{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06a261b7-546c-4701-8563-4b20562648f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/general/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import gseapy as gp\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from gears import PertData, GEARS\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b825122f-1780-41e2-926c-8eb7445a9671",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('/home/ubuntu/data/decoder')\n",
    "eval_dir = data_dir / 'tokenized'\n",
    "splits= ['train','val','test']\n",
    "\n",
    "chunk_size = 20000        # How many cells per file\n",
    "n_pathways = 1024          # Number of pathway \"tokens\" per cell\n",
    "n_genes = 4096\n",
    "count_normalize_target = 1e4 # normalize each cell to this count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5fe419-38a5-46f3-8f2a-6a8d3555e4c5",
   "metadata": {},
   "source": [
    "# Download Data\n",
    "\n",
    "We'll start by downloading the GEARS version of Replogle K562 Data. Since we're benchmarking we want to ensure we use the EXACT same data/splits as the SOTA models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ad61932-b379-4c8e-b716-7e22088b59a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n",
      "Found local copy...\n",
      "Found local copy...\n",
      "These perturbations are not in the GO graph and their perturbation can thus not be predicted\n",
      "['C7orf26+ctrl' 'C14orf178+ctrl' 'RPS10-NUDT3+ctrl' 'SEM1+ctrl' 'FAU+ctrl']\n",
      "Local copy of pyg dataset is detected. Loading...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "pert_data = PertData(eval_dir) \n",
    "pert_data.load(data_name='replogle_k562_essential')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258e309c-d4a9-4857-919f-9f7c14dec346",
   "metadata": {},
   "source": [
    "**Create Splits**\n",
    "Now we'll create the same splits that GEARS used: \n",
    "* Train: Seen perturbations\n",
    "* Val: Seen perturbations (held out cells)\n",
    "* Test: Unseen perturbations (The real challenge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf0c4926-2691-43a5-a9b4-524cb0961abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Local copy of split is detected. Loading...\n",
      "Simulation split test composition:\n",
      "combo_seen0:0\n",
      "combo_seen1:0\n",
      "combo_seen2:0\n",
      "unseen_single:272\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n"
     ]
    }
   ],
   "source": [
    "pert_data.prepare_split(split='simulation', seed=1) \n",
    "adata = pert_data.adata "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92e19ed-b289-4425-ab01-531cae7ff123",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "Now we need to do the same preprocessing we did before for our model. \n",
    "\n",
    "**Total Counts**\n",
    "\n",
    "We'll start by adding the log of total counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8516d60f-7907-4e4b-9da4-844a00eb85cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_counts = np.array(adata.X.sum(axis=1)).flatten()\n",
    "adata.obs['log_total_counts'] = np.log1p(total_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c44ea3a-ea01-4977-add5-becf79611fd7",
   "metadata": {},
   "source": [
    "**Count Normalization and filtering** \n",
    "Now we'll normalize our counts to account for differing read depths. After normalizing we'll take the log to squeeze the order of magnitude differences and then do our gene based filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00709a0e-9a48-46bf-9fe0-50a85c233431",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.normalize_total(adata, target_sum=count_normalize_target)\n",
    "sc.pp.log1p(adata)\n",
    "sc.pp.highly_variable_genes(adata, n_top_genes=n_genes, subset=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201dcf10-de0f-4d64-b609-f69ec250fc5e",
   "metadata": {},
   "source": [
    "Let's see the gene list that we ended up keeping. Also we'll save the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d87f9ec-898b-4137-b501-2f0a6df2ef7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Feature Space: 4096 Genes\n"
     ]
    }
   ],
   "source": [
    "genes = adata.var.gene_name.tolist()\n",
    "print(f'Final Feature Space: {len(genes)} Genes')\n",
    "\n",
    "with open(eval_dir / 'gene_names.json', 'w') as f:\n",
    "    json.dump(genes, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde14652-363b-4b7b-ada7-d91501601f4c",
   "metadata": {},
   "source": [
    "## Pathway Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7487f99a-abd0-485a-8620-547723d5a2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use DSigDB or Reactome to get the gene sets\n",
    "gs_res = gp.get_library(name='DSigDB', organism='Human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd7de7fb-b934-4165-a39d-6fd8e88b8b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pathways = {k: v for k, v in gs_res.items() if 80 <= len(v) <= 1400}\n",
    "pathway_names = list(valid_pathways.keys())[:n_pathways]\n",
    "# Save Pathway Names\n",
    "with open(eval_dir / 'pathway_names.json', 'w') as f:\n",
    "    json.dump(pathway_names, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bff633d-c762-4638-8c4a-a03a8dc026d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_mask = np.zeros((len(genes), len(pathway_names)), dtype=np.float32)\n",
    "gene_to_idx = {gene: i for i, gene in enumerate(genes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c62b19e0-b8fa-493b-8235-31a777ecf687",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p_idx, p_name in enumerate(pathway_names):\n",
    "    hit_count = 0\n",
    "    genes_in_pathway = valid_pathways[p_name]\n",
    "    for g in genes_in_pathway:\n",
    "        if g in gene_to_idx:\n",
    "            binary_mask[gene_to_idx[g], p_idx] = 1.0\n",
    "            hit_count += 1\n",
    "\n",
    "    if hit_count <= 1:\n",
    "    \tprint(f'pathway {p_name} had {hit_count} gene hits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2792833-4778-4cce-9e0f-3065fff60768",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(eval_dir / 'binary_pathway_mask.npy', binary_mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad10dbdb-183f-4c55-806c-f49efce4b738",
   "metadata": {},
   "source": [
    "## Prepare Controls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a96394ec-d0a5-427f-ade1-09763e176ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_mask = adata.obs['condition'] == 'ctrl'\n",
    "control_indices = np.where(control_mask)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97852944-9815-4922-865b-7772092f7bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format: List of (Gene_Vector, Total_Count_Scalar)\n",
    "control_bank = {\n",
    "    'X': adata.X[control_indices].toarray().astype(np.float32),\n",
    "    'total': adata.obs['log_total_counts'].values[control_indices].astype(np.float32)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8de8049-da5e-4a9a-baf0-07445f5d0b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10691 control cells.\n"
     ]
    }
   ],
   "source": [
    "print(f'Found {len(control_indices)} control cells.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4211d868-9c48-4684-bea8-755337134d2c",
   "metadata": {},
   "source": [
    "## Generate Shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd3c16a2-7978-4df9-b36b-fb79a77911a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's extract the official splits from the GEARS object\n",
    "# split_dict = {'train': [pert1, pert2...], 'val': [...], 'test': [...]}\n",
    "split_map = pert_data.set2conditions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fb03df-0ee0-4435-85c7-23cb4cde17a7",
   "metadata": {},
   "source": [
    "**Perturbation Map** we actually need to load our training perturbation map so that we reuse the ID for any overlapping perturbations and generate new IDs for new ones.  \n",
    "\n",
    "Also, the perturbation names are slightly different so we'll need to modify the names of the GEARS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63d51078-0246-41df-bb19-9eb95dd473f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2057"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_pert_map = eval_dir / 'perturbation_map_original.json'\n",
    "with open(original_pert_map, 'r') as f:\n",
    "    training_map = json.load(f)\n",
    "\n",
    "# invert map to find ID:gene name \n",
    "max_pert_id = max([v for k, v in training_map.items()])\n",
    "max_pert_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae036041-1866-436b-a36c-3bfb6180b45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_gears_name(name):\n",
    "    # GEARS format is 'Gene+ctrl' -> We want 'Gene'\n",
    "    if name.endswith('+ctrl'):\n",
    "        return name.replace('+ctrl', '')\n",
    "    elif name == 'ctrl':\n",
    "        return 'control'\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "094b278b-fb03-454b-bdec-cd4dc4588a89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'overlap 1088 | new additions 0'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_perts = adata.obs['condition'].unique()\n",
    "overlap = 0\n",
    "new = 0\n",
    "id_to_pert = {v:k for k, v in training_map.items()}\n",
    "for a in all_perts:\n",
    "    clean_name = clean_gears_name(a)\n",
    "    if clean_name in training_map:\n",
    "        overlap += 1\n",
    "    else:\n",
    "        max_pert_id += 1\n",
    "        new += 1\n",
    "        id_to_pert[max_pert_id] = clean_name\n",
    "f'overlap {overlap} | new additions {new}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9816fcff-d2c4-4004-b278-4dffd3e2eb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pert_to_id = {v: k for k, v in id_to_pert.items()}\n",
    "with open(eval_dir / 'perturbation_map.json', 'w') as f:\n",
    "    json.dump({str(k): int(v) for k, v in pert_to_id.items()}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd104e5e-cde2-453e-a1dc-ac3a81216e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_shards(split_name, condition_list):\n",
    "    \"\"\"\n",
    "    Iterates through cells belonging to the given conditions, \n",
    "    pairs them with random controls, and saves .npz shards.\n",
    "    \"\"\"\n",
    "    print(f'Split: {split_name.upper()}')\n",
    "    \n",
    "    # Filter cells belonging to these perturbations\n",
    "    # Note: We exclude 'ctrl' from the 'Treated' side of the pair\n",
    "    mask = adata.obs['condition'].isin(condition_list) & (adata.obs['condition'] != 'ctrl')\n",
    "    indices = np.where(mask)[0]\n",
    "    \n",
    "    # Shuffle for randomness\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    # Buffer for current shard\n",
    "    buffer = {\n",
    "        'control_x': [], \n",
    "        'control_total': [],\n",
    "        'case_x': [], \n",
    "        'case_total': [],\n",
    "        'action_ids': []\n",
    "    }\n",
    "    \n",
    "    shard_count = 0\n",
    "    save_path = eval_dir / split_name\n",
    "    \n",
    "    for idx in tqdm(indices):\n",
    "        # 1. Get Case Data\n",
    "        case_x = adata.X[idx].toarray().flatten().astype(np.float32)\n",
    "        case_tot = adata.obs['log_total_counts'].iloc[idx].astype(np.float32)\n",
    "        pert_name = adata.obs['condition'].iloc[idx]\n",
    "        \n",
    "        # 2. Get Random Control Pair\n",
    "        # Ideally we match batch, but Replogle K562 is often batch-corrected or single batch.\n",
    "        # For simplicity/speed here, we sample global control.\n",
    "        # (Improvement: dictionary mapping batch_id -> control_indices)\n",
    "        rand_idx = np.random.randint(len(control_bank['X']))\n",
    "        ctrl_x = control_bank['X'][rand_idx]\n",
    "        ctrl_tot = control_bank['total'][rand_idx]\n",
    "        \n",
    "        # 3. Add to Buffer\n",
    "        buffer['control_x'].append(ctrl_x)\n",
    "        buffer['control_total'].append(ctrl_tot)\n",
    "        buffer['case_x'].append(case_x)\n",
    "        buffer['case_total'].append(case_tot)\n",
    "        buffer['action_ids'].append(pert_to_id[clean_gears_name(pert_name)])\n",
    "        \n",
    "        # 4. Save if buffer full\n",
    "        if len(buffer['case_x']) >= chunk_size:\n",
    "            np.savez(\n",
    "                save_path / f'shard_{split_name}_{shard_count:04d}.npz',\n",
    "                control=np.array(buffer['control_x']),\n",
    "                control_total=np.array(buffer['control_total']),\n",
    "                case=np.array(buffer['case_x']),\n",
    "                case_total=np.array(buffer['case_total']),\n",
    "                action_ids=np.array(buffer['action_ids'], dtype=np.int16)\n",
    "            )\n",
    "            # Reset\n",
    "            buffer = {k: [] for k in buffer}\n",
    "            shard_count += 1\n",
    "            \n",
    "    # Save leftovers\n",
    "    if len(buffer['case_x']) > 0:\n",
    "        np.savez(\n",
    "            save_path / f'shard_{split_name}_{shard_count:04d}.npz',\n",
    "            control=np.array(buffer['control_x']),\n",
    "            control_total=np.array(buffer['control_total']),\n",
    "            case=np.array(buffer['case_x']),\n",
    "            case_total=np.array(buffer['case_total']),\n",
    "            action_ids=np.array(buffer['action_ids'], dtype=np.int16)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c62aded-2539-48af-8cb9-49b68062f194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split: TRAIN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 101682/101682 [00:06<00:00, 15660.38it/s]\n"
     ]
    }
   ],
   "source": [
    "write_shards('train', split_map['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd53570f-fd4b-4084-a9bf-d7f94f518eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split: VAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 11044/11044 [00:00<00:00, 23390.93it/s]\n"
     ]
    }
   ],
   "source": [
    "write_shards('val', split_map['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f4cdecd9-c5ef-400f-a518-bae4b117b3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split: TEST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 38829/38829 [00:02<00:00, 18542.89it/s]\n"
     ]
    }
   ],
   "source": [
    "write_shards('test', split_map['test']) # Unseen perturbations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef11bc2-a85b-40ad-97e6-bfa14ff6d4ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
