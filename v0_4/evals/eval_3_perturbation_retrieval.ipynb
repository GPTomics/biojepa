{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Eval 3: Perturbation Retrieval\n",
    "\n",
    "**Biological question**: Given a desired cellular outcome, can we identify which perturbation would achieve it?\n",
    "\n",
    "**Metrics**: Recall@K, Mean Reciprocal Rank (MRR)\n",
    "\n",
    "**Story**: This directly tests the target discovery use case. If a user has a disease signature and wants to find what perturbation reverses it, the model needs to correctly rank perturbations by their predicted effect similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "from eval_common import EvalContext, update_eval_report, load_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "load-context",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cpu\n",
      "Input Bank (DNA): torch.Size([1250, 1536])\n",
      "<All keys matched successfully>\n",
      "Decoder loaded\n",
      "Total perturbations in bank: 1250\n"
     ]
    }
   ],
   "source": [
    "ctx = EvalContext()\n",
    "n_genes = ctx.config['n_genes']\n",
    "n_perturbations = ctx.input_bank.shape[0]\n",
    "print(f'Total perturbations in bank: {n_perturbations}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gt-header",
   "metadata": {},
   "source": [
    "## Step 1: Compute Ground Truth Deltas\n",
    "\n",
    "First, aggregate ground truth expression deltas for each test perturbation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ground-truth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 4 shards for split test\n",
      "loading /Users/djemec/data/jepa/v0_4/training/test/shard_k562e_test_0003.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting ground truth:  16%|███▍                  | 191/1213 [00:00<00:00, 1907.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading /Users/djemec/data/jepa/v0_4/training/test/shard_k562e_test_0000.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting ground truth:  31%|███████▏               | 382/1213 [00:00<00:01, 682.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading /Users/djemec/data/jepa/v0_4/training/test/shard_k562e_test_0002.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting ground truth:  67%|███████████████▎       | 807/1213 [00:00<00:00, 859.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading /Users/djemec/data/jepa/v0_4/training/test/shard_k562e_test_0001.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting ground truth: 100%|██████████████████████| 1213/1213 [00:01<00:00, 846.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test perturbations: 286\n"
     ]
    }
   ],
   "source": [
    "test_loader, test_steps = load_test_data(ctx.paths, ctx.device, ctx.config)\n",
    "\n",
    "bulk_real_deltas = defaultdict(list)\n",
    "bulk_control_states = defaultdict(list)\n",
    "\n",
    "for step in tqdm(range(test_steps), desc='Collecting ground truth'):\n",
    "    cont_x, cont_tot, case_x, case_tot, p_idx, p_mod, p_mode = test_loader.next_batch()\n",
    "    B = cont_x.shape[0]\n",
    "    \n",
    "    real_delta = (case_x - cont_x).cpu().numpy()\n",
    "    cont_x_np = cont_x.cpu().numpy()\n",
    "    p_idx_np = p_idx.cpu().numpy().flatten()\n",
    "    \n",
    "    for i in range(B):\n",
    "        pid = p_idx_np[i]\n",
    "        bulk_real_deltas[pid].append(real_delta[i])\n",
    "        bulk_control_states[pid].append(cont_x_np[i])\n",
    "\n",
    "test_pert_ids = list(bulk_real_deltas.keys())\n",
    "mean_real_deltas = {pid: np.mean(np.stack(bulk_real_deltas[pid]), axis=0) for pid in test_pert_ids}\n",
    "mean_control_states = {pid: np.mean(np.stack(bulk_control_states[pid]), axis=0) for pid in test_pert_ids}\n",
    "\n",
    "print(f'Test perturbations: {len(test_pert_ids)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pred-header",
   "metadata": {},
   "source": [
    "## Step 2: Predict Deltas for All Perturbations\n",
    "\n",
    "For each test perturbation's control state, predict what delta each perturbation in the bank would cause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "pert-metadata",
   "metadata": {},
   "outputs": [],
   "source": [
    "pert_mod = torch.zeros(n_perturbations, dtype=torch.long, device=ctx.device)\n",
    "pert_mode = torch.zeros(n_perturbations, dtype=torch.long, device=ctx.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "batch-predict-fn",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_all_deltas_for_control(control_x_np, batch_size=64):\n",
    "    control_x = torch.from_numpy(control_x_np).float().to(ctx.device)\n",
    "    control_tot = control_x.sum()\n",
    "    \n",
    "    all_pred_deltas = []\n",
    "    \n",
    "    for start_idx in range(0, n_perturbations, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, n_perturbations)\n",
    "        batch_pert_idx = torch.arange(start_idx, end_idx, device=ctx.device)\n",
    "        \n",
    "        B = end_idx - start_idx\n",
    "        control_x_batch = control_x.unsqueeze(0).expand(B, -1)\n",
    "        control_tot_batch = control_tot.unsqueeze(0).expand(B)\n",
    "        \n",
    "        p_feats = ctx.input_bank[batch_pert_idx]\n",
    "        p_mod = pert_mod[batch_pert_idx]\n",
    "        p_mode = pert_mode[batch_pert_idx]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            z_context = ctx.biojepa.student(control_x_batch, control_tot_batch, mask_idx=None)\n",
    "            action_latents = ctx.biojepa.composer(p_feats, p_mod, p_mode)\n",
    "            target_indices = torch.arange(n_genes, device=ctx.device).unsqueeze(0).expand(B, -1)\n",
    "            z_pred_mu, _ = ctx.biojepa.predictor(z_context, action_latents, target_indices)\n",
    "            \n",
    "            pred_delta = ctx.decoder(z_pred_mu) - ctx.decoder(z_context)\n",
    "        \n",
    "        all_pred_deltas.append(pred_delta.cpu().numpy())\n",
    "    \n",
    "    return np.concatenate(all_pred_deltas, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retrieval-header",
   "metadata": {},
   "source": [
    "## Step 3: Retrieval Evaluation\n",
    "\n",
    "For each test perturbation:\n",
    "1. Take its ground truth delta\n",
    "2. Predict deltas for all perturbations in the bank\n",
    "3. Rank by similarity to ground truth\n",
    "4. Check where the true perturbation ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "similarity-fn",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    a_norm = a / (np.linalg.norm(a, axis=-1, keepdims=True) + 1e-8)\n",
    "    b_norm = b / (np.linalg.norm(b) + 1e-8)\n",
    "    return np.dot(a_norm, b_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retrieval-eval",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating retrieval:   7%|██                            | 7/100 [01:23<19:24, 12.52s/it]"
     ]
    }
   ],
   "source": [
    "ranks = []\n",
    "reciprocal_ranks = []\n",
    "\n",
    "n_eval = min(len(test_pert_ids), 100)\n",
    "eval_pert_ids = test_pert_ids[:n_eval]\n",
    "\n",
    "for pid in tqdm(eval_pert_ids, desc='Evaluating retrieval'):\n",
    "    ground_truth_delta = mean_real_deltas[pid]\n",
    "    control_state = mean_control_states[pid]\n",
    "    \n",
    "    all_pred_deltas = predict_all_deltas_for_control(control_state)\n",
    "    similarities = cosine_similarity(all_pred_deltas, ground_truth_delta)\n",
    "    \n",
    "    ranking = np.argsort(similarities)[::-1]\n",
    "    rank = np.where(ranking == pid)[0][0] + 1\n",
    "    \n",
    "    ranks.append(rank)\n",
    "    reciprocal_ranks.append(1.0 / rank)\n",
    "\n",
    "ranks = np.array(ranks)\n",
    "reciprocal_ranks = np.array(reciprocal_ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metrics-header",
   "metadata": {},
   "source": [
    "## Retrieval Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compute-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_VALUES = [1, 5, 10, 20, 50]\n",
    "\n",
    "print('PERTURBATION RETRIEVAL METRICS')\n",
    "print('=' * 60)\n",
    "print()\n",
    "print(f'Mean Reciprocal Rank (MRR): {np.mean(reciprocal_ranks):.4f}')\n",
    "print(f'Median Rank: {np.median(ranks):.0f}')\n",
    "print(f'Mean Rank: {np.mean(ranks):.1f}')\n",
    "print()\n",
    "print('Recall@K:')\n",
    "for k in K_VALUES:\n",
    "    recall_at_k = np.mean(ranks <= k)\n",
    "    print(f'  Recall@{k:>2}: {recall_at_k:.4f} ({int(recall_at_k * len(ranks))}/{len(ranks)} perturbations)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dist-header",
   "metadata": {},
   "source": [
    "## Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rank-distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(ranks, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(np.median(ranks), color='red', linestyle='--', label=f'Median: {np.median(ranks):.0f}')\n",
    "axes[0].set_xlabel('Rank of True Perturbation')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Distribution of True Perturbation Ranks')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(ranks, bins=50, edgecolor='black', alpha=0.7, cumulative=True, density=True)\n",
    "axes[1].axhline(0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "for k in [1, 5, 10, 20]:\n",
    "    recall = np.mean(ranks <= k)\n",
    "    axes[1].axvline(k, color='red', linestyle=':', alpha=0.7)\n",
    "    axes[1].text(k + 1, recall + 0.02, f'R@{k}={recall:.2f}', fontsize=9)\n",
    "axes[1].set_xlabel('Rank')\n",
    "axes[1].set_ylabel('Cumulative Proportion')\n",
    "axes[1].set_title('Cumulative Distribution of Ranks')\n",
    "axes[1].set_xlim(0, min(100, ranks.max()))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baseline-header",
   "metadata": {},
   "source": [
    "## Comparison to Random Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baseline-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_expected_mrr = 1.0 / n_perturbations * sum(1.0/i for i in range(1, n_perturbations + 1))\n",
    "random_median_rank = n_perturbations / 2\n",
    "\n",
    "print('COMPARISON TO RANDOM BASELINE')\n",
    "print('=' * 60)\n",
    "print(f'Total perturbations in bank: {n_perturbations}')\n",
    "print()\n",
    "print(f'{\"Metric\":>20} | {\"Model\":>10} | {\"Random\":>10} | {\"Improvement\":>12}')\n",
    "print('-' * 60)\n",
    "print(f'{\"MRR\":>20} | {np.mean(reciprocal_ranks):>10.4f} | {random_expected_mrr:>10.4f} | {np.mean(reciprocal_ranks)/random_expected_mrr:>11.1f}x')\n",
    "print(f'{\"Median Rank\":>20} | {np.median(ranks):>10.0f} | {random_median_rank:>10.0f} | {random_median_rank/np.median(ranks):>11.1f}x')\n",
    "\n",
    "for k in K_VALUES:\n",
    "    model_recall = np.mean(ranks <= k)\n",
    "    random_recall = k / n_perturbations\n",
    "    improvement = model_recall / random_recall if random_recall > 0 else float('inf')\n",
    "    print(f'{f\"Recall@{k}\":>20} | {model_recall:>10.4f} | {random_recall:>10.4f} | {improvement:>11.1f}x')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "error-header",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "error-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_idx = np.argsort(ranks)[-5:][::-1]\n",
    "best_idx = np.argsort(ranks)[:5]\n",
    "\n",
    "print('BEST RETRIEVALS (lowest rank = best):')\n",
    "for idx in best_idx:\n",
    "    pid = eval_pert_ids[idx]\n",
    "    print(f'  Perturbation {pid}: Rank {ranks[idx]}')\n",
    "\n",
    "print()\n",
    "print('WORST RETRIEVALS (highest rank = worst):')\n",
    "for idx in worst_idx:\n",
    "    pid = eval_pert_ids[idx]\n",
    "    print(f'  Perturbation {pid}: Rank {ranks[idx]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## Benchmark Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 70)\n",
    "print('EVAL 3: PERTURBATION RETRIEVAL - BENCHMARK RESULTS')\n",
    "print('=' * 70)\n",
    "print()\n",
    "print('CONFIGURATION')\n",
    "print(f'  Test perturbations evaluated: {len(eval_pert_ids)}')\n",
    "print(f'  Total perturbations in bank: {n_perturbations}')\n",
    "print(f'  Genes: {n_genes}')\n",
    "print(f'  Similarity metric: Cosine')\n",
    "print()\n",
    "print('RETRIEVAL METRICS')\n",
    "print(f'  Mean Reciprocal Rank (MRR): {np.mean(reciprocal_ranks):.4f}')\n",
    "print(f'  Median Rank: {np.median(ranks):.0f} / {n_perturbations}')\n",
    "print(f'  Mean Rank: {np.mean(ranks):.1f} / {n_perturbations}')\n",
    "print()\n",
    "print('RECALL@K')\n",
    "for k in K_VALUES:\n",
    "    recall = np.mean(ranks <= k)\n",
    "    random = k / n_perturbations\n",
    "    bar = '█' * int(recall * 20)\n",
    "    print(f'  @{k:>2}: {recall:.3f} {bar} ({recall/random:.0f}x random)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "update-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_eval_report('eval_3_perturbation_retrieval', {\n",
    "    'config': {\n",
    "        'test_perturbations_evaluated': len(eval_pert_ids),\n",
    "        'total_perturbations_in_bank': n_perturbations,\n",
    "        'genes': n_genes,\n",
    "        'similarity_metric': 'cosine'\n",
    "    },\n",
    "    'metrics': {\n",
    "        'mrr': float(np.mean(reciprocal_ranks)),\n",
    "        'median_rank': float(np.median(ranks)),\n",
    "        'mean_rank': float(np.mean(ranks))\n",
    "    },\n",
    "    'recall_at_k': {\n",
    "        str(k): {\n",
    "            'recall': float(np.mean(ranks <= k)),\n",
    "            'vs_random': float(np.mean(ranks <= k) / (k / n_perturbations))\n",
    "        } for k in K_VALUES\n",
    "    }\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
