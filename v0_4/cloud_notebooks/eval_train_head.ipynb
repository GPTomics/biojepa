{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71522d3b-8805-48e6-bb36-1c9a7bd15482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.serialization\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "import biojepa_v0_4 as model\n",
    "from bio_dataloader import TrainingLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe98e32-54b3-4da6-adf9-8b3e76e3bc0b",
   "metadata": {},
   "source": [
    "## BioJEPA Model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f67335c-63ea-4f80-9c10-96b4d3e09c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda\n"
     ]
    }
   ],
   "source": [
    "random.seed(1337)\n",
    "def get_device():\n",
    "    device = 'cpu'\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(1337)\n",
    "        device = 'cuda'\n",
    "    print(f'using {device}')\n",
    "    return device\n",
    "\n",
    "DEVICE = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5ec64e1-0d7a-4dee-b478-b99f901ce9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "n_genes = 5000\n",
    "n_layers = 6\n",
    "n_heads = 4\n",
    "n_embd = 256\n",
    "pert_latent_dim = 320\n",
    "pert_mode_dim = 64\n",
    "\n",
    "training_file_chunk = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "025d6281-f460-46eb-89d9-842d1ca6fa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('/home/ubuntu')\n",
    "train_dir = data_dir / 'training'\n",
    "checkpoint_dir = data_dir / 'checkpoints'\n",
    "pert_dir = data_dir / 'pert_embd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9671701-5cc4-4016-a354-b71238c98947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Banks Loaded. Input(DNA): torch.Size([1250, 1536]), Anchor(Prot): torch.Size([1087, 320])\n"
     ]
    }
   ],
   "source": [
    "input_bank = torch.from_numpy(np.load(pert_dir / 'input_embeddings_dna.npy')).float().to(DEVICE)\n",
    "anchor_bank = torch.from_numpy(np.load(pert_dir / 'action_embeddings_esm2.npy')).float().to(DEVICE)\n",
    "print(f'Banks Loaded. Input(DNA): {input_bank.shape}, Anchor(Prot): {anchor_bank.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fe46b1b-aff7-4355-909f-9f71761f11c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high')\n",
    "config = model.BioJepaConfig(\n",
    "    num_genes = n_genes,\n",
    "    n_layer= n_layers,\n",
    "    heads= n_heads,\n",
    "    embed_dim = n_embd,\n",
    "    n_pre_layer= n_layers,\n",
    "    pert_latent_dim=pert_latent_dim,\n",
    "    pert_mode_dim=pert_mode_dim\n",
    ")\n",
    "model = model.BioJepa(config).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a609e628-943e-41c2-a4d7-a71b9075a640",
   "metadata": {},
   "source": [
    "**Load Checkpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4aac628e-6ab7-4768-955a-3f99e79f58ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = checkpoint_dir / 'bio_jepa_ckpt_63539_final.pt'\n",
    "checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "\n",
    "keys = model.load_state_dict(checkpoint['model'])\n",
    "keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8287a64f-9d0d-4fbe-8c9b-a03c463a1487",
   "metadata": {},
   "source": [
    "**Freeze Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63812672-368f-483a-be2a-8b65cc849a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze_encoders()\n",
    "model.eval()\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c98b4a-5df8-4894-9c56-ae0dbcbc3acd",
   "metadata": {},
   "source": [
    "## Build Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3dec1f3-d9cc-4849-aaa8-6a7d60199432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights_robust(module):\n",
    "    if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            fan_in = module.embedding_dim\n",
    "        else:\n",
    "            fan_in = module.weight.size(1)\n",
    "        std = 1.0 / math.sqrt(fan_in) if fan_in > 0 else 0.02\n",
    "        nn.init.trunc_normal_(module.weight, mean=0.0, std=std, a=-2*std, b=2*std)\n",
    "        if hasattr(module, 'bias') and module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "    elif isinstance(module, nn.LayerNorm):\n",
    "        nn.init.zeros_(module.bias)\n",
    "        nn.init.ones_(module.weight)\n",
    "\n",
    "@dataclass\n",
    "class BenchmarkDecoderConfig:\n",
    "    embed_dim: int = 256\n",
    "    \n",
    "class BenchmarkDecoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.head = nn.Linear(config.embed_dim, 1)\n",
    "\n",
    "        self.apply(init_weights_robust)\n",
    "        \n",
    "    def forward(self, latents):\n",
    "\n",
    "        gene_preds = self.head(latents)        \n",
    "        gene_preds = gene_preds.squeeze(-1)\n",
    "        \n",
    "        return gene_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c316fe59-e4aa-4d12-9db4-2e38781f53bc",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e1d5000-f278-4f9b-9227-9554f31e9a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 6 shards for split train\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "found 1 shards for split val\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n"
     ]
    }
   ],
   "source": [
    "train_loader = TrainingLoader(batch_size=batch_size, split='train', data_dir=train_dir, device=DEVICE)\n",
    "val_loader = TrainingLoader(batch_size=batch_size, split='val', data_dir=train_dir, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9025bf-05d7-4ff8-ae9a-c1b87471c3e5",
   "metadata": {},
   "source": [
    "## Training Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f484f1f2-2933-413d-80c0-f0c970656453",
   "metadata": {},
   "source": [
    "### Training Config/Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad700ee3-e710-43de-be89-4003c922a41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_decoder = 1e-3\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0494e7de-e96d-4989-b47b-9229c050a263",
   "metadata": {},
   "source": [
    "**Initialize Decoder** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74224c15-ff49-41d9-9552-b8ec9ed83dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BenchmarkDecoderConfig(\n",
    "    embed_dim= n_embd\n",
    ")\n",
    "\n",
    "decoder = BenchmarkDecoder(config).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d5141e-064c-4ec0-9584-ef4a87237125",
   "metadata": {},
   "source": [
    "**Load Checkpoint (only Sometimes)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d3b286-5a21-4bff-aa8e-cd1a5135df08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder_checkpoint_path = checkpoint_dir / 'biojepa_decoder_ckpt_15884_final.pt'\n",
    "# decode_checkpoint = torch.load(decoder_checkpoint_path, map_location=DEVICE)\n",
    "\n",
    "# d_keys = decoder.load_state_dict(decode_checkpoint['model'])\n",
    "# d_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2da30fa-9c44-40be-921c-5184c764f0e5",
   "metadata": {},
   "source": [
    "**Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b197e689-8b98-4cf9-8382-6116bf2ce440",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(decoder.parameters(), lr=lr_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958f11cf-8c9f-49b9-b526-74be69d2126f",
   "metadata": {},
   "source": [
    "**Training Lenght**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "009526cc-7703-497d-87cb-4a50f9c1c1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_total_examples = 101682"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "766a254e-2fd7-4274-91c4-7ceab21d8d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3177, 63540)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch = train_total_examples // batch_size\n",
    "max_steps = epochs * steps_per_epoch\n",
    "steps_per_epoch, max_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cf4803-b42e-4d1b-89db-c9b43d6e3558",
   "metadata": {},
   "source": [
    "**Scheduler**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c683627d-3da6-45e9-9827-06d8586a1f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=lr_decoder, total_steps=max_steps, pct_start=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15c5c7c-b741-486e-9ce2-99c2e253b05d",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82cb876c-e45d-43f7-94aa-2fa7f44b15c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossi = []\n",
    "step = 0\n",
    "total_epoch_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69468a01-365b-42b9-b041-5d6969e5b4ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss: 2.0668\n",
      "Step 0 | Loss: 2.07401 | LR: 4.00e-05\n",
      "Step 25 | Loss: 2.11025 | LR: 4.02e-05\n",
      "Step 50 | Loss: 1.92717 | LR: 4.06e-05\n",
      "Step 75 | Loss: 1.96266 | LR: 4.14e-05\n",
      "val loss: 1.9126\n",
      "Step 100 | Loss: 1.84268 | LR: 4.24e-05\n",
      "Step 125 | Loss: 2.25234 | LR: 4.37e-05\n",
      "Step 150 | Loss: 1.92479 | LR: 4.53e-05\n",
      "Step 175 | Loss: 1.63595 | LR: 4.73e-05\n",
      "val loss: 1.6797\n",
      "Step 200 | Loss: 1.73721 | LR: 4.95e-05\n",
      "Step 225 | Loss: 1.61439 | LR: 5.19e-05\n",
      "Step 250 | Loss: 1.52583 | LR: 5.47e-05\n",
      "Step 275 | Loss: 1.49018 | LR: 5.78e-05\n",
      "val loss: 1.4609\n",
      "Step 300 | Loss: 1.48922 | LR: 6.11e-05\n",
      "Step 325 | Loss: 1.54646 | LR: 6.47e-05\n",
      "Step 350 | Loss: 1.47880 | LR: 6.86e-05\n",
      "Step 375 | Loss: 1.27609 | LR: 7.28e-05\n",
      "val loss: 1.3350\n",
      "Step 400 | Loss: 1.38779 | LR: 7.73e-05\n",
      "Step 425 | Loss: 1.21407 | LR: 8.20e-05\n",
      "Step 450 | Loss: 1.19211 | LR: 8.70e-05\n",
      "Step 475 | Loss: 1.18399 | LR: 9.22e-05\n",
      "val loss: 1.1532\n",
      "Step 500 | Loss: 1.10416 | LR: 9.77e-05\n",
      "Step 525 | Loss: 1.14963 | LR: 1.04e-04\n",
      "Step 550 | Loss: 1.05179 | LR: 1.10e-04\n",
      "Step 575 | Loss: 1.03692 | LR: 1.16e-04\n",
      "val loss: 1.0058\n",
      "Step 600 | Loss: 1.00159 | LR: 1.22e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 625 | Loss: 0.97677 | LR: 1.29e-04\n",
      "Step 650 | Loss: 0.93628 | LR: 1.36e-04\n",
      "Step 675 | Loss: 0.90036 | LR: 1.43e-04\n",
      "val loss: 0.8614\n",
      "Step 700 | Loss: 0.87646 | LR: 1.51e-04\n",
      "Step 725 | Loss: 0.85056 | LR: 1.59e-04\n",
      "Step 750 | Loss: 0.84616 | LR: 1.66e-04\n",
      "Step 775 | Loss: 0.78264 | LR: 1.75e-04\n",
      "val loss: 0.7614\n",
      "Step 800 | Loss: 0.77236 | LR: 1.83e-04\n",
      "Step 825 | Loss: 0.74981 | LR: 1.92e-04\n",
      "Step 850 | Loss: 0.71055 | LR: 2.00e-04\n",
      "Step 875 | Loss: 0.69481 | LR: 2.09e-04\n",
      "val loss: 0.6855\n",
      "Step 900 | Loss: 0.69082 | LR: 2.18e-04\n",
      "Step 925 | Loss: 0.66943 | LR: 2.28e-04\n",
      "Step 950 | Loss: 0.64107 | LR: 2.37e-04\n",
      "Step 975 | Loss: 0.66490 | LR: 2.47e-04\n",
      "val loss: 0.6247\n",
      "Step 1000 | Loss: 0.61341 | LR: 2.57e-04\n",
      "Step 1025 | Loss: 0.61940 | LR: 2.67e-04\n",
      "Step 1050 | Loss: 0.62181 | LR: 2.77e-04\n",
      "Step 1075 | Loss: 0.58919 | LR: 2.87e-04\n",
      "val loss: 0.5600\n",
      "Step 1100 | Loss: 0.58960 | LR: 2.98e-04\n",
      "Step 1125 | Loss: 0.56095 | LR: 3.08e-04\n",
      "Step 1150 | Loss: 0.53603 | LR: 3.19e-04\n",
      "Step 1175 | Loss: 0.56281 | LR: 3.30e-04\n",
      "val loss: 0.5377\n",
      "Step 1200 | Loss: 0.53431 | LR: 3.41e-04\n",
      "Step 1225 | Loss: 0.53432 | LR: 3.52e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0003.npz\n",
      "Step 1250 | Loss: 0.50071 | LR: 3.63e-04\n",
      "Step 1275 | Loss: 0.56166 | LR: 3.74e-04\n",
      "val loss: 0.5339\n",
      "Step 1300 | Loss: 0.52556 | LR: 3.86e-04\n",
      "Step 1325 | Loss: 0.49972 | LR: 3.97e-04\n",
      "Step 1350 | Loss: 0.50172 | LR: 4.09e-04\n",
      "Step 1375 | Loss: 0.52212 | LR: 4.20e-04\n",
      "val loss: 0.5149\n",
      "Step 1400 | Loss: 0.54823 | LR: 4.32e-04\n",
      "Step 1425 | Loss: 0.49077 | LR: 4.43e-04\n",
      "Step 1450 | Loss: 0.52450 | LR: 4.55e-04\n",
      "Step 1475 | Loss: 0.51286 | LR: 4.67e-04\n",
      "val loss: 0.5093\n",
      "Step 1500 | Loss: 0.50750 | LR: 4.79e-04\n",
      "Step 1525 | Loss: 0.47993 | LR: 4.91e-04\n",
      "Step 1550 | Loss: 0.51330 | LR: 5.02e-04\n",
      "Step 1575 | Loss: 0.49875 | LR: 5.14e-04\n",
      "val loss: 0.5023\n",
      "Step 1600 | Loss: 0.50035 | LR: 5.26e-04\n",
      "Step 1625 | Loss: 0.50353 | LR: 5.38e-04\n",
      "Step 1650 | Loss: 0.49945 | LR: 5.50e-04\n",
      "Step 1675 | Loss: 0.48473 | LR: 5.62e-04\n",
      "val loss: 0.5025\n",
      "Step 1700 | Loss: 0.50133 | LR: 5.74e-04\n",
      "Step 1725 | Loss: 0.50257 | LR: 5.85e-04\n",
      "Step 1750 | Loss: 0.50072 | LR: 5.97e-04\n",
      "Step 1775 | Loss: 0.50402 | LR: 6.09e-04\n",
      "val loss: 0.4986\n",
      "Step 1800 | Loss: 0.50142 | LR: 6.20e-04\n",
      "Step 1825 | Loss: 0.49367 | LR: 6.32e-04\n",
      "Step 1850 | Loss: 0.50351 | LR: 6.43e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "Step 1875 | Loss: 0.52187 | LR: 6.55e-04\n",
      "val loss: 0.4857\n",
      "Step 1900 | Loss: 0.51827 | LR: 6.66e-04\n",
      "Step 1925 | Loss: 0.51214 | LR: 6.78e-04\n",
      "Step 1950 | Loss: 0.50240 | LR: 6.89e-04\n",
      "Step 1975 | Loss: 0.51476 | LR: 7.00e-04\n",
      "val loss: 0.5035\n",
      "Step 2000 | Loss: 0.51904 | LR: 7.11e-04\n",
      "Step 2025 | Loss: 0.52531 | LR: 7.22e-04\n",
      "Step 2050 | Loss: 0.48037 | LR: 7.32e-04\n",
      "Step 2075 | Loss: 0.51406 | LR: 7.43e-04\n",
      "val loss: 0.5022\n",
      "Step 2100 | Loss: 0.49812 | LR: 7.53e-04\n",
      "Step 2125 | Loss: 0.52481 | LR: 7.64e-04\n",
      "Step 2150 | Loss: 0.51552 | LR: 7.74e-04\n",
      "Step 2175 | Loss: 0.47397 | LR: 7.84e-04\n",
      "val loss: 0.5098\n",
      "Step 2200 | Loss: 0.52156 | LR: 7.94e-04\n",
      "Step 2225 | Loss: 0.50291 | LR: 8.03e-04\n",
      "Step 2250 | Loss: 0.49130 | LR: 8.13e-04\n",
      "Step 2275 | Loss: 0.47957 | LR: 8.22e-04\n",
      "val loss: 0.5004\n",
      "Step 2300 | Loss: 0.47512 | LR: 8.31e-04\n",
      "Step 2325 | Loss: 0.51660 | LR: 8.40e-04\n",
      "Step 2350 | Loss: 0.49290 | LR: 8.49e-04\n",
      "Step 2375 | Loss: 0.46186 | LR: 8.57e-04\n",
      "val loss: 0.5006\n",
      "Step 2400 | Loss: 0.48777 | LR: 8.66e-04\n",
      "Step 2425 | Loss: 0.47482 | LR: 8.74e-04\n",
      "Step 2450 | Loss: 0.51475 | LR: 8.82e-04\n",
      "Step 2475 | Loss: 0.48580 | LR: 8.89e-04\n",
      "val loss: 0.5040\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0005.npz\n",
      "Step 2500 | Loss: 0.52463 | LR: 8.97e-04\n",
      "Step 2525 | Loss: 0.49857 | LR: 9.04e-04\n",
      "Step 2550 | Loss: 0.47290 | LR: 9.11e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0004.npz\n",
      "Step 2575 | Loss: 0.48420 | LR: 9.18e-04\n",
      "val loss: 0.4952\n",
      "Step 2600 | Loss: 0.48875 | LR: 9.24e-04\n",
      "Step 2625 | Loss: 0.50193 | LR: 9.31e-04\n",
      "Step 2650 | Loss: 0.50570 | LR: 9.37e-04\n",
      "Step 2675 | Loss: 0.48375 | LR: 9.42e-04\n",
      "val loss: 0.5040\n",
      "Step 2700 | Loss: 0.49955 | LR: 9.48e-04\n",
      "Step 2725 | Loss: 0.51081 | LR: 9.53e-04\n",
      "Step 2750 | Loss: 0.51849 | LR: 9.58e-04\n",
      "Step 2775 | Loss: 0.50047 | LR: 9.63e-04\n",
      "val loss: 0.5042\n",
      "Step 2800 | Loss: 0.49454 | LR: 9.67e-04\n",
      "Step 2825 | Loss: 0.51177 | LR: 9.72e-04\n",
      "Step 2850 | Loss: 0.49928 | LR: 9.75e-04\n",
      "Step 2875 | Loss: 0.49722 | LR: 9.79e-04\n",
      "val loss: 0.4999\n",
      "Step 2900 | Loss: 0.50089 | LR: 9.82e-04\n",
      "Step 2925 | Loss: 0.51110 | LR: 9.85e-04\n",
      "Step 2950 | Loss: 0.51453 | LR: 9.88e-04\n",
      "Step 2975 | Loss: 0.49979 | LR: 9.91e-04\n",
      "val loss: 0.5005\n",
      "Step 3000 | Loss: 0.49440 | LR: 9.93e-04\n",
      "Step 3025 | Loss: 0.48694 | LR: 9.95e-04\n",
      "Step 3050 | Loss: 0.52273 | LR: 9.96e-04\n",
      "Step 3075 | Loss: 0.50133 | LR: 9.98e-04\n",
      "val loss: 0.4997\n",
      "Step 3100 | Loss: 0.51584 | LR: 9.99e-04\n",
      "Step 3125 | Loss: 0.47787 | LR: 9.99e-04\n",
      "Step 3150 | Loss: 0.53163 | LR: 1.00e-03\n",
      "Step 3175 | Loss: 0.49385 | LR: 1.00e-03\n",
      "=== Step 3176 Done. Avg Loss: 0.73915 ===\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "val loss: 0.5045\n",
      "Step 3200 | Loss: 0.50616 | LR: 1.00e-03\n",
      "Step 3225 | Loss: 0.50793 | LR: 1.00e-03\n",
      "Step 3250 | Loss: 0.51808 | LR: 1.00e-03\n",
      "Step 3275 | Loss: 0.47954 | LR: 1.00e-03\n",
      "val loss: 0.4957\n",
      "Step 3300 | Loss: 0.50658 | LR: 1.00e-03\n",
      "Step 3325 | Loss: 0.51229 | LR: 1.00e-03\n",
      "Step 3350 | Loss: 0.54110 | LR: 1.00e-03\n",
      "Step 3375 | Loss: 0.51057 | LR: 1.00e-03\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.4981\n",
      "Step 3400 | Loss: 0.50929 | LR: 1.00e-03\n",
      "Step 3425 | Loss: 0.51177 | LR: 1.00e-03\n",
      "Step 3450 | Loss: 0.51051 | LR: 1.00e-03\n",
      "Step 3475 | Loss: 0.52753 | LR: 1.00e-03\n",
      "val loss: 0.5039\n",
      "Step 3500 | Loss: 0.48493 | LR: 1.00e-03\n",
      "Step 3525 | Loss: 0.49943 | LR: 1.00e-03\n",
      "Step 3550 | Loss: 0.50324 | LR: 1.00e-03\n",
      "Step 3575 | Loss: 0.49009 | LR: 1.00e-03\n",
      "val loss: 0.4991\n",
      "Step 3600 | Loss: 0.50401 | LR: 1.00e-03\n",
      "Step 3625 | Loss: 0.53724 | LR: 1.00e-03\n",
      "Step 3650 | Loss: 0.48400 | LR: 1.00e-03\n",
      "Step 3675 | Loss: 0.51481 | LR: 1.00e-03\n",
      "val loss: 0.5059\n",
      "Step 3700 | Loss: 0.50342 | LR: 1.00e-03\n",
      "Step 3725 | Loss: 0.50070 | LR: 1.00e-03\n",
      "Step 3750 | Loss: 0.49477 | LR: 1.00e-03\n",
      "Step 3775 | Loss: 0.51511 | LR: 1.00e-03\n",
      "val loss: 0.4971\n",
      "Step 3800 | Loss: 0.52428 | LR: 1.00e-03\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0004.npz\n",
      "Step 3825 | Loss: 0.52977 | LR: 1.00e-03\n",
      "Step 3850 | Loss: 0.48095 | LR: 1.00e-03\n",
      "Step 3875 | Loss: 0.48965 | LR: 1.00e-03\n",
      "val loss: 0.4964\n",
      "Step 3900 | Loss: 0.54272 | LR: 1.00e-03\n",
      "Step 3925 | Loss: 0.51280 | LR: 1.00e-03\n",
      "Step 3950 | Loss: 0.52860 | LR: 1.00e-03\n",
      "Step 3975 | Loss: 0.48603 | LR: 1.00e-03\n",
      "val loss: 0.5019\n",
      "Step 4000 | Loss: 0.48052 | LR: 1.00e-03\n",
      "Step 4025 | Loss: 0.51086 | LR: 1.00e-03\n",
      "Step 4050 | Loss: 0.47745 | LR: 9.99e-04\n",
      "Step 4075 | Loss: 0.47263 | LR: 9.99e-04\n",
      "val loss: 0.5087\n",
      "Step 4100 | Loss: 0.50160 | LR: 9.99e-04\n",
      "Step 4125 | Loss: 0.52042 | LR: 9.99e-04\n",
      "Step 4150 | Loss: 0.49416 | LR: 9.99e-04\n",
      "Step 4175 | Loss: 0.51240 | LR: 9.99e-04\n",
      "val loss: 0.5056\n",
      "Step 4200 | Loss: 0.52152 | LR: 9.99e-04\n",
      "Step 4225 | Loss: 0.50733 | LR: 9.99e-04\n",
      "Step 4250 | Loss: 0.53172 | LR: 9.99e-04\n",
      "Step 4275 | Loss: 0.49058 | LR: 9.99e-04\n",
      "val loss: 0.4962\n",
      "Step 4300 | Loss: 0.49175 | LR: 9.99e-04\n",
      "Step 4325 | Loss: 0.49657 | LR: 9.99e-04\n",
      "Step 4350 | Loss: 0.46999 | LR: 9.99e-04\n",
      "Step 4375 | Loss: 0.47619 | LR: 9.99e-04\n",
      "val loss: 0.5120\n",
      "Step 4400 | Loss: 0.50159 | LR: 9.99e-04\n",
      "Step 4425 | Loss: 0.48263 | LR: 9.99e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "Step 4450 | Loss: 0.49294 | LR: 9.99e-04\n",
      "Step 4475 | Loss: 0.45896 | LR: 9.99e-04\n",
      "val loss: 0.4978\n",
      "Step 4500 | Loss: 0.51551 | LR: 9.99e-04\n",
      "Step 4525 | Loss: 0.48721 | LR: 9.99e-04\n",
      "Step 4550 | Loss: 0.46721 | LR: 9.99e-04\n",
      "Step 4575 | Loss: 0.51778 | LR: 9.99e-04\n",
      "val loss: 0.5091\n",
      "Step 4600 | Loss: 0.49541 | LR: 9.99e-04\n",
      "Step 4625 | Loss: 0.48205 | LR: 9.99e-04\n",
      "Step 4650 | Loss: 0.50596 | LR: 9.99e-04\n",
      "Step 4675 | Loss: 0.49997 | LR: 9.98e-04\n",
      "val loss: 0.5002\n",
      "Step 4700 | Loss: 0.49298 | LR: 9.98e-04\n",
      "Step 4725 | Loss: 0.51043 | LR: 9.98e-04\n",
      "Step 4750 | Loss: 0.48624 | LR: 9.98e-04\n",
      "Step 4775 | Loss: 0.52740 | LR: 9.98e-04\n",
      "val loss: 0.5020\n",
      "Step 4800 | Loss: 0.51897 | LR: 9.98e-04\n",
      "Step 4825 | Loss: 0.47846 | LR: 9.98e-04\n",
      "Step 4850 | Loss: 0.51904 | LR: 9.98e-04\n",
      "Step 4875 | Loss: 0.48442 | LR: 9.98e-04\n",
      "val loss: 0.5081\n",
      "Step 4900 | Loss: 0.46701 | LR: 9.98e-04\n",
      "Step 4925 | Loss: 0.50435 | LR: 9.98e-04\n",
      "Step 4950 | Loss: 0.48944 | LR: 9.98e-04\n",
      "Step 4975 | Loss: 0.48510 | LR: 9.98e-04\n",
      "val loss: 0.4989\n",
      "Step 5000 | Loss: 0.49560 | LR: 9.98e-04\n",
      "Step 5025 | Loss: 0.50155 | LR: 9.98e-04\n",
      "Step 5050 | Loss: 0.46088 | LR: 9.98e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0005.npz\n",
      "Step 5075 | Loss: 0.50267 | LR: 9.98e-04\n",
      "val loss: 0.5042\n",
      "Step 5100 | Loss: 0.51507 | LR: 9.97e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0003.npz\n",
      "Step 5125 | Loss: 0.50163 | LR: 9.97e-04\n",
      "Step 5150 | Loss: 0.51214 | LR: 9.97e-04\n",
      "Step 5175 | Loss: 0.49140 | LR: 9.97e-04\n",
      "val loss: 0.5048\n",
      "Step 5200 | Loss: 0.51039 | LR: 9.97e-04\n",
      "Step 5225 | Loss: 0.47969 | LR: 9.97e-04\n",
      "Step 5250 | Loss: 0.51269 | LR: 9.97e-04\n",
      "Step 5275 | Loss: 0.50682 | LR: 9.97e-04\n",
      "val loss: 0.5039\n",
      "Step 5300 | Loss: 0.51536 | LR: 9.97e-04\n",
      "Step 5325 | Loss: 0.53176 | LR: 9.97e-04\n",
      "Step 5350 | Loss: 0.48096 | LR: 9.97e-04\n",
      "Step 5375 | Loss: 0.48601 | LR: 9.97e-04\n",
      "val loss: 0.4935\n",
      "Step 5400 | Loss: 0.53951 | LR: 9.97e-04\n",
      "Step 5425 | Loss: 0.54299 | LR: 9.97e-04\n",
      "Step 5450 | Loss: 0.51720 | LR: 9.96e-04\n",
      "Step 5475 | Loss: 0.51587 | LR: 9.96e-04\n",
      "val loss: 0.5027\n",
      "Step 5500 | Loss: 0.50064 | LR: 9.96e-04\n",
      "Step 5525 | Loss: 0.47340 | LR: 9.96e-04\n",
      "Step 5550 | Loss: 0.51312 | LR: 9.96e-04\n",
      "Step 5575 | Loss: 0.47832 | LR: 9.96e-04\n",
      "val loss: 0.4967\n",
      "Step 5600 | Loss: 0.49192 | LR: 9.96e-04\n",
      "Step 5625 | Loss: 0.47673 | LR: 9.96e-04\n",
      "Step 5650 | Loss: 0.50449 | LR: 9.96e-04\n",
      "Step 5675 | Loss: 0.48736 | LR: 9.96e-04\n",
      "val loss: 0.4877\n",
      "Step 5700 | Loss: 0.50477 | LR: 9.96e-04\n",
      "Step 5725 | Loss: 0.52609 | LR: 9.96e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "Step 5750 | Loss: 0.47821 | LR: 9.96e-04\n",
      "Step 5775 | Loss: 0.51972 | LR: 9.95e-04\n",
      "val loss: 0.4992\n",
      "Step 5800 | Loss: 0.47600 | LR: 9.95e-04\n",
      "Step 5825 | Loss: 0.48319 | LR: 9.95e-04\n",
      "Step 5850 | Loss: 0.51254 | LR: 9.95e-04\n",
      "Step 5875 | Loss: 0.47537 | LR: 9.95e-04\n",
      "val loss: 0.5036\n",
      "Step 5900 | Loss: 0.49872 | LR: 9.95e-04\n",
      "Step 5925 | Loss: 0.52846 | LR: 9.95e-04\n",
      "Step 5950 | Loss: 0.52928 | LR: 9.95e-04\n",
      "Step 5975 | Loss: 0.47612 | LR: 9.95e-04\n",
      "val loss: 0.5032\n",
      "Step 6000 | Loss: 0.50083 | LR: 9.95e-04\n",
      "Step 6025 | Loss: 0.52547 | LR: 9.95e-04\n",
      "Step 6050 | Loss: 0.49959 | LR: 9.94e-04\n",
      "Step 6075 | Loss: 0.48707 | LR: 9.94e-04\n",
      "val loss: 0.5064\n",
      "Step 6100 | Loss: 0.50352 | LR: 9.94e-04\n",
      "Step 6125 | Loss: 0.51535 | LR: 9.94e-04\n",
      "Step 6150 | Loss: 0.48587 | LR: 9.94e-04\n",
      "Step 6175 | Loss: 0.50391 | LR: 9.94e-04\n",
      "val loss: 0.5009\n",
      "Step 6200 | Loss: 0.49888 | LR: 9.94e-04\n",
      "Step 6225 | Loss: 0.49976 | LR: 9.94e-04\n",
      "Step 6250 | Loss: 0.49889 | LR: 9.94e-04\n",
      "Step 6275 | Loss: 0.47253 | LR: 9.94e-04\n",
      "val loss: 0.4988\n",
      "Step 6300 | Loss: 0.53179 | LR: 9.93e-04\n",
      "Step 6325 | Loss: 0.49600 | LR: 9.93e-04\n",
      "Step 6350 | Loss: 0.51187 | LR: 9.93e-04\n",
      "=== Step 6353 Done. Avg Loss: 0.50366 ===\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0003.npz\n",
      "Step 6375 | Loss: 0.49821 | LR: 9.93e-04\n",
      "val loss: 0.5071\n",
      "Step 6400 | Loss: 0.47698 | LR: 9.93e-04\n",
      "Step 6425 | Loss: 0.50910 | LR: 9.93e-04\n",
      "Step 6450 | Loss: 0.50640 | LR: 9.93e-04\n",
      "Step 6475 | Loss: 0.52537 | LR: 9.93e-04\n",
      "val loss: 0.4983\n",
      "Step 6500 | Loss: 0.51510 | LR: 9.93e-04\n",
      "Step 6525 | Loss: 0.47400 | LR: 9.92e-04\n",
      "Step 6550 | Loss: 0.48992 | LR: 9.92e-04\n",
      "Step 6575 | Loss: 0.49821 | LR: 9.92e-04\n",
      "val loss: 0.5008\n",
      "Step 6600 | Loss: 0.48892 | LR: 9.92e-04\n",
      "Step 6625 | Loss: 0.50300 | LR: 9.92e-04\n",
      "Step 6650 | Loss: 0.48765 | LR: 9.92e-04\n",
      "Step 6675 | Loss: 0.51696 | LR: 9.92e-04\n",
      "val loss: 0.4951\n",
      "Step 6700 | Loss: 0.45710 | LR: 9.92e-04\n",
      "Step 6725 | Loss: 0.48641 | LR: 9.91e-04\n",
      "Step 6750 | Loss: 0.51146 | LR: 9.91e-04\n",
      "Step 6775 | Loss: 0.49937 | LR: 9.91e-04\n",
      "val loss: 0.5001\n",
      "Step 6800 | Loss: 0.53357 | LR: 9.91e-04\n",
      "Step 6825 | Loss: 0.51172 | LR: 9.91e-04\n",
      "Step 6850 | Loss: 0.53072 | LR: 9.91e-04\n",
      "Step 6875 | Loss: 0.49763 | LR: 9.91e-04\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.4965\n",
      "Step 6900 | Loss: 0.48253 | LR: 9.91e-04\n",
      "Step 6925 | Loss: 0.48054 | LR: 9.91e-04\n",
      "Step 6950 | Loss: 0.49680 | LR: 9.90e-04\n",
      "Step 6975 | Loss: 0.51554 | LR: 9.90e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0004.npz\n",
      "val loss: 0.5027\n",
      "Step 7000 | Loss: 0.50882 | LR: 9.90e-04\n",
      "Step 7025 | Loss: 0.47788 | LR: 9.90e-04\n",
      "Step 7050 | Loss: 0.49964 | LR: 9.90e-04\n",
      "Step 7075 | Loss: 0.50281 | LR: 9.90e-04\n",
      "val loss: 0.4959\n",
      "Step 7100 | Loss: 0.49113 | LR: 9.90e-04\n",
      "Step 7125 | Loss: 0.48332 | LR: 9.89e-04\n",
      "Step 7150 | Loss: 0.48775 | LR: 9.89e-04\n",
      "Step 7175 | Loss: 0.48415 | LR: 9.89e-04\n",
      "val loss: 0.5124\n",
      "Step 7200 | Loss: 0.50757 | LR: 9.89e-04\n",
      "Step 7225 | Loss: 0.48285 | LR: 9.89e-04\n",
      "Step 7250 | Loss: 0.49334 | LR: 9.89e-04\n",
      "Step 7275 | Loss: 0.51223 | LR: 9.89e-04\n",
      "val loss: 0.5000\n",
      "Step 7300 | Loss: 0.49459 | LR: 9.89e-04\n",
      "Step 7325 | Loss: 0.49757 | LR: 9.88e-04\n",
      "Step 7350 | Loss: 0.50899 | LR: 9.88e-04\n",
      "Step 7375 | Loss: 0.47458 | LR: 9.88e-04\n",
      "val loss: 0.4929\n",
      "Step 7400 | Loss: 0.50101 | LR: 9.88e-04\n",
      "Step 7425 | Loss: 0.52619 | LR: 9.88e-04\n",
      "Step 7450 | Loss: 0.51767 | LR: 9.88e-04\n",
      "Step 7475 | Loss: 0.51616 | LR: 9.88e-04\n",
      "val loss: 0.4982\n",
      "Step 7500 | Loss: 0.50873 | LR: 9.87e-04\n",
      "Step 7525 | Loss: 0.47313 | LR: 9.87e-04\n",
      "Step 7550 | Loss: 0.51538 | LR: 9.87e-04\n",
      "Step 7575 | Loss: 0.48786 | LR: 9.87e-04\n",
      "val loss: 0.5051\n",
      "Step 7600 | Loss: 0.51859 | LR: 9.87e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 7625 | Loss: 0.51648 | LR: 9.87e-04\n",
      "Step 7650 | Loss: 0.49777 | LR: 9.87e-04\n",
      "Step 7675 | Loss: 0.50392 | LR: 9.86e-04\n",
      "val loss: 0.5020\n",
      "Step 7700 | Loss: 0.50414 | LR: 9.86e-04\n",
      "Step 7725 | Loss: 0.50736 | LR: 9.86e-04\n",
      "Step 7750 | Loss: 0.51555 | LR: 9.86e-04\n",
      "Step 7775 | Loss: 0.49015 | LR: 9.86e-04\n",
      "val loss: 0.5049\n",
      "Step 7800 | Loss: 0.53667 | LR: 9.86e-04\n",
      "Step 7825 | Loss: 0.51828 | LR: 9.85e-04\n",
      "Step 7850 | Loss: 0.51427 | LR: 9.85e-04\n",
      "Step 7875 | Loss: 0.52100 | LR: 9.85e-04\n",
      "val loss: 0.4961\n",
      "Step 7900 | Loss: 0.50670 | LR: 9.85e-04\n",
      "Step 7925 | Loss: 0.49261 | LR: 9.85e-04\n",
      "Step 7950 | Loss: 0.50085 | LR: 9.85e-04\n",
      "Step 7975 | Loss: 0.50980 | LR: 9.84e-04\n",
      "val loss: 0.5015\n",
      "Step 8000 | Loss: 0.47674 | LR: 9.84e-04\n",
      "Step 8025 | Loss: 0.51068 | LR: 9.84e-04\n",
      "Step 8050 | Loss: 0.49802 | LR: 9.84e-04\n",
      "Step 8075 | Loss: 0.49044 | LR: 9.84e-04\n",
      "val loss: 0.5030\n",
      "Step 8100 | Loss: 0.47490 | LR: 9.84e-04\n",
      "Step 8125 | Loss: 0.49365 | LR: 9.83e-04\n",
      "Step 8150 | Loss: 0.50367 | LR: 9.83e-04\n",
      "Step 8175 | Loss: 0.46933 | LR: 9.83e-04\n",
      "val loss: 0.4994\n",
      "Step 8200 | Loss: 0.48448 | LR: 9.83e-04\n",
      "Step 8225 | Loss: 0.49293 | LR: 9.83e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "Step 8250 | Loss: 0.48665 | LR: 9.83e-04\n",
      "Step 8275 | Loss: 0.51484 | LR: 9.82e-04\n",
      "val loss: 0.5023\n",
      "Step 8300 | Loss: 0.50525 | LR: 9.82e-04\n",
      "Step 8325 | Loss: 0.48413 | LR: 9.82e-04\n",
      "Step 8350 | Loss: 0.48601 | LR: 9.82e-04\n",
      "Step 8375 | Loss: 0.52051 | LR: 9.82e-04\n",
      "val loss: 0.5052\n",
      "Step 8400 | Loss: 0.49103 | LR: 9.82e-04\n",
      "Step 8425 | Loss: 0.49539 | LR: 9.81e-04\n",
      "Step 8450 | Loss: 0.47052 | LR: 9.81e-04\n",
      "Step 8475 | Loss: 0.50020 | LR: 9.81e-04\n",
      "val loss: 0.5014\n",
      "Step 8500 | Loss: 0.50387 | LR: 9.81e-04\n",
      "Step 8525 | Loss: 0.54254 | LR: 9.81e-04\n",
      "Step 8550 | Loss: 0.50816 | LR: 9.81e-04\n",
      "Step 8575 | Loss: 0.52594 | LR: 9.80e-04\n",
      "val loss: 0.5019\n",
      "Step 8600 | Loss: 0.54301 | LR: 9.80e-04\n",
      "Step 8625 | Loss: 0.50248 | LR: 9.80e-04\n",
      "Step 8650 | Loss: 0.52384 | LR: 9.80e-04\n",
      "Step 8675 | Loss: 0.51789 | LR: 9.80e-04\n",
      "val loss: 0.5079\n",
      "Step 8700 | Loss: 0.48375 | LR: 9.79e-04\n",
      "Step 8725 | Loss: 0.51655 | LR: 9.79e-04\n",
      "Step 8750 | Loss: 0.51853 | LR: 9.79e-04\n",
      "Step 8775 | Loss: 0.47644 | LR: 9.79e-04\n",
      "val loss: 0.5055\n",
      "Step 8800 | Loss: 0.51724 | LR: 9.79e-04\n",
      "Step 8825 | Loss: 0.49875 | LR: 9.79e-04\n",
      "Step 8850 | Loss: 0.46523 | LR: 9.78e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "Step 8875 | Loss: 0.52329 | LR: 9.78e-04\n",
      "val loss: 0.5033\n",
      "Step 8900 | Loss: 0.50284 | LR: 9.78e-04\n",
      "Step 8925 | Loss: 0.50998 | LR: 9.78e-04\n",
      "Step 8950 | Loss: 0.52979 | LR: 9.78e-04\n",
      "Step 8975 | Loss: 0.51874 | LR: 9.77e-04\n",
      "val loss: 0.5048\n",
      "Step 9000 | Loss: 0.50462 | LR: 9.77e-04\n",
      "Step 9025 | Loss: 0.49063 | LR: 9.77e-04\n",
      "Step 9050 | Loss: 0.50905 | LR: 9.77e-04\n",
      "Step 9075 | Loss: 0.49484 | LR: 9.77e-04\n",
      "val loss: 0.4882\n",
      "Step 9100 | Loss: 0.47101 | LR: 9.76e-04\n",
      "Step 9125 | Loss: 0.48937 | LR: 9.76e-04\n",
      "Step 9150 | Loss: 0.45777 | LR: 9.76e-04\n",
      "Step 9175 | Loss: 0.49468 | LR: 9.76e-04\n",
      "val loss: 0.5106\n",
      "Step 9200 | Loss: 0.50576 | LR: 9.76e-04\n",
      "Step 9225 | Loss: 0.50241 | LR: 9.75e-04\n",
      "Step 9250 | Loss: 0.51485 | LR: 9.75e-04\n",
      "Step 9275 | Loss: 0.49126 | LR: 9.75e-04\n",
      "val loss: 0.5034\n",
      "Step 9300 | Loss: 0.51264 | LR: 9.75e-04\n",
      "Step 9325 | Loss: 0.48985 | LR: 9.75e-04\n",
      "Step 9350 | Loss: 0.50135 | LR: 9.74e-04\n",
      "Step 9375 | Loss: 0.50168 | LR: 9.74e-04\n",
      "val loss: 0.4972\n",
      "Step 9400 | Loss: 0.48296 | LR: 9.74e-04\n",
      "Step 9425 | Loss: 0.49836 | LR: 9.74e-04\n",
      "Step 9450 | Loss: 0.53740 | LR: 9.74e-04\n",
      "Step 9475 | Loss: 0.53046 | LR: 9.73e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0005.npz\n",
      "val loss: 0.5022\n",
      "Step 9500 | Loss: 0.50790 | LR: 9.73e-04\n",
      "Step 9525 | Loss: 0.49825 | LR: 9.73e-04\n",
      "=== Step 9530 Done. Avg Loss: 0.50389 ===\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0004.npz\n",
      "Step 9550 | Loss: 0.50464 | LR: 9.73e-04\n",
      "Step 9575 | Loss: 0.47887 | LR: 9.73e-04\n",
      "val loss: 0.5040\n",
      "Step 9600 | Loss: 0.48766 | LR: 9.72e-04\n",
      "Step 9625 | Loss: 0.52498 | LR: 9.72e-04\n",
      "Step 9650 | Loss: 0.50332 | LR: 9.72e-04\n",
      "Step 9675 | Loss: 0.50630 | LR: 9.72e-04\n",
      "val loss: 0.4962\n",
      "Step 9700 | Loss: 0.50203 | LR: 9.71e-04\n",
      "Step 9725 | Loss: 0.48788 | LR: 9.71e-04\n",
      "Step 9750 | Loss: 0.50431 | LR: 9.71e-04\n",
      "Step 9775 | Loss: 0.49649 | LR: 9.71e-04\n",
      "val loss: 0.5015\n",
      "Step 9800 | Loss: 0.48395 | LR: 9.71e-04\n",
      "Step 9825 | Loss: 0.49629 | LR: 9.70e-04\n",
      "Step 9850 | Loss: 0.48610 | LR: 9.70e-04\n",
      "Step 9875 | Loss: 0.49235 | LR: 9.70e-04\n",
      "val loss: 0.5044\n",
      "Step 9900 | Loss: 0.48643 | LR: 9.70e-04\n",
      "Step 9925 | Loss: 0.48715 | LR: 9.69e-04\n",
      "Step 9950 | Loss: 0.48437 | LR: 9.69e-04\n",
      "Step 9975 | Loss: 0.52546 | LR: 9.69e-04\n",
      "val loss: 0.4965\n",
      "Step 10000 | Loss: 0.52645 | LR: 9.69e-04\n",
      "Step 10025 | Loss: 0.49950 | LR: 9.69e-04\n",
      "Step 10050 | Loss: 0.48982 | LR: 9.68e-04\n",
      "Step 10075 | Loss: 0.46609 | LR: 9.68e-04\n",
      "val loss: 0.5013\n",
      "Step 10100 | Loss: 0.51936 | LR: 9.68e-04\n",
      "Step 10125 | Loss: 0.48987 | LR: 9.68e-04\n",
      "Step 10150 | Loss: 0.50593 | LR: 9.67e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "Step 10175 | Loss: 0.47764 | LR: 9.67e-04\n",
      "val loss: 0.5026\n",
      "Step 10200 | Loss: 0.48242 | LR: 9.67e-04\n",
      "Step 10225 | Loss: 0.54752 | LR: 9.67e-04\n",
      "Step 10250 | Loss: 0.50029 | LR: 9.66e-04\n",
      "Step 10275 | Loss: 0.49954 | LR: 9.66e-04\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5038\n",
      "Step 10300 | Loss: 0.48264 | LR: 9.66e-04\n",
      "Step 10325 | Loss: 0.53533 | LR: 9.66e-04\n",
      "Step 10350 | Loss: 0.50618 | LR: 9.66e-04\n",
      "Step 10375 | Loss: 0.49684 | LR: 9.65e-04\n",
      "val loss: 0.5051\n",
      "Step 10400 | Loss: 0.51333 | LR: 9.65e-04\n",
      "Step 10425 | Loss: 0.48466 | LR: 9.65e-04\n",
      "Step 10450 | Loss: 0.51371 | LR: 9.65e-04\n",
      "Step 10475 | Loss: 0.54274 | LR: 9.64e-04\n",
      "val loss: 0.5018\n",
      "Step 10500 | Loss: 0.48285 | LR: 9.64e-04\n",
      "Step 10525 | Loss: 0.51217 | LR: 9.64e-04\n",
      "Step 10550 | Loss: 0.51332 | LR: 9.64e-04\n",
      "Step 10575 | Loss: 0.48367 | LR: 9.63e-04\n",
      "val loss: 0.4891\n",
      "Step 10600 | Loss: 0.50511 | LR: 9.63e-04\n",
      "Step 10625 | Loss: 0.51384 | LR: 9.63e-04\n",
      "Step 10650 | Loss: 0.48935 | LR: 9.63e-04\n",
      "Step 10675 | Loss: 0.49719 | LR: 9.62e-04\n",
      "val loss: 0.5072\n",
      "Step 10700 | Loss: 0.49291 | LR: 9.62e-04\n",
      "Step 10725 | Loss: 0.47829 | LR: 9.62e-04\n",
      "Step 10750 | Loss: 0.48227 | LR: 9.62e-04\n",
      "Step 10775 | Loss: 0.50578 | LR: 9.61e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "val loss: 0.5059\n",
      "Step 10800 | Loss: 0.47883 | LR: 9.61e-04\n",
      "Step 10825 | Loss: 0.50761 | LR: 9.61e-04\n",
      "Step 10850 | Loss: 0.51385 | LR: 9.61e-04\n",
      "Step 10875 | Loss: 0.50677 | LR: 9.60e-04\n",
      "val loss: 0.5020\n",
      "Step 10900 | Loss: 0.52036 | LR: 9.60e-04\n",
      "Step 10925 | Loss: 0.47475 | LR: 9.60e-04\n",
      "Step 10950 | Loss: 0.49835 | LR: 9.60e-04\n",
      "Step 10975 | Loss: 0.48987 | LR: 9.59e-04\n",
      "val loss: 0.5060\n",
      "Step 11000 | Loss: 0.50716 | LR: 9.59e-04\n",
      "Step 11025 | Loss: 0.48365 | LR: 9.59e-04\n",
      "Step 11050 | Loss: 0.47774 | LR: 9.59e-04\n",
      "Step 11075 | Loss: 0.51737 | LR: 9.58e-04\n",
      "val loss: 0.5096\n",
      "Step 11100 | Loss: 0.48946 | LR: 9.58e-04\n",
      "Step 11125 | Loss: 0.53456 | LR: 9.58e-04\n",
      "Step 11150 | Loss: 0.52749 | LR: 9.58e-04\n",
      "Step 11175 | Loss: 0.49929 | LR: 9.57e-04\n",
      "val loss: 0.5046\n",
      "Step 11200 | Loss: 0.51654 | LR: 9.57e-04\n",
      "Step 11225 | Loss: 0.50959 | LR: 9.57e-04\n",
      "Step 11250 | Loss: 0.50213 | LR: 9.56e-04\n",
      "Step 11275 | Loss: 0.53414 | LR: 9.56e-04\n",
      "val loss: 0.4950\n",
      "Step 11300 | Loss: 0.50136 | LR: 9.56e-04\n",
      "Step 11325 | Loss: 0.49643 | LR: 9.56e-04\n",
      "Step 11350 | Loss: 0.51476 | LR: 9.55e-04\n",
      "Step 11375 | Loss: 0.51043 | LR: 9.55e-04\n",
      "val loss: 0.5073\n",
      "Step 11400 | Loss: 0.50346 | LR: 9.55e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 11425 | Loss: 0.48273 | LR: 9.55e-04\n",
      "Step 11450 | Loss: 0.50906 | LR: 9.54e-04\n",
      "Step 11475 | Loss: 0.52437 | LR: 9.54e-04\n",
      "val loss: 0.4997\n",
      "Step 11500 | Loss: 0.53093 | LR: 9.54e-04\n",
      "Step 11525 | Loss: 0.52096 | LR: 9.54e-04\n",
      "Step 11550 | Loss: 0.46916 | LR: 9.53e-04\n",
      "Step 11575 | Loss: 0.51108 | LR: 9.53e-04\n",
      "val loss: 0.5091\n",
      "Step 11600 | Loss: 0.51890 | LR: 9.53e-04\n",
      "Step 11625 | Loss: 0.52836 | LR: 9.52e-04\n",
      "Step 11650 | Loss: 0.47114 | LR: 9.52e-04\n",
      "Step 11675 | Loss: 0.52208 | LR: 9.52e-04\n",
      "val loss: 0.5025\n",
      "Step 11700 | Loss: 0.50730 | LR: 9.52e-04\n",
      "Step 11725 | Loss: 0.52209 | LR: 9.51e-04\n",
      "Step 11750 | Loss: 0.50209 | LR: 9.51e-04\n",
      "Step 11775 | Loss: 0.49731 | LR: 9.51e-04\n",
      "val loss: 0.5079\n",
      "Step 11800 | Loss: 0.50184 | LR: 9.50e-04\n",
      "Step 11825 | Loss: 0.51088 | LR: 9.50e-04\n",
      "Step 11850 | Loss: 0.49871 | LR: 9.50e-04\n",
      "Step 11875 | Loss: 0.50142 | LR: 9.50e-04\n",
      "val loss: 0.5037\n",
      "Step 11900 | Loss: 0.48316 | LR: 9.49e-04\n",
      "Step 11925 | Loss: 0.51174 | LR: 9.49e-04\n",
      "Step 11950 | Loss: 0.48904 | LR: 9.49e-04\n",
      "Step 11975 | Loss: 0.53344 | LR: 9.48e-04\n",
      "val loss: 0.4915\n",
      "Step 12000 | Loss: 0.52483 | LR: 9.48e-04\n",
      "Step 12025 | Loss: 0.50029 | LR: 9.48e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0003.npz\n",
      "Step 12050 | Loss: 0.49048 | LR: 9.48e-04\n",
      "Step 12075 | Loss: 0.50283 | LR: 9.47e-04\n",
      "val loss: 0.5062\n",
      "Step 12100 | Loss: 0.51775 | LR: 9.47e-04\n",
      "Step 12125 | Loss: 0.48299 | LR: 9.47e-04\n",
      "Step 12150 | Loss: 0.50472 | LR: 9.46e-04\n",
      "Step 12175 | Loss: 0.47545 | LR: 9.46e-04\n",
      "val loss: 0.4978\n",
      "Step 12200 | Loss: 0.51402 | LR: 9.46e-04\n",
      "Step 12225 | Loss: 0.49646 | LR: 9.46e-04\n",
      "Step 12250 | Loss: 0.52083 | LR: 9.45e-04\n",
      "Step 12275 | Loss: 0.52861 | LR: 9.45e-04\n",
      "val loss: 0.5059\n",
      "Step 12300 | Loss: 0.48775 | LR: 9.45e-04\n",
      "Step 12325 | Loss: 0.51851 | LR: 9.44e-04\n",
      "Step 12350 | Loss: 0.51977 | LR: 9.44e-04\n",
      "Step 12375 | Loss: 0.52020 | LR: 9.44e-04\n",
      "val loss: 0.5057\n",
      "Step 12400 | Loss: 0.48711 | LR: 9.43e-04\n",
      "Step 12425 | Loss: 0.48573 | LR: 9.43e-04\n",
      "Step 12450 | Loss: 0.47944 | LR: 9.43e-04\n",
      "Step 12475 | Loss: 0.48549 | LR: 9.43e-04\n",
      "val loss: 0.4907\n",
      "Step 12500 | Loss: 0.50371 | LR: 9.42e-04\n",
      "Step 12525 | Loss: 0.49951 | LR: 9.42e-04\n",
      "Step 12550 | Loss: 0.50126 | LR: 9.42e-04\n",
      "Step 12575 | Loss: 0.49484 | LR: 9.41e-04\n",
      "val loss: 0.5072\n",
      "Step 12600 | Loss: 0.48752 | LR: 9.41e-04\n",
      "Step 12625 | Loss: 0.50762 | LR: 9.41e-04\n",
      "Step 12650 | Loss: 0.50465 | LR: 9.40e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0005.npz\n",
      "Step 12675 | Loss: 0.50891 | LR: 9.40e-04\n",
      "val loss: 0.5055\n",
      "Step 12700 | Loss: 0.53345 | LR: 9.40e-04\n",
      "=== Step 12707 Done. Avg Loss: 0.50385 ===\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "Step 12725 | Loss: 0.50212 | LR: 9.40e-04\n",
      "Step 12750 | Loss: 0.50586 | LR: 9.39e-04\n",
      "Step 12775 | Loss: 0.53116 | LR: 9.39e-04\n",
      "val loss: 0.5004\n",
      "Step 12800 | Loss: 0.47407 | LR: 9.39e-04\n",
      "Step 12825 | Loss: 0.48684 | LR: 9.38e-04\n",
      "Step 12850 | Loss: 0.52667 | LR: 9.38e-04\n",
      "Step 12875 | Loss: 0.52351 | LR: 9.38e-04\n",
      "val loss: 0.5017\n",
      "Step 12900 | Loss: 0.49367 | LR: 9.37e-04\n",
      "Step 12925 | Loss: 0.49929 | LR: 9.37e-04\n",
      "Step 12950 | Loss: 0.48924 | LR: 9.37e-04\n",
      "Step 12975 | Loss: 0.48402 | LR: 9.36e-04\n",
      "val loss: 0.4991\n",
      "Step 13000 | Loss: 0.52594 | LR: 9.36e-04\n",
      "Step 13025 | Loss: 0.51874 | LR: 9.36e-04\n",
      "Step 13050 | Loss: 0.48792 | LR: 9.35e-04\n",
      "Step 13075 | Loss: 0.51031 | LR: 9.35e-04\n",
      "val loss: 0.4976\n",
      "Step 13100 | Loss: 0.49064 | LR: 9.35e-04\n",
      "Step 13125 | Loss: 0.53728 | LR: 9.34e-04\n",
      "Step 13150 | Loss: 0.53442 | LR: 9.34e-04\n",
      "Step 13175 | Loss: 0.47557 | LR: 9.34e-04\n",
      "val loss: 0.5017\n",
      "Step 13200 | Loss: 0.51925 | LR: 9.33e-04\n",
      "Step 13225 | Loss: 0.50369 | LR: 9.33e-04\n",
      "Step 13250 | Loss: 0.54178 | LR: 9.33e-04\n",
      "Step 13275 | Loss: 0.49558 | LR: 9.32e-04\n",
      "val loss: 0.4935\n",
      "Step 13300 | Loss: 0.49697 | LR: 9.32e-04\n",
      "Step 13325 | Loss: 0.52163 | LR: 9.32e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 13350 | Loss: 0.53462 | LR: 9.32e-04\n",
      "Step 13375 | Loss: 0.52927 | LR: 9.31e-04\n",
      "val loss: 0.4944\n",
      "Step 13400 | Loss: 0.53267 | LR: 9.31e-04\n",
      "Step 13425 | Loss: 0.50199 | LR: 9.31e-04\n",
      "Step 13450 | Loss: 0.50507 | LR: 9.30e-04\n",
      "Step 13475 | Loss: 0.50301 | LR: 9.30e-04\n",
      "val loss: 0.5110\n",
      "Step 13500 | Loss: 0.51136 | LR: 9.30e-04\n",
      "Step 13525 | Loss: 0.51742 | LR: 9.29e-04\n",
      "Step 13550 | Loss: 0.50508 | LR: 9.29e-04\n",
      "Step 13575 | Loss: 0.49313 | LR: 9.29e-04\n",
      "val loss: 0.4951\n",
      "Step 13600 | Loss: 0.49166 | LR: 9.28e-04\n",
      "Step 13625 | Loss: 0.52845 | LR: 9.28e-04\n",
      "Step 13650 | Loss: 0.53672 | LR: 9.28e-04\n",
      "Step 13675 | Loss: 0.51311 | LR: 9.27e-04\n",
      "val loss: 0.4927\n",
      "Step 13700 | Loss: 0.53497 | LR: 9.27e-04\n",
      "Step 13725 | Loss: 0.50168 | LR: 9.27e-04\n",
      "Step 13750 | Loss: 0.50085 | LR: 9.26e-04\n",
      "Step 13775 | Loss: 0.54647 | LR: 9.26e-04\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5066\n",
      "Step 13800 | Loss: 0.48241 | LR: 9.25e-04\n",
      "Step 13825 | Loss: 0.49801 | LR: 9.25e-04\n",
      "Step 13850 | Loss: 0.48345 | LR: 9.25e-04\n",
      "Step 13875 | Loss: 0.48729 | LR: 9.24e-04\n",
      "val loss: 0.4964\n",
      "Step 13900 | Loss: 0.48767 | LR: 9.24e-04\n",
      "Step 13925 | Loss: 0.50706 | LR: 9.24e-04\n",
      "Step 13950 | Loss: 0.52578 | LR: 9.23e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0004.npz\n",
      "Step 13975 | Loss: 0.51552 | LR: 9.23e-04\n",
      "val loss: 0.5020\n",
      "Step 14000 | Loss: 0.52404 | LR: 9.23e-04\n",
      "Step 14025 | Loss: 0.51696 | LR: 9.22e-04\n",
      "Step 14050 | Loss: 0.51761 | LR: 9.22e-04\n",
      "Step 14075 | Loss: 0.50140 | LR: 9.22e-04\n",
      "val loss: 0.5072\n",
      "Step 14100 | Loss: 0.51873 | LR: 9.21e-04\n",
      "Step 14125 | Loss: 0.49808 | LR: 9.21e-04\n",
      "Step 14150 | Loss: 0.50504 | LR: 9.21e-04\n",
      "Step 14175 | Loss: 0.48049 | LR: 9.20e-04\n",
      "val loss: 0.5069\n",
      "Step 14200 | Loss: 0.52290 | LR: 9.20e-04\n",
      "Step 14225 | Loss: 0.53296 | LR: 9.20e-04\n",
      "Step 14250 | Loss: 0.49902 | LR: 9.19e-04\n",
      "Step 14275 | Loss: 0.48959 | LR: 9.19e-04\n",
      "val loss: 0.5061\n",
      "Step 14300 | Loss: 0.51886 | LR: 9.19e-04\n",
      "Step 14325 | Loss: 0.47704 | LR: 9.18e-04\n",
      "Step 14350 | Loss: 0.51029 | LR: 9.18e-04\n",
      "Step 14375 | Loss: 0.49901 | LR: 9.17e-04\n",
      "val loss: 0.5078\n",
      "Step 14400 | Loss: 0.51219 | LR: 9.17e-04\n",
      "Step 14425 | Loss: 0.53432 | LR: 9.17e-04\n",
      "Step 14450 | Loss: 0.50907 | LR: 9.16e-04\n",
      "Step 14475 | Loss: 0.51244 | LR: 9.16e-04\n",
      "val loss: 0.5026\n",
      "Step 14500 | Loss: 0.47707 | LR: 9.16e-04\n",
      "Step 14525 | Loss: 0.46059 | LR: 9.15e-04\n",
      "Step 14550 | Loss: 0.49131 | LR: 9.15e-04\n",
      "Step 14575 | Loss: 0.50743 | LR: 9.15e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "val loss: 0.4967\n",
      "Step 14600 | Loss: 0.47134 | LR: 9.14e-04\n",
      "Step 14625 | Loss: 0.52253 | LR: 9.14e-04\n",
      "Step 14650 | Loss: 0.50989 | LR: 9.13e-04\n",
      "Step 14675 | Loss: 0.49188 | LR: 9.13e-04\n",
      "val loss: 0.4989\n",
      "Step 14700 | Loss: 0.51116 | LR: 9.13e-04\n",
      "Step 14725 | Loss: 0.51580 | LR: 9.12e-04\n",
      "Step 14750 | Loss: 0.50561 | LR: 9.12e-04\n",
      "Step 14775 | Loss: 0.49201 | LR: 9.12e-04\n",
      "val loss: 0.4987\n",
      "Step 14800 | Loss: 0.50556 | LR: 9.11e-04\n",
      "Step 14825 | Loss: 0.51794 | LR: 9.11e-04\n",
      "Step 14850 | Loss: 0.50725 | LR: 9.11e-04\n",
      "Step 14875 | Loss: 0.49360 | LR: 9.10e-04\n",
      "val loss: 0.5060\n",
      "Step 14900 | Loss: 0.47935 | LR: 9.10e-04\n",
      "Step 14925 | Loss: 0.50210 | LR: 9.09e-04\n",
      "Step 14950 | Loss: 0.50556 | LR: 9.09e-04\n",
      "Step 14975 | Loss: 0.48413 | LR: 9.09e-04\n",
      "val loss: 0.4966\n",
      "Step 15000 | Loss: 0.49524 | LR: 9.08e-04\n",
      "Step 15025 | Loss: 0.47516 | LR: 9.08e-04\n",
      "Step 15050 | Loss: 0.53410 | LR: 9.08e-04\n",
      "Step 15075 | Loss: 0.50808 | LR: 9.07e-04\n",
      "val loss: 0.5072\n",
      "Step 15100 | Loss: 0.50036 | LR: 9.07e-04\n",
      "Step 15125 | Loss: 0.49470 | LR: 9.06e-04\n",
      "Step 15150 | Loss: 0.52870 | LR: 9.06e-04\n",
      "Step 15175 | Loss: 0.53288 | LR: 9.06e-04\n",
      "val loss: 0.4950\n",
      "Step 15200 | Loss: 0.50198 | LR: 9.05e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0005.npz\n",
      "Step 15225 | Loss: 0.48694 | LR: 9.05e-04\n",
      "Step 15250 | Loss: 0.47582 | LR: 9.04e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0003.npz\n",
      "Step 15275 | Loss: 0.50410 | LR: 9.04e-04\n",
      "val loss: 0.4993\n",
      "Step 15300 | Loss: 0.49455 | LR: 9.04e-04\n",
      "Step 15325 | Loss: 0.51926 | LR: 9.03e-04\n",
      "Step 15350 | Loss: 0.49153 | LR: 9.03e-04\n",
      "Step 15375 | Loss: 0.47409 | LR: 9.03e-04\n",
      "val loss: 0.4998\n",
      "Step 15400 | Loss: 0.49177 | LR: 9.02e-04\n",
      "Step 15425 | Loss: 0.49129 | LR: 9.02e-04\n",
      "Step 15450 | Loss: 0.49735 | LR: 9.01e-04\n",
      "Step 15475 | Loss: 0.50460 | LR: 9.01e-04\n",
      "val loss: 0.4959\n",
      "Step 15500 | Loss: 0.50181 | LR: 9.01e-04\n",
      "Step 15525 | Loss: 0.51126 | LR: 9.00e-04\n",
      "Step 15550 | Loss: 0.48022 | LR: 9.00e-04\n",
      "Step 15575 | Loss: 0.49732 | LR: 8.99e-04\n",
      "val loss: 0.4986\n",
      "Step 15600 | Loss: 0.44440 | LR: 8.99e-04\n",
      "Step 15625 | Loss: 0.50125 | LR: 8.99e-04\n",
      "Step 15650 | Loss: 0.51919 | LR: 8.98e-04\n",
      "Step 15675 | Loss: 0.49635 | LR: 8.98e-04\n",
      "val loss: 0.5004\n",
      "Step 15700 | Loss: 0.50965 | LR: 8.97e-04\n",
      "Step 15725 | Loss: 0.50924 | LR: 8.97e-04\n",
      "Step 15750 | Loss: 0.50064 | LR: 8.97e-04\n",
      "Step 15775 | Loss: 0.52904 | LR: 8.96e-04\n",
      "val loss: 0.5027\n",
      "Step 15800 | Loss: 0.52069 | LR: 8.96e-04\n",
      "Step 15825 | Loss: 0.50591 | LR: 8.95e-04\n",
      "Step 15850 | Loss: 0.50103 | LR: 8.95e-04\n",
      "Step 15875 | Loss: 0.50018 | LR: 8.95e-04\n",
      "=== Step 15884 Done. Avg Loss: 0.50385 ===\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0005.npz\n",
      "val loss: 0.4952\n",
      "Step 15900 | Loss: 0.50689 | LR: 8.94e-04\n",
      "Step 15925 | Loss: 0.51309 | LR: 8.94e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0003.npz\n",
      "Step 15950 | Loss: 0.53610 | LR: 8.93e-04\n",
      "Step 15975 | Loss: 0.50295 | LR: 8.93e-04\n",
      "val loss: 0.5060\n",
      "Step 16000 | Loss: 0.52112 | LR: 8.93e-04\n",
      "Step 16025 | Loss: 0.50272 | LR: 8.92e-04\n",
      "Step 16050 | Loss: 0.50955 | LR: 8.92e-04\n",
      "Step 16075 | Loss: 0.54396 | LR: 8.91e-04\n",
      "val loss: 0.4951\n",
      "Step 16100 | Loss: 0.54370 | LR: 8.91e-04\n",
      "Step 16125 | Loss: 0.48313 | LR: 8.91e-04\n",
      "Step 16150 | Loss: 0.51546 | LR: 8.90e-04\n",
      "Step 16175 | Loss: 0.49499 | LR: 8.90e-04\n",
      "val loss: 0.5045\n",
      "Step 16200 | Loss: 0.48958 | LR: 8.89e-04\n",
      "Step 16225 | Loss: 0.51450 | LR: 8.89e-04\n",
      "Step 16250 | Loss: 0.51944 | LR: 8.89e-04\n",
      "Step 16275 | Loss: 0.50662 | LR: 8.88e-04\n",
      "val loss: 0.4969\n",
      "Step 16300 | Loss: 0.52713 | LR: 8.88e-04\n",
      "Step 16325 | Loss: 0.49264 | LR: 8.87e-04\n",
      "Step 16350 | Loss: 0.49902 | LR: 8.87e-04\n",
      "Step 16375 | Loss: 0.52199 | LR: 8.87e-04\n",
      "val loss: 0.4941\n",
      "Step 16400 | Loss: 0.51547 | LR: 8.86e-04\n",
      "Step 16425 | Loss: 0.52197 | LR: 8.86e-04\n",
      "Step 16450 | Loss: 0.49602 | LR: 8.85e-04\n",
      "Step 16475 | Loss: 0.49899 | LR: 8.85e-04\n",
      "val loss: 0.5087\n",
      "Step 16500 | Loss: 0.52669 | LR: 8.85e-04\n",
      "Step 16525 | Loss: 0.49306 | LR: 8.84e-04\n",
      "Step 16550 | Loss: 0.53181 | LR: 8.84e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "Step 16575 | Loss: 0.52302 | LR: 8.83e-04\n",
      "val loss: 0.5039\n",
      "Step 16600 | Loss: 0.52767 | LR: 8.83e-04\n",
      "Step 16625 | Loss: 0.47948 | LR: 8.82e-04\n",
      "Step 16650 | Loss: 0.49409 | LR: 8.82e-04\n",
      "Step 16675 | Loss: 0.49383 | LR: 8.82e-04\n",
      "val loss: 0.5011\n",
      "Step 16700 | Loss: 0.51064 | LR: 8.81e-04\n",
      "Step 16725 | Loss: 0.51690 | LR: 8.81e-04\n",
      "Step 16750 | Loss: 0.46587 | LR: 8.80e-04\n",
      "Step 16775 | Loss: 0.48678 | LR: 8.80e-04\n",
      "val loss: 0.5007\n",
      "Step 16800 | Loss: 0.48608 | LR: 8.79e-04\n",
      "Step 16825 | Loss: 0.50712 | LR: 8.79e-04\n",
      "Step 16850 | Loss: 0.50163 | LR: 8.79e-04\n",
      "Step 16875 | Loss: 0.51159 | LR: 8.78e-04\n",
      "val loss: 0.5033\n",
      "Step 16900 | Loss: 0.48559 | LR: 8.78e-04\n",
      "Step 16925 | Loss: 0.46514 | LR: 8.77e-04\n",
      "Step 16950 | Loss: 0.49470 | LR: 8.77e-04\n",
      "Step 16975 | Loss: 0.50792 | LR: 8.76e-04\n",
      "val loss: 0.4981\n",
      "Step 17000 | Loss: 0.50674 | LR: 8.76e-04\n",
      "Step 17025 | Loss: 0.50817 | LR: 8.76e-04\n",
      "Step 17050 | Loss: 0.45994 | LR: 8.75e-04\n",
      "Step 17075 | Loss: 0.54260 | LR: 8.75e-04\n",
      "val loss: 0.5038\n",
      "Step 17100 | Loss: 0.54555 | LR: 8.74e-04\n",
      "Step 17125 | Loss: 0.49479 | LR: 8.74e-04\n",
      "Step 17150 | Loss: 0.47515 | LR: 8.73e-04\n",
      "Step 17175 | Loss: 0.49173 | LR: 8.73e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5057\n",
      "Step 17200 | Loss: 0.49389 | LR: 8.73e-04\n",
      "Step 17225 | Loss: 0.46624 | LR: 8.72e-04\n",
      "Step 17250 | Loss: 0.48037 | LR: 8.72e-04\n",
      "Step 17275 | Loss: 0.49144 | LR: 8.71e-04\n",
      "val loss: 0.5004\n",
      "Step 17300 | Loss: 0.48859 | LR: 8.71e-04\n",
      "Step 17325 | Loss: 0.50765 | LR: 8.70e-04\n",
      "Step 17350 | Loss: 0.52922 | LR: 8.70e-04\n",
      "Step 17375 | Loss: 0.48383 | LR: 8.70e-04\n",
      "val loss: 0.5023\n",
      "Step 17400 | Loss: 0.47240 | LR: 8.69e-04\n",
      "Step 17425 | Loss: 0.51181 | LR: 8.69e-04\n",
      "Step 17450 | Loss: 0.48485 | LR: 8.68e-04\n",
      "Step 17475 | Loss: 0.50675 | LR: 8.68e-04\n",
      "val loss: 0.5095\n",
      "Step 17500 | Loss: 0.47166 | LR: 8.67e-04\n",
      "Step 17525 | Loss: 0.49710 | LR: 8.67e-04\n",
      "Step 17550 | Loss: 0.49967 | LR: 8.66e-04\n",
      "Step 17575 | Loss: 0.51004 | LR: 8.66e-04\n",
      "val loss: 0.4997\n",
      "Step 17600 | Loss: 0.52205 | LR: 8.66e-04\n",
      "Step 17625 | Loss: 0.54379 | LR: 8.65e-04\n",
      "Step 17650 | Loss: 0.50293 | LR: 8.65e-04\n",
      "Step 17675 | Loss: 0.51991 | LR: 8.64e-04\n",
      "val loss: 0.5002\n",
      "Step 17700 | Loss: 0.50004 | LR: 8.64e-04\n",
      "Step 17725 | Loss: 0.51339 | LR: 8.63e-04\n",
      "Step 17750 | Loss: 0.46969 | LR: 8.63e-04\n",
      "Step 17775 | Loss: 0.51418 | LR: 8.62e-04\n",
      "val loss: 0.5034\n",
      "Step 17800 | Loss: 0.49365 | LR: 8.62e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0004.npz\n",
      "Step 17825 | Loss: 0.51951 | LR: 8.62e-04\n",
      "Step 17850 | Loss: 0.50543 | LR: 8.61e-04\n",
      "Step 17875 | Loss: 0.53882 | LR: 8.61e-04\n",
      "val loss: 0.5051\n",
      "Step 17900 | Loss: 0.49366 | LR: 8.60e-04\n",
      "Step 17925 | Loss: 0.51620 | LR: 8.60e-04\n",
      "Step 17950 | Loss: 0.49302 | LR: 8.59e-04\n",
      "Step 17975 | Loss: 0.49642 | LR: 8.59e-04\n",
      "val loss: 0.5034\n",
      "Step 18000 | Loss: 0.55234 | LR: 8.58e-04\n",
      "Step 18025 | Loss: 0.52183 | LR: 8.58e-04\n",
      "Step 18050 | Loss: 0.53688 | LR: 8.58e-04\n",
      "Step 18075 | Loss: 0.51496 | LR: 8.57e-04\n",
      "val loss: 0.5036\n",
      "Step 18100 | Loss: 0.51083 | LR: 8.57e-04\n",
      "Step 18125 | Loss: 0.52494 | LR: 8.56e-04\n",
      "Step 18150 | Loss: 0.47039 | LR: 8.56e-04\n",
      "Step 18175 | Loss: 0.49842 | LR: 8.55e-04\n",
      "val loss: 0.5020\n",
      "Step 18200 | Loss: 0.50378 | LR: 8.55e-04\n",
      "Step 18225 | Loss: 0.49094 | LR: 8.54e-04\n",
      "Step 18250 | Loss: 0.49661 | LR: 8.54e-04\n",
      "Step 18275 | Loss: 0.51747 | LR: 8.53e-04\n",
      "val loss: 0.4964\n",
      "Step 18300 | Loss: 0.50081 | LR: 8.53e-04\n",
      "Step 18325 | Loss: 0.51313 | LR: 8.52e-04\n",
      "Step 18350 | Loss: 0.48937 | LR: 8.52e-04\n",
      "Step 18375 | Loss: 0.50186 | LR: 8.52e-04\n",
      "val loss: 0.5027\n",
      "Step 18400 | Loss: 0.50724 | LR: 8.51e-04\n",
      "Step 18425 | Loss: 0.52101 | LR: 8.51e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "Step 18450 | Loss: 0.51917 | LR: 8.50e-04\n",
      "Step 18475 | Loss: 0.48612 | LR: 8.50e-04\n",
      "val loss: 0.4958\n",
      "Step 18500 | Loss: 0.51710 | LR: 8.49e-04\n",
      "Step 18525 | Loss: 0.49228 | LR: 8.49e-04\n",
      "Step 18550 | Loss: 0.49648 | LR: 8.48e-04\n",
      "Step 18575 | Loss: 0.48713 | LR: 8.48e-04\n",
      "val loss: 0.5038\n",
      "Step 18600 | Loss: 0.50757 | LR: 8.47e-04\n",
      "Step 18625 | Loss: 0.49574 | LR: 8.47e-04\n",
      "Step 18650 | Loss: 0.49892 | LR: 8.46e-04\n",
      "Step 18675 | Loss: 0.50704 | LR: 8.46e-04\n",
      "val loss: 0.5053\n",
      "Step 18700 | Loss: 0.47687 | LR: 8.45e-04\n",
      "Step 18725 | Loss: 0.50036 | LR: 8.45e-04\n",
      "Step 18750 | Loss: 0.49891 | LR: 8.45e-04\n",
      "Step 18775 | Loss: 0.49575 | LR: 8.44e-04\n",
      "val loss: 0.5034\n",
      "Step 18800 | Loss: 0.51674 | LR: 8.44e-04\n",
      "Step 18825 | Loss: 0.50784 | LR: 8.43e-04\n",
      "Step 18850 | Loss: 0.48428 | LR: 8.43e-04\n",
      "Step 18875 | Loss: 0.48983 | LR: 8.42e-04\n",
      "val loss: 0.4950\n",
      "Step 18900 | Loss: 0.47868 | LR: 8.42e-04\n",
      "Step 18925 | Loss: 0.51336 | LR: 8.41e-04\n",
      "Step 18950 | Loss: 0.48743 | LR: 8.41e-04\n",
      "Step 18975 | Loss: 0.52776 | LR: 8.40e-04\n",
      "val loss: 0.5043\n",
      "Step 19000 | Loss: 0.49710 | LR: 8.40e-04\n",
      "Step 19025 | Loss: 0.48707 | LR: 8.39e-04\n",
      "Step 19050 | Loss: 0.52487 | LR: 8.39e-04\n",
      "=== Step 19061 Done. Avg Loss: 0.50381 ===\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 19075 | Loss: 0.50709 | LR: 8.38e-04\n",
      "val loss: 0.5012\n",
      "Step 19100 | Loss: 0.47854 | LR: 8.38e-04\n",
      "Step 19125 | Loss: 0.48735 | LR: 8.37e-04\n",
      "Step 19150 | Loss: 0.51413 | LR: 8.37e-04\n",
      "Step 19175 | Loss: 0.48215 | LR: 8.36e-04\n",
      "val loss: 0.5022\n",
      "Step 19200 | Loss: 0.47376 | LR: 8.36e-04\n",
      "Step 19225 | Loss: 0.50064 | LR: 8.35e-04\n",
      "Step 19250 | Loss: 0.52808 | LR: 8.35e-04\n",
      "Step 19275 | Loss: 0.47780 | LR: 8.35e-04\n",
      "val loss: 0.5054\n",
      "Step 19300 | Loss: 0.50333 | LR: 8.34e-04\n",
      "Step 19325 | Loss: 0.48281 | LR: 8.34e-04\n",
      "Step 19350 | Loss: 0.49361 | LR: 8.33e-04\n",
      "Step 19375 | Loss: 0.48618 | LR: 8.33e-04\n",
      "val loss: 0.5047\n",
      "Step 19400 | Loss: 0.50641 | LR: 8.32e-04\n",
      "Step 19425 | Loss: 0.49988 | LR: 8.32e-04\n",
      "Step 19450 | Loss: 0.50987 | LR: 8.31e-04\n",
      "Step 19475 | Loss: 0.50301 | LR: 8.31e-04\n",
      "val loss: 0.5031\n",
      "Step 19500 | Loss: 0.51463 | LR: 8.30e-04\n",
      "Step 19525 | Loss: 0.50411 | LR: 8.30e-04\n",
      "Step 19550 | Loss: 0.50769 | LR: 8.29e-04\n",
      "Step 19575 | Loss: 0.51167 | LR: 8.29e-04\n",
      "val loss: 0.5091\n",
      "Step 19600 | Loss: 0.47364 | LR: 8.28e-04\n",
      "Step 19625 | Loss: 0.52603 | LR: 8.28e-04\n",
      "Step 19650 | Loss: 0.48664 | LR: 8.27e-04\n",
      "Step 19675 | Loss: 0.48448 | LR: 8.27e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0005.npz\n",
      "val loss: 0.4960\n",
      "Step 19700 | Loss: 0.51458 | LR: 8.26e-04\n",
      "Step 19725 | Loss: 0.51218 | LR: 8.26e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0004.npz\n",
      "Step 19750 | Loss: 0.50887 | LR: 8.25e-04\n",
      "Step 19775 | Loss: 0.50550 | LR: 8.25e-04\n",
      "val loss: 0.5014\n",
      "Step 19800 | Loss: 0.50994 | LR: 8.24e-04\n",
      "Step 19825 | Loss: 0.48883 | LR: 8.24e-04\n",
      "Step 19850 | Loss: 0.49471 | LR: 8.23e-04\n",
      "Step 19875 | Loss: 0.48967 | LR: 8.23e-04\n",
      "val loss: 0.4995\n",
      "Step 19900 | Loss: 0.50849 | LR: 8.22e-04\n",
      "Step 19925 | Loss: 0.47660 | LR: 8.22e-04\n",
      "Step 19950 | Loss: 0.50940 | LR: 8.21e-04\n",
      "Step 19975 | Loss: 0.49663 | LR: 8.21e-04\n",
      "val loss: 0.4985\n",
      "Step 20000 | Loss: 0.50434 | LR: 8.20e-04\n",
      "Step 20025 | Loss: 0.48372 | LR: 8.20e-04\n",
      "Step 20050 | Loss: 0.50442 | LR: 8.19e-04\n",
      "Step 20075 | Loss: 0.51957 | LR: 8.19e-04\n",
      "val loss: 0.4877\n",
      "Step 20100 | Loss: 0.49206 | LR: 8.18e-04\n",
      "Step 20125 | Loss: 0.48772 | LR: 8.18e-04\n",
      "Step 20150 | Loss: 0.51639 | LR: 8.17e-04\n",
      "Step 20175 | Loss: 0.53130 | LR: 8.17e-04\n",
      "val loss: 0.5024\n",
      "Step 20200 | Loss: 0.51547 | LR: 8.16e-04\n",
      "Step 20225 | Loss: 0.49624 | LR: 8.16e-04\n",
      "Step 20250 | Loss: 0.49289 | LR: 8.15e-04\n",
      "Step 20275 | Loss: 0.49051 | LR: 8.15e-04\n",
      "val loss: 0.4980\n",
      "Step 20300 | Loss: 0.53177 | LR: 8.14e-04\n",
      "Step 20325 | Loss: 0.49943 | LR: 8.14e-04\n",
      "Step 20350 | Loss: 0.52155 | LR: 8.13e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0003.npz\n",
      "Step 20375 | Loss: 0.49585 | LR: 8.13e-04\n",
      "val loss: 0.5087\n",
      "Step 20400 | Loss: 0.50531 | LR: 8.12e-04\n",
      "Step 20425 | Loss: 0.53218 | LR: 8.12e-04\n",
      "Step 20450 | Loss: 0.51222 | LR: 8.11e-04\n",
      "Step 20475 | Loss: 0.48523 | LR: 8.11e-04\n",
      "val loss: 0.4941\n",
      "Step 20500 | Loss: 0.50552 | LR: 8.10e-04\n",
      "Step 20525 | Loss: 0.49472 | LR: 8.10e-04\n",
      "Step 20550 | Loss: 0.49625 | LR: 8.09e-04\n",
      "Step 20575 | Loss: 0.52360 | LR: 8.09e-04\n",
      "val loss: 0.5028\n",
      "Step 20600 | Loss: 0.49344 | LR: 8.08e-04\n",
      "Step 20625 | Loss: 0.50633 | LR: 8.08e-04\n",
      "Step 20650 | Loss: 0.48042 | LR: 8.07e-04\n",
      "Step 20675 | Loss: 0.50095 | LR: 8.07e-04\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.4958\n",
      "Step 20700 | Loss: 0.47861 | LR: 8.06e-04\n",
      "Step 20725 | Loss: 0.50474 | LR: 8.06e-04\n",
      "Step 20750 | Loss: 0.49726 | LR: 8.05e-04\n",
      "Step 20775 | Loss: 0.54106 | LR: 8.05e-04\n",
      "val loss: 0.5022\n",
      "Step 20800 | Loss: 0.50800 | LR: 8.04e-04\n",
      "Step 20825 | Loss: 0.49252 | LR: 8.03e-04\n",
      "Step 20850 | Loss: 0.50224 | LR: 8.03e-04\n",
      "Step 20875 | Loss: 0.52192 | LR: 8.02e-04\n",
      "val loss: 0.4987\n",
      "Step 20900 | Loss: 0.50488 | LR: 8.02e-04\n",
      "Step 20925 | Loss: 0.50137 | LR: 8.01e-04\n",
      "Step 20950 | Loss: 0.51027 | LR: 8.01e-04\n",
      "Step 20975 | Loss: 0.52100 | LR: 8.00e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "val loss: 0.5022\n",
      "Step 21000 | Loss: 0.50763 | LR: 8.00e-04\n",
      "Step 21025 | Loss: 0.51207 | LR: 7.99e-04\n",
      "Step 21050 | Loss: 0.51069 | LR: 7.99e-04\n",
      "Step 21075 | Loss: 0.50034 | LR: 7.98e-04\n",
      "val loss: 0.5001\n",
      "Step 21100 | Loss: 0.49553 | LR: 7.98e-04\n",
      "Step 21125 | Loss: 0.49842 | LR: 7.97e-04\n",
      "Step 21150 | Loss: 0.53316 | LR: 7.97e-04\n",
      "Step 21175 | Loss: 0.52421 | LR: 7.96e-04\n",
      "val loss: 0.5008\n",
      "Step 21200 | Loss: 0.50295 | LR: 7.96e-04\n",
      "Step 21225 | Loss: 0.48403 | LR: 7.95e-04\n",
      "Step 21250 | Loss: 0.52967 | LR: 7.95e-04\n",
      "Step 21275 | Loss: 0.52134 | LR: 7.94e-04\n",
      "val loss: 0.5008\n",
      "Step 21300 | Loss: 0.48990 | LR: 7.94e-04\n",
      "Step 21325 | Loss: 0.51110 | LR: 7.93e-04\n",
      "Step 21350 | Loss: 0.49460 | LR: 7.93e-04\n",
      "Step 21375 | Loss: 0.47324 | LR: 7.92e-04\n",
      "val loss: 0.5030\n",
      "Step 21400 | Loss: 0.49968 | LR: 7.91e-04\n",
      "Step 21425 | Loss: 0.51599 | LR: 7.91e-04\n",
      "Step 21450 | Loss: 0.52472 | LR: 7.90e-04\n",
      "Step 21475 | Loss: 0.49739 | LR: 7.90e-04\n",
      "val loss: 0.5023\n",
      "Step 21500 | Loss: 0.47115 | LR: 7.89e-04\n",
      "Step 21525 | Loss: 0.49922 | LR: 7.89e-04\n",
      "Step 21550 | Loss: 0.49861 | LR: 7.88e-04\n",
      "Step 21575 | Loss: 0.48760 | LR: 7.88e-04\n",
      "val loss: 0.5016\n",
      "Step 21600 | Loss: 0.49820 | LR: 7.87e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "Step 21625 | Loss: 0.49498 | LR: 7.87e-04\n",
      "Step 21650 | Loss: 0.51991 | LR: 7.86e-04\n",
      "Step 21675 | Loss: 0.50303 | LR: 7.86e-04\n",
      "val loss: 0.5103\n",
      "Step 21700 | Loss: 0.49847 | LR: 7.85e-04\n",
      "Step 21725 | Loss: 0.50087 | LR: 7.85e-04\n",
      "Step 21750 | Loss: 0.50991 | LR: 7.84e-04\n",
      "Step 21775 | Loss: 0.52028 | LR: 7.83e-04\n",
      "val loss: 0.4957\n",
      "Step 21800 | Loss: 0.50113 | LR: 7.83e-04\n",
      "Step 21825 | Loss: 0.50511 | LR: 7.82e-04\n",
      "Step 21850 | Loss: 0.51066 | LR: 7.82e-04\n",
      "Step 21875 | Loss: 0.50829 | LR: 7.81e-04\n",
      "val loss: 0.5000\n",
      "Step 21900 | Loss: 0.48038 | LR: 7.81e-04\n",
      "Step 21925 | Loss: 0.50395 | LR: 7.80e-04\n",
      "Step 21950 | Loss: 0.49804 | LR: 7.80e-04\n",
      "Step 21975 | Loss: 0.51405 | LR: 7.79e-04\n",
      "val loss: 0.5057\n",
      "Step 22000 | Loss: 0.50010 | LR: 7.79e-04\n",
      "Step 22025 | Loss: 0.50192 | LR: 7.78e-04\n",
      "Step 22050 | Loss: 0.51863 | LR: 7.78e-04\n",
      "Step 22075 | Loss: 0.50228 | LR: 7.77e-04\n",
      "val loss: 0.4921\n",
      "Step 22100 | Loss: 0.50261 | LR: 7.76e-04\n",
      "Step 22125 | Loss: 0.52687 | LR: 7.76e-04\n",
      "Step 22150 | Loss: 0.49858 | LR: 7.75e-04\n",
      "Step 22175 | Loss: 0.53058 | LR: 7.75e-04\n",
      "val loss: 0.4995\n",
      "Step 22200 | Loss: 0.52028 | LR: 7.74e-04\n",
      "Step 22225 | Loss: 0.49611 | LR: 7.74e-04\n",
      "=== Step 22238 Done. Avg Loss: 0.50372 ===\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0005.npz\n",
      "Step 22250 | Loss: 0.53512 | LR: 7.73e-04\n",
      "Step 22275 | Loss: 0.50258 | LR: 7.73e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0003.npz\n",
      "val loss: 0.4931\n",
      "Step 22300 | Loss: 0.47705 | LR: 7.72e-04\n",
      "Step 22325 | Loss: 0.47444 | LR: 7.72e-04\n",
      "Step 22350 | Loss: 0.47622 | LR: 7.71e-04\n",
      "Step 22375 | Loss: 0.52724 | LR: 7.70e-04\n",
      "val loss: 0.5009\n",
      "Step 22400 | Loss: 0.50609 | LR: 7.70e-04\n",
      "Step 22425 | Loss: 0.52201 | LR: 7.69e-04\n",
      "Step 22450 | Loss: 0.49481 | LR: 7.69e-04\n",
      "Step 22475 | Loss: 0.48249 | LR: 7.68e-04\n",
      "val loss: 0.5008\n",
      "Step 22500 | Loss: 0.49414 | LR: 7.68e-04\n",
      "Step 22525 | Loss: 0.51057 | LR: 7.67e-04\n",
      "Step 22550 | Loss: 0.48484 | LR: 7.67e-04\n",
      "Step 22575 | Loss: 0.50594 | LR: 7.66e-04\n",
      "val loss: 0.5002\n",
      "Step 22600 | Loss: 0.51914 | LR: 7.66e-04\n",
      "Step 22625 | Loss: 0.49594 | LR: 7.65e-04\n",
      "Step 22650 | Loss: 0.49467 | LR: 7.64e-04\n",
      "Step 22675 | Loss: 0.51441 | LR: 7.64e-04\n",
      "val loss: 0.5021\n",
      "Step 22700 | Loss: 0.48240 | LR: 7.63e-04\n",
      "Step 22725 | Loss: 0.51227 | LR: 7.63e-04\n",
      "Step 22750 | Loss: 0.53536 | LR: 7.62e-04\n",
      "Step 22775 | Loss: 0.50621 | LR: 7.62e-04\n",
      "val loss: 0.4962\n",
      "Step 22800 | Loss: 0.52495 | LR: 7.61e-04\n",
      "Step 22825 | Loss: 0.51419 | LR: 7.61e-04\n",
      "Step 22850 | Loss: 0.45877 | LR: 7.60e-04\n",
      "Step 22875 | Loss: 0.51022 | LR: 7.59e-04\n",
      "val loss: 0.4966\n",
      "Step 22900 | Loss: 0.50415 | LR: 7.59e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "Step 22925 | Loss: 0.51403 | LR: 7.58e-04\n",
      "Step 22950 | Loss: 0.53461 | LR: 7.58e-04\n",
      "Step 22975 | Loss: 0.50495 | LR: 7.57e-04\n",
      "val loss: 0.5035\n",
      "Step 23000 | Loss: 0.51105 | LR: 7.57e-04\n",
      "Step 23025 | Loss: 0.52978 | LR: 7.56e-04\n",
      "Step 23050 | Loss: 0.50327 | LR: 7.56e-04\n",
      "Step 23075 | Loss: 0.52211 | LR: 7.55e-04\n",
      "val loss: 0.4963\n",
      "Step 23100 | Loss: 0.52920 | LR: 7.54e-04\n",
      "Step 23125 | Loss: 0.51096 | LR: 7.54e-04\n",
      "Step 23150 | Loss: 0.49424 | LR: 7.53e-04\n",
      "Step 23175 | Loss: 0.49781 | LR: 7.53e-04\n",
      "val loss: 0.5091\n",
      "Step 23200 | Loss: 0.47712 | LR: 7.52e-04\n",
      "Step 23225 | Loss: 0.52332 | LR: 7.52e-04\n",
      "Step 23250 | Loss: 0.49708 | LR: 7.51e-04\n",
      "Step 23275 | Loss: 0.46329 | LR: 7.50e-04\n",
      "val loss: 0.5068\n",
      "Step 23300 | Loss: 0.47064 | LR: 7.50e-04\n",
      "Step 23325 | Loss: 0.51043 | LR: 7.49e-04\n",
      "Step 23350 | Loss: 0.50054 | LR: 7.49e-04\n",
      "Step 23375 | Loss: 0.46419 | LR: 7.48e-04\n",
      "val loss: 0.5140\n",
      "Step 23400 | Loss: 0.50970 | LR: 7.48e-04\n",
      "Step 23425 | Loss: 0.50625 | LR: 7.47e-04\n",
      "Step 23450 | Loss: 0.51031 | LR: 7.47e-04\n",
      "Step 23475 | Loss: 0.47566 | LR: 7.46e-04\n",
      "val loss: 0.4964\n",
      "Step 23500 | Loss: 0.49123 | LR: 7.45e-04\n",
      "Step 23525 | Loss: 0.48513 | LR: 7.45e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 23550 | Loss: 0.49789 | LR: 7.44e-04\n",
      "Step 23575 | Loss: 0.49933 | LR: 7.44e-04\n",
      "val loss: 0.5131\n",
      "Step 23600 | Loss: 0.51677 | LR: 7.43e-04\n",
      "Step 23625 | Loss: 0.50042 | LR: 7.43e-04\n",
      "Step 23650 | Loss: 0.50326 | LR: 7.42e-04\n",
      "Step 23675 | Loss: 0.49444 | LR: 7.41e-04\n",
      "val loss: 0.4998\n",
      "Step 23700 | Loss: 0.49932 | LR: 7.41e-04\n",
      "Step 23725 | Loss: 0.55676 | LR: 7.40e-04\n",
      "Step 23750 | Loss: 0.49607 | LR: 7.40e-04\n",
      "Step 23775 | Loss: 0.49989 | LR: 7.39e-04\n",
      "val loss: 0.5050\n",
      "Step 23800 | Loss: 0.52074 | LR: 7.39e-04\n",
      "Step 23825 | Loss: 0.51648 | LR: 7.38e-04\n",
      "Step 23850 | Loss: 0.50285 | LR: 7.37e-04\n",
      "Step 23875 | Loss: 0.49218 | LR: 7.37e-04\n",
      "val loss: 0.4945\n",
      "Step 23900 | Loss: 0.48752 | LR: 7.36e-04\n",
      "Step 23925 | Loss: 0.50766 | LR: 7.36e-04\n",
      "Step 23950 | Loss: 0.52609 | LR: 7.35e-04\n",
      "Step 23975 | Loss: 0.49260 | LR: 7.35e-04\n",
      "val loss: 0.5080\n",
      "Step 24000 | Loss: 0.47850 | LR: 7.34e-04\n",
      "Step 24025 | Loss: 0.52465 | LR: 7.33e-04\n",
      "Step 24050 | Loss: 0.48432 | LR: 7.33e-04\n",
      "Step 24075 | Loss: 0.50715 | LR: 7.32e-04\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.4956\n",
      "Step 24100 | Loss: 0.51336 | LR: 7.32e-04\n",
      "Step 24125 | Loss: 0.54275 | LR: 7.31e-04\n",
      "Step 24150 | Loss: 0.50380 | LR: 7.31e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0004.npz\n",
      "Step 24175 | Loss: 0.49766 | LR: 7.30e-04\n",
      "val loss: 0.5031\n",
      "Step 24200 | Loss: 0.50898 | LR: 7.29e-04\n",
      "Step 24225 | Loss: 0.53653 | LR: 7.29e-04\n",
      "Step 24250 | Loss: 0.48361 | LR: 7.28e-04\n",
      "Step 24275 | Loss: 0.48257 | LR: 7.28e-04\n",
      "val loss: 0.5001\n",
      "Step 24300 | Loss: 0.49676 | LR: 7.27e-04\n",
      "Step 24325 | Loss: 0.50846 | LR: 7.26e-04\n",
      "Step 24350 | Loss: 0.51082 | LR: 7.26e-04\n",
      "Step 24375 | Loss: 0.47526 | LR: 7.25e-04\n",
      "val loss: 0.4899\n",
      "Step 24400 | Loss: 0.50397 | LR: 7.25e-04\n",
      "Step 24425 | Loss: 0.51276 | LR: 7.24e-04\n",
      "Step 24450 | Loss: 0.49222 | LR: 7.24e-04\n",
      "Step 24475 | Loss: 0.51697 | LR: 7.23e-04\n",
      "val loss: 0.4967\n",
      "Step 24500 | Loss: 0.52392 | LR: 7.22e-04\n",
      "Step 24525 | Loss: 0.50636 | LR: 7.22e-04\n",
      "Step 24550 | Loss: 0.51701 | LR: 7.21e-04\n",
      "Step 24575 | Loss: 0.52419 | LR: 7.21e-04\n",
      "val loss: 0.5039\n",
      "Step 24600 | Loss: 0.52447 | LR: 7.20e-04\n",
      "Step 24625 | Loss: 0.53694 | LR: 7.19e-04\n",
      "Step 24650 | Loss: 0.49850 | LR: 7.19e-04\n",
      "Step 24675 | Loss: 0.50101 | LR: 7.18e-04\n",
      "val loss: 0.4957\n",
      "Step 24700 | Loss: 0.48180 | LR: 7.18e-04\n",
      "Step 24725 | Loss: 0.49640 | LR: 7.17e-04\n",
      "Step 24750 | Loss: 0.51736 | LR: 7.17e-04\n",
      "Step 24775 | Loss: 0.51198 | LR: 7.16e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "val loss: 0.5101\n",
      "Step 24800 | Loss: 0.54116 | LR: 7.15e-04\n",
      "Step 24825 | Loss: 0.50623 | LR: 7.15e-04\n",
      "Step 24850 | Loss: 0.51105 | LR: 7.14e-04\n",
      "Step 24875 | Loss: 0.49232 | LR: 7.14e-04\n",
      "val loss: 0.5028\n",
      "Step 24900 | Loss: 0.47998 | LR: 7.13e-04\n",
      "Step 24925 | Loss: 0.52092 | LR: 7.12e-04\n",
      "Step 24950 | Loss: 0.48002 | LR: 7.12e-04\n",
      "Step 24975 | Loss: 0.50666 | LR: 7.11e-04\n",
      "val loss: 0.4998\n",
      "Step 25000 | Loss: 0.50491 | LR: 7.11e-04\n",
      "Step 25025 | Loss: 0.50969 | LR: 7.10e-04\n",
      "Step 25050 | Loss: 0.47614 | LR: 7.09e-04\n",
      "Step 25075 | Loss: 0.49717 | LR: 7.09e-04\n",
      "val loss: 0.5007\n",
      "Step 25100 | Loss: 0.49659 | LR: 7.08e-04\n",
      "Step 25125 | Loss: 0.47151 | LR: 7.08e-04\n",
      "Step 25150 | Loss: 0.52129 | LR: 7.07e-04\n",
      "Step 25175 | Loss: 0.50461 | LR: 7.07e-04\n",
      "val loss: 0.5059\n",
      "Step 25200 | Loss: 0.51106 | LR: 7.06e-04\n",
      "Step 25225 | Loss: 0.50485 | LR: 7.05e-04\n",
      "Step 25250 | Loss: 0.51793 | LR: 7.05e-04\n",
      "Step 25275 | Loss: 0.49773 | LR: 7.04e-04\n",
      "val loss: 0.4945\n",
      "Step 25300 | Loss: 0.51565 | LR: 7.04e-04\n",
      "Step 25325 | Loss: 0.47265 | LR: 7.03e-04\n",
      "Step 25350 | Loss: 0.52321 | LR: 7.02e-04\n",
      "Step 25375 | Loss: 0.47843 | LR: 7.02e-04\n",
      "val loss: 0.5062\n",
      "Step 25400 | Loss: 0.49691 | LR: 7.01e-04\n",
      "=== Step 25415 Done. Avg Loss: 0.50367 ===\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "Step 25425 | Loss: 0.48439 | LR: 7.01e-04\n",
      "Step 25450 | Loss: 0.49806 | LR: 7.00e-04\n",
      "Step 25475 | Loss: 0.51765 | LR: 6.99e-04\n",
      "val loss: 0.5067\n",
      "Step 25500 | Loss: 0.50661 | LR: 6.99e-04\n",
      "Step 25525 | Loss: 0.45530 | LR: 6.98e-04\n",
      "Step 25550 | Loss: 0.52004 | LR: 6.98e-04\n",
      "Step 25575 | Loss: 0.50680 | LR: 6.97e-04\n",
      "val loss: 0.5000\n",
      "Step 25600 | Loss: 0.53206 | LR: 6.96e-04\n",
      "Step 25625 | Loss: 0.51696 | LR: 6.96e-04\n",
      "Step 25650 | Loss: 0.49242 | LR: 6.95e-04\n",
      "Step 25675 | Loss: 0.50542 | LR: 6.95e-04\n",
      "val loss: 0.5053\n",
      "Step 25700 | Loss: 0.49398 | LR: 6.94e-04\n",
      "Step 25725 | Loss: 0.49598 | LR: 6.93e-04\n",
      "Step 25750 | Loss: 0.51295 | LR: 6.93e-04\n",
      "Step 25775 | Loss: 0.53839 | LR: 6.92e-04\n",
      "val loss: 0.5025\n",
      "Step 25800 | Loss: 0.49971 | LR: 6.92e-04\n",
      "Step 25825 | Loss: 0.53416 | LR: 6.91e-04\n",
      "Step 25850 | Loss: 0.48568 | LR: 6.90e-04\n",
      "Step 25875 | Loss: 0.49682 | LR: 6.90e-04\n",
      "val loss: 0.5156\n",
      "Step 25900 | Loss: 0.50841 | LR: 6.89e-04\n",
      "Step 25925 | Loss: 0.50572 | LR: 6.89e-04\n",
      "Step 25950 | Loss: 0.50830 | LR: 6.88e-04\n",
      "Step 25975 | Loss: 0.51881 | LR: 6.87e-04\n",
      "val loss: 0.5020\n",
      "Step 26000 | Loss: 0.48748 | LR: 6.87e-04\n",
      "Step 26025 | Loss: 0.50250 | LR: 6.86e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0004.npz\n",
      "Step 26050 | Loss: 0.49746 | LR: 6.86e-04\n",
      "Step 26075 | Loss: 0.51382 | LR: 6.85e-04\n",
      "val loss: 0.5068\n",
      "Step 26100 | Loss: 0.50131 | LR: 6.84e-04\n",
      "Step 26125 | Loss: 0.50902 | LR: 6.84e-04\n",
      "Step 26150 | Loss: 0.51119 | LR: 6.83e-04\n",
      "Step 26175 | Loss: 0.53377 | LR: 6.83e-04\n",
      "val loss: 0.4910\n",
      "Step 26200 | Loss: 0.51593 | LR: 6.82e-04\n",
      "Step 26225 | Loss: 0.51080 | LR: 6.81e-04\n",
      "Step 26250 | Loss: 0.50078 | LR: 6.81e-04\n",
      "Step 26275 | Loss: 0.47290 | LR: 6.80e-04\n",
      "val loss: 0.4982\n",
      "Step 26300 | Loss: 0.53327 | LR: 6.80e-04\n",
      "Step 26325 | Loss: 0.49723 | LR: 6.79e-04\n",
      "Step 26350 | Loss: 0.49539 | LR: 6.78e-04\n",
      "Step 26375 | Loss: 0.51462 | LR: 6.78e-04\n",
      "val loss: 0.4968\n",
      "Step 26400 | Loss: 0.49083 | LR: 6.77e-04\n",
      "Step 26425 | Loss: 0.49415 | LR: 6.76e-04\n",
      "Step 26450 | Loss: 0.50506 | LR: 6.76e-04\n",
      "Step 26475 | Loss: 0.49813 | LR: 6.75e-04\n",
      "val loss: 0.4898\n",
      "Step 26500 | Loss: 0.51011 | LR: 6.75e-04\n",
      "Step 26525 | Loss: 0.52771 | LR: 6.74e-04\n",
      "Step 26550 | Loss: 0.50470 | LR: 6.73e-04\n",
      "Step 26575 | Loss: 0.56189 | LR: 6.73e-04\n",
      "val loss: 0.5041\n",
      "Step 26600 | Loss: 0.48477 | LR: 6.72e-04\n",
      "Step 26625 | Loss: 0.49969 | LR: 6.72e-04\n",
      "Step 26650 | Loss: 0.49275 | LR: 6.71e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "Step 26675 | Loss: 0.51914 | LR: 6.70e-04\n",
      "val loss: 0.4935\n",
      "Step 26700 | Loss: 0.48908 | LR: 6.70e-04\n",
      "Step 26725 | Loss: 0.49918 | LR: 6.69e-04\n",
      "Step 26750 | Loss: 0.46348 | LR: 6.69e-04\n",
      "Step 26775 | Loss: 0.49559 | LR: 6.68e-04\n",
      "val loss: 0.5012\n",
      "Step 26800 | Loss: 0.50840 | LR: 6.67e-04\n",
      "Step 26825 | Loss: 0.47345 | LR: 6.67e-04\n",
      "Step 26850 | Loss: 0.50976 | LR: 6.66e-04\n",
      "Step 26875 | Loss: 0.49134 | LR: 6.65e-04\n",
      "val loss: 0.5026\n",
      "Step 26900 | Loss: 0.48578 | LR: 6.65e-04\n",
      "Step 26925 | Loss: 0.53116 | LR: 6.64e-04\n",
      "Step 26950 | Loss: 0.51367 | LR: 6.64e-04\n",
      "Step 26975 | Loss: 0.50211 | LR: 6.63e-04\n",
      "val loss: 0.5092\n",
      "Step 27000 | Loss: 0.50851 | LR: 6.62e-04\n",
      "Step 27025 | Loss: 0.49023 | LR: 6.62e-04\n",
      "Step 27050 | Loss: 0.48365 | LR: 6.61e-04\n",
      "Step 27075 | Loss: 0.51176 | LR: 6.61e-04\n",
      "val loss: 0.5090\n",
      "Step 27100 | Loss: 0.53694 | LR: 6.60e-04\n",
      "Step 27125 | Loss: 0.50734 | LR: 6.59e-04\n",
      "Step 27150 | Loss: 0.51165 | LR: 6.59e-04\n",
      "Step 27175 | Loss: 0.50067 | LR: 6.58e-04\n",
      "val loss: 0.4985\n",
      "Step 27200 | Loss: 0.50707 | LR: 6.57e-04\n",
      "Step 27225 | Loss: 0.50889 | LR: 6.57e-04\n",
      "Step 27250 | Loss: 0.49481 | LR: 6.56e-04\n",
      "Step 27275 | Loss: 0.51845 | LR: 6.56e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0005.npz\n",
      "val loss: 0.4985\n",
      "Step 27300 | Loss: 0.51007 | LR: 6.55e-04\n",
      "Step 27325 | Loss: 0.53142 | LR: 6.54e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0003.npz\n",
      "Step 27350 | Loss: 0.50610 | LR: 6.54e-04\n",
      "Step 27375 | Loss: 0.52490 | LR: 6.53e-04\n",
      "val loss: 0.5048\n",
      "Step 27400 | Loss: 0.48941 | LR: 6.53e-04\n",
      "Step 27425 | Loss: 0.50013 | LR: 6.52e-04\n",
      "Step 27450 | Loss: 0.49075 | LR: 6.51e-04\n",
      "Step 27475 | Loss: 0.49176 | LR: 6.51e-04\n",
      "val loss: 0.4994\n",
      "Step 27500 | Loss: 0.48949 | LR: 6.50e-04\n",
      "Step 27525 | Loss: 0.53116 | LR: 6.49e-04\n",
      "Step 27550 | Loss: 0.51693 | LR: 6.49e-04\n",
      "Step 27575 | Loss: 0.46647 | LR: 6.48e-04\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5016\n",
      "Step 27600 | Loss: 0.47915 | LR: 6.48e-04\n",
      "Step 27625 | Loss: 0.49895 | LR: 6.47e-04\n",
      "Step 27650 | Loss: 0.50689 | LR: 6.46e-04\n",
      "Step 27675 | Loss: 0.49723 | LR: 6.46e-04\n",
      "val loss: 0.5024\n",
      "Step 27700 | Loss: 0.51680 | LR: 6.45e-04\n",
      "Step 27725 | Loss: 0.48959 | LR: 6.44e-04\n",
      "Step 27750 | Loss: 0.53301 | LR: 6.44e-04\n",
      "Step 27775 | Loss: 0.50168 | LR: 6.43e-04\n",
      "val loss: 0.5009\n",
      "Step 27800 | Loss: 0.53419 | LR: 6.43e-04\n",
      "Step 27825 | Loss: 0.50331 | LR: 6.42e-04\n",
      "Step 27850 | Loss: 0.49685 | LR: 6.41e-04\n",
      "Step 27875 | Loss: 0.50059 | LR: 6.41e-04\n",
      "val loss: 0.5006\n",
      "Step 27900 | Loss: 0.53991 | LR: 6.40e-04\n",
      "Step 27925 | Loss: 0.48136 | LR: 6.39e-04\n",
      "Step 27950 | Loss: 0.53678 | LR: 6.39e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 27975 | Loss: 0.50742 | LR: 6.38e-04\n",
      "val loss: 0.5093\n",
      "Step 28000 | Loss: 0.51491 | LR: 6.38e-04\n",
      "Step 28025 | Loss: 0.50860 | LR: 6.37e-04\n",
      "Step 28050 | Loss: 0.48941 | LR: 6.36e-04\n",
      "Step 28075 | Loss: 0.51423 | LR: 6.36e-04\n",
      "val loss: 0.5051\n",
      "Step 28100 | Loss: 0.50893 | LR: 6.35e-04\n",
      "Step 28125 | Loss: 0.51585 | LR: 6.34e-04\n",
      "Step 28150 | Loss: 0.52301 | LR: 6.34e-04\n",
      "Step 28175 | Loss: 0.50701 | LR: 6.33e-04\n",
      "val loss: 0.4978\n",
      "Step 28200 | Loss: 0.51399 | LR: 6.33e-04\n",
      "Step 28225 | Loss: 0.52127 | LR: 6.32e-04\n",
      "Step 28250 | Loss: 0.50028 | LR: 6.31e-04\n",
      "Step 28275 | Loss: 0.48609 | LR: 6.31e-04\n",
      "val loss: 0.5034\n",
      "Step 28300 | Loss: 0.49911 | LR: 6.30e-04\n",
      "Step 28325 | Loss: 0.52782 | LR: 6.29e-04\n",
      "Step 28350 | Loss: 0.49798 | LR: 6.29e-04\n",
      "Step 28375 | Loss: 0.47866 | LR: 6.28e-04\n",
      "val loss: 0.5022\n",
      "Step 28400 | Loss: 0.52604 | LR: 6.28e-04\n",
      "Step 28425 | Loss: 0.51901 | LR: 6.27e-04\n",
      "Step 28450 | Loss: 0.51312 | LR: 6.26e-04\n",
      "Step 28475 | Loss: 0.49968 | LR: 6.26e-04\n",
      "val loss: 0.4976\n",
      "Step 28500 | Loss: 0.53693 | LR: 6.25e-04\n",
      "Step 28525 | Loss: 0.52306 | LR: 6.24e-04\n",
      "Step 28550 | Loss: 0.50820 | LR: 6.24e-04\n",
      "Step 28575 | Loss: 0.52325 | LR: 6.23e-04\n",
      "=== Step 28592 Done. Avg Loss: 0.50362 ===\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "val loss: 0.4976\n",
      "Step 28600 | Loss: 0.51923 | LR: 6.23e-04\n",
      "Step 28625 | Loss: 0.52703 | LR: 6.22e-04\n",
      "Step 28650 | Loss: 0.51186 | LR: 6.21e-04\n",
      "Step 28675 | Loss: 0.48244 | LR: 6.21e-04\n",
      "val loss: 0.5102\n",
      "Step 28700 | Loss: 0.51531 | LR: 6.20e-04\n",
      "Step 28725 | Loss: 0.48698 | LR: 6.19e-04\n",
      "Step 28750 | Loss: 0.52135 | LR: 6.19e-04\n",
      "Step 28775 | Loss: 0.50648 | LR: 6.18e-04\n",
      "val loss: 0.4991\n",
      "Step 28800 | Loss: 0.50663 | LR: 6.17e-04\n",
      "Step 28825 | Loss: 0.49172 | LR: 6.17e-04\n",
      "Step 28850 | Loss: 0.46652 | LR: 6.16e-04\n",
      "Step 28875 | Loss: 0.53127 | LR: 6.16e-04\n",
      "val loss: 0.5044\n",
      "Step 28900 | Loss: 0.50871 | LR: 6.15e-04\n",
      "Step 28925 | Loss: 0.46656 | LR: 6.14e-04\n",
      "Step 28950 | Loss: 0.48404 | LR: 6.14e-04\n",
      "Step 28975 | Loss: 0.52935 | LR: 6.13e-04\n",
      "val loss: 0.5079\n",
      "Step 29000 | Loss: 0.50465 | LR: 6.12e-04\n",
      "Step 29025 | Loss: 0.50977 | LR: 6.12e-04\n",
      "Step 29050 | Loss: 0.52566 | LR: 6.11e-04\n",
      "Step 29075 | Loss: 0.52542 | LR: 6.10e-04\n",
      "val loss: 0.5062\n",
      "Step 29100 | Loss: 0.49104 | LR: 6.10e-04\n",
      "Step 29125 | Loss: 0.49846 | LR: 6.09e-04\n",
      "Step 29150 | Loss: 0.50308 | LR: 6.09e-04\n",
      "Step 29175 | Loss: 0.49248 | LR: 6.08e-04\n",
      "val loss: 0.5047\n",
      "Step 29200 | Loss: 0.48784 | LR: 6.07e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0003.npz\n",
      "Step 29225 | Loss: 0.49524 | LR: 6.07e-04\n",
      "Step 29250 | Loss: 0.49043 | LR: 6.06e-04\n",
      "Step 29275 | Loss: 0.49899 | LR: 6.05e-04\n",
      "val loss: 0.5028\n",
      "Step 29300 | Loss: 0.53390 | LR: 6.05e-04\n",
      "Step 29325 | Loss: 0.50274 | LR: 6.04e-04\n",
      "Step 29350 | Loss: 0.49298 | LR: 6.04e-04\n",
      "Step 29375 | Loss: 0.46697 | LR: 6.03e-04\n",
      "val loss: 0.5053\n",
      "Step 29400 | Loss: 0.49555 | LR: 6.02e-04\n",
      "Step 29425 | Loss: 0.50827 | LR: 6.02e-04\n",
      "Step 29450 | Loss: 0.48853 | LR: 6.01e-04\n",
      "Step 29475 | Loss: 0.47151 | LR: 6.00e-04\n",
      "val loss: 0.5049\n",
      "Step 29500 | Loss: 0.50798 | LR: 6.00e-04\n",
      "Step 29525 | Loss: 0.52855 | LR: 5.99e-04\n",
      "Step 29550 | Loss: 0.48537 | LR: 5.98e-04\n",
      "Step 29575 | Loss: 0.50068 | LR: 5.98e-04\n",
      "val loss: 0.5021\n",
      "Step 29600 | Loss: 0.49670 | LR: 5.97e-04\n",
      "Step 29625 | Loss: 0.52142 | LR: 5.96e-04\n",
      "Step 29650 | Loss: 0.50369 | LR: 5.96e-04\n",
      "Step 29675 | Loss: 0.48629 | LR: 5.95e-04\n",
      "val loss: 0.4920\n",
      "Step 29700 | Loss: 0.50102 | LR: 5.95e-04\n",
      "Step 29725 | Loss: 0.49881 | LR: 5.94e-04\n",
      "Step 29750 | Loss: 0.49993 | LR: 5.93e-04\n",
      "Step 29775 | Loss: 0.49640 | LR: 5.93e-04\n",
      "val loss: 0.5020\n",
      "Step 29800 | Loss: 0.47834 | LR: 5.92e-04\n",
      "Step 29825 | Loss: 0.49201 | LR: 5.91e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 29850 | Loss: 0.51641 | LR: 5.91e-04\n",
      "Step 29875 | Loss: 0.47426 | LR: 5.90e-04\n",
      "val loss: 0.4949\n",
      "Step 29900 | Loss: 0.49605 | LR: 5.89e-04\n",
      "Step 29925 | Loss: 0.50164 | LR: 5.89e-04\n",
      "Step 29950 | Loss: 0.51427 | LR: 5.88e-04\n",
      "Step 29975 | Loss: 0.53762 | LR: 5.88e-04\n",
      "val loss: 0.5013\n",
      "Step 30000 | Loss: 0.49998 | LR: 5.87e-04\n",
      "Step 30025 | Loss: 0.52087 | LR: 5.86e-04\n",
      "Step 30050 | Loss: 0.50484 | LR: 5.86e-04\n",
      "Step 30075 | Loss: 0.50928 | LR: 5.85e-04\n",
      "val loss: 0.5030\n",
      "Step 30100 | Loss: 0.52392 | LR: 5.84e-04\n",
      "Step 30125 | Loss: 0.49575 | LR: 5.84e-04\n",
      "Step 30150 | Loss: 0.50815 | LR: 5.83e-04\n",
      "Step 30175 | Loss: 0.51060 | LR: 5.82e-04\n",
      "val loss: 0.5059\n",
      "Step 30200 | Loss: 0.50453 | LR: 5.82e-04\n",
      "Step 30225 | Loss: 0.50329 | LR: 5.81e-04\n",
      "Step 30250 | Loss: 0.53608 | LR: 5.80e-04\n",
      "Step 30275 | Loss: 0.50914 | LR: 5.80e-04\n",
      "val loss: 0.4951\n",
      "Step 30300 | Loss: 0.48869 | LR: 5.79e-04\n",
      "Step 30325 | Loss: 0.50450 | LR: 5.79e-04\n",
      "Step 30350 | Loss: 0.50680 | LR: 5.78e-04\n",
      "Step 30375 | Loss: 0.48799 | LR: 5.77e-04\n",
      "val loss: 0.4952\n",
      "Step 30400 | Loss: 0.51475 | LR: 5.77e-04\n",
      "Step 30425 | Loss: 0.51836 | LR: 5.76e-04\n",
      "Step 30450 | Loss: 0.50583 | LR: 5.75e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0004.npz\n",
      "Step 30475 | Loss: 0.45081 | LR: 5.75e-04\n",
      "val loss: 0.4994\n",
      "Step 30500 | Loss: 0.51868 | LR: 5.74e-04\n",
      "Step 30525 | Loss: 0.50198 | LR: 5.73e-04\n",
      "Step 30550 | Loss: 0.50225 | LR: 5.73e-04\n",
      "Step 30575 | Loss: 0.51131 | LR: 5.72e-04\n",
      "val loss: 0.4994\n",
      "Step 30600 | Loss: 0.51749 | LR: 5.71e-04\n",
      "Step 30625 | Loss: 0.49338 | LR: 5.71e-04\n",
      "Step 30650 | Loss: 0.51311 | LR: 5.70e-04\n",
      "Step 30675 | Loss: 0.47814 | LR: 5.70e-04\n",
      "val loss: 0.4935\n",
      "Step 30700 | Loss: 0.50771 | LR: 5.69e-04\n",
      "Step 30725 | Loss: 0.45738 | LR: 5.68e-04\n",
      "Step 30750 | Loss: 0.52623 | LR: 5.68e-04\n",
      "Step 30775 | Loss: 0.49797 | LR: 5.67e-04\n",
      "val loss: 0.4944\n",
      "Step 30800 | Loss: 0.48908 | LR: 5.66e-04\n",
      "Step 30825 | Loss: 0.51694 | LR: 5.66e-04\n",
      "Step 30850 | Loss: 0.53714 | LR: 5.65e-04\n",
      "Step 30875 | Loss: 0.47592 | LR: 5.64e-04\n",
      "val loss: 0.4975\n",
      "Step 30900 | Loss: 0.52844 | LR: 5.64e-04\n",
      "Step 30925 | Loss: 0.52100 | LR: 5.63e-04\n",
      "Step 30950 | Loss: 0.50430 | LR: 5.62e-04\n",
      "Step 30975 | Loss: 0.53285 | LR: 5.62e-04\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5044\n",
      "Step 31000 | Loss: 0.49199 | LR: 5.61e-04\n",
      "Step 31025 | Loss: 0.48769 | LR: 5.61e-04\n",
      "Step 31050 | Loss: 0.50675 | LR: 5.60e-04\n",
      "Step 31075 | Loss: 0.51742 | LR: 5.59e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "val loss: 0.5043\n",
      "Step 31100 | Loss: 0.50654 | LR: 5.59e-04\n",
      "Step 31125 | Loss: 0.51967 | LR: 5.58e-04\n",
      "Step 31150 | Loss: 0.49568 | LR: 5.57e-04\n",
      "Step 31175 | Loss: 0.52787 | LR: 5.57e-04\n",
      "val loss: 0.4942\n",
      "Step 31200 | Loss: 0.49840 | LR: 5.56e-04\n",
      "Step 31225 | Loss: 0.50806 | LR: 5.55e-04\n",
      "Step 31250 | Loss: 0.51859 | LR: 5.55e-04\n",
      "Step 31275 | Loss: 0.51539 | LR: 5.54e-04\n",
      "val loss: 0.4976\n",
      "Step 31300 | Loss: 0.48909 | LR: 5.53e-04\n",
      "Step 31325 | Loss: 0.51735 | LR: 5.53e-04\n",
      "Step 31350 | Loss: 0.50578 | LR: 5.52e-04\n",
      "Step 31375 | Loss: 0.50300 | LR: 5.51e-04\n",
      "val loss: 0.5011\n",
      "Step 31400 | Loss: 0.48142 | LR: 5.51e-04\n",
      "Step 31425 | Loss: 0.51749 | LR: 5.50e-04\n",
      "Step 31450 | Loss: 0.51305 | LR: 5.50e-04\n",
      "Step 31475 | Loss: 0.49008 | LR: 5.49e-04\n",
      "val loss: 0.5018\n",
      "Step 31500 | Loss: 0.49341 | LR: 5.48e-04\n",
      "Step 31525 | Loss: 0.49536 | LR: 5.48e-04\n",
      "Step 31550 | Loss: 0.52351 | LR: 5.47e-04\n",
      "Step 31575 | Loss: 0.49826 | LR: 5.46e-04\n",
      "val loss: 0.4951\n",
      "Step 31600 | Loss: 0.51506 | LR: 5.46e-04\n",
      "Step 31625 | Loss: 0.47773 | LR: 5.45e-04\n",
      "Step 31650 | Loss: 0.53926 | LR: 5.44e-04\n",
      "Step 31675 | Loss: 0.49133 | LR: 5.44e-04\n",
      "val loss: 0.4994\n",
      "Step 31700 | Loss: 0.50778 | LR: 5.43e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0005.npz\n",
      "Step 31725 | Loss: 0.49485 | LR: 5.42e-04\n",
      "Step 31750 | Loss: 0.49090 | LR: 5.42e-04\n",
      "=== Step 31769 Done. Avg Loss: 0.50353 ===\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0004.npz\n",
      "Step 31775 | Loss: 0.49077 | LR: 5.41e-04\n",
      "val loss: 0.4986\n",
      "Step 31800 | Loss: 0.49421 | LR: 5.40e-04\n",
      "Step 31825 | Loss: 0.51166 | LR: 5.40e-04\n",
      "Step 31850 | Loss: 0.50337 | LR: 5.39e-04\n",
      "Step 31875 | Loss: 0.52964 | LR: 5.39e-04\n",
      "val loss: 0.5023\n",
      "Step 31900 | Loss: 0.52351 | LR: 5.38e-04\n",
      "Step 31925 | Loss: 0.47055 | LR: 5.37e-04\n",
      "Step 31950 | Loss: 0.48743 | LR: 5.37e-04\n",
      "Step 31975 | Loss: 0.49039 | LR: 5.36e-04\n",
      "val loss: 0.5041\n",
      "Step 32000 | Loss: 0.50295 | LR: 5.35e-04\n",
      "Step 32025 | Loss: 0.52416 | LR: 5.35e-04\n",
      "Step 32050 | Loss: 0.52341 | LR: 5.34e-04\n",
      "Step 32075 | Loss: 0.55546 | LR: 5.33e-04\n",
      "val loss: 0.4953\n",
      "Step 32100 | Loss: 0.49005 | LR: 5.33e-04\n",
      "Step 32125 | Loss: 0.50514 | LR: 5.32e-04\n",
      "Step 32150 | Loss: 0.50705 | LR: 5.31e-04\n",
      "Step 32175 | Loss: 0.51655 | LR: 5.31e-04\n",
      "val loss: 0.4955\n",
      "Step 32200 | Loss: 0.50027 | LR: 5.30e-04\n",
      "Step 32225 | Loss: 0.50032 | LR: 5.29e-04\n",
      "Step 32250 | Loss: 0.51887 | LR: 5.29e-04\n",
      "Step 32275 | Loss: 0.51502 | LR: 5.28e-04\n",
      "val loss: 0.4988\n",
      "Step 32300 | Loss: 0.51707 | LR: 5.27e-04\n",
      "Step 32325 | Loss: 0.47792 | LR: 5.27e-04\n",
      "Step 32350 | Loss: 0.51665 | LR: 5.26e-04\n",
      "Step 32375 | Loss: 0.49905 | LR: 5.26e-04\n",
      "val loss: 0.4911\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0005.npz\n",
      "Step 32400 | Loss: 0.50339 | LR: 5.25e-04\n",
      "Step 32425 | Loss: 0.52463 | LR: 5.24e-04\n",
      "Step 32450 | Loss: 0.50271 | LR: 5.24e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 32475 | Loss: 0.50828 | LR: 5.23e-04\n",
      "val loss: 0.4952\n",
      "Step 32500 | Loss: 0.49821 | LR: 5.22e-04\n",
      "Step 32525 | Loss: 0.49594 | LR: 5.22e-04\n",
      "Step 32550 | Loss: 0.51089 | LR: 5.21e-04\n",
      "Step 32575 | Loss: 0.49930 | LR: 5.20e-04\n",
      "val loss: 0.5037\n",
      "Step 32600 | Loss: 0.53292 | LR: 5.20e-04\n",
      "Step 32625 | Loss: 0.48722 | LR: 5.19e-04\n",
      "Step 32650 | Loss: 0.46531 | LR: 5.18e-04\n",
      "Step 32675 | Loss: 0.49492 | LR: 5.18e-04\n",
      "val loss: 0.5079\n",
      "Step 32700 | Loss: 0.53698 | LR: 5.17e-04\n",
      "Step 32725 | Loss: 0.50091 | LR: 5.16e-04\n",
      "Step 32750 | Loss: 0.50370 | LR: 5.16e-04\n",
      "Step 32775 | Loss: 0.48130 | LR: 5.15e-04\n",
      "val loss: 0.5081\n",
      "Step 32800 | Loss: 0.49295 | LR: 5.14e-04\n",
      "Step 32825 | Loss: 0.55455 | LR: 5.14e-04\n",
      "Step 32850 | Loss: 0.52494 | LR: 5.13e-04\n",
      "Step 32875 | Loss: 0.47461 | LR: 5.13e-04\n",
      "val loss: 0.4978\n",
      "Step 32900 | Loss: 0.49503 | LR: 5.12e-04\n",
      "Step 32925 | Loss: 0.49475 | LR: 5.11e-04\n",
      "Step 32950 | Loss: 0.52135 | LR: 5.11e-04\n",
      "Step 32975 | Loss: 0.50220 | LR: 5.10e-04\n",
      "val loss: 0.5106\n",
      "Step 33000 | Loss: 0.51700 | LR: 5.09e-04\n",
      "Step 33025 | Loss: 0.50173 | LR: 5.09e-04\n",
      "Step 33050 | Loss: 0.51561 | LR: 5.08e-04\n",
      "Step 33075 | Loss: 0.51696 | LR: 5.07e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "val loss: 0.4958\n",
      "Step 33100 | Loss: 0.49448 | LR: 5.07e-04\n",
      "Step 33125 | Loss: 0.50675 | LR: 5.06e-04\n",
      "Step 33150 | Loss: 0.49058 | LR: 5.05e-04\n",
      "Step 33175 | Loss: 0.49451 | LR: 5.05e-04\n",
      "val loss: 0.5058\n",
      "Step 33200 | Loss: 0.49338 | LR: 5.04e-04\n",
      "Step 33225 | Loss: 0.52854 | LR: 5.03e-04\n",
      "Step 33250 | Loss: 0.50643 | LR: 5.03e-04\n",
      "Step 33275 | Loss: 0.50149 | LR: 5.02e-04\n",
      "val loss: 0.5083\n",
      "Step 33300 | Loss: 0.51981 | LR: 5.01e-04\n",
      "Step 33325 | Loss: 0.50244 | LR: 5.01e-04\n",
      "Step 33350 | Loss: 0.50361 | LR: 5.00e-04\n",
      "Step 33375 | Loss: 0.50803 | LR: 5.00e-04\n",
      "val loss: 0.4957\n",
      "Step 33400 | Loss: 0.50602 | LR: 4.99e-04\n",
      "Step 33425 | Loss: 0.49684 | LR: 4.98e-04\n",
      "Step 33450 | Loss: 0.50675 | LR: 4.98e-04\n",
      "Step 33475 | Loss: 0.46477 | LR: 4.97e-04\n",
      "val loss: 0.4966\n",
      "Step 33500 | Loss: 0.51120 | LR: 4.96e-04\n",
      "Step 33525 | Loss: 0.50806 | LR: 4.96e-04\n",
      "Step 33550 | Loss: 0.47145 | LR: 4.95e-04\n",
      "Step 33575 | Loss: 0.48177 | LR: 4.94e-04\n",
      "val loss: 0.5046\n",
      "Step 33600 | Loss: 0.51911 | LR: 4.94e-04\n",
      "Step 33625 | Loss: 0.47625 | LR: 4.93e-04\n",
      "Step 33650 | Loss: 0.47950 | LR: 4.92e-04\n",
      "Step 33675 | Loss: 0.51090 | LR: 4.92e-04\n",
      "val loss: 0.5074\n",
      "Step 33700 | Loss: 0.47302 | LR: 4.91e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0003.npz\n",
      "Step 33725 | Loss: 0.47396 | LR: 4.90e-04\n",
      "Step 33750 | Loss: 0.47006 | LR: 4.90e-04\n",
      "Step 33775 | Loss: 0.50966 | LR: 4.89e-04\n",
      "val loss: 0.5093\n",
      "Step 33800 | Loss: 0.50076 | LR: 4.88e-04\n",
      "Step 33825 | Loss: 0.50463 | LR: 4.88e-04\n",
      "Step 33850 | Loss: 0.51044 | LR: 4.87e-04\n",
      "Step 33875 | Loss: 0.49768 | LR: 4.87e-04\n",
      "val loss: 0.5036\n",
      "Step 33900 | Loss: 0.47338 | LR: 4.86e-04\n",
      "Step 33925 | Loss: 0.50458 | LR: 4.85e-04\n",
      "Step 33950 | Loss: 0.48836 | LR: 4.85e-04\n",
      "Step 33975 | Loss: 0.52027 | LR: 4.84e-04\n",
      "val loss: 0.5081\n",
      "Step 34000 | Loss: 0.50417 | LR: 4.83e-04\n",
      "Step 34025 | Loss: 0.51265 | LR: 4.83e-04\n",
      "Step 34050 | Loss: 0.50159 | LR: 4.82e-04\n",
      "Step 34075 | Loss: 0.51133 | LR: 4.81e-04\n",
      "val loss: 0.5052\n",
      "Step 34100 | Loss: 0.51685 | LR: 4.81e-04\n",
      "Step 34125 | Loss: 0.47653 | LR: 4.80e-04\n",
      "Step 34150 | Loss: 0.51472 | LR: 4.79e-04\n",
      "Step 34175 | Loss: 0.49957 | LR: 4.79e-04\n",
      "val loss: 0.4998\n",
      "Step 34200 | Loss: 0.52392 | LR: 4.78e-04\n",
      "Step 34225 | Loss: 0.49839 | LR: 4.77e-04\n",
      "Step 34250 | Loss: 0.52051 | LR: 4.77e-04\n",
      "Step 34275 | Loss: 0.51235 | LR: 4.76e-04\n",
      "val loss: 0.5017\n",
      "Step 34300 | Loss: 0.50131 | LR: 4.75e-04\n",
      "Step 34325 | Loss: 0.48065 | LR: 4.75e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "Step 34350 | Loss: 0.50646 | LR: 4.74e-04\n",
      "Step 34375 | Loss: 0.49350 | LR: 4.74e-04\n",
      "val loss: 0.4956\n",
      "Step 34400 | Loss: 0.48990 | LR: 4.73e-04\n",
      "Step 34425 | Loss: 0.52069 | LR: 4.72e-04\n",
      "Step 34450 | Loss: 0.52084 | LR: 4.72e-04\n",
      "Step 34475 | Loss: 0.48101 | LR: 4.71e-04\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5088\n",
      "Step 34500 | Loss: 0.50592 | LR: 4.70e-04\n",
      "Step 34525 | Loss: 0.52057 | LR: 4.70e-04\n",
      "Step 34550 | Loss: 0.50677 | LR: 4.69e-04\n",
      "Step 34575 | Loss: 0.51286 | LR: 4.68e-04\n",
      "val loss: 0.4953\n",
      "Step 34600 | Loss: 0.52313 | LR: 4.68e-04\n",
      "Step 34625 | Loss: 0.49754 | LR: 4.67e-04\n",
      "Step 34650 | Loss: 0.48072 | LR: 4.66e-04\n",
      "Step 34675 | Loss: 0.51391 | LR: 4.66e-04\n",
      "val loss: 0.4988\n",
      "Step 34700 | Loss: 0.52768 | LR: 4.65e-04\n",
      "Step 34725 | Loss: 0.48930 | LR: 4.64e-04\n",
      "Step 34750 | Loss: 0.48385 | LR: 4.64e-04\n",
      "Step 34775 | Loss: 0.51246 | LR: 4.63e-04\n",
      "val loss: 0.4962\n",
      "Step 34800 | Loss: 0.50485 | LR: 4.62e-04\n",
      "Step 34825 | Loss: 0.50001 | LR: 4.62e-04\n",
      "Step 34850 | Loss: 0.50787 | LR: 4.61e-04\n",
      "Step 34875 | Loss: 0.51594 | LR: 4.61e-04\n",
      "val loss: 0.5045\n",
      "Step 34900 | Loss: 0.48430 | LR: 4.60e-04\n",
      "Step 34925 | Loss: 0.51143 | LR: 4.59e-04\n",
      "=== Step 34946 Done. Avg Loss: 0.50344 ===\n",
      "Step 34950 | Loss: 0.49394 | LR: 4.59e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0005.npz\n",
      "Step 34975 | Loss: 0.52463 | LR: 4.58e-04\n",
      "val loss: 0.4979\n",
      "Step 35000 | Loss: 0.50750 | LR: 4.57e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 35025 | Loss: 0.48682 | LR: 4.57e-04\n",
      "Step 35050 | Loss: 0.50444 | LR: 4.56e-04\n",
      "Step 35075 | Loss: 0.51720 | LR: 4.55e-04\n",
      "val loss: 0.5006\n",
      "Step 35100 | Loss: 0.51129 | LR: 4.55e-04\n",
      "Step 35125 | Loss: 0.48034 | LR: 4.54e-04\n",
      "Step 35150 | Loss: 0.50229 | LR: 4.53e-04\n",
      "Step 35175 | Loss: 0.51400 | LR: 4.53e-04\n",
      "val loss: 0.5055\n",
      "Step 35200 | Loss: 0.49612 | LR: 4.52e-04\n",
      "Step 35225 | Loss: 0.49565 | LR: 4.51e-04\n",
      "Step 35250 | Loss: 0.47683 | LR: 4.51e-04\n",
      "Step 35275 | Loss: 0.49124 | LR: 4.50e-04\n",
      "val loss: 0.5080\n",
      "Step 35300 | Loss: 0.48797 | LR: 4.50e-04\n",
      "Step 35325 | Loss: 0.50220 | LR: 4.49e-04\n",
      "Step 35350 | Loss: 0.50212 | LR: 4.48e-04\n",
      "Step 35375 | Loss: 0.52484 | LR: 4.48e-04\n",
      "val loss: 0.4970\n",
      "Step 35400 | Loss: 0.50427 | LR: 4.47e-04\n",
      "Step 35425 | Loss: 0.50089 | LR: 4.46e-04\n",
      "Step 35450 | Loss: 0.52843 | LR: 4.46e-04\n",
      "Step 35475 | Loss: 0.53264 | LR: 4.45e-04\n",
      "val loss: 0.5168\n",
      "Step 35500 | Loss: 0.49923 | LR: 4.44e-04\n",
      "Step 35525 | Loss: 0.52219 | LR: 4.44e-04\n",
      "Step 35550 | Loss: 0.50549 | LR: 4.43e-04\n",
      "Step 35575 | Loss: 0.51543 | LR: 4.42e-04\n",
      "val loss: 0.5055\n",
      "Step 35600 | Loss: 0.50622 | LR: 4.42e-04\n",
      "Step 35625 | Loss: 0.51536 | LR: 4.41e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "Step 35650 | Loss: 0.49221 | LR: 4.40e-04\n",
      "Step 35675 | Loss: 0.47971 | LR: 4.40e-04\n",
      "val loss: 0.5024\n",
      "Step 35700 | Loss: 0.50169 | LR: 4.39e-04\n",
      "Step 35725 | Loss: 0.50597 | LR: 4.39e-04\n",
      "Step 35750 | Loss: 0.49749 | LR: 4.38e-04\n",
      "Step 35775 | Loss: 0.50791 | LR: 4.37e-04\n",
      "val loss: 0.5083\n",
      "Step 35800 | Loss: 0.51842 | LR: 4.37e-04\n",
      "Step 35825 | Loss: 0.47676 | LR: 4.36e-04\n",
      "Step 35850 | Loss: 0.49196 | LR: 4.35e-04\n",
      "Step 35875 | Loss: 0.52090 | LR: 4.35e-04\n",
      "val loss: 0.5050\n",
      "Step 35900 | Loss: 0.48005 | LR: 4.34e-04\n",
      "Step 35925 | Loss: 0.49925 | LR: 4.33e-04\n",
      "Step 35950 | Loss: 0.52542 | LR: 4.33e-04\n",
      "Step 35975 | Loss: 0.51505 | LR: 4.32e-04\n",
      "val loss: 0.4989\n",
      "Step 36000 | Loss: 0.50695 | LR: 4.31e-04\n",
      "Step 36025 | Loss: 0.51874 | LR: 4.31e-04\n",
      "Step 36050 | Loss: 0.50903 | LR: 4.30e-04\n",
      "Step 36075 | Loss: 0.51032 | LR: 4.29e-04\n",
      "val loss: 0.4999\n",
      "Step 36100 | Loss: 0.49807 | LR: 4.29e-04\n",
      "Step 36125 | Loss: 0.52548 | LR: 4.28e-04\n",
      "Step 36150 | Loss: 0.50097 | LR: 4.28e-04\n",
      "Step 36175 | Loss: 0.48313 | LR: 4.27e-04\n",
      "val loss: 0.4961\n",
      "Step 36200 | Loss: 0.48419 | LR: 4.26e-04\n",
      "Step 36225 | Loss: 0.49113 | LR: 4.26e-04\n",
      "Step 36250 | Loss: 0.50964 | LR: 4.25e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "Step 36275 | Loss: 0.53064 | LR: 4.24e-04\n",
      "val loss: 0.5088\n",
      "Step 36300 | Loss: 0.48390 | LR: 4.24e-04\n",
      "Step 36325 | Loss: 0.53010 | LR: 4.23e-04\n",
      "Step 36350 | Loss: 0.51518 | LR: 4.22e-04\n",
      "Step 36375 | Loss: 0.47286 | LR: 4.22e-04\n",
      "val loss: 0.5080\n",
      "Step 36400 | Loss: 0.54060 | LR: 4.21e-04\n",
      "Step 36425 | Loss: 0.50595 | LR: 4.20e-04\n",
      "Step 36450 | Loss: 0.50378 | LR: 4.20e-04\n",
      "Step 36475 | Loss: 0.52758 | LR: 4.19e-04\n",
      "val loss: 0.5043\n",
      "Step 36500 | Loss: 0.49325 | LR: 4.19e-04\n",
      "Step 36525 | Loss: 0.49181 | LR: 4.18e-04\n",
      "Step 36550 | Loss: 0.48957 | LR: 4.17e-04\n",
      "Step 36575 | Loss: 0.47957 | LR: 4.17e-04\n",
      "val loss: 0.4981\n",
      "Step 36600 | Loss: 0.50487 | LR: 4.16e-04\n",
      "Step 36625 | Loss: 0.51830 | LR: 4.15e-04\n",
      "Step 36650 | Loss: 0.51554 | LR: 4.15e-04\n",
      "Step 36675 | Loss: 0.49170 | LR: 4.14e-04\n",
      "val loss: 0.4978\n",
      "Step 36700 | Loss: 0.49961 | LR: 4.13e-04\n",
      "Step 36725 | Loss: 0.49076 | LR: 4.13e-04\n",
      "Step 36750 | Loss: 0.48096 | LR: 4.12e-04\n",
      "Step 36775 | Loss: 0.51316 | LR: 4.12e-04\n",
      "val loss: 0.5043\n",
      "Step 36800 | Loss: 0.51400 | LR: 4.11e-04\n",
      "Step 36825 | Loss: 0.50963 | LR: 4.10e-04\n",
      "Step 36850 | Loss: 0.50319 | LR: 4.10e-04\n",
      "Step 36875 | Loss: 0.52138 | LR: 4.09e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0004.npz\n",
      "val loss: 0.5003\n",
      "Step 36900 | Loss: 0.49969 | LR: 4.08e-04\n",
      "Step 36925 | Loss: 0.45901 | LR: 4.08e-04\n",
      "Step 36950 | Loss: 0.49106 | LR: 4.07e-04\n",
      "Step 36975 | Loss: 0.50954 | LR: 4.06e-04\n",
      "val loss: 0.5017\n",
      "Step 37000 | Loss: 0.52311 | LR: 4.06e-04\n",
      "Step 37025 | Loss: 0.51327 | LR: 4.05e-04\n",
      "Step 37050 | Loss: 0.51645 | LR: 4.04e-04\n",
      "Step 37075 | Loss: 0.51119 | LR: 4.04e-04\n",
      "val loss: 0.4848\n",
      "Step 37100 | Loss: 0.49887 | LR: 4.03e-04\n",
      "Step 37125 | Loss: 0.53682 | LR: 4.03e-04\n",
      "Step 37150 | Loss: 0.53044 | LR: 4.02e-04\n",
      "Step 37175 | Loss: 0.51709 | LR: 4.01e-04\n",
      "val loss: 0.4940\n",
      "Step 37200 | Loss: 0.47779 | LR: 4.01e-04\n",
      "Step 37225 | Loss: 0.51655 | LR: 4.00e-04\n",
      "Step 37250 | Loss: 0.50964 | LR: 3.99e-04\n",
      "Step 37275 | Loss: 0.51726 | LR: 3.99e-04\n",
      "val loss: 0.4933\n",
      "Step 37300 | Loss: 0.49742 | LR: 3.98e-04\n",
      "Step 37325 | Loss: 0.50565 | LR: 3.97e-04\n",
      "Step 37350 | Loss: 0.50690 | LR: 3.97e-04\n",
      "Step 37375 | Loss: 0.49522 | LR: 3.96e-04\n",
      "val loss: 0.5034\n",
      "Step 37400 | Loss: 0.48689 | LR: 3.96e-04\n",
      "Step 37425 | Loss: 0.49888 | LR: 3.95e-04\n",
      "Step 37450 | Loss: 0.47529 | LR: 3.94e-04\n",
      "Step 37475 | Loss: 0.48671 | LR: 3.94e-04\n",
      "val loss: 0.4994\n",
      "Step 37500 | Loss: 0.54744 | LR: 3.93e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0003.npz\n",
      "Step 37525 | Loss: 0.47934 | LR: 3.92e-04\n",
      "Step 37550 | Loss: 0.49350 | LR: 3.92e-04\n",
      "Step 37575 | Loss: 0.48485 | LR: 3.91e-04\n",
      "val loss: 0.4964\n",
      "Step 37600 | Loss: 0.49923 | LR: 3.90e-04\n",
      "Step 37625 | Loss: 0.48643 | LR: 3.90e-04\n",
      "Step 37650 | Loss: 0.49662 | LR: 3.89e-04\n",
      "Step 37675 | Loss: 0.47485 | LR: 3.89e-04\n",
      "val loss: 0.4969\n",
      "Step 37700 | Loss: 0.50481 | LR: 3.88e-04\n",
      "Step 37725 | Loss: 0.49364 | LR: 3.87e-04\n",
      "Step 37750 | Loss: 0.48051 | LR: 3.87e-04\n",
      "Step 37775 | Loss: 0.49241 | LR: 3.86e-04\n",
      "val loss: 0.4965\n",
      "Step 37800 | Loss: 0.50434 | LR: 3.85e-04\n",
      "Step 37825 | Loss: 0.50568 | LR: 3.85e-04\n",
      "Step 37850 | Loss: 0.48566 | LR: 3.84e-04\n",
      "Step 37875 | Loss: 0.49560 | LR: 3.84e-04\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5042\n",
      "Step 37900 | Loss: 0.50444 | LR: 3.83e-04\n",
      "Step 37925 | Loss: 0.48139 | LR: 3.82e-04\n",
      "Step 37950 | Loss: 0.46987 | LR: 3.82e-04\n",
      "Step 37975 | Loss: 0.49823 | LR: 3.81e-04\n",
      "val loss: 0.5076\n",
      "Step 38000 | Loss: 0.53344 | LR: 3.80e-04\n",
      "Step 38025 | Loss: 0.49149 | LR: 3.80e-04\n",
      "Step 38050 | Loss: 0.51952 | LR: 3.79e-04\n",
      "Step 38075 | Loss: 0.50243 | LR: 3.78e-04\n",
      "val loss: 0.5023\n",
      "Step 38100 | Loss: 0.48572 | LR: 3.78e-04\n",
      "=== Step 38123 Done. Avg Loss: 0.50331 ===\n",
      "Step 38125 | Loss: 0.47757 | LR: 3.77e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0003.npz\n",
      "Step 38150 | Loss: 0.49438 | LR: 3.77e-04\n",
      "Step 38175 | Loss: 0.48390 | LR: 3.76e-04\n",
      "val loss: 0.5093\n",
      "Step 38200 | Loss: 0.49853 | LR: 3.75e-04\n",
      "Step 38225 | Loss: 0.48375 | LR: 3.75e-04\n",
      "Step 38250 | Loss: 0.48599 | LR: 3.74e-04\n",
      "Step 38275 | Loss: 0.50236 | LR: 3.73e-04\n",
      "val loss: 0.4968\n",
      "Step 38300 | Loss: 0.47075 | LR: 3.73e-04\n",
      "Step 38325 | Loss: 0.50801 | LR: 3.72e-04\n",
      "Step 38350 | Loss: 0.46833 | LR: 3.72e-04\n",
      "Step 38375 | Loss: 0.50923 | LR: 3.71e-04\n",
      "val loss: 0.4951\n",
      "Step 38400 | Loss: 0.51333 | LR: 3.70e-04\n",
      "Step 38425 | Loss: 0.47694 | LR: 3.70e-04\n",
      "Step 38450 | Loss: 0.51056 | LR: 3.69e-04\n",
      "Step 38475 | Loss: 0.52869 | LR: 3.68e-04\n",
      "val loss: 0.4997\n",
      "Step 38500 | Loss: 0.51643 | LR: 3.68e-04\n",
      "Step 38525 | Loss: 0.52029 | LR: 3.67e-04\n",
      "Step 38550 | Loss: 0.48062 | LR: 3.66e-04\n",
      "Step 38575 | Loss: 0.50692 | LR: 3.66e-04\n",
      "val loss: 0.5006\n",
      "Step 38600 | Loss: 0.49995 | LR: 3.65e-04\n",
      "Step 38625 | Loss: 0.48766 | LR: 3.65e-04\n",
      "Step 38650 | Loss: 0.52208 | LR: 3.64e-04\n",
      "Step 38675 | Loss: 0.53867 | LR: 3.63e-04\n",
      "val loss: 0.5010\n",
      "Step 38700 | Loss: 0.51215 | LR: 3.63e-04\n",
      "Step 38725 | Loss: 0.49073 | LR: 3.62e-04\n",
      "Step 38750 | Loss: 0.50013 | LR: 3.61e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 38775 | Loss: 0.47795 | LR: 3.61e-04\n",
      "val loss: 0.4971\n",
      "Step 38800 | Loss: 0.48136 | LR: 3.60e-04\n",
      "Step 38825 | Loss: 0.52070 | LR: 3.60e-04\n",
      "Step 38850 | Loss: 0.50716 | LR: 3.59e-04\n",
      "Step 38875 | Loss: 0.50181 | LR: 3.58e-04\n",
      "val loss: 0.4999\n",
      "Step 38900 | Loss: 0.50543 | LR: 3.58e-04\n",
      "Step 38925 | Loss: 0.51222 | LR: 3.57e-04\n",
      "Step 38950 | Loss: 0.50699 | LR: 3.56e-04\n",
      "Step 38975 | Loss: 0.49824 | LR: 3.56e-04\n",
      "val loss: 0.4952\n",
      "Step 39000 | Loss: 0.49388 | LR: 3.55e-04\n",
      "Step 39025 | Loss: 0.52151 | LR: 3.55e-04\n",
      "Step 39050 | Loss: 0.48993 | LR: 3.54e-04\n",
      "Step 39075 | Loss: 0.50068 | LR: 3.53e-04\n",
      "val loss: 0.5085\n",
      "Step 39100 | Loss: 0.46166 | LR: 3.53e-04\n",
      "Step 39125 | Loss: 0.48059 | LR: 3.52e-04\n",
      "Step 39150 | Loss: 0.49188 | LR: 3.52e-04\n",
      "Step 39175 | Loss: 0.51355 | LR: 3.51e-04\n",
      "val loss: 0.5011\n",
      "Step 39200 | Loss: 0.49667 | LR: 3.50e-04\n",
      "Step 39225 | Loss: 0.50820 | LR: 3.50e-04\n",
      "Step 39250 | Loss: 0.47981 | LR: 3.49e-04\n",
      "Step 39275 | Loss: 0.52586 | LR: 3.48e-04\n",
      "val loss: 0.5057\n",
      "Step 39300 | Loss: 0.49741 | LR: 3.48e-04\n",
      "Step 39325 | Loss: 0.50619 | LR: 3.47e-04\n",
      "Step 39350 | Loss: 0.49129 | LR: 3.47e-04\n",
      "Step 39375 | Loss: 0.50854 | LR: 3.46e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0005.npz\n",
      "val loss: 0.5123\n",
      "Step 39400 | Loss: 0.47305 | LR: 3.45e-04\n",
      "Step 39425 | Loss: 0.51034 | LR: 3.45e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "Step 39450 | Loss: 0.51491 | LR: 3.44e-04\n",
      "Step 39475 | Loss: 0.48849 | LR: 3.43e-04\n",
      "val loss: 0.5016\n",
      "Step 39500 | Loss: 0.49704 | LR: 3.43e-04\n",
      "Step 39525 | Loss: 0.47549 | LR: 3.42e-04\n",
      "Step 39550 | Loss: 0.48911 | LR: 3.42e-04\n",
      "Step 39575 | Loss: 0.52462 | LR: 3.41e-04\n",
      "val loss: 0.4925\n",
      "Step 39600 | Loss: 0.51896 | LR: 3.40e-04\n",
      "Step 39625 | Loss: 0.52418 | LR: 3.40e-04\n",
      "Step 39650 | Loss: 0.50398 | LR: 3.39e-04\n",
      "Step 39675 | Loss: 0.52542 | LR: 3.39e-04\n",
      "val loss: 0.4905\n",
      "Step 39700 | Loss: 0.47738 | LR: 3.38e-04\n",
      "Step 39725 | Loss: 0.50320 | LR: 3.37e-04\n",
      "Step 39750 | Loss: 0.48128 | LR: 3.37e-04\n",
      "Step 39775 | Loss: 0.47166 | LR: 3.36e-04\n",
      "val loss: 0.4982\n",
      "Step 39800 | Loss: 0.47771 | LR: 3.35e-04\n",
      "Step 39825 | Loss: 0.47435 | LR: 3.35e-04\n",
      "Step 39850 | Loss: 0.49184 | LR: 3.34e-04\n",
      "Step 39875 | Loss: 0.50236 | LR: 3.34e-04\n",
      "val loss: 0.4978\n",
      "Step 39900 | Loss: 0.49231 | LR: 3.33e-04\n",
      "Step 39925 | Loss: 0.50270 | LR: 3.32e-04\n",
      "Step 39950 | Loss: 0.51768 | LR: 3.32e-04\n",
      "Step 39975 | Loss: 0.48747 | LR: 3.31e-04\n",
      "val loss: 0.5017\n",
      "Step 40000 | Loss: 0.51555 | LR: 3.31e-04\n",
      "Step 40025 | Loss: 0.50707 | LR: 3.30e-04\n",
      "Step 40050 | Loss: 0.51413 | LR: 3.29e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "Step 40075 | Loss: 0.47512 | LR: 3.29e-04\n",
      "val loss: 0.4950\n",
      "Step 40100 | Loss: 0.48467 | LR: 3.28e-04\n",
      "Step 40125 | Loss: 0.50913 | LR: 3.27e-04\n",
      "Step 40150 | Loss: 0.49053 | LR: 3.27e-04\n",
      "Step 40175 | Loss: 0.48303 | LR: 3.26e-04\n",
      "val loss: 0.5039\n",
      "Step 40200 | Loss: 0.50924 | LR: 3.26e-04\n",
      "Step 40225 | Loss: 0.50603 | LR: 3.25e-04\n",
      "Step 40250 | Loss: 0.49791 | LR: 3.24e-04\n",
      "Step 40275 | Loss: 0.52132 | LR: 3.24e-04\n",
      "val loss: 0.4909\n",
      "Step 40300 | Loss: 0.52115 | LR: 3.23e-04\n",
      "Step 40325 | Loss: 0.50744 | LR: 3.23e-04\n",
      "Step 40350 | Loss: 0.48635 | LR: 3.22e-04\n",
      "Step 40375 | Loss: 0.49301 | LR: 3.21e-04\n",
      "val loss: 0.5018\n",
      "Step 40400 | Loss: 0.48931 | LR: 3.21e-04\n",
      "Step 40425 | Loss: 0.50976 | LR: 3.20e-04\n",
      "Step 40450 | Loss: 0.48282 | LR: 3.20e-04\n",
      "Step 40475 | Loss: 0.49508 | LR: 3.19e-04\n",
      "val loss: 0.5047\n",
      "Step 40500 | Loss: 0.49641 | LR: 3.18e-04\n",
      "Step 40525 | Loss: 0.52664 | LR: 3.18e-04\n",
      "Step 40550 | Loss: 0.50758 | LR: 3.17e-04\n",
      "Step 40575 | Loss: 0.51006 | LR: 3.17e-04\n",
      "val loss: 0.5011\n",
      "Step 40600 | Loss: 0.50471 | LR: 3.16e-04\n",
      "Step 40625 | Loss: 0.49872 | LR: 3.15e-04\n",
      "Step 40650 | Loss: 0.49211 | LR: 3.15e-04\n",
      "Step 40675 | Loss: 0.53413 | LR: 3.14e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0004.npz\n",
      "val loss: 0.5018\n",
      "Step 40700 | Loss: 0.51943 | LR: 3.14e-04\n",
      "Step 40725 | Loss: 0.51384 | LR: 3.13e-04\n",
      "Step 40750 | Loss: 0.48371 | LR: 3.12e-04\n",
      "Step 40775 | Loss: 0.50227 | LR: 3.12e-04\n",
      "val loss: 0.5089\n",
      "Step 40800 | Loss: 0.46569 | LR: 3.11e-04\n",
      "Step 40825 | Loss: 0.49979 | LR: 3.11e-04\n",
      "Step 40850 | Loss: 0.50330 | LR: 3.10e-04\n",
      "Step 40875 | Loss: 0.52705 | LR: 3.09e-04\n",
      "val loss: 0.5044\n",
      "Step 40900 | Loss: 0.52123 | LR: 3.09e-04\n",
      "Step 40925 | Loss: 0.47975 | LR: 3.08e-04\n",
      "Step 40950 | Loss: 0.47549 | LR: 3.08e-04\n",
      "Step 40975 | Loss: 0.53497 | LR: 3.07e-04\n",
      "val loss: 0.4979\n",
      "Step 41000 | Loss: 0.50816 | LR: 3.06e-04\n",
      "Step 41025 | Loss: 0.49257 | LR: 3.06e-04\n",
      "Step 41050 | Loss: 0.47315 | LR: 3.05e-04\n",
      "Step 41075 | Loss: 0.49806 | LR: 3.05e-04\n",
      "val loss: 0.5043\n",
      "Step 41100 | Loss: 0.47756 | LR: 3.04e-04\n",
      "Step 41125 | Loss: 0.52587 | LR: 3.03e-04\n",
      "Step 41150 | Loss: 0.49061 | LR: 3.03e-04\n",
      "Step 41175 | Loss: 0.49730 | LR: 3.02e-04\n",
      "val loss: 0.5000\n",
      "Step 41200 | Loss: 0.50676 | LR: 3.02e-04\n",
      "Step 41225 | Loss: 0.49384 | LR: 3.01e-04\n",
      "Step 41250 | Loss: 0.49106 | LR: 3.00e-04\n",
      "Step 41275 | Loss: 0.48292 | LR: 3.00e-04\n",
      "val loss: 0.5043\n",
      "Step 41300 | Loss: 0.50472 | LR: 2.99e-04\n",
      "=== Step 41300 Done. Avg Loss: 0.50334 ===\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0003.npz\n",
      "Step 41325 | Loss: 0.50531 | LR: 2.99e-04\n",
      "Step 41350 | Loss: 0.49010 | LR: 2.98e-04\n",
      "Step 41375 | Loss: 0.46427 | LR: 2.97e-04\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.4986\n",
      "Step 41400 | Loss: 0.49111 | LR: 2.97e-04\n",
      "Step 41425 | Loss: 0.50030 | LR: 2.96e-04\n",
      "Step 41450 | Loss: 0.49271 | LR: 2.96e-04\n",
      "Step 41475 | Loss: 0.52035 | LR: 2.95e-04\n",
      "val loss: 0.5060\n",
      "Step 41500 | Loss: 0.49048 | LR: 2.94e-04\n",
      "Step 41525 | Loss: 0.49544 | LR: 2.94e-04\n",
      "Step 41550 | Loss: 0.48418 | LR: 2.93e-04\n",
      "Step 41575 | Loss: 0.50179 | LR: 2.93e-04\n",
      "val loss: 0.4956\n",
      "Step 41600 | Loss: 0.50930 | LR: 2.92e-04\n",
      "Step 41625 | Loss: 0.51125 | LR: 2.91e-04\n",
      "Step 41650 | Loss: 0.50235 | LR: 2.91e-04\n",
      "Step 41675 | Loss: 0.48895 | LR: 2.90e-04\n",
      "val loss: 0.4972\n",
      "Step 41700 | Loss: 0.47713 | LR: 2.90e-04\n",
      "Step 41725 | Loss: 0.47310 | LR: 2.89e-04\n",
      "Step 41750 | Loss: 0.49125 | LR: 2.88e-04\n",
      "Step 41775 | Loss: 0.46692 | LR: 2.88e-04\n",
      "val loss: 0.5054\n",
      "Step 41800 | Loss: 0.49758 | LR: 2.87e-04\n",
      "Step 41825 | Loss: 0.51216 | LR: 2.87e-04\n",
      "Step 41850 | Loss: 0.51082 | LR: 2.86e-04\n",
      "Step 41875 | Loss: 0.48839 | LR: 2.86e-04\n",
      "val loss: 0.4932\n",
      "Step 41900 | Loss: 0.49091 | LR: 2.85e-04\n",
      "Step 41925 | Loss: 0.50327 | LR: 2.84e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 41950 | Loss: 0.49111 | LR: 2.84e-04\n",
      "Step 41975 | Loss: 0.49017 | LR: 2.83e-04\n",
      "val loss: 0.4992\n",
      "Step 42000 | Loss: 0.47503 | LR: 2.83e-04\n",
      "Step 42025 | Loss: 0.48869 | LR: 2.82e-04\n",
      "Step 42050 | Loss: 0.50957 | LR: 2.81e-04\n",
      "Step 42075 | Loss: 0.51714 | LR: 2.81e-04\n",
      "val loss: 0.5038\n",
      "Step 42100 | Loss: 0.51302 | LR: 2.80e-04\n",
      "Step 42125 | Loss: 0.47585 | LR: 2.80e-04\n",
      "Step 42150 | Loss: 0.51469 | LR: 2.79e-04\n",
      "Step 42175 | Loss: 0.48474 | LR: 2.78e-04\n",
      "val loss: 0.4940\n",
      "Step 42200 | Loss: 0.52597 | LR: 2.78e-04\n",
      "Step 42225 | Loss: 0.51025 | LR: 2.77e-04\n",
      "Step 42250 | Loss: 0.52472 | LR: 2.77e-04\n",
      "Step 42275 | Loss: 0.51668 | LR: 2.76e-04\n",
      "val loss: 0.4950\n",
      "Step 42300 | Loss: 0.50839 | LR: 2.76e-04\n",
      "Step 42325 | Loss: 0.47679 | LR: 2.75e-04\n",
      "Step 42350 | Loss: 0.50652 | LR: 2.74e-04\n",
      "Step 42375 | Loss: 0.50422 | LR: 2.74e-04\n",
      "val loss: 0.4982\n",
      "Step 42400 | Loss: 0.51478 | LR: 2.73e-04\n",
      "Step 42425 | Loss: 0.50086 | LR: 2.73e-04\n",
      "Step 42450 | Loss: 0.50470 | LR: 2.72e-04\n",
      "Step 42475 | Loss: 0.48927 | LR: 2.72e-04\n",
      "val loss: 0.4989\n",
      "Step 42500 | Loss: 0.47972 | LR: 2.71e-04\n",
      "Step 42525 | Loss: 0.47949 | LR: 2.70e-04\n",
      "Step 42550 | Loss: 0.51934 | LR: 2.70e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "Step 42575 | Loss: 0.48731 | LR: 2.69e-04\n",
      "val loss: 0.4953\n",
      "Step 42600 | Loss: 0.51712 | LR: 2.69e-04\n",
      "Step 42625 | Loss: 0.49391 | LR: 2.68e-04\n",
      "Step 42650 | Loss: 0.48219 | LR: 2.67e-04\n",
      "Step 42675 | Loss: 0.49744 | LR: 2.67e-04\n",
      "val loss: 0.5016\n",
      "Step 42700 | Loss: 0.51814 | LR: 2.66e-04\n",
      "Step 42725 | Loss: 0.46993 | LR: 2.66e-04\n",
      "Step 42750 | Loss: 0.53016 | LR: 2.65e-04\n",
      "Step 42775 | Loss: 0.53272 | LR: 2.65e-04\n",
      "val loss: 0.5159\n",
      "Step 42800 | Loss: 0.48172 | LR: 2.64e-04\n",
      "Step 42825 | Loss: 0.51689 | LR: 2.63e-04\n",
      "Step 42850 | Loss: 0.50309 | LR: 2.63e-04\n",
      "Step 42875 | Loss: 0.48940 | LR: 2.62e-04\n",
      "val loss: 0.5089\n",
      "Step 42900 | Loss: 0.49992 | LR: 2.62e-04\n",
      "Step 42925 | Loss: 0.47926 | LR: 2.61e-04\n",
      "Step 42950 | Loss: 0.48840 | LR: 2.61e-04\n",
      "Step 42975 | Loss: 0.49888 | LR: 2.60e-04\n",
      "val loss: 0.5075\n",
      "Step 43000 | Loss: 0.51300 | LR: 2.59e-04\n",
      "Step 43025 | Loss: 0.47443 | LR: 2.59e-04\n",
      "Step 43050 | Loss: 0.48766 | LR: 2.58e-04\n",
      "Step 43075 | Loss: 0.50431 | LR: 2.58e-04\n",
      "val loss: 0.5052\n",
      "Step 43100 | Loss: 0.51712 | LR: 2.57e-04\n",
      "Step 43125 | Loss: 0.55193 | LR: 2.57e-04\n",
      "Step 43150 | Loss: 0.51381 | LR: 2.56e-04\n",
      "Step 43175 | Loss: 0.48336 | LR: 2.55e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "val loss: 0.5055\n",
      "Step 43200 | Loss: 0.54118 | LR: 2.55e-04\n",
      "Step 43225 | Loss: 0.49023 | LR: 2.54e-04\n",
      "Step 43250 | Loss: 0.48755 | LR: 2.54e-04\n",
      "Step 43275 | Loss: 0.52482 | LR: 2.53e-04\n",
      "val loss: 0.5040\n",
      "Step 43300 | Loss: 0.49638 | LR: 2.53e-04\n",
      "Step 43325 | Loss: 0.47718 | LR: 2.52e-04\n",
      "Step 43350 | Loss: 0.48360 | LR: 2.52e-04\n",
      "Step 43375 | Loss: 0.49677 | LR: 2.51e-04\n",
      "val loss: 0.5047\n",
      "Step 43400 | Loss: 0.51035 | LR: 2.50e-04\n",
      "Step 43425 | Loss: 0.48829 | LR: 2.50e-04\n",
      "Step 43450 | Loss: 0.53622 | LR: 2.49e-04\n",
      "Step 43475 | Loss: 0.47450 | LR: 2.49e-04\n",
      "val loss: 0.4980\n",
      "Step 43500 | Loss: 0.46528 | LR: 2.48e-04\n",
      "Step 43525 | Loss: 0.50568 | LR: 2.48e-04\n",
      "Step 43550 | Loss: 0.50105 | LR: 2.47e-04\n",
      "Step 43575 | Loss: 0.51571 | LR: 2.46e-04\n",
      "val loss: 0.5058\n",
      "Step 43600 | Loss: 0.51437 | LR: 2.46e-04\n",
      "Step 43625 | Loss: 0.50612 | LR: 2.45e-04\n",
      "Step 43650 | Loss: 0.53706 | LR: 2.45e-04\n",
      "Step 43675 | Loss: 0.49088 | LR: 2.44e-04\n",
      "val loss: 0.5097\n",
      "Step 43700 | Loss: 0.53227 | LR: 2.44e-04\n",
      "Step 43725 | Loss: 0.50797 | LR: 2.43e-04\n",
      "Step 43750 | Loss: 0.49641 | LR: 2.43e-04\n",
      "Step 43775 | Loss: 0.48359 | LR: 2.42e-04\n",
      "val loss: 0.5032\n",
      "Step 43800 | Loss: 0.48643 | LR: 2.41e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0005.npz\n",
      "Step 43825 | Loss: 0.51606 | LR: 2.41e-04\n",
      "Step 43850 | Loss: 0.50784 | LR: 2.40e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0004.npz\n",
      "Step 43875 | Loss: 0.52423 | LR: 2.40e-04\n",
      "val loss: 0.4959\n",
      "Step 43900 | Loss: 0.52303 | LR: 2.39e-04\n",
      "Step 43925 | Loss: 0.49495 | LR: 2.39e-04\n",
      "Step 43950 | Loss: 0.49240 | LR: 2.38e-04\n",
      "Step 43975 | Loss: 0.49528 | LR: 2.38e-04\n",
      "val loss: 0.5021\n",
      "Step 44000 | Loss: 0.50845 | LR: 2.37e-04\n",
      "Step 44025 | Loss: 0.52015 | LR: 2.36e-04\n",
      "Step 44050 | Loss: 0.51974 | LR: 2.36e-04\n",
      "Step 44075 | Loss: 0.50613 | LR: 2.35e-04\n",
      "val loss: 0.4989\n",
      "Step 44100 | Loss: 0.49431 | LR: 2.35e-04\n",
      "Step 44125 | Loss: 0.48831 | LR: 2.34e-04\n",
      "Step 44150 | Loss: 0.51520 | LR: 2.34e-04\n",
      "Step 44175 | Loss: 0.50822 | LR: 2.33e-04\n",
      "val loss: 0.4989\n",
      "Step 44200 | Loss: 0.49403 | LR: 2.33e-04\n",
      "Step 44225 | Loss: 0.51337 | LR: 2.32e-04\n",
      "Step 44250 | Loss: 0.52738 | LR: 2.31e-04\n",
      "Step 44275 | Loss: 0.51389 | LR: 2.31e-04\n",
      "val loss: 0.5006\n",
      "Step 44300 | Loss: 0.47817 | LR: 2.30e-04\n",
      "Step 44325 | Loss: 0.50150 | LR: 2.30e-04\n",
      "Step 44350 | Loss: 0.48280 | LR: 2.29e-04\n",
      "Step 44375 | Loss: 0.47572 | LR: 2.29e-04\n",
      "val loss: 0.4994\n",
      "Step 44400 | Loss: 0.47725 | LR: 2.28e-04\n",
      "Step 44425 | Loss: 0.50955 | LR: 2.28e-04\n",
      "Step 44450 | Loss: 0.52188 | LR: 2.27e-04\n",
      "Step 44475 | Loss: 0.52365 | LR: 2.27e-04\n",
      "=== Step 44477 Done. Avg Loss: 0.50318 ===\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "val loss: 0.4903\n",
      "Step 44500 | Loss: 0.50920 | LR: 2.26e-04\n",
      "Step 44525 | Loss: 0.51552 | LR: 2.25e-04\n",
      "Step 44550 | Loss: 0.47066 | LR: 2.25e-04\n",
      "Step 44575 | Loss: 0.49066 | LR: 2.24e-04\n",
      "val loss: 0.5046\n",
      "Step 44600 | Loss: 0.51167 | LR: 2.24e-04\n",
      "Step 44625 | Loss: 0.49815 | LR: 2.23e-04\n",
      "Step 44650 | Loss: 0.50381 | LR: 2.23e-04\n",
      "Step 44675 | Loss: 0.49443 | LR: 2.22e-04\n",
      "val loss: 0.4878\n",
      "Step 44700 | Loss: 0.51323 | LR: 2.22e-04\n",
      "Step 44725 | Loss: 0.48758 | LR: 2.21e-04\n",
      "Step 44750 | Loss: 0.46031 | LR: 2.21e-04\n",
      "Step 44775 | Loss: 0.50324 | LR: 2.20e-04\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.4949\n",
      "Step 44800 | Loss: 0.50345 | LR: 2.20e-04\n",
      "Step 44825 | Loss: 0.50831 | LR: 2.19e-04\n",
      "Step 44850 | Loss: 0.51579 | LR: 2.18e-04\n",
      "Step 44875 | Loss: 0.52889 | LR: 2.18e-04\n",
      "val loss: 0.5040\n",
      "Step 44900 | Loss: 0.49789 | LR: 2.17e-04\n",
      "Step 44925 | Loss: 0.50232 | LR: 2.17e-04\n",
      "Step 44950 | Loss: 0.46091 | LR: 2.16e-04\n",
      "Step 44975 | Loss: 0.48417 | LR: 2.16e-04\n",
      "val loss: 0.5026\n",
      "Step 45000 | Loss: 0.46948 | LR: 2.15e-04\n",
      "Step 45025 | Loss: 0.50564 | LR: 2.15e-04\n",
      "Step 45050 | Loss: 0.47726 | LR: 2.14e-04\n",
      "Step 45075 | Loss: 0.50092 | LR: 2.14e-04\n",
      "val loss: 0.5020\n",
      "Step 45100 | Loss: 0.51748 | LR: 2.13e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "Step 45125 | Loss: 0.50637 | LR: 2.13e-04\n",
      "Step 45150 | Loss: 0.50372 | LR: 2.12e-04\n",
      "Step 45175 | Loss: 0.51868 | LR: 2.11e-04\n",
      "val loss: 0.4986\n",
      "Step 45200 | Loss: 0.51991 | LR: 2.11e-04\n",
      "Step 45225 | Loss: 0.46944 | LR: 2.10e-04\n",
      "Step 45250 | Loss: 0.50963 | LR: 2.10e-04\n",
      "Step 45275 | Loss: 0.48843 | LR: 2.09e-04\n",
      "val loss: 0.5002\n",
      "Step 45300 | Loss: 0.50242 | LR: 2.09e-04\n",
      "Step 45325 | Loss: 0.51958 | LR: 2.08e-04\n",
      "Step 45350 | Loss: 0.47511 | LR: 2.08e-04\n",
      "Step 45375 | Loss: 0.50543 | LR: 2.07e-04\n",
      "val loss: 0.4898\n",
      "Step 45400 | Loss: 0.47098 | LR: 2.07e-04\n",
      "Step 45425 | Loss: 0.48814 | LR: 2.06e-04\n",
      "Step 45450 | Loss: 0.52642 | LR: 2.06e-04\n",
      "Step 45475 | Loss: 0.52417 | LR: 2.05e-04\n",
      "val loss: 0.4983\n",
      "Step 45500 | Loss: 0.49577 | LR: 2.05e-04\n",
      "Step 45525 | Loss: 0.46979 | LR: 2.04e-04\n",
      "Step 45550 | Loss: 0.48034 | LR: 2.04e-04\n",
      "Step 45575 | Loss: 0.49748 | LR: 2.03e-04\n",
      "val loss: 0.4982\n",
      "Step 45600 | Loss: 0.51184 | LR: 2.03e-04\n",
      "Step 45625 | Loss: 0.49999 | LR: 2.02e-04\n",
      "Step 45650 | Loss: 0.50802 | LR: 2.01e-04\n",
      "Step 45675 | Loss: 0.51517 | LR: 2.01e-04\n",
      "val loss: 0.5042\n",
      "Step 45700 | Loss: 0.49338 | LR: 2.00e-04\n",
      "Step 45725 | Loss: 0.48682 | LR: 2.00e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 45750 | Loss: 0.48018 | LR: 1.99e-04\n",
      "Step 45775 | Loss: 0.52139 | LR: 1.99e-04\n",
      "val loss: 0.5042\n",
      "Step 45800 | Loss: 0.52704 | LR: 1.98e-04\n",
      "Step 45825 | Loss: 0.51959 | LR: 1.98e-04\n",
      "Step 45850 | Loss: 0.52299 | LR: 1.97e-04\n",
      "Step 45875 | Loss: 0.51974 | LR: 1.97e-04\n",
      "val loss: 0.5045\n",
      "Step 45900 | Loss: 0.49943 | LR: 1.96e-04\n",
      "Step 45925 | Loss: 0.49653 | LR: 1.96e-04\n",
      "Step 45950 | Loss: 0.48663 | LR: 1.95e-04\n",
      "Step 45975 | Loss: 0.51138 | LR: 1.95e-04\n",
      "val loss: 0.5055\n",
      "Step 46000 | Loss: 0.48317 | LR: 1.94e-04\n",
      "Step 46025 | Loss: 0.52730 | LR: 1.94e-04\n",
      "Step 46050 | Loss: 0.48671 | LR: 1.93e-04\n",
      "Step 46075 | Loss: 0.50911 | LR: 1.93e-04\n",
      "val loss: 0.4974\n",
      "Step 46100 | Loss: 0.49238 | LR: 1.92e-04\n",
      "Step 46125 | Loss: 0.48349 | LR: 1.92e-04\n",
      "Step 46150 | Loss: 0.50072 | LR: 1.91e-04\n",
      "Step 46175 | Loss: 0.47829 | LR: 1.91e-04\n",
      "val loss: 0.5017\n",
      "Step 46200 | Loss: 0.51280 | LR: 1.90e-04\n",
      "Step 46225 | Loss: 0.50764 | LR: 1.90e-04\n",
      "Step 46250 | Loss: 0.51580 | LR: 1.89e-04\n",
      "Step 46275 | Loss: 0.50238 | LR: 1.89e-04\n",
      "val loss: 0.4978\n",
      "Step 46300 | Loss: 0.49798 | LR: 1.88e-04\n",
      "Step 46325 | Loss: 0.50827 | LR: 1.88e-04\n",
      "Step 46350 | Loss: 0.52817 | LR: 1.87e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0004.npz\n",
      "Step 46375 | Loss: 0.49221 | LR: 1.87e-04\n",
      "val loss: 0.5036\n",
      "Step 46400 | Loss: 0.47124 | LR: 1.86e-04\n",
      "Step 46425 | Loss: 0.49019 | LR: 1.86e-04\n",
      "Step 46450 | Loss: 0.48498 | LR: 1.85e-04\n",
      "Step 46475 | Loss: 0.49750 | LR: 1.85e-04\n",
      "val loss: 0.5063\n",
      "Step 46500 | Loss: 0.51528 | LR: 1.84e-04\n",
      "Step 46525 | Loss: 0.49737 | LR: 1.84e-04\n",
      "Step 46550 | Loss: 0.52508 | LR: 1.83e-04\n",
      "Step 46575 | Loss: 0.50853 | LR: 1.83e-04\n",
      "val loss: 0.5120\n",
      "Step 46600 | Loss: 0.49088 | LR: 1.82e-04\n",
      "Step 46625 | Loss: 0.54712 | LR: 1.82e-04\n",
      "Step 46650 | Loss: 0.51746 | LR: 1.81e-04\n",
      "Step 46675 | Loss: 0.49413 | LR: 1.81e-04\n",
      "val loss: 0.5034\n",
      "Step 46700 | Loss: 0.51044 | LR: 1.80e-04\n",
      "Step 46725 | Loss: 0.49605 | LR: 1.80e-04\n",
      "Step 46750 | Loss: 0.49524 | LR: 1.79e-04\n",
      "Step 46775 | Loss: 0.51834 | LR: 1.79e-04\n",
      "val loss: 0.4976\n",
      "Step 46800 | Loss: 0.48482 | LR: 1.78e-04\n",
      "Step 46825 | Loss: 0.50045 | LR: 1.78e-04\n",
      "Step 46850 | Loss: 0.49599 | LR: 1.77e-04\n",
      "Step 46875 | Loss: 0.49374 | LR: 1.77e-04\n",
      "val loss: 0.5056\n",
      "Step 46900 | Loss: 0.53259 | LR: 1.76e-04\n",
      "Step 46925 | Loss: 0.48738 | LR: 1.76e-04\n",
      "Step 46950 | Loss: 0.48836 | LR: 1.75e-04\n",
      "Step 46975 | Loss: 0.49081 | LR: 1.75e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0003.npz\n",
      "val loss: 0.4945\n",
      "Step 47000 | Loss: 0.49299 | LR: 1.74e-04\n",
      "Step 47025 | Loss: 0.52929 | LR: 1.74e-04\n",
      "Step 47050 | Loss: 0.48872 | LR: 1.73e-04\n",
      "Step 47075 | Loss: 0.51367 | LR: 1.73e-04\n",
      "val loss: 0.4931\n",
      "Step 47100 | Loss: 0.47979 | LR: 1.72e-04\n",
      "Step 47125 | Loss: 0.49855 | LR: 1.72e-04\n",
      "Step 47150 | Loss: 0.49239 | LR: 1.71e-04\n",
      "Step 47175 | Loss: 0.54637 | LR: 1.71e-04\n",
      "val loss: 0.5023\n",
      "Step 47200 | Loss: 0.50017 | LR: 1.70e-04\n",
      "Step 47225 | Loss: 0.50194 | LR: 1.70e-04\n",
      "Step 47250 | Loss: 0.49787 | LR: 1.69e-04\n",
      "Step 47275 | Loss: 0.48881 | LR: 1.69e-04\n",
      "val loss: 0.5011\n",
      "Step 47300 | Loss: 0.48787 | LR: 1.68e-04\n",
      "Step 47325 | Loss: 0.48900 | LR: 1.68e-04\n",
      "Step 47350 | Loss: 0.48790 | LR: 1.67e-04\n",
      "Step 47375 | Loss: 0.53360 | LR: 1.67e-04\n",
      "val loss: 0.5023\n",
      "Step 47400 | Loss: 0.52399 | LR: 1.66e-04\n",
      "Step 47425 | Loss: 0.50962 | LR: 1.66e-04\n",
      "Step 47450 | Loss: 0.50466 | LR: 1.65e-04\n",
      "Step 47475 | Loss: 0.50222 | LR: 1.65e-04\n",
      "val loss: 0.4984\n",
      "Step 47500 | Loss: 0.50935 | LR: 1.64e-04\n",
      "Step 47525 | Loss: 0.49559 | LR: 1.64e-04\n",
      "Step 47550 | Loss: 0.52478 | LR: 1.63e-04\n",
      "Step 47575 | Loss: 0.48448 | LR: 1.63e-04\n",
      "val loss: 0.5045\n",
      "Step 47600 | Loss: 0.47302 | LR: 1.62e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0005.npz\n",
      "Step 47625 | Loss: 0.48333 | LR: 1.62e-04\n",
      "Step 47650 | Loss: 0.51322 | LR: 1.61e-04\n",
      "=== Step 47654 Done. Avg Loss: 0.50314 ===\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0003.npz\n",
      "Step 47675 | Loss: 0.48738 | LR: 1.61e-04\n",
      "val loss: 0.5056\n",
      "Step 47700 | Loss: 0.50670 | LR: 1.60e-04\n",
      "Step 47725 | Loss: 0.50621 | LR: 1.60e-04\n",
      "Step 47750 | Loss: 0.52083 | LR: 1.60e-04\n",
      "Step 47775 | Loss: 0.50655 | LR: 1.59e-04\n",
      "val loss: 0.5007\n",
      "Step 47800 | Loss: 0.50489 | LR: 1.59e-04\n",
      "Step 47825 | Loss: 0.52936 | LR: 1.58e-04\n",
      "Step 47850 | Loss: 0.48534 | LR: 1.58e-04\n",
      "Step 47875 | Loss: 0.53124 | LR: 1.57e-04\n",
      "val loss: 0.5004\n",
      "Step 47900 | Loss: 0.50817 | LR: 1.57e-04\n",
      "Step 47925 | Loss: 0.48902 | LR: 1.56e-04\n",
      "Step 47950 | Loss: 0.48567 | LR: 1.56e-04\n",
      "Step 47975 | Loss: 0.53762 | LR: 1.55e-04\n",
      "val loss: 0.5005\n",
      "Step 48000 | Loss: 0.53271 | LR: 1.55e-04\n",
      "Step 48025 | Loss: 0.49181 | LR: 1.54e-04\n",
      "Step 48050 | Loss: 0.49465 | LR: 1.54e-04\n",
      "Step 48075 | Loss: 0.47899 | LR: 1.53e-04\n",
      "val loss: 0.4926\n",
      "Step 48100 | Loss: 0.49654 | LR: 1.53e-04\n",
      "Step 48125 | Loss: 0.49351 | LR: 1.52e-04\n",
      "Step 48150 | Loss: 0.49469 | LR: 1.52e-04\n",
      "Step 48175 | Loss: 0.50111 | LR: 1.51e-04\n",
      "val loss: 0.4979\n",
      "Step 48200 | Loss: 0.52180 | LR: 1.51e-04\n",
      "Step 48225 | Loss: 0.49341 | LR: 1.51e-04\n",
      "Step 48250 | Loss: 0.50189 | LR: 1.50e-04\n",
      "Step 48275 | Loss: 0.52758 | LR: 1.50e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5080\n",
      "Step 48300 | Loss: 0.52685 | LR: 1.49e-04\n",
      "Step 48325 | Loss: 0.51334 | LR: 1.49e-04\n",
      "Step 48350 | Loss: 0.47406 | LR: 1.48e-04\n",
      "Step 48375 | Loss: 0.49154 | LR: 1.48e-04\n",
      "val loss: 0.5081\n",
      "Step 48400 | Loss: 0.49587 | LR: 1.47e-04\n",
      "Step 48425 | Loss: 0.49835 | LR: 1.47e-04\n",
      "Step 48450 | Loss: 0.49805 | LR: 1.46e-04\n",
      "Step 48475 | Loss: 0.49262 | LR: 1.46e-04\n",
      "val loss: 0.4968\n",
      "Step 48500 | Loss: 0.50400 | LR: 1.45e-04\n",
      "Step 48525 | Loss: 0.50015 | LR: 1.45e-04\n",
      "Step 48550 | Loss: 0.48980 | LR: 1.45e-04\n",
      "Step 48575 | Loss: 0.54596 | LR: 1.44e-04\n",
      "val loss: 0.4999\n",
      "Step 48600 | Loss: 0.49496 | LR: 1.44e-04\n",
      "Step 48625 | Loss: 0.50004 | LR: 1.43e-04\n",
      "Step 48650 | Loss: 0.49198 | LR: 1.43e-04\n",
      "Step 48675 | Loss: 0.51989 | LR: 1.42e-04\n",
      "val loss: 0.4961\n",
      "Step 48700 | Loss: 0.48478 | LR: 1.42e-04\n",
      "Step 48725 | Loss: 0.49613 | LR: 1.41e-04\n",
      "Step 48750 | Loss: 0.53924 | LR: 1.41e-04\n",
      "Step 48775 | Loss: 0.51307 | LR: 1.40e-04\n",
      "val loss: 0.5058\n",
      "Step 48800 | Loss: 0.45879 | LR: 1.40e-04\n",
      "Step 48825 | Loss: 0.50380 | LR: 1.40e-04\n",
      "Step 48850 | Loss: 0.48572 | LR: 1.39e-04\n",
      "Step 48875 | Loss: 0.52210 | LR: 1.39e-04\n",
      "val loss: 0.5027\n",
      "Step 48900 | Loss: 0.51569 | LR: 1.38e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "Step 48925 | Loss: 0.52114 | LR: 1.38e-04\n",
      "Step 48950 | Loss: 0.48216 | LR: 1.37e-04\n",
      "Step 48975 | Loss: 0.49579 | LR: 1.37e-04\n",
      "val loss: 0.5014\n",
      "Step 49000 | Loss: 0.51740 | LR: 1.36e-04\n",
      "Step 49025 | Loss: 0.49527 | LR: 1.36e-04\n",
      "Step 49050 | Loss: 0.49794 | LR: 1.36e-04\n",
      "Step 49075 | Loss: 0.51377 | LR: 1.35e-04\n",
      "val loss: 0.5010\n",
      "Step 49100 | Loss: 0.49880 | LR: 1.35e-04\n",
      "Step 49125 | Loss: 0.50482 | LR: 1.34e-04\n",
      "Step 49150 | Loss: 0.50686 | LR: 1.34e-04\n",
      "Step 49175 | Loss: 0.51544 | LR: 1.33e-04\n",
      "val loss: 0.4993\n",
      "Step 49200 | Loss: 0.49523 | LR: 1.33e-04\n",
      "Step 49225 | Loss: 0.52109 | LR: 1.32e-04\n",
      "Step 49250 | Loss: 0.49195 | LR: 1.32e-04\n",
      "Step 49275 | Loss: 0.48654 | LR: 1.32e-04\n",
      "val loss: 0.5041\n",
      "Step 49300 | Loss: 0.51175 | LR: 1.31e-04\n",
      "Step 49325 | Loss: 0.47372 | LR: 1.31e-04\n",
      "Step 49350 | Loss: 0.48197 | LR: 1.30e-04\n",
      "Step 49375 | Loss: 0.46216 | LR: 1.30e-04\n",
      "val loss: 0.4977\n",
      "Step 49400 | Loss: 0.49487 | LR: 1.29e-04\n",
      "Step 49425 | Loss: 0.50706 | LR: 1.29e-04\n",
      "Step 49450 | Loss: 0.50896 | LR: 1.28e-04\n",
      "Step 49475 | Loss: 0.49594 | LR: 1.28e-04\n",
      "val loss: 0.5043\n",
      "Step 49500 | Loss: 0.48028 | LR: 1.28e-04\n",
      "Step 49525 | Loss: 0.48701 | LR: 1.27e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0005.npz\n",
      "Step 49550 | Loss: 0.48676 | LR: 1.27e-04\n",
      "Step 49575 | Loss: 0.52359 | LR: 1.26e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "val loss: 0.4985\n",
      "Step 49600 | Loss: 0.52292 | LR: 1.26e-04\n",
      "Step 49625 | Loss: 0.50793 | LR: 1.25e-04\n",
      "Step 49650 | Loss: 0.50825 | LR: 1.25e-04\n",
      "Step 49675 | Loss: 0.48047 | LR: 1.25e-04\n",
      "val loss: 0.4918\n",
      "Step 49700 | Loss: 0.51693 | LR: 1.24e-04\n",
      "Step 49725 | Loss: 0.49366 | LR: 1.24e-04\n",
      "Step 49750 | Loss: 0.49838 | LR: 1.23e-04\n",
      "Step 49775 | Loss: 0.51215 | LR: 1.23e-04\n",
      "val loss: 0.5013\n",
      "Step 49800 | Loss: 0.52286 | LR: 1.22e-04\n",
      "Step 49825 | Loss: 0.50401 | LR: 1.22e-04\n",
      "Step 49850 | Loss: 0.53208 | LR: 1.22e-04\n",
      "Step 49875 | Loss: 0.49796 | LR: 1.21e-04\n",
      "val loss: 0.5075\n",
      "Step 49900 | Loss: 0.53076 | LR: 1.21e-04\n",
      "Step 49925 | Loss: 0.51766 | LR: 1.20e-04\n",
      "Step 49950 | Loss: 0.52158 | LR: 1.20e-04\n",
      "Step 49975 | Loss: 0.47524 | LR: 1.19e-04\n",
      "val loss: 0.5064\n",
      "Step 50000 | Loss: 0.48592 | LR: 1.19e-04\n",
      "Step 50025 | Loss: 0.47348 | LR: 1.19e-04\n",
      "Step 50050 | Loss: 0.48753 | LR: 1.18e-04\n",
      "Step 50075 | Loss: 0.48282 | LR: 1.18e-04\n",
      "val loss: 0.4975\n",
      "Step 50100 | Loss: 0.48991 | LR: 1.17e-04\n",
      "Step 50125 | Loss: 0.53039 | LR: 1.17e-04\n",
      "Step 50150 | Loss: 0.48617 | LR: 1.17e-04\n",
      "Step 50175 | Loss: 0.50266 | LR: 1.16e-04\n",
      "val loss: 0.4980\n",
      "Step 50200 | Loss: 0.49116 | LR: 1.16e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0004.npz\n",
      "Step 50225 | Loss: 0.49042 | LR: 1.15e-04\n",
      "Step 50250 | Loss: 0.52912 | LR: 1.15e-04\n",
      "Step 50275 | Loss: 0.50918 | LR: 1.14e-04\n",
      "val loss: 0.4877\n",
      "Step 50300 | Loss: 0.45888 | LR: 1.14e-04\n",
      "Step 50325 | Loss: 0.52647 | LR: 1.14e-04\n",
      "Step 50350 | Loss: 0.48969 | LR: 1.13e-04\n",
      "Step 50375 | Loss: 0.49698 | LR: 1.13e-04\n",
      "val loss: 0.4891\n",
      "Step 50400 | Loss: 0.49971 | LR: 1.12e-04\n",
      "Step 50425 | Loss: 0.48916 | LR: 1.12e-04\n",
      "Step 50450 | Loss: 0.51369 | LR: 1.12e-04\n",
      "Step 50475 | Loss: 0.48887 | LR: 1.11e-04\n",
      "val loss: 0.4952\n",
      "Step 50500 | Loss: 0.48123 | LR: 1.11e-04\n",
      "Step 50525 | Loss: 0.53086 | LR: 1.10e-04\n",
      "Step 50550 | Loss: 0.50319 | LR: 1.10e-04\n",
      "Step 50575 | Loss: 0.50081 | LR: 1.10e-04\n",
      "val loss: 0.5053\n",
      "Step 50600 | Loss: 0.52159 | LR: 1.09e-04\n",
      "Step 50625 | Loss: 0.47966 | LR: 1.09e-04\n",
      "Step 50650 | Loss: 0.51455 | LR: 1.08e-04\n",
      "Step 50675 | Loss: 0.49623 | LR: 1.08e-04\n",
      "val loss: 0.5088\n",
      "Step 50700 | Loss: 0.49849 | LR: 1.08e-04\n",
      "Step 50725 | Loss: 0.49559 | LR: 1.07e-04\n",
      "Step 50750 | Loss: 0.49691 | LR: 1.07e-04\n",
      "Step 50775 | Loss: 0.48731 | LR: 1.06e-04\n",
      "val loss: 0.5029\n",
      "Step 50800 | Loss: 0.50895 | LR: 1.06e-04\n",
      "Step 50825 | Loss: 0.57185 | LR: 1.06e-04\n",
      "=== Step 50831 Done. Avg Loss: 0.50307 ===\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "Step 50850 | Loss: 0.50556 | LR: 1.05e-04\n",
      "Step 50875 | Loss: 0.51002 | LR: 1.05e-04\n",
      "val loss: 0.4981\n",
      "Step 50900 | Loss: 0.47814 | LR: 1.04e-04\n",
      "Step 50925 | Loss: 0.49196 | LR: 1.04e-04\n",
      "Step 50950 | Loss: 0.49538 | LR: 1.04e-04\n",
      "Step 50975 | Loss: 0.52241 | LR: 1.03e-04\n",
      "val loss: 0.4949\n",
      "Step 51000 | Loss: 0.54799 | LR: 1.03e-04\n",
      "Step 51025 | Loss: 0.48678 | LR: 1.02e-04\n",
      "Step 51050 | Loss: 0.50117 | LR: 1.02e-04\n",
      "Step 51075 | Loss: 0.48761 | LR: 1.02e-04\n",
      "val loss: 0.4983\n",
      "Step 51100 | Loss: 0.50856 | LR: 1.01e-04\n",
      "Step 51125 | Loss: 0.48928 | LR: 1.01e-04\n",
      "Step 51150 | Loss: 0.47733 | LR: 1.00e-04\n",
      "Step 51175 | Loss: 0.51844 | LR: 1.00e-04\n",
      "val loss: 0.5062\n",
      "Step 51200 | Loss: 0.51831 | LR: 9.96e-05\n",
      "Step 51225 | Loss: 0.50224 | LR: 9.92e-05\n",
      "Step 51250 | Loss: 0.47611 | LR: 9.88e-05\n",
      "Step 51275 | Loss: 0.50980 | LR: 9.84e-05\n",
      "val loss: 0.5107\n",
      "Step 51300 | Loss: 0.51380 | LR: 9.80e-05\n",
      "Step 51325 | Loss: 0.50980 | LR: 9.77e-05\n",
      "Step 51350 | Loss: 0.49990 | LR: 9.73e-05\n",
      "Step 51375 | Loss: 0.50738 | LR: 9.69e-05\n",
      "val loss: 0.5013\n",
      "Step 51400 | Loss: 0.48490 | LR: 9.65e-05\n",
      "Step 51425 | Loss: 0.50871 | LR: 9.61e-05\n",
      "Step 51450 | Loss: 0.49605 | LR: 9.57e-05\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0004.npz\n",
      "Step 51475 | Loss: 0.53361 | LR: 9.53e-05\n",
      "val loss: 0.5039\n",
      "Step 51500 | Loss: 0.50769 | LR: 9.50e-05\n",
      "Step 51525 | Loss: 0.48523 | LR: 9.46e-05\n",
      "Step 51550 | Loss: 0.48500 | LR: 9.42e-05\n",
      "Step 51575 | Loss: 0.54082 | LR: 9.38e-05\n",
      "val loss: 0.5025\n",
      "Step 51600 | Loss: 0.49542 | LR: 9.34e-05\n",
      "Step 51625 | Loss: 0.48411 | LR: 9.31e-05\n",
      "Step 51650 | Loss: 0.50301 | LR: 9.27e-05\n",
      "Step 51675 | Loss: 0.47014 | LR: 9.23e-05\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.4972\n",
      "Step 51700 | Loss: 0.50334 | LR: 9.19e-05\n",
      "Step 51725 | Loss: 0.51040 | LR: 9.16e-05\n",
      "Step 51750 | Loss: 0.50765 | LR: 9.12e-05\n",
      "Step 51775 | Loss: 0.48250 | LR: 9.08e-05\n",
      "val loss: 0.4964\n",
      "Step 51800 | Loss: 0.51566 | LR: 9.04e-05\n",
      "Step 51825 | Loss: 0.49365 | LR: 9.01e-05\n",
      "Step 51850 | Loss: 0.51783 | LR: 8.97e-05\n",
      "Step 51875 | Loss: 0.46966 | LR: 8.93e-05\n",
      "val loss: 0.4945\n",
      "Step 51900 | Loss: 0.47234 | LR: 8.90e-05\n",
      "Step 51925 | Loss: 0.52206 | LR: 8.86e-05\n",
      "Step 51950 | Loss: 0.48941 | LR: 8.82e-05\n",
      "Step 51975 | Loss: 0.49476 | LR: 8.78e-05\n",
      "val loss: 0.4990\n",
      "Step 52000 | Loss: 0.49500 | LR: 8.75e-05\n",
      "Step 52025 | Loss: 0.48746 | LR: 8.71e-05\n",
      "Step 52050 | Loss: 0.53090 | LR: 8.67e-05\n",
      "Step 52075 | Loss: 0.48264 | LR: 8.64e-05\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0003.npz\n",
      "val loss: 0.4946\n",
      "Step 52100 | Loss: 0.52426 | LR: 8.60e-05\n",
      "Step 52125 | Loss: 0.51935 | LR: 8.56e-05\n",
      "Step 52150 | Loss: 0.49786 | LR: 8.53e-05\n",
      "Step 52175 | Loss: 0.51221 | LR: 8.49e-05\n",
      "val loss: 0.5050\n",
      "Step 52200 | Loss: 0.50686 | LR: 8.46e-05\n",
      "Step 52225 | Loss: 0.52462 | LR: 8.42e-05\n",
      "Step 52250 | Loss: 0.49880 | LR: 8.38e-05\n",
      "Step 52275 | Loss: 0.50455 | LR: 8.35e-05\n",
      "val loss: 0.4945\n",
      "Step 52300 | Loss: 0.50342 | LR: 8.31e-05\n",
      "Step 52325 | Loss: 0.48928 | LR: 8.28e-05\n",
      "Step 52350 | Loss: 0.53638 | LR: 8.24e-05\n",
      "Step 52375 | Loss: 0.47008 | LR: 8.20e-05\n",
      "val loss: 0.5036\n",
      "Step 52400 | Loss: 0.50090 | LR: 8.17e-05\n",
      "Step 52425 | Loss: 0.52955 | LR: 8.13e-05\n",
      "Step 52450 | Loss: 0.53661 | LR: 8.10e-05\n",
      "Step 52475 | Loss: 0.51058 | LR: 8.06e-05\n",
      "val loss: 0.4951\n",
      "Step 52500 | Loss: 0.51204 | LR: 8.03e-05\n",
      "Step 52525 | Loss: 0.47264 | LR: 7.99e-05\n",
      "Step 52550 | Loss: 0.52161 | LR: 7.96e-05\n",
      "Step 52575 | Loss: 0.49249 | LR: 7.92e-05\n",
      "val loss: 0.5116\n",
      "Step 52600 | Loss: 0.51762 | LR: 7.89e-05\n",
      "Step 52625 | Loss: 0.49819 | LR: 7.85e-05\n",
      "Step 52650 | Loss: 0.51557 | LR: 7.82e-05\n",
      "Step 52675 | Loss: 0.50144 | LR: 7.78e-05\n",
      "val loss: 0.4996\n",
      "Step 52700 | Loss: 0.49250 | LR: 7.75e-05\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 52725 | Loss: 0.49748 | LR: 7.71e-05\n",
      "Step 52750 | Loss: 0.48828 | LR: 7.68e-05\n",
      "Step 52775 | Loss: 0.49827 | LR: 7.64e-05\n",
      "val loss: 0.5007\n",
      "Step 52800 | Loss: 0.51497 | LR: 7.61e-05\n",
      "Step 52825 | Loss: 0.53774 | LR: 7.57e-05\n",
      "Step 52850 | Loss: 0.53338 | LR: 7.54e-05\n",
      "Step 52875 | Loss: 0.50460 | LR: 7.50e-05\n",
      "val loss: 0.5039\n",
      "Step 52900 | Loss: 0.53775 | LR: 7.47e-05\n",
      "Step 52925 | Loss: 0.49419 | LR: 7.44e-05\n",
      "Step 52950 | Loss: 0.50489 | LR: 7.40e-05\n",
      "Step 52975 | Loss: 0.50564 | LR: 7.37e-05\n",
      "val loss: 0.5018\n",
      "Step 53000 | Loss: 0.49995 | LR: 7.33e-05\n",
      "Step 53025 | Loss: 0.49304 | LR: 7.30e-05\n",
      "Step 53050 | Loss: 0.52264 | LR: 7.27e-05\n",
      "Step 53075 | Loss: 0.49147 | LR: 7.23e-05\n",
      "val loss: 0.5030\n",
      "Step 53100 | Loss: 0.49935 | LR: 7.20e-05\n",
      "Step 53125 | Loss: 0.49306 | LR: 7.17e-05\n",
      "Step 53150 | Loss: 0.48675 | LR: 7.13e-05\n",
      "Step 53175 | Loss: 0.49286 | LR: 7.10e-05\n",
      "val loss: 0.5052\n",
      "Step 53200 | Loss: 0.48629 | LR: 7.06e-05\n",
      "Step 53225 | Loss: 0.47826 | LR: 7.03e-05\n",
      "Step 53250 | Loss: 0.51703 | LR: 7.00e-05\n",
      "Step 53275 | Loss: 0.51808 | LR: 6.96e-05\n",
      "val loss: 0.5047\n",
      "Step 53300 | Loss: 0.51343 | LR: 6.93e-05\n",
      "Step 53325 | Loss: 0.49445 | LR: 6.90e-05\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "Step 53350 | Loss: 0.52192 | LR: 6.87e-05\n",
      "Step 53375 | Loss: 0.49069 | LR: 6.83e-05\n",
      "val loss: 0.4977\n",
      "Step 53400 | Loss: 0.50860 | LR: 6.80e-05\n",
      "Step 53425 | Loss: 0.49537 | LR: 6.77e-05\n",
      "Step 53450 | Loss: 0.47870 | LR: 6.73e-05\n",
      "Step 53475 | Loss: 0.50628 | LR: 6.70e-05\n",
      "val loss: 0.5102\n",
      "Step 53500 | Loss: 0.48164 | LR: 6.67e-05\n",
      "Step 53525 | Loss: 0.48439 | LR: 6.64e-05\n",
      "Step 53550 | Loss: 0.49833 | LR: 6.61e-05\n",
      "Step 53575 | Loss: 0.50197 | LR: 6.57e-05\n",
      "val loss: 0.4989\n",
      "Step 53600 | Loss: 0.50531 | LR: 6.54e-05\n",
      "Step 53625 | Loss: 0.53573 | LR: 6.51e-05\n",
      "Step 53650 | Loss: 0.51399 | LR: 6.48e-05\n",
      "Step 53675 | Loss: 0.50511 | LR: 6.44e-05\n",
      "val loss: 0.4990\n",
      "Step 53700 | Loss: 0.49090 | LR: 6.41e-05\n",
      "Step 53725 | Loss: 0.48825 | LR: 6.38e-05\n",
      "Step 53750 | Loss: 0.51335 | LR: 6.35e-05\n",
      "Step 53775 | Loss: 0.51106 | LR: 6.32e-05\n",
      "val loss: 0.5091\n",
      "Step 53800 | Loss: 0.51637 | LR: 6.29e-05\n",
      "Step 53825 | Loss: 0.48623 | LR: 6.25e-05\n",
      "Step 53850 | Loss: 0.48823 | LR: 6.22e-05\n",
      "Step 53875 | Loss: 0.51437 | LR: 6.19e-05\n",
      "val loss: 0.4987\n",
      "Step 53900 | Loss: 0.47281 | LR: 6.16e-05\n",
      "Step 53925 | Loss: 0.47246 | LR: 6.13e-05\n",
      "Step 53950 | Loss: 0.48115 | LR: 6.10e-05\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0005.npz\n",
      "Step 53975 | Loss: 0.52243 | LR: 6.07e-05\n",
      "val loss: 0.4926\n",
      "Step 54000 | Loss: 0.51963 | LR: 6.04e-05\n",
      "=== Step 54008 Done. Avg Loss: 0.50303 ===\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0005.npz\n",
      "Step 54025 | Loss: 0.51440 | LR: 6.00e-05\n",
      "Step 54050 | Loss: 0.49685 | LR: 5.97e-05\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0004.npz\n",
      "Step 54075 | Loss: 0.49180 | LR: 5.94e-05\n",
      "val loss: 0.5007\n",
      "Step 54100 | Loss: 0.48931 | LR: 5.91e-05\n",
      "Step 54125 | Loss: 0.48630 | LR: 5.88e-05\n",
      "Step 54150 | Loss: 0.50320 | LR: 5.85e-05\n",
      "Step 54175 | Loss: 0.53557 | LR: 5.82e-05\n",
      "val loss: 0.5047\n",
      "Step 54200 | Loss: 0.48807 | LR: 5.79e-05\n",
      "Step 54225 | Loss: 0.48357 | LR: 5.76e-05\n",
      "Step 54250 | Loss: 0.53848 | LR: 5.73e-05\n",
      "Step 54275 | Loss: 0.51924 | LR: 5.70e-05\n",
      "val loss: 0.5037\n",
      "Step 54300 | Loss: 0.46546 | LR: 5.67e-05\n",
      "Step 54325 | Loss: 0.53578 | LR: 5.64e-05\n",
      "Step 54350 | Loss: 0.49886 | LR: 5.61e-05\n",
      "Step 54375 | Loss: 0.51818 | LR: 5.58e-05\n",
      "val loss: 0.5016\n",
      "Step 54400 | Loss: 0.49244 | LR: 5.55e-05\n",
      "Step 54425 | Loss: 0.48223 | LR: 5.52e-05\n",
      "Step 54450 | Loss: 0.52786 | LR: 5.49e-05\n",
      "Step 54475 | Loss: 0.46624 | LR: 5.46e-05\n",
      "val loss: 0.4988\n",
      "Step 54500 | Loss: 0.47603 | LR: 5.43e-05\n",
      "Step 54525 | Loss: 0.47404 | LR: 5.40e-05\n",
      "Step 54550 | Loss: 0.50698 | LR: 5.37e-05\n",
      "Step 54575 | Loss: 0.50646 | LR: 5.34e-05\n",
      "val loss: 0.4942\n",
      "Step 54600 | Loss: 0.52223 | LR: 5.31e-05\n",
      "Step 54625 | Loss: 0.50468 | LR: 5.28e-05\n",
      "Step 54650 | Loss: 0.52382 | LR: 5.26e-05\n",
      "Step 54675 | Loss: 0.50557 | LR: 5.23e-05\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "val loss: 0.5036\n",
      "Step 54700 | Loss: 0.50636 | LR: 5.20e-05\n",
      "Step 54725 | Loss: 0.50225 | LR: 5.17e-05\n",
      "Step 54750 | Loss: 0.51841 | LR: 5.14e-05\n",
      "Step 54775 | Loss: 0.51697 | LR: 5.11e-05\n",
      "val loss: 0.4948\n",
      "Step 54800 | Loss: 0.52744 | LR: 5.08e-05\n",
      "Step 54825 | Loss: 0.49518 | LR: 5.05e-05\n",
      "Step 54850 | Loss: 0.54163 | LR: 5.03e-05\n",
      "Step 54875 | Loss: 0.49441 | LR: 5.00e-05\n",
      "val loss: 0.4972\n",
      "Step 54900 | Loss: 0.52487 | LR: 4.97e-05\n",
      "Step 54925 | Loss: 0.55445 | LR: 4.94e-05\n",
      "Step 54950 | Loss: 0.51198 | LR: 4.91e-05\n",
      "Step 54975 | Loss: 0.48017 | LR: 4.88e-05\n",
      "val loss: 0.5075\n",
      "Step 55000 | Loss: 0.49953 | LR: 4.86e-05\n",
      "Step 55025 | Loss: 0.48941 | LR: 4.83e-05\n",
      "Step 55050 | Loss: 0.47671 | LR: 4.80e-05\n",
      "Step 55075 | Loss: 0.48568 | LR: 4.77e-05\n",
      "val loss: 0.4999\n",
      "Step 55100 | Loss: 0.48709 | LR: 4.74e-05\n",
      "Step 55125 | Loss: 0.50600 | LR: 4.72e-05\n",
      "Step 55150 | Loss: 0.53769 | LR: 4.69e-05\n",
      "Step 55175 | Loss: 0.49656 | LR: 4.66e-05\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5038\n",
      "Step 55200 | Loss: 0.51067 | LR: 4.63e-05\n",
      "Step 55225 | Loss: 0.49938 | LR: 4.61e-05\n",
      "Step 55250 | Loss: 0.48127 | LR: 4.58e-05\n",
      "Step 55275 | Loss: 0.50842 | LR: 4.55e-05\n",
      "val loss: 0.4970\n",
      "Step 55300 | Loss: 0.50038 | LR: 4.53e-05\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0003.npz\n",
      "Step 55325 | Loss: 0.47954 | LR: 4.50e-05\n",
      "Step 55350 | Loss: 0.50777 | LR: 4.47e-05\n",
      "Step 55375 | Loss: 0.50283 | LR: 4.45e-05\n",
      "val loss: 0.4974\n",
      "Step 55400 | Loss: 0.50574 | LR: 4.42e-05\n",
      "Step 55425 | Loss: 0.47002 | LR: 4.39e-05\n",
      "Step 55450 | Loss: 0.48768 | LR: 4.37e-05\n",
      "Step 55475 | Loss: 0.48212 | LR: 4.34e-05\n",
      "val loss: 0.4942\n",
      "Step 55500 | Loss: 0.52360 | LR: 4.31e-05\n",
      "Step 55525 | Loss: 0.48252 | LR: 4.29e-05\n",
      "Step 55550 | Loss: 0.48647 | LR: 4.26e-05\n",
      "Step 55575 | Loss: 0.50764 | LR: 4.23e-05\n",
      "val loss: 0.4928\n",
      "Step 55600 | Loss: 0.51163 | LR: 4.21e-05\n",
      "Step 55625 | Loss: 0.50693 | LR: 4.18e-05\n",
      "Step 55650 | Loss: 0.51745 | LR: 4.15e-05\n",
      "Step 55675 | Loss: 0.49200 | LR: 4.13e-05\n",
      "val loss: 0.4996\n",
      "Step 55700 | Loss: 0.50513 | LR: 4.10e-05\n",
      "Step 55725 | Loss: 0.47372 | LR: 4.08e-05\n",
      "Step 55750 | Loss: 0.53848 | LR: 4.05e-05\n",
      "Step 55775 | Loss: 0.49641 | LR: 4.03e-05\n",
      "val loss: 0.4991\n",
      "Step 55800 | Loss: 0.50792 | LR: 4.00e-05\n",
      "Step 55825 | Loss: 0.49764 | LR: 3.98e-05\n",
      "Step 55850 | Loss: 0.51792 | LR: 3.95e-05\n",
      "Step 55875 | Loss: 0.50400 | LR: 3.92e-05\n",
      "val loss: 0.5008\n",
      "Step 55900 | Loss: 0.49382 | LR: 3.90e-05\n",
      "Step 55925 | Loss: 0.49788 | LR: 3.87e-05\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "Step 55950 | Loss: 0.53098 | LR: 3.85e-05\n",
      "Step 55975 | Loss: 0.50620 | LR: 3.82e-05\n",
      "val loss: 0.5029\n",
      "Step 56000 | Loss: 0.50097 | LR: 3.80e-05\n",
      "Step 56025 | Loss: 0.48950 | LR: 3.77e-05\n",
      "Step 56050 | Loss: 0.48704 | LR: 3.75e-05\n",
      "Step 56075 | Loss: 0.51237 | LR: 3.72e-05\n",
      "val loss: 0.4976\n",
      "Step 56100 | Loss: 0.49486 | LR: 3.70e-05\n",
      "Step 56125 | Loss: 0.49441 | LR: 3.68e-05\n",
      "Step 56150 | Loss: 0.52698 | LR: 3.65e-05\n",
      "Step 56175 | Loss: 0.50449 | LR: 3.63e-05\n",
      "val loss: 0.5054\n",
      "Step 56200 | Loss: 0.49853 | LR: 3.60e-05\n",
      "Step 56225 | Loss: 0.52708 | LR: 3.58e-05\n",
      "Step 56250 | Loss: 0.50321 | LR: 3.55e-05\n",
      "Step 56275 | Loss: 0.49380 | LR: 3.53e-05\n",
      "val loss: 0.4920\n",
      "Step 56300 | Loss: 0.48946 | LR: 3.51e-05\n",
      "Step 56325 | Loss: 0.51099 | LR: 3.48e-05\n",
      "Step 56350 | Loss: 0.52332 | LR: 3.46e-05\n",
      "Step 56375 | Loss: 0.49253 | LR: 3.43e-05\n",
      "val loss: 0.4996\n",
      "Step 56400 | Loss: 0.50258 | LR: 3.41e-05\n",
      "Step 56425 | Loss: 0.51983 | LR: 3.39e-05\n",
      "Step 56450 | Loss: 0.51303 | LR: 3.36e-05\n",
      "Step 56475 | Loss: 0.50537 | LR: 3.34e-05\n",
      "val loss: 0.4984\n",
      "Step 56500 | Loss: 0.50311 | LR: 3.32e-05\n",
      "Step 56525 | Loss: 0.50702 | LR: 3.29e-05\n",
      "Step 56550 | Loss: 0.51135 | LR: 3.27e-05\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 56575 | Loss: 0.51260 | LR: 3.25e-05\n",
      "val loss: 0.4962\n",
      "Step 56600 | Loss: 0.51791 | LR: 3.22e-05\n",
      "Step 56625 | Loss: 0.51607 | LR: 3.20e-05\n",
      "Step 56650 | Loss: 0.51870 | LR: 3.18e-05\n",
      "Step 56675 | Loss: 0.52077 | LR: 3.16e-05\n",
      "val loss: 0.4925\n",
      "Step 56700 | Loss: 0.50617 | LR: 3.13e-05\n",
      "Step 56725 | Loss: 0.50634 | LR: 3.11e-05\n",
      "Step 56750 | Loss: 0.48383 | LR: 3.09e-05\n",
      "Step 56775 | Loss: 0.50180 | LR: 3.07e-05\n",
      "val loss: 0.4992\n",
      "Step 56800 | Loss: 0.48895 | LR: 3.04e-05\n",
      "Step 56825 | Loss: 0.46671 | LR: 3.02e-05\n",
      "Step 56850 | Loss: 0.49043 | LR: 3.00e-05\n",
      "Step 56875 | Loss: 0.52754 | LR: 2.98e-05\n",
      "val loss: 0.5005\n",
      "Step 56900 | Loss: 0.50417 | LR: 2.95e-05\n",
      "Step 56925 | Loss: 0.51782 | LR: 2.93e-05\n",
      "Step 56950 | Loss: 0.49121 | LR: 2.91e-05\n",
      "Step 56975 | Loss: 0.50591 | LR: 2.89e-05\n",
      "val loss: 0.5065\n",
      "Step 57000 | Loss: 0.50981 | LR: 2.87e-05\n",
      "Step 57025 | Loss: 0.50196 | LR: 2.85e-05\n",
      "Step 57050 | Loss: 0.51098 | LR: 2.82e-05\n",
      "Step 57075 | Loss: 0.52773 | LR: 2.80e-05\n",
      "val loss: 0.5110\n",
      "Step 57100 | Loss: 0.52999 | LR: 2.78e-05\n",
      "Step 57125 | Loss: 0.51060 | LR: 2.76e-05\n",
      "Step 57150 | Loss: 0.51253 | LR: 2.74e-05\n",
      "Step 57175 | Loss: 0.50137 | LR: 2.72e-05\n",
      "=== Step 57185 Done. Avg Loss: 0.50299 ===\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0005.npz\n",
      "val loss: 0.4954\n",
      "Step 57200 | Loss: 0.52276 | LR: 2.70e-05\n",
      "Step 57225 | Loss: 0.53129 | LR: 2.67e-05\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "Step 57250 | Loss: 0.50621 | LR: 2.65e-05\n",
      "Step 57275 | Loss: 0.46757 | LR: 2.63e-05\n",
      "val loss: 0.5065\n",
      "Step 57300 | Loss: 0.48705 | LR: 2.61e-05\n",
      "Step 57325 | Loss: 0.48069 | LR: 2.59e-05\n",
      "Step 57350 | Loss: 0.49793 | LR: 2.57e-05\n",
      "Step 57375 | Loss: 0.48850 | LR: 2.55e-05\n",
      "val loss: 0.4971\n",
      "Step 57400 | Loss: 0.52079 | LR: 2.53e-05\n",
      "Step 57425 | Loss: 0.49531 | LR: 2.51e-05\n",
      "Step 57450 | Loss: 0.51006 | LR: 2.49e-05\n",
      "Step 57475 | Loss: 0.48555 | LR: 2.47e-05\n",
      "val loss: 0.4967\n",
      "Step 57500 | Loss: 0.47803 | LR: 2.45e-05\n",
      "Step 57525 | Loss: 0.50149 | LR: 2.43e-05\n",
      "Step 57550 | Loss: 0.51572 | LR: 2.41e-05\n",
      "Step 57575 | Loss: 0.50600 | LR: 2.39e-05\n",
      "val loss: 0.5025\n",
      "Step 57600 | Loss: 0.48445 | LR: 2.37e-05\n",
      "Step 57625 | Loss: 0.47647 | LR: 2.35e-05\n",
      "Step 57650 | Loss: 0.49461 | LR: 2.33e-05\n",
      "Step 57675 | Loss: 0.47811 | LR: 2.31e-05\n",
      "val loss: 0.5028\n",
      "Step 57700 | Loss: 0.48146 | LR: 2.29e-05\n",
      "Step 57725 | Loss: 0.48309 | LR: 2.27e-05\n",
      "Step 57750 | Loss: 0.51236 | LR: 2.25e-05\n",
      "Step 57775 | Loss: 0.49354 | LR: 2.23e-05\n",
      "val loss: 0.5095\n",
      "Step 57800 | Loss: 0.49222 | LR: 2.21e-05\n",
      "Step 57825 | Loss: 0.52787 | LR: 2.19e-05\n",
      "Step 57850 | Loss: 0.50572 | LR: 2.18e-05\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0003.npz\n",
      "Step 57875 | Loss: 0.51869 | LR: 2.16e-05\n",
      "val loss: 0.5044\n",
      "Step 57900 | Loss: 0.50738 | LR: 2.14e-05\n",
      "Step 57925 | Loss: 0.46874 | LR: 2.12e-05\n",
      "Step 57950 | Loss: 0.49293 | LR: 2.10e-05\n",
      "Step 57975 | Loss: 0.52148 | LR: 2.08e-05\n",
      "val loss: 0.4917\n",
      "Step 58000 | Loss: 0.51448 | LR: 2.06e-05\n",
      "Step 58025 | Loss: 0.52571 | LR: 2.04e-05\n",
      "Step 58050 | Loss: 0.49246 | LR: 2.03e-05\n",
      "Step 58075 | Loss: 0.51841 | LR: 2.01e-05\n",
      "val loss: 0.5077\n",
      "Step 58100 | Loss: 0.48431 | LR: 1.99e-05\n",
      "Step 58125 | Loss: 0.48214 | LR: 1.97e-05\n",
      "Step 58150 | Loss: 0.53010 | LR: 1.95e-05\n",
      "Step 58175 | Loss: 0.52110 | LR: 1.94e-05\n",
      "val loss: 0.5073\n",
      "Step 58200 | Loss: 0.50254 | LR: 1.92e-05\n",
      "Step 58225 | Loss: 0.50433 | LR: 1.90e-05\n",
      "Step 58250 | Loss: 0.52263 | LR: 1.88e-05\n",
      "Step 58275 | Loss: 0.51283 | LR: 1.86e-05\n",
      "val loss: 0.5056\n",
      "Step 58300 | Loss: 0.53444 | LR: 1.85e-05\n",
      "Step 58325 | Loss: 0.48121 | LR: 1.83e-05\n",
      "Step 58350 | Loss: 0.50506 | LR: 1.81e-05\n",
      "Step 58375 | Loss: 0.51290 | LR: 1.79e-05\n",
      "val loss: 0.5044\n",
      "Step 58400 | Loss: 0.50168 | LR: 1.78e-05\n",
      "Step 58425 | Loss: 0.49045 | LR: 1.76e-05\n",
      "Step 58450 | Loss: 0.50471 | LR: 1.74e-05\n",
      "Step 58475 | Loss: 0.48525 | LR: 1.73e-05\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "val loss: 0.4923\n",
      "Step 58500 | Loss: 0.49098 | LR: 1.71e-05\n",
      "Step 58525 | Loss: 0.49279 | LR: 1.69e-05\n",
      "Step 58550 | Loss: 0.49103 | LR: 1.68e-05\n",
      "Step 58575 | Loss: 0.52309 | LR: 1.66e-05\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5091\n",
      "Step 58600 | Loss: 0.48195 | LR: 1.64e-05\n",
      "Step 58625 | Loss: 0.50593 | LR: 1.63e-05\n",
      "Step 58650 | Loss: 0.49514 | LR: 1.61e-05\n",
      "Step 58675 | Loss: 0.53284 | LR: 1.59e-05\n",
      "val loss: 0.5060\n",
      "Step 58700 | Loss: 0.48487 | LR: 1.58e-05\n",
      "Step 58725 | Loss: 0.50152 | LR: 1.56e-05\n",
      "Step 58750 | Loss: 0.51465 | LR: 1.54e-05\n",
      "Step 58775 | Loss: 0.48243 | LR: 1.53e-05\n",
      "val loss: 0.5029\n",
      "Step 58800 | Loss: 0.50570 | LR: 1.51e-05\n",
      "Step 58825 | Loss: 0.50687 | LR: 1.50e-05\n",
      "Step 58850 | Loss: 0.51435 | LR: 1.48e-05\n",
      "Step 58875 | Loss: 0.47500 | LR: 1.47e-05\n",
      "val loss: 0.5052\n",
      "Step 58900 | Loss: 0.49154 | LR: 1.45e-05\n",
      "Step 58925 | Loss: 0.47678 | LR: 1.43e-05\n",
      "Step 58950 | Loss: 0.51812 | LR: 1.42e-05\n",
      "Step 58975 | Loss: 0.50091 | LR: 1.40e-05\n",
      "val loss: 0.5019\n",
      "Step 59000 | Loss: 0.49236 | LR: 1.39e-05\n",
      "Step 59025 | Loss: 0.50265 | LR: 1.37e-05\n",
      "Step 59050 | Loss: 0.47940 | LR: 1.36e-05\n",
      "Step 59075 | Loss: 0.52525 | LR: 1.34e-05\n",
      "val loss: 0.5004\n",
      "Step 59100 | Loss: 0.47509 | LR: 1.33e-05\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0004.npz\n",
      "Step 59125 | Loss: 0.50102 | LR: 1.31e-05\n",
      "Step 59150 | Loss: 0.50054 | LR: 1.30e-05\n",
      "Step 59175 | Loss: 0.50777 | LR: 1.28e-05\n",
      "val loss: 0.5018\n",
      "Step 59200 | Loss: 0.48531 | LR: 1.27e-05\n",
      "Step 59225 | Loss: 0.45311 | LR: 1.25e-05\n",
      "Step 59250 | Loss: 0.48283 | LR: 1.24e-05\n",
      "Step 59275 | Loss: 0.52715 | LR: 1.23e-05\n",
      "val loss: 0.5014\n",
      "Step 59300 | Loss: 0.49291 | LR: 1.21e-05\n",
      "Step 59325 | Loss: 0.51843 | LR: 1.20e-05\n",
      "Step 59350 | Loss: 0.50692 | LR: 1.18e-05\n",
      "Step 59375 | Loss: 0.46118 | LR: 1.17e-05\n",
      "val loss: 0.4960\n",
      "Step 59400 | Loss: 0.52513 | LR: 1.16e-05\n",
      "Step 59425 | Loss: 0.50493 | LR: 1.14e-05\n",
      "Step 59450 | Loss: 0.52942 | LR: 1.13e-05\n",
      "Step 59475 | Loss: 0.53350 | LR: 1.11e-05\n",
      "val loss: 0.4906\n",
      "Step 59500 | Loss: 0.51112 | LR: 1.10e-05\n",
      "Step 59525 | Loss: 0.47122 | LR: 1.09e-05\n",
      "Step 59550 | Loss: 0.50989 | LR: 1.07e-05\n",
      "Step 59575 | Loss: 0.49573 | LR: 1.06e-05\n",
      "val loss: 0.4906\n",
      "Step 59600 | Loss: 0.51710 | LR: 1.05e-05\n",
      "Step 59625 | Loss: 0.48099 | LR: 1.03e-05\n",
      "Step 59650 | Loss: 0.51395 | LR: 1.02e-05\n",
      "Step 59675 | Loss: 0.49605 | LR: 1.01e-05\n",
      "val loss: 0.4983\n",
      "Step 59700 | Loss: 0.53654 | LR: 9.95e-06\n",
      "Step 59725 | Loss: 0.48040 | LR: 9.82e-06\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 59750 | Loss: 0.49231 | LR: 9.69e-06\n",
      "Step 59775 | Loss: 0.51713 | LR: 9.56e-06\n",
      "val loss: 0.5071\n",
      "Step 59800 | Loss: 0.49901 | LR: 9.44e-06\n",
      "Step 59825 | Loss: 0.49444 | LR: 9.31e-06\n",
      "Step 59850 | Loss: 0.48093 | LR: 9.19e-06\n",
      "Step 59875 | Loss: 0.48017 | LR: 9.06e-06\n",
      "val loss: 0.4949\n",
      "Step 59900 | Loss: 0.50736 | LR: 8.94e-06\n",
      "Step 59925 | Loss: 0.50172 | LR: 8.82e-06\n",
      "Step 59950 | Loss: 0.51209 | LR: 8.70e-06\n",
      "Step 59975 | Loss: 0.49796 | LR: 8.58e-06\n",
      "val loss: 0.5122\n",
      "Step 60000 | Loss: 0.49365 | LR: 8.46e-06\n",
      "Step 60025 | Loss: 0.50116 | LR: 8.34e-06\n",
      "Step 60050 | Loss: 0.50711 | LR: 8.22e-06\n",
      "Step 60075 | Loss: 0.50197 | LR: 8.10e-06\n",
      "val loss: 0.4986\n",
      "Step 60100 | Loss: 0.48433 | LR: 7.99e-06\n",
      "Step 60125 | Loss: 0.53028 | LR: 7.87e-06\n",
      "Step 60150 | Loss: 0.48152 | LR: 7.76e-06\n",
      "Step 60175 | Loss: 0.47656 | LR: 7.64e-06\n",
      "val loss: 0.4977\n",
      "Step 60200 | Loss: 0.47814 | LR: 7.53e-06\n",
      "Step 60225 | Loss: 0.49633 | LR: 7.42e-06\n",
      "Step 60250 | Loss: 0.51783 | LR: 7.31e-06\n",
      "Step 60275 | Loss: 0.47301 | LR: 7.20e-06\n",
      "val loss: 0.5079\n",
      "Step 60300 | Loss: 0.50902 | LR: 7.09e-06\n",
      "Step 60325 | Loss: 0.50445 | LR: 6.98e-06\n",
      "Step 60350 | Loss: 0.50088 | LR: 6.87e-06\n",
      "=== Step 60362 Done. Avg Loss: 0.50294 ===\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "Step 60375 | Loss: 0.51482 | LR: 6.76e-06\n",
      "val loss: 0.4972\n",
      "Step 60400 | Loss: 0.47679 | LR: 6.66e-06\n",
      "Step 60425 | Loss: 0.51337 | LR: 6.55e-06\n",
      "Step 60450 | Loss: 0.49677 | LR: 6.45e-06\n",
      "Step 60475 | Loss: 0.51543 | LR: 6.34e-06\n",
      "val loss: 0.4988\n",
      "Step 60500 | Loss: 0.49308 | LR: 6.24e-06\n",
      "Step 60525 | Loss: 0.47731 | LR: 6.14e-06\n",
      "Step 60550 | Loss: 0.50802 | LR: 6.04e-06\n",
      "Step 60575 | Loss: 0.48278 | LR: 5.94e-06\n",
      "val loss: 0.4951\n",
      "Step 60600 | Loss: 0.50860 | LR: 5.84e-06\n",
      "Step 60625 | Loss: 0.50361 | LR: 5.74e-06\n",
      "Step 60650 | Loss: 0.50276 | LR: 5.64e-06\n",
      "Step 60675 | Loss: 0.48594 | LR: 5.54e-06\n",
      "val loss: 0.4954\n",
      "Step 60700 | Loss: 0.50284 | LR: 5.45e-06\n",
      "Step 60725 | Loss: 0.48075 | LR: 5.35e-06\n",
      "Step 60750 | Loss: 0.49594 | LR: 5.26e-06\n",
      "Step 60775 | Loss: 0.49443 | LR: 5.16e-06\n",
      "val loss: 0.5093\n",
      "Step 60800 | Loss: 0.47406 | LR: 5.07e-06\n",
      "Step 60825 | Loss: 0.50435 | LR: 4.98e-06\n",
      "Step 60850 | Loss: 0.49477 | LR: 4.89e-06\n",
      "Step 60875 | Loss: 0.51527 | LR: 4.80e-06\n",
      "val loss: 0.5075\n",
      "Step 60900 | Loss: 0.49458 | LR: 4.71e-06\n",
      "Step 60925 | Loss: 0.47932 | LR: 4.62e-06\n",
      "Step 60950 | Loss: 0.48517 | LR: 4.53e-06\n",
      "Step 60975 | Loss: 0.52296 | LR: 4.45e-06\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0005.npz\n",
      "val loss: 0.5005\n",
      "Step 61000 | Loss: 0.48457 | LR: 4.36e-06\n",
      "Step 61025 | Loss: 0.50543 | LR: 4.27e-06\n",
      "Step 61050 | Loss: 0.48542 | LR: 4.19e-06\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "Step 61075 | Loss: 0.52244 | LR: 4.11e-06\n",
      "val loss: 0.4922\n",
      "Step 61100 | Loss: 0.52110 | LR: 4.02e-06\n",
      "Step 61125 | Loss: 0.48057 | LR: 3.94e-06\n",
      "Step 61150 | Loss: 0.51880 | LR: 3.86e-06\n",
      "Step 61175 | Loss: 0.51479 | LR: 3.78e-06\n",
      "val loss: 0.4975\n",
      "Step 61200 | Loss: 0.49307 | LR: 3.70e-06\n",
      "Step 61225 | Loss: 0.50821 | LR: 3.62e-06\n",
      "Step 61250 | Loss: 0.51565 | LR: 3.54e-06\n",
      "Step 61275 | Loss: 0.49464 | LR: 3.47e-06\n",
      "val loss: 0.4875\n",
      "Step 61300 | Loss: 0.51750 | LR: 3.39e-06\n",
      "Step 61325 | Loss: 0.50620 | LR: 3.32e-06\n",
      "Step 61350 | Loss: 0.49371 | LR: 3.24e-06\n",
      "Step 61375 | Loss: 0.49853 | LR: 3.17e-06\n",
      "val loss: 0.5008\n",
      "Step 61400 | Loss: 0.51361 | LR: 3.10e-06\n",
      "Step 61425 | Loss: 0.50815 | LR: 3.02e-06\n",
      "Step 61450 | Loss: 0.46705 | LR: 2.95e-06\n",
      "Step 61475 | Loss: 0.50444 | LR: 2.88e-06\n",
      "val loss: 0.5018\n",
      "Step 61500 | Loss: 0.47617 | LR: 2.81e-06\n",
      "Step 61525 | Loss: 0.50191 | LR: 2.75e-06\n",
      "Step 61550 | Loss: 0.51617 | LR: 2.68e-06\n",
      "Step 61575 | Loss: 0.49388 | LR: 2.61e-06\n",
      "val loss: 0.5024\n",
      "Step 61600 | Loss: 0.50299 | LR: 2.55e-06\n",
      "Step 61625 | Loss: 0.47836 | LR: 2.48e-06\n",
      "Step 61650 | Loss: 0.48197 | LR: 2.42e-06\n",
      "Step 61675 | Loss: 0.48986 | LR: 2.35e-06\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0003.npz\n",
      "val loss: 0.5044\n",
      "Step 61700 | Loss: 0.53606 | LR: 2.29e-06\n",
      "Step 61725 | Loss: 0.47862 | LR: 2.23e-06\n",
      "Step 61750 | Loss: 0.49020 | LR: 2.17e-06\n",
      "Step 61775 | Loss: 0.52646 | LR: 2.11e-06\n",
      "val loss: 0.5071\n",
      "Step 61800 | Loss: 0.47864 | LR: 2.05e-06\n",
      "Step 61825 | Loss: 0.48948 | LR: 1.99e-06\n",
      "Step 61850 | Loss: 0.53120 | LR: 1.93e-06\n",
      "Step 61875 | Loss: 0.50795 | LR: 1.88e-06\n",
      "val loss: 0.5064\n",
      "Step 61900 | Loss: 0.47651 | LR: 1.82e-06\n",
      "Step 61925 | Loss: 0.47368 | LR: 1.76e-06\n",
      "Step 61950 | Loss: 0.49935 | LR: 1.71e-06\n",
      "Step 61975 | Loss: 0.52651 | LR: 1.66e-06\n",
      "val loss: 0.4994\n",
      "Step 62000 | Loss: 0.51566 | LR: 1.60e-06\n",
      "Step 62025 | Loss: 0.51106 | LR: 1.55e-06\n",
      "Step 62050 | Loss: 0.49528 | LR: 1.50e-06\n",
      "Step 62075 | Loss: 0.49321 | LR: 1.45e-06\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5029\n",
      "Step 62100 | Loss: 0.49745 | LR: 1.40e-06\n",
      "Step 62125 | Loss: 0.48382 | LR: 1.36e-06\n",
      "Step 62150 | Loss: 0.48886 | LR: 1.31e-06\n",
      "Step 62175 | Loss: 0.50945 | LR: 1.26e-06\n",
      "val loss: 0.5014\n",
      "Step 62200 | Loss: 0.52395 | LR: 1.22e-06\n",
      "Step 62225 | Loss: 0.50398 | LR: 1.17e-06\n",
      "Step 62250 | Loss: 0.48224 | LR: 1.13e-06\n",
      "Step 62275 | Loss: 0.50565 | LR: 1.08e-06\n",
      "val loss: 0.5056\n",
      "Step 62300 | Loss: 0.50590 | LR: 1.04e-06\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 62325 | Loss: 0.50003 | LR: 1.00e-06\n",
      "Step 62350 | Loss: 0.51145 | LR: 9.59e-07\n",
      "Step 62375 | Loss: 0.52009 | LR: 9.20e-07\n",
      "val loss: 0.5004\n",
      "Step 62400 | Loss: 0.46724 | LR: 8.81e-07\n",
      "Step 62425 | Loss: 0.48993 | LR: 8.43e-07\n",
      "Step 62450 | Loss: 0.52460 | LR: 8.05e-07\n",
      "Step 62475 | Loss: 0.46494 | LR: 7.69e-07\n",
      "val loss: 0.5012\n",
      "Step 62500 | Loss: 0.50898 | LR: 7.33e-07\n",
      "Step 62525 | Loss: 0.54340 | LR: 6.99e-07\n",
      "Step 62550 | Loss: 0.51879 | LR: 6.65e-07\n",
      "Step 62575 | Loss: 0.50732 | LR: 6.32e-07\n",
      "val loss: 0.4995\n",
      "Step 62600 | Loss: 0.51490 | LR: 6.00e-07\n",
      "Step 62625 | Loss: 0.51460 | LR: 5.68e-07\n",
      "Step 62650 | Loss: 0.49439 | LR: 5.38e-07\n",
      "Step 62675 | Loss: 0.49967 | LR: 5.08e-07\n",
      "val loss: 0.4997\n",
      "Step 62700 | Loss: 0.49232 | LR: 4.79e-07\n",
      "Step 62725 | Loss: 0.48451 | LR: 4.52e-07\n",
      "Step 62750 | Loss: 0.49814 | LR: 4.24e-07\n",
      "Step 62775 | Loss: 0.53130 | LR: 3.98e-07\n",
      "val loss: 0.4946\n",
      "Step 62800 | Loss: 0.47673 | LR: 3.73e-07\n",
      "Step 62825 | Loss: 0.50306 | LR: 3.48e-07\n",
      "Step 62850 | Loss: 0.47009 | LR: 3.24e-07\n",
      "Step 62875 | Loss: 0.50336 | LR: 3.02e-07\n",
      "val loss: 0.5000\n",
      "Step 62900 | Loss: 0.48966 | LR: 2.80e-07\n",
      "Step 62925 | Loss: 0.50985 | LR: 2.58e-07\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0004.npz\n",
      "Step 62950 | Loss: 0.48235 | LR: 2.38e-07\n",
      "Step 62975 | Loss: 0.49205 | LR: 2.19e-07\n",
      "val loss: 0.5028\n",
      "Step 63000 | Loss: 0.46251 | LR: 2.00e-07\n",
      "Step 63025 | Loss: 0.48481 | LR: 1.82e-07\n",
      "Step 63050 | Loss: 0.50635 | LR: 1.65e-07\n",
      "Step 63075 | Loss: 0.49274 | LR: 1.49e-07\n",
      "val loss: 0.5071\n",
      "Step 63100 | Loss: 0.49916 | LR: 1.34e-07\n",
      "Step 63125 | Loss: 0.50014 | LR: 1.19e-07\n",
      "Step 63150 | Loss: 0.48598 | LR: 1.06e-07\n",
      "Step 63175 | Loss: 0.52400 | LR: 9.32e-08\n",
      "val loss: 0.4951\n",
      "Step 63200 | Loss: 0.51431 | LR: 8.14e-08\n",
      "Step 63225 | Loss: 0.49657 | LR: 7.03e-08\n",
      "Step 63250 | Loss: 0.49205 | LR: 6.02e-08\n",
      "Step 63275 | Loss: 0.48819 | LR: 5.08e-08\n",
      "val loss: 0.5063\n",
      "Step 63300 | Loss: 0.53695 | LR: 4.24e-08\n",
      "Step 63325 | Loss: 0.49076 | LR: 3.47e-08\n",
      "Step 63350 | Loss: 0.48960 | LR: 2.79e-08\n",
      "Step 63375 | Loss: 0.49012 | LR: 2.20e-08\n",
      "val loss: 0.5014\n",
      "Step 63400 | Loss: 0.48188 | LR: 1.69e-08\n",
      "Step 63425 | Loss: 0.51481 | LR: 1.26e-08\n",
      "Step 63450 | Loss: 0.48477 | LR: 9.24e-09\n",
      "Step 63475 | Loss: 0.48611 | LR: 6.69e-09\n",
      "val loss: 0.4992\n",
      "Step 63500 | Loss: 0.48763 | LR: 4.98e-09\n",
      "Step 63525 | Loss: 0.51922 | LR: 4.11e-09\n",
      "val loss: 0.5018\n",
      "=== Step 63539 Done. Avg Loss: 0.50295 ===\n"
     ]
    }
   ],
   "source": [
    "decoder.train()\n",
    "\n",
    "for step in range(max_steps):\n",
    "\n",
    "    last_step = (step == max_steps - 1)\n",
    "\n",
    "    # once in a while evaluate our validation set loss\n",
    "    if step % 100 == 0 or last_step:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss_accum = 0.0\n",
    "            val_loss_steps = 10\n",
    "            for i in range(val_loss_steps):\n",
    "                xc, xct, xt, xtt, p_idx, p_mod, p_mode = val_loader.next_batch()\n",
    "                p_feats = input_bank[p_idx]\n",
    "                B, N = xc.shape\n",
    "\n",
    "                # get perturbation latents\n",
    "                action_latents = model.composer(p_feats, p_mod, p_mode)\n",
    "\n",
    "                # run BioJEPA\n",
    "                z_context = model.student(xc, xct, mask_idx=None)\n",
    "                target_indices = torch.arange(N, device=DEVICE).expand(B, N)\n",
    "                z_pred_mu, _ = model.predictor(z_context, action_latents, target_indices)\n",
    "\n",
    "                # run new decoder\n",
    "                pred_delta = decoder(z_pred_mu) - decoder(z_context)\n",
    "                real_delta = xt - xc\n",
    "\n",
    "                loss = F.mse_loss(pred_delta, real_delta)\n",
    "                val_loss_accum += loss.item()\n",
    "\n",
    "            avg_val_loss = val_loss_accum / val_loss_steps\n",
    "            print(f'val loss: {avg_val_loss:.4f}')\n",
    "\n",
    "        decoder.train()\n",
    "\n",
    "    # periodically save checkpoint\n",
    "    if step > 0 and  (step+1) % steps_per_epoch ==0 and not last_step:\n",
    "        # Save Checkpoint\n",
    "        torch.save({\n",
    "            'model': decoder.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'step': step\n",
    "        }, checkpoint_dir / f'biojepa_decoder_ckpt_{step}.pt')\n",
    "\n",
    "    # actual training\n",
    "\n",
    "    xc, xct, xt, xtt, p_idx, p_mod, p_mode = train_loader.next_batch()\n",
    "    p_feats = input_bank[p_idx]\n",
    "    B, N = xc.shape\n",
    "\n",
    "    # run frozen BioJEPA\n",
    "    with torch.no_grad():\n",
    "        z_context = model.student(xc, xct, mask_idx=None)\n",
    "        action_latents = model.composer(p_feats, p_mod, p_mode)\n",
    "        \n",
    "        target_indices = torch.arange(N, device=DEVICE).expand(B, N)\n",
    "        z_pred_mu, _ = model.predictor(z_context, action_latents, target_indices)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # run decoder\n",
    "    pred_case = decoder(z_pred_mu)\n",
    "    pred_control = decoder(z_context)\n",
    "\n",
    "    pred_delta = pred_case - pred_control\n",
    "    real_delta = xt - xc\n",
    "\n",
    "    # loss\n",
    "    loss = F.mse_loss(pred_delta, real_delta)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    # loss caching\n",
    "    lossi.append(loss.item())\n",
    "    total_epoch_loss += loss.item()\n",
    "\n",
    "    if step % 25 == 0:\n",
    "        print(f\"Step {step} | Loss: {loss.item():.5f} | LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "    \n",
    "    if step > 0 and (step+1) % steps_per_epoch == 0:   \n",
    "        avg_loss = total_epoch_loss / steps_per_epoch\n",
    "        print(f\"=== Step {step} Done. Avg Loss: {avg_loss:.5f} ===\")\n",
    "        total_epoch_loss = 0\n",
    "    \n",
    "    if last_step:\n",
    "        # Save Checkpoint\n",
    "        torch.save({\n",
    "            'model': decoder.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'step': step\n",
    "        }, checkpoint_dir / f'biojepa_decoder_ckpt_{step}_final.pt')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c924050-b2d8-4973-a3c1-f9dd0a3d1f03",
   "metadata": {},
   "source": [
    "**Training Loss Plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b11fde37-0cac-429d-be0c-351390bac22d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0xe64cc7ec3770>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lossi)\n",
    "#plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8aaa78-e607-4bc4-8cde-49a529984c1b",
   "metadata": {},
   "source": [
    "## Trained Decoder Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "974730ff-1cda-43e0-a72f-df0986bb9daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from scipy.stats import ConstantInputWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94fd5355-31c8-47fc-a2fd-9b1fa9f91576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkDecoder(\n",
       "  (head): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "240d54bf-e521-433d-9c79-541dbebab4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_preds = defaultdict(list)      \n",
    "bulk_reals = defaultdict(list)     \n",
    "bulk_reals_delta = defaultdict(list)\n",
    "\n",
    "val_r2_all = []\n",
    "val_r2_top50 = []\n",
    "val_correlations = []\n",
    "val_mses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "417a9438-c9f1-4cb0-8da2-e9fe7ec831a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 1 shards for split val\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n"
     ]
    }
   ],
   "source": [
    "val_total_examples = 11044\n",
    "val_steps_per_epoch = val_total_examples // batch_size\n",
    "val_loader = TrainingLoader(batch_size=batch_size, split='val', data_dir=train_dir, device=DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27c89be7-508c-4e06-9dfc-9f4809807077",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Benchmarking: 100%|| 345/345 [00:35<00:00,  9.86it/s]\n"
     ]
    }
   ],
   "source": [
    "for step in tqdm(range(val_steps_per_epoch), desc='Benchmarking'):\n",
    "    \n",
    "    cont_x, cont_tot, case_x, case_tot, p_idx, p_mod, p_mode = val_loader.next_batch()\n",
    "    p_feats = input_bank[p_idx]\n",
    "    B, N = cont_x.shape\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        z_context = model.student(cont_x, cont_tot, mask_idx=None)\n",
    "        action_latents = model.composer(p_feats, p_mod, p_mode)\n",
    "        \n",
    "        target_indices = torch.arange(N, device=DEVICE).expand(B, N)\n",
    "        z_pred_mu, _ = model.predictor(z_context, action_latents, target_indices)\n",
    "        \n",
    "        pred_delta = decoder(z_pred_mu) - decoder(z_context)\n",
    "        real_delta = case_x - cont_x\n",
    "\n",
    "        pred_absolute = cont_x + pred_delta\n",
    "    \n",
    "    pred_delta_np = pred_delta.cpu().numpy()\n",
    "    real_delta_np = real_delta.cpu().numpy()\n",
    "\n",
    "    pred_abs_np = pred_absolute.cpu().numpy()\n",
    "    real_abs_np = case_x.cpu().numpy()\n",
    "    \n",
    "    act_id_np = p_idx.cpu().numpy().flatten()\n",
    "\n",
    "    for i in range(B):\n",
    "        p_delta = pred_delta_np[i]\n",
    "        t_delta = real_delta_np[i]\n",
    "        pid = act_id_np[i] \n",
    "\n",
    "        val_mses.append(np.mean((p_delta - t_delta)**2))\n",
    "\n",
    "        top_20_idx = np.argsort(np.abs(t_delta))[-20:]\n",
    "        p_top = p_delta[top_20_idx]\n",
    "        t_top = t_delta[top_20_idx]\n",
    "        \n",
    "        if np.std(p_top) > 1e-9 and np.std(t_top) > 1e-9:\n",
    "            corr, _ = pearsonr(p_top, t_top)\n",
    "            val_correlations.append(0.0 if np.isnan(corr) else corr)\n",
    "        else:\n",
    "            val_correlations.append(0.0)\n",
    "\n",
    "        bulk_preds[pid].append(pred_abs_np[i])\n",
    "        bulk_reals[pid].append(real_abs_np[i])\n",
    "        bulk_reals_delta[pid].append(real_delta_np[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c99d0783-9717-4fd2-933b-9b1d42a66cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pid in bulk_preds:\n",
    "    p_mean = np.mean(np.stack(bulk_preds[pid]), axis=0)\n",
    "    t_mean = np.mean(np.stack(bulk_reals[pid]), axis=0)\n",
    "    t_mean_delta = np.mean(np.stack(bulk_reals_delta[pid]), axis=0)\n",
    "\n",
    "    if np.std(t_mean) > 1e-9:\n",
    "        val_r2_all.append(r2_score(t_mean, p_mean))\n",
    "    \n",
    "    top_50_idx = np.argsort(np.abs(t_mean_delta))[-50:] \n",
    "    \n",
    "    val_r2_top50.append(r2_score(t_mean[top_50_idx], p_mean[top_50_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c9fa6eb-5426-42e6-bc77-4a796c2eba4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_mean_mse = np.mean(val_mses)\n",
    "val_mean_corr = np.mean(val_correlations)\n",
    "val_mean_r2_all = np.mean(val_r2_all)\n",
    "val_median_r2_all = np.median(val_r2_all)\n",
    "val_mean_r2_top50 = np.mean(val_r2_top50)\n",
    "val_median_r2_top50 = np.median(val_r2_top50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "750afdb9-ba92-4a7a-874d-0cc221525de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global MSE: 0.5006\n",
      "Top-20 Pearson R: 0.9271\n",
      "R^2 (All Genes): Mean: 0.9166, Median: 0.9235\n",
      "R^2 (Top 50 DEGs): Mean: 0.0659, Median: 0.2326\n"
     ]
    }
   ],
   "source": [
    "print(f'Global MSE: {val_mean_mse:.4f}')\n",
    "print(f'Top-20 Pearson R: {val_mean_corr:.4f}')\n",
    "print(f'R^2 (All Genes): Mean: {val_mean_r2_all:.4f}, Median: {val_median_r2_all:.4f}')\n",
    "print(f'R^2 (Top 50 DEGs): Mean: {val_mean_r2_top50:.4f}, Median: {val_median_r2_top50:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6b2741-9067-4cee-b37f-2ee471d3c7e6",
   "metadata": {},
   "source": [
    "## Trained Decoder Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ecd5cb77-d9a4-43d8-ad77-b6d102d00caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_preds = defaultdict(list)\n",
    "bulk_reals = defaultdict(list)\n",
    "bulk_reals_delta = defaultdict(list)\n",
    "\n",
    "test_r2_all = []\n",
    "test_r2_top50 = []\n",
    "test_correlations = []\n",
    "test_mses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "17c58931-04dc-4c75-a84d-23fbf3f7abc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 2 shards for split test\n",
      "loading /home/ubuntu/training/test/shard_k562e_test_0000.npz\n"
     ]
    }
   ],
   "source": [
    "test_total_examples = 38829\n",
    "test_loader = TrainingLoader(batch_size=batch_size, split='test', data_dir=train_dir, device=DEVICE)\n",
    "test_steps_per_epoch = test_total_examples // batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9022a3d0-68f1-4de8-bb53-33e5bd05e212",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Benchmarking:  43%|                             | 525/1213 [00:53<01:09,  9.83it/s]/tmp/ipykernel_14905/2029734354.py:41: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  corr, _ = pearsonr(p_top, t_top)\n",
      "Benchmarking:  52%|                        | 625/1213 [01:03<00:59,  9.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading /home/ubuntu/training/test/shard_k562e_test_0001.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Benchmarking: 100%|| 1213/1213 [02:03<00:00,  9.81it/s]\n"
     ]
    }
   ],
   "source": [
    "for step in tqdm(range(test_steps_per_epoch), desc='Benchmarking'):\n",
    "    \n",
    "    cont_x, cont_tot, case_x, case_tot, p_idx, p_mod, p_mode = test_loader.next_batch()\n",
    "    p_feats = input_bank[p_idx]\n",
    "    B, N = cont_x.shape\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        z_context = model.student(cont_x, cont_tot, mask_idx=None)\n",
    "        action_latents = model.composer(p_feats, p_mod, p_mode)\n",
    "        \n",
    "        target_indices = torch.arange(N, device=DEVICE).expand(B, N)\n",
    "        z_pred_mu, _ = model.predictor(z_context, action_latents, target_indices)\n",
    "        \n",
    "        pred_delta = decoder(z_pred_mu) - decoder(z_context)\n",
    "        real_delta = case_x - cont_x\n",
    "\n",
    "        pred_absolute = cont_x + pred_delta\n",
    "        pred_absolute = torch.clamp(pred_absolute, min=0.0)\n",
    "    \n",
    "    pred_delta_np = pred_delta.cpu().numpy()\n",
    "    real_delta_np = real_delta.cpu().numpy()\n",
    "\n",
    "    pred_abs_np = pred_absolute.cpu().numpy()\n",
    "    real_abs_np = case_x.cpu().numpy()\n",
    "    \n",
    "    act_id_np = p_idx.cpu().numpy().flatten()\n",
    "\n",
    "\n",
    "    for i in range(B):\n",
    "        p_delta = pred_delta_np[i]\n",
    "        t_delta = real_delta_np[i]\n",
    "        pid = act_id_np[i] \n",
    "\n",
    "        test_mses.append(np.mean((p_delta - t_delta)**2))\n",
    "\n",
    "        top_20_idx = np.argsort(np.abs(t_delta))[-20:]\n",
    "        p_top = p_delta[top_20_idx]\n",
    "        t_top = t_delta[top_20_idx]\n",
    "        \n",
    "        if np.std(p_top) > 1e-9 and np.std(t_top) > 1e-9:\n",
    "            corr, _ = pearsonr(p_top, t_top)\n",
    "            test_correlations.append(0.0 if np.isnan(corr) else corr)\n",
    "        else:\n",
    "            test_correlations.append(0.0)\n",
    "\n",
    "        bulk_preds[pid].append(pred_abs_np[i])\n",
    "        bulk_reals[pid].append(real_abs_np[i])\n",
    "        bulk_reals_delta[pid].append(real_delta_np[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa48799e-604f-4100-bb2b-3e1f5ac5e0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pid in bulk_preds:\n",
    "    p_mean = np.mean(np.stack(bulk_preds[pid]), axis=0)\n",
    "    t_mean = np.mean(np.stack(bulk_reals[pid]), axis=0)\n",
    "    t_mean_delta = np.mean(np.stack(bulk_reals_delta[pid]), axis=0)\n",
    "\n",
    "    if np.std(t_mean) > 1e-9:\n",
    "        test_r2_all.append(r2_score(t_mean, p_mean))\n",
    "    \n",
    "    top_50_idx = np.argsort(np.abs(t_mean_delta))[-50:] \n",
    "    \n",
    "    test_r2_top50.append(r2_score(t_mean[top_50_idx], p_mean[top_50_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "efaae9f8-c745-4044-b9cd-9d3cf0e944e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mean_mse = np.mean(test_mses)\n",
    "test_mean_corr = np.mean(test_correlations)\n",
    "test_mean_r2_all = np.mean(test_r2_all)\n",
    "test_median_r2_all = np.median(test_r2_all)\n",
    "test_mean_r2_top50 = np.mean(test_r2_top50)\n",
    "test_median_r2_top50 = np.median(test_r2_top50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "40ee16bc-f9f8-4ab6-bc12-9e72fafe1b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global MSE: 0.4979\n",
      "Top-20 Pearson R: 0.9266\n",
      "R^2 (All Genes) - Mean: 0.9175, Median: 0.9272\n",
      "R^2 (Top 50 DEGs) - Mean: 0.0962, Median: 0.3211\n"
     ]
    }
   ],
   "source": [
    "print(f'Global MSE: {test_mean_mse:.4f}')\n",
    "print(f'Top-20 Pearson R: {test_mean_corr:.4f}')\n",
    "print(f'R^2 (All Genes) - Mean: {test_mean_r2_all:.4f}, Median: {test_median_r2_all:.4f}')\n",
    "print(f'R^2 (Top 50 DEGs) - Mean: {test_mean_r2_top50:.4f}, Median: {test_median_r2_top50:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4370f0b7-232f-4fe4-8777-3d08ab4519be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Predicted Shift magnitude: 0.5379\n"
     ]
    }
   ],
   "source": [
    "diff = np.abs(pred_absolute.cpu().numpy() - cont_x.cpu().numpy()).mean()\n",
    "print(f\"Average Predicted Shift magnitude: {diff:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0803aec4-f2d9-4557-93df-6fbbca94a90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Change | Pred | True | Error | How much of change missed\n",
      "0.8229 | 0.4887 | 0.8229 | -0.3341 | -0.41\n",
      "0.8242 | 0.2069 | 0.9653 | -0.7584 | -0.92\n",
      "0.8246 | 0.8343 | 1.6169 | -0.7826 | -0.95\n",
      "-0.8279 | 0.3082 | 0.0000 | 0.3082 | -0.37\n",
      "0.8334 | 0.6302 | 0.9833 | -0.3530 | -0.42\n",
      "0.8338 | 0.7487 | 1.0090 | -0.2603 | -0.31\n",
      "0.8353 | 0.4204 | 0.9895 | -0.5691 | -0.68\n",
      "0.8486 | 0.3425 | 0.8486 | -0.5061 | -0.60\n",
      "-0.8495 | 1.1056 | 0.7253 | 0.3803 | -0.45\n",
      "-0.8530 | 1.0213 | 0.8724 | 0.1489 | -0.17\n",
      "0.8541 | 0.6795 | 1.5019 | -0.8224 | -0.96\n",
      "0.8554 | 0.7172 | 1.3824 | -0.6652 | -0.78\n",
      "0.8572 | 0.9089 | 1.5562 | -0.6473 | -0.76\n",
      "-0.8582 | 0.4284 | 0.1519 | 0.2765 | -0.32\n",
      "0.8601 | 1.3922 | 2.1323 | -0.7401 | -0.86\n",
      "0.8629 | 0.5675 | 1.2034 | -0.6358 | -0.74\n",
      "0.8631 | 0.8959 | 1.5717 | -0.6758 | -0.78\n",
      "0.8644 | 0.3559 | 1.1751 | -0.8192 | -0.95\n",
      "0.8705 | 0.6710 | 1.0491 | -0.3781 | -0.43\n",
      "-0.8720 | 0.5656 | 0.1487 | 0.4169 | -0.48\n",
      "-0.8731 | 0.9748 | 0.5155 | 0.4594 | -0.53\n",
      "-0.8832 | 0.4774 | 0.0000 | 0.4774 | -0.54\n",
      "-0.8840 | 1.4588 | 0.8479 | 0.6109 | -0.69\n",
      "0.8849 | 1.0415 | 1.9115 | -0.8700 | -0.98\n",
      "0.8863 | 0.6549 | 1.0405 | -0.3856 | -0.44\n",
      "0.8951 | 0.9242 | 1.5175 | -0.5933 | -0.66\n",
      "-0.8969 | 0.4617 | 0.2979 | 0.1638 | -0.18\n",
      "0.9061 | 0.8441 | 1.3918 | -0.5477 | -0.60\n",
      "-0.9089 | 0.6984 | 0.9867 | -0.2884 | 0.32\n",
      "0.9098 | 0.2380 | 0.9098 | -0.6718 | -0.74\n",
      "0.9152 | 0.7753 | 1.0905 | -0.3152 | -0.34\n",
      "0.9244 | 0.5740 | 1.0655 | -0.4915 | -0.53\n",
      "0.9256 | 1.2526 | 1.8359 | -0.5833 | -0.63\n",
      "-0.9258 | 1.3677 | 0.8306 | 0.5371 | -0.58\n",
      "0.9286 | 0.6338 | 1.2504 | -0.6166 | -0.66\n",
      "-0.9532 | 0.9633 | 0.3268 | 0.6365 | -0.67\n",
      "0.9661 | 0.7428 | 1.6294 | -0.8865 | -0.92\n",
      "-0.9668 | 0.2473 | 0.0000 | 0.2473 | -0.26\n",
      "0.9744 | 1.1176 | 1.8579 | -0.7403 | -0.76\n",
      "0.9746 | 0.8784 | 1.8343 | -0.9559 | -0.98\n",
      "-0.9841 | 0.6089 | 0.1691 | 0.4398 | -0.45\n",
      "0.9870 | 0.8291 | 1.5156 | -0.6865 | -0.70\n",
      "0.9960 | 0.5003 | 0.9960 | -0.4957 | -0.50\n",
      "1.0105 | 0.5784 | 1.1647 | -0.5863 | -0.58\n",
      "1.0474 | 0.3394 | 1.0474 | -0.7079 | -0.68\n",
      "-1.0765 | 1.0515 | 0.3214 | 0.7300 | -0.68\n",
      "1.0803 | 0.7046 | 1.5248 | -0.8201 | -0.76\n",
      "1.1011 | 1.0092 | 1.7784 | -0.7692 | -0.70\n",
      "1.1016 | 0.3171 | 1.1016 | -0.7844 | -0.71\n",
      "-1.7307 | 1.4680 | 0.0000 | 1.4680 | -0.85\n"
     ]
    }
   ],
   "source": [
    "## most of these are either turning on (true change == true) or off (true == 0) \n",
    "print(f'True Change | Pred | True | Error | How much of change missed')\n",
    "for i in top_50_idx:\n",
    "    print(f'{t_mean_delta[i]:.4f} | {p_mean[i]:.4f} | {t_mean[i]:.4f} | {(p_mean[i] - t_mean[i]):.4f} | {((p_mean[i] - t_mean[i])/(t_mean_delta[i])):.2f}')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
