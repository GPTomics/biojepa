5. 


## 7. Future Directions: In Silico Biology

1. Expand our validation suite to other tasks to better represent the flexibility of our model. 
2. Expand our "unseen" validation to fully excluded cell lines and perturbations to highlight the generalization capabilities of the model. 
3. Expand our dataset to include different cell lines, different perturbation types/modalities, and more cell state measurements.

## Works Cited

[1] Replogle, J. M., Saunders, R. A., Pogson, A. N., et al. (2022). *Mapping information-rich genotype–phenotype landscapes with genome-scale Perturb-seq.* Cell. DOI: 10.1016/j.cell.2022.05.013.

[2] LeCun, Y. (2022). *A Path Towards Autonomous Machine Intelligence.* arXiv:2205.12723.

[3] *PerturbNet predicts single-cell responses to unseen chemical and genetic perturbations.* Molecular Systems Biology (2025). DOI: 10.1038/s44320-025-00131-3.

[4] *Scouter predicts transcriptional responses to genetic perturbations with large language model embeddings.* Nature Computational Science (2025). DOI: 10.1038/s43588-025-00912-8.

[5] *V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction, and Planning.* arXiv:2506.09985 (2025).

[6] Bardes, A., Ponce, J., Sharma, H., LeCun, Y., & Misra, I. (2021). *VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning.* arXiv:2105.04906.

[7] van den Oord, A., Li, Y., & Vinyals, O. (2018). *Representation Learning with Contrastive Predictive Coding.* arXiv:1807.03748.

[8] Lotfollahi, M., Naghipourfar, M., Theis, F. J., & Wolf, F. A. (2023). *Predicting cellular responses to complex perturbations in high-throughput screens: a comparative study of models and datasets.* Molecular Systems Biology. DOI: 10.15252/msb.202211517.

[9] *Predicting transcriptional outcomes of novel multigene perturbations with graph neural networks (GEARS).* Nature Biotechnology (2024). DOI: 10.1038/s41587-023-01905-6.

[10] *scGPT: Towards building a foundation model for single-cell multi-omics using generative pre-training.* Nature Methods (2024). DOI: 10.1038/s41592-024-02201-0.

[11] Theodoris, C. V., Xiao, L., Chopra, A., et al. (2023). *Transfer learning enables predictions in network biology.* Nature. DOI: 10.1038/s41586-023-06139-9.

[12] *STATE: Generalizable representation learning for perturbation response prediction.* bioRxiv (2025). DOI: 10.1101/2025.06.26.661135.

[13] *STACK: Scaling up perturbation response prediction with compositional prompting.* bioRxiv (2026). DOI: 10.64898/2026.01.09.698608.

[14] Grill, J.-B., Strub, F., Altché, F., et al. (2020). *Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning.* arXiv:2006.07733.

[15] He, K., Chen, X., Xie, S., et al. (2021). *Masked Autoencoders Are Scalable Vision Learners.* arXiv:2111.06377.

[16] Katharopoulos, A., Vyas, A., Pappas, N., & Fleuret, F. (2020). *Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention.* arXiv:2006.16236.

[17] Perez, E., Strub, F., de Vries, H., Dumoulin, V., & Courville, A. (2017). *FiLM: Visual Reasoning with a General Conditioning Layer.* arXiv:1709.07871.

[18] *The Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics.* Nature Methods (2024). DOI: 10.1038/s41592-024-02523-z.

[19] *Evolutionary-scale prediction of atomic-level protein structure with a language model.* Science (2023). DOI: 10.1126/science.ade2574.
