{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e33d7757-9243-4c30-8f6f-a8643e5b4fe2",
   "metadata": {},
   "source": [
    "# v0.3 Updates\n",
    "Merging multiple datasets together. Perturbations were already turned into embeddings. Since we're merging multiple datasets together, we have to handle the fact that some datasets are missing expression counts for some of the genes in our map and so we need to handle the difference between missing and 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38ae8c96-009d-404f-bdf7-6f62c85bde41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ff95ef8-023e-4411-a8cd-2e6d0cef9e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('/Users/djemec/data/jepa/v0_3')\n",
    "raw_dir = data_dir / 'raw_files'\n",
    "pert_dir = data_dir / 'pert_embd'\n",
    "training_dir = data_dir / 'training'\n",
    "pretraining_dir = data_dir / 'pretraining'\n",
    "splits= ['train','val','test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22063d76-75dc-4a95-bee2-f1454dd0931b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets= {\n",
    "    'rep1e':raw_dir /'rep1e'/'perturb_processed.h5ad',\n",
    "    'adamson':raw_dir /'adamson'/'perturb_processed.h5ad',\n",
    "    'k562gw':raw_dir /'k562gw'/'replogle_k562_gw_expanded_8k.h5ad',\n",
    "    'k562e':raw_dir / 'k562e' / 'ReplogleWeissman2022_K562_essential.h5ad',\n",
    "}\n",
    "big_ds = ['k562gw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6007b51f-a56a-4a06-a48b-39af29f89c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 10000\n",
    "pt_chunk_size = 15000\n",
    "count_normalize_target = 1e4\n",
    "val_split_pct = 0.05\n",
    "train_cells = 0\n",
    "test_cells = 0\n",
    "val_cells = 0\n",
    "pt_train_cells = 0\n",
    "pt_val_cells = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b788e7-4637-46b0-908d-a0e68ec0c34e",
   "metadata": {},
   "source": [
    "**Load genes/perturbations previously identified**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c7dce90-b9d3-48a8-939e-9c84db844bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8192"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(raw_dir / 'gene_and_perts.json', 'r') as f:\n",
    "    meta = json.load(f)\n",
    "    all_genes = meta['all_genes']\n",
    "    hold_out_perts = set(meta['hold_out_perts'])\n",
    "\n",
    "n_genes = len(all_genes)\n",
    "n_genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0aa7a9fc-4fe9-4b21-a3e1-3736d8c255ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9876, 272)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(pert_dir / 'pert_to_id.json', 'r') as f:\n",
    "    pert_to_id = json.load(f)\n",
    "n_perts = len(pert_to_id.keys())\n",
    "n_perts, len(hold_out_perts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be2e04a-79ef-42fe-90d6-6b725f182aea",
   "metadata": {},
   "source": [
    "**Create All Gene Index Map**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b23fe58-9ea3-4193-bf68-62a98516bcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_to_id = {g: i for i, g in enumerate(all_genes)}\n",
    "n_genes = len(all_genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0797b281-8ef1-4ca5-95b7-01f3242df85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_gears_name(name):\n",
    "    # GEARS format is 'Gene+ctrl' -> We want 'Gene'\n",
    "    if name.endswith('+ctrl'):\n",
    "        return name.replace('+ctrl', '')\n",
    "    if name.startswith('ctrl+'):\n",
    "        return name.replace('ctrl+', '')\n",
    "    elif name == 'ctrl':\n",
    "        return 'control'\n",
    "    return str.strip(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578f6fa0-17e5-4d85-883a-628bf9b294dd",
   "metadata": {},
   "source": [
    "## Save file shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00eecc6c-9bd3-4763-8a66-1d50cf3949c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_shard_split(ds_key, split_name, indices, adata, ctrl_bank_X, ctrl_bank_totals, \n",
    "                     local_indices, global_indices, dataset_valid_mask,\n",
    "                     batch_col, batch_to_ctrl_indices, condition_col):\n",
    "    if len(indices) == 0:\n",
    "        return\n",
    "    \n",
    "    save_path = training_dir / split_name\n",
    "\n",
    "    indices = np.sort(indices)\n",
    "    saved_mask = dataset_valid_mask.astype(np.int8)\n",
    "    meta_conds = adata.obs[condition_col].values\n",
    "    meta_sf = adata.obs['size_factor'].values\n",
    "    meta_log_tot = adata.obs['log_total_counts'].values\n",
    "    meta_batches = adata.obs[batch_col].values if batch_col else None\n",
    "\n",
    "    shard_count = 0\n",
    "    \n",
    "    for i in tqdm(range(0, len(indices), chunk_size), desc=f'Processing {split_name}'):\n",
    "        idx_chunk = indices[i : i + chunk_size]\n",
    "        current_batch_len = len(idx_chunk)\n",
    "\n",
    "        # 1. Vectorized Read & Norm (Case)\n",
    "        raw_batch = adata.X[idx_chunk]\n",
    "        if hasattr(raw_batch, 'toarray'):\n",
    "            raw_batch = raw_batch.toarray() # Convert sparse to dense immediately\n",
    "\n",
    "        sf_chunk = meta_sf[idx_chunk].reshape(-1, 1)\n",
    "        norm_batch = np.log1p(raw_batch * sf_chunk).astype(np.float32)\n",
    "\n",
    "        # 2. Map to Global\n",
    "        case_global = np.zeros((current_batch_len, n_genes), dtype=np.float32)\n",
    "        case_global[:, global_indices] = norm_batch[:, local_indices]\n",
    "\n",
    "        # 3. Metadata\n",
    "        cond_chunk = meta_conds[idx_chunk]\n",
    "        case_tot_chunk = meta_log_tot[idx_chunk].astype(np.float32)\n",
    "\n",
    "        # 4. Control Sampling\n",
    "        batch_ctrl_indices = np.zeros(current_batch_len, dtype=int)\n",
    "\n",
    "        if batch_col:\n",
    "            batch_id_chunk = meta_batches[idx_chunk]\n",
    "            unique_batches, inverse_indices = np.unique(batch_id_chunk, return_inverse=True)\n",
    "            \n",
    "            for b_idx, b_id in enumerate(unique_batches):\n",
    "                mask = (inverse_indices == b_idx)\n",
    "                count = np.count_nonzero(mask)\n",
    "                \n",
    "                # Get candidates\n",
    "                candidates = batch_to_ctrl_indices.get(b_id, [])\n",
    "                if len(candidates) > 0:\n",
    "                    choices = np.random.choice(candidates, size=count)\n",
    "                    batch_ctrl_indices[mask] = choices\n",
    "                else:\n",
    "                    batch_ctrl_indices[mask] = np.random.randint(len(ctrl_bank_X), size=count)\n",
    "        else:\n",
    "            batch_ctrl_indices = np.random.randint(len(ctrl_bank_X), size=current_batch_len)\n",
    "\n",
    "        ctrl_vecs = ctrl_bank_X[batch_ctrl_indices]\n",
    "        ctrl_tots = ctrl_bank_totals[batch_ctrl_indices]\n",
    "        act_ids = np.array([pert_to_id[clean_gears_name(c)] for c in cond_chunk], dtype=np.int16)\n",
    "\n",
    "        # shuffle then save\n",
    "        perm = np.random.permutation(current_batch_len)\n",
    "        \n",
    "        np.savez(\n",
    "            save_path / f'shard_{ds_key}_{split_name}_{shard_count:04d}.npz',\n",
    "            control=ctrl_vecs[perm],\n",
    "            control_total=ctrl_tots[perm],\n",
    "            case=case_global[perm],\n",
    "            case_total=case_tot_chunk[perm],\n",
    "            action_ids=act_ids[perm],\n",
    "            valid_mask=saved_mask\n",
    "        )\n",
    "        shard_count += 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce7fd08e-a34b-4200-87d6-4cc3eef38628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pretrain_shard(ds_key, split_name, indices, adata, \n",
    "                        local_indices, global_indices, dataset_valid_mask):\n",
    "    \n",
    "    save_path = pretraining_dir / split_name\n",
    "\n",
    "    indices = np.sort(indices)\n",
    "    saved_mask = dataset_valid_mask.astype(np.int8)\n",
    "    meta_sf = adata.obs['size_factor'].values\n",
    "    meta_log_tot = adata.obs['log_total_counts'].values\n",
    "\n",
    "    shard_count = 0\n",
    "    \n",
    "    for i in tqdm(range(0, len(indices), pt_chunk_size), desc=f'PT {split_name}'):\n",
    "        idx_chunk = indices[i : i + pt_chunk_size]\n",
    "        current_batch_len = len(idx_chunk)\n",
    "\n",
    "        # 1. Vectorized Read\n",
    "        raw_batch = adata.X[idx_chunk]\n",
    "        if hasattr(raw_batch, 'toarray'):\n",
    "            raw_batch = raw_batch.toarray()\n",
    "\n",
    "        # 2. Vectorized Norm\n",
    "        sf_chunk = meta_sf[idx_chunk].reshape(-1, 1)\n",
    "        norm_batch = np.log1p(raw_batch * sf_chunk).astype(np.float32)\n",
    "\n",
    "        # 3. Map to Global\n",
    "        x_global = np.zeros((current_batch_len, n_genes), dtype=np.float32)\n",
    "        x_global[:, global_indices] = norm_batch[:, local_indices]\n",
    "\n",
    "        # 4. Metadata\n",
    "        tot_chunk = meta_log_tot[idx_chunk].astype(np.float32)\n",
    "\n",
    "        # 5. shuffle and save\n",
    "        perm = np.random.permutation(current_batch_len)\n",
    "        \n",
    "        np.savez(\n",
    "            save_path / f'pt_shard_{ds_key}_{split_name}_{shard_count:04d}.npz',\n",
    "            x=x_global[perm],\n",
    "            total=tot_chunk[perm],\n",
    "            valid_mask=saved_mask\n",
    "        )\n",
    "        shard_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd45360-cfbe-466b-b13b-7876598dec94",
   "metadata": {},
   "source": [
    "## Process datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f2bbe92-b8d7-44fb-9ca6-d899b69caa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(ds_key, ds_path):\n",
    "    print(f'Processing {ds_key}')\n",
    "\n",
    "    # load dataset\n",
    "    if ds_key in big_ds:\n",
    "        adata = sc.read_h5ad(ds_path, backed='r')\n",
    "        total_counts = []\n",
    "        n_genes_per_cell = []\n",
    "        batched_size = chunk_size\n",
    "        \n",
    "        for i in tqdm(range(0, adata.n_obs, batched_size), desc=\"Metrics\"):\n",
    "            chunk = adata.X[i:i+batched_size]\n",
    "            \n",
    "            # Sum\n",
    "            batch_sum = chunk.sum(axis=1)\n",
    "            if hasattr(batch_sum, 'A1'): \n",
    "                batch_sum = batch_sum.A1 \n",
    "            else: \n",
    "                batch_sum = np.array(batch_sum).flatten()\n",
    "            total_counts.append(batch_sum)\n",
    "\n",
    "            # Count Genes\n",
    "            batch_genes = (chunk > 0).sum(axis=1)\n",
    "            if hasattr(batch_genes, 'A1'): \n",
    "                batch_genes = batch_genes.A1\n",
    "            else: \n",
    "                batch_genes = np.array(batch_genes).flatten()\n",
    "            n_genes_per_cell.append(batch_genes)\n",
    "            \n",
    "        total_counts = np.concatenate(total_counts)\n",
    "        n_genes_per_cell = np.concatenate(n_genes_per_cell)\n",
    "        \n",
    "    else:\n",
    "        adata = sc.read_h5ad(ds_path)\n",
    "        if hasattr(adata.X, 'tocsr'):\n",
    "            print('Converting to CSR for fast row-slicing')\n",
    "            adata.X = adata.X.tocsr()\n",
    "        \n",
    "        raw_sums = adata.X.sum(axis=1)\n",
    "        if hasattr(raw_sums, 'A1'):\n",
    "            total_counts = raw_sums.A1 \n",
    "        else:\n",
    "            total_counts = np.array(raw_sums).flatten()\n",
    "\n",
    "        # 2. N Genes (Count Non-Zero)\n",
    "        # For sparse matrices, getting nnz per row is extremely fast\n",
    "        if hasattr(adata.X, 'getnnz'):\n",
    "            n_genes_per_cell = adata.X.getnnz(axis=1)\n",
    "        else:\n",
    "            # Dense fallback\n",
    "            n_genes_per_cell = np.count_nonzero(adata.X, axis=1)\n",
    "    #print(adata.var.columns)\n",
    "    #print(adata.obs.columns)\n",
    "    \n",
    "    # Preprocessing\n",
    "    \n",
    "    adata.obs['total_counts'] = total_counts\n",
    "    adata.obs['n_genes'] = n_genes_per_cell\n",
    "\n",
    "    # Calculate Factors (Allowed on parent object)\n",
    "    adata.obs['log_total_counts'] = np.log1p(adata.obs['total_counts'])\n",
    "    adata.obs['size_factor'] = count_normalize_target / adata.obs['total_counts']\n",
    "\n",
    "    # Filter (Creates View)\n",
    "    adata = adata[adata.obs['n_genes'] >= 200]\n",
    "    \n",
    "    # Global Alignment\n",
    "    if 'gene_name' in adata.var.columns:\n",
    "        local_genes = adata.var.gene_name.tolist()\n",
    "    else:\n",
    "        local_genes = adata.var_names.tolist()\n",
    "        \n",
    "    print(f'genes {local_genes[:5]}')\n",
    "    valid_map = []\n",
    "    dataset_valid_mask = np.zeros(n_genes, dtype=np.float32)\n",
    "\n",
    "    for local_i, gene in enumerate(local_genes):\n",
    "        if gene in gene_to_id:\n",
    "            global_i = gene_to_id[gene]\n",
    "            valid_map.append((local_i, global_i))\n",
    "            dataset_valid_mask[global_i] = 1.0\n",
    "            \n",
    "    print(f'overlapping genes {len(valid_map)} | skipped genes {len(local_genes) - len(valid_map)}')\n",
    "\n",
    "    local_indices, global_indices = zip(*valid_map)\n",
    "    local_indices = np.array(local_indices)\n",
    "    global_indices = np.array(global_indices)\n",
    "\n",
    "    # Handle Batch & Conditions\n",
    "    #print(list(adata.obs.columns))\n",
    "    batch_col = None\n",
    "    candidates = ['batch', 'batch_id', 'gem_group', 'sequencing_batch']\n",
    "    for col in candidates:\n",
    "        if col in adata.obs.columns:\n",
    "            batch_col = col\n",
    "            print(f'Batch column: {batch_col}')\n",
    "            break\n",
    "    \n",
    "    if not batch_col:\n",
    "        print('No batch column. using global pool')\n",
    "\n",
    "    condition_col = None\n",
    "    cond_candidates = ['perturbation','condition']\n",
    "    for col in cond_candidates:\n",
    "        if col in adata.obs.columns:\n",
    "            condition_col = col\n",
    "            print(f'Condition column: {condition_col}')\n",
    "            break\n",
    "\n",
    "    # Build Normalized Control Bank\n",
    "    cntrl_names = ['ctrl', 'control', 'non-targeting']\n",
    "    is_ctrl = pd.Series(adata.obs[condition_col].isin(cntrl_names))\n",
    "    ctrl_indices = np.where(is_ctrl)[0]\n",
    "\n",
    "    if len(ctrl_indices) == 0:\n",
    "        print('!!!!!no control')\n",
    "    \n",
    "    raw_ctrls = adata.X[ctrl_indices]\n",
    "    if hasattr(raw_ctrls, 'toarray'):\n",
    "        raw_ctrls = raw_ctrls.toarray()\n",
    "\n",
    "    ctrl_sf = adata.obs['size_factor'].iloc[ctrl_indices].values.reshape(-1, 1)\n",
    "    norm_ctrls = np.log1p(raw_ctrls * ctrl_sf).astype(np.float32)\n",
    "    \n",
    "    ctrl_bank_X = np.zeros((len(ctrl_indices), n_genes), dtype=np.float32)\n",
    "    ctrl_bank_X[:, global_indices] = norm_ctrls[:, local_indices]\n",
    "    ctrl_bank_totals = adata.obs['log_total_counts'].iloc[ctrl_indices].values.astype(np.float32)\n",
    "\n",
    "    batch_to_ctrl_indices = {}\n",
    "    if batch_col:\n",
    "        ctrl_batches = adata.obs[batch_col].iloc[ctrl_indices].values\n",
    "        for i, batch_id in enumerate(ctrl_batches):\n",
    "            if batch_id not in batch_to_ctrl_indices:\n",
    "                batch_to_ctrl_indices[batch_id] = []\n",
    "            batch_to_ctrl_indices[batch_id].append(i)\n",
    "\n",
    "    # Categorize Cells\n",
    "    conditions = pd.Series(adata.obs[condition_col].values)\n",
    "\n",
    "    train_pool_indices = []\n",
    "    test_indices = []\n",
    "    pt_pool_indices = []\n",
    "    stats = {'hold_out': 0, 'unknown': 0, 'train_pool': 0}\n",
    "\n",
    "    for idx in range(len(adata)):\n",
    "        if is_ctrl.iloc[idx]: \n",
    "            pt_pool_indices.append(idx)\n",
    "            continue\n",
    "            \n",
    "        clean_cond = clean_gears_name(conditions.iloc[idx])\n",
    "        if clean_cond not in pert_to_id:\n",
    "            stats['unknown'] += 1\n",
    "            pt_pool_indices.append(idx)\n",
    "            continue\n",
    "            \n",
    "        if clean_cond in hold_out_perts:\n",
    "            test_indices.append(idx)\n",
    "            stats['hold_out'] += 1\n",
    "        else:\n",
    "            train_pool_indices.append(idx)\n",
    "            pt_pool_indices.append(idx)\n",
    "            stats['train_pool'] += 1\n",
    "    print(f'Trainable: {stats['train_pool']} | Test {stats['hold_out']} | Skipped {stats['unknown']}')\n",
    "    print(f'Pretraining: {len(pt_pool_indices)}')\n",
    "\n",
    "    # set train and val splits\n",
    "    np.random.shuffle(train_pool_indices)\n",
    "    num_val = int(len(train_pool_indices) * val_split_pct)\n",
    "    \n",
    "    val_indices = train_pool_indices[:num_val]\n",
    "    train_indices = train_pool_indices[num_val:]\n",
    "\n",
    "    save_shard_split(ds_key, 'train', train_indices, adata, ctrl_bank_X, ctrl_bank_totals,\n",
    "                     local_indices, global_indices, dataset_valid_mask, \n",
    "                     batch_col, batch_to_ctrl_indices, condition_col)\n",
    "    \n",
    "    save_shard_split(ds_key, 'val', val_indices, adata, ctrl_bank_X, ctrl_bank_totals,\n",
    "                     local_indices, global_indices, dataset_valid_mask, \n",
    "                     batch_col, batch_to_ctrl_indices, condition_col)\n",
    "    \n",
    "    save_shard_split(ds_key, 'test', test_indices, adata, ctrl_bank_X, ctrl_bank_totals,\n",
    "                     local_indices, global_indices, dataset_valid_mask, \n",
    "                     batch_col, batch_to_ctrl_indices, condition_col)\n",
    "\n",
    "    # set and save pretraining\n",
    "    np.random.shuffle(pt_pool_indices)\n",
    "    \n",
    "    # We can use the same Validation Split Percentage\n",
    "    num_pt_val = int(len(pt_pool_indices) * val_split_pct)\n",
    "    \n",
    "    pt_val_indices = pt_pool_indices[:num_pt_val]\n",
    "    pt_train_indices = pt_pool_indices[num_pt_val:]\n",
    "    \n",
    "    save_pretrain_shard(ds_key, 'train', pt_train_indices, adata, \n",
    "                        local_indices, global_indices, dataset_valid_mask)\n",
    "    \n",
    "    save_pretrain_shard(ds_key, 'val', pt_val_indices, adata, \n",
    "                        local_indices, global_indices, dataset_valid_mask)\n",
    "\n",
    "\n",
    "    del adata\n",
    "    del ctrl_bank_X\n",
    "    gc.collect()\n",
    "    \n",
    "    print('\\n')\n",
    "    return len(train_indices), len(val_indices), len(test_indices), len(pt_train_indices), len(pt_val_indices)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41ba957-7470-493c-9c79-e471a730ebf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing rep1e\n",
      "Converting to CSR for fast row-slicing\n",
      "genes ['PLEKHN1', 'HES4', 'ISG15', 'AGRN', 'B3GALT6']\n",
      "overlapping genes 3739 | skipped genes 1261\n",
      "No batch column. using global pool\n",
      "Condition column: condition\n",
      "Trainable: 133225 | Test 18023 | Skipped 0\n",
      "Pretraining: 144710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train: 100%|█████████████████████████████████████████████████████████| 13/13 [00:12<00:00,  1.07it/s]\n",
      "Processing val: 100%|█████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
      "Processing test: 100%|████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.00s/it]\n",
      "PT train: 100%|█████████████████████████████████████████████████████████████████| 10/10 [00:11<00:00,  1.12s/it]\n",
      "PT val: 100%|█████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Processing adamson\n",
      "Converting to CSR for fast row-slicing\n",
      "genes ['AP006222.2', 'RP11-54O7.16', 'RP11-54O7.1', 'RP11-54O7.3', 'SAMD11']\n",
      "overlapping genes 2089 | skipped genes 2971\n",
      "No batch column. using global pool\n",
      "Condition column: condition\n",
      "Trainable: 39462 | Test 4878 | Skipped 0\n",
      "Pretraining: 63721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train: 100%|███████████████████████████████████████████████████████████| 4/4 [00:02<00:00,  1.66it/s]\n",
      "Processing val: 100%|█████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.41it/s]\n",
      "Processing test: 100%|████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.16it/s]\n",
      "PT train: 100%|███████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  1.69it/s]\n",
      "PT val: 100%|█████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Processing k562gw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metrics: 100%|████████████████████████████████████████████████████████████████| 195/195 [00:39<00:00,  4.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genes ['A1BG', 'AAAS', 'AACS', 'AAGAB', 'AAK1']\n",
      "overlapping genes 8192 | skipped genes 0\n",
      "Batch column: batch\n",
      "Condition column: perturbation\n"
     ]
    }
   ],
   "source": [
    "for key, path in list(datasets.items()):\n",
    "    c_tr, c_val, c_tes, pt_t, pt_v = process_dataset(key, path)\n",
    "    train_cells += c_tr\n",
    "    val_cells +=  c_val\n",
    "    test_cells += c_tes\n",
    "    pt_train_cells += pt_t\n",
    "    pt_val_cells +=  pt_v\n",
    "    \n",
    "train_cells, val_cells, test_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c008e15-d03e-4a12-8345-e8566ac50dca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
