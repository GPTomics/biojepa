{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecde55a-b2ae-48c4-b33b-81e327eb2c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import pearsonr\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "\n",
    "import biojepa_ac_model as model\n",
    "from bio_dataloader import PretrainLoader, TrainingLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f002c8c-dc2a-45ee-83c4-334648fcf033",
   "metadata": {},
   "source": [
    "## BioJEPA Model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df05ee08-24ef-46b5-be25-9820a8304b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bcd644-b8eb-413f-8614-28ebda4ecd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e089507f-69f6-4883-aca7-43b361c9da21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    device = 'cpu'\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(1337)\n",
    "        device = 'cuda'\n",
    "    print(f'using {device}')\n",
    "    return device\n",
    "\n",
    "DEVICE = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efa224b-1884-4f79-b84c-d1518a1f54d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "random.seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79fa3b1-d191-40bc-90f1-fe2b4a62057d",
   "metadata": {},
   "source": [
    "## Training Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b646069-a51f-48c3-ac6c-d8fda740de7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('/Users/djemec/data/jepa/v0_2')\n",
    "train_dir = data_dir / 'training'\n",
    "pretrain_dir = data_dir / 'pretraining'\n",
    "checkpoint_dir = Path('/Users/djemec/data/jepa/v0_3') / 'checkpoints'\n",
    "pert_dir = data_dir / 'pert_embd'\n",
    "pert_embd_path = pert_dir / 'action_embeddings_esm2.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671d9f97-c31e-412c-a84a-b860ed561be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading Action Embedding ...')\n",
    "pert_embd = np.load(pert_embd_path)\n",
    "print(f'Bank Loaded. Shape: {pert_embd.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d668759-740c-4b86-913f-98817ef29a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n_embd = 8\n",
    "pt_epochs = 10\n",
    "training_file_chunk = 25000\n",
    "pretraining_file_chunk = 50000\n",
    "n_heads = 2\n",
    "n_layers = 2\n",
    "n_genes = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e53d0e-fb0c-41a8-82c1-b81c18afb03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high')\n",
    "config = model.BioJepaConfig(\n",
    "    num_genes = n_genes,\n",
    "    n_layer= n_layers,\n",
    "    heads= n_heads,\n",
    "    embed_dim = n_embd,\n",
    "    n_pre_layer= n_layers\n",
    ")\n",
    "model = model.BioJepa(config, pert_embd=pert_embd).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0160cf-2e8f-4e0a-bda0-2a5fe9c80b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Student/Teacher: {sum(p.numel() for p in model.student.parameters() if p.requires_grad)}')\n",
    "print(f'ACpredictor: {sum(p.numel() for p in model.predictor.parameters() if p.requires_grad)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978f697d-0c6e-4da1-bf29-5500fa68a893",
   "metadata": {},
   "source": [
    "### Pre-Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e3082c-515e-428e-94d4-8b281501f8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_train_loader = PretrainLoader(batch_size=batch_size, split='train', data_dir=pretrain_dir, device=DEVICE)\n",
    "pt_val_loader = PretrainLoader(batch_size=batch_size, split='val', data_dir=pretrain_dir, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41728e1e-e69e-4fb3-8f34-952e1608a5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_train_total = 112373\n",
    "pt_val_total = 11044\n",
    "steps_per_epoch = pt_train_total // batch_size\n",
    "pt_max_steps = pt_epochs * steps_per_epoch\n",
    "steps_per_epoch, pt_max_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59997f1-43ca-4945-a881-4222f6f88b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_LR = 4e-3\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=pt_LR, weight_decay=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e4c635-c8eb-4ad7-8c06-b81677e9df88",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=pt_LR, total_steps=pt_max_steps, pct_start=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7ca9f7-1586-4978-b562-adfc325aa6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_lossi = []\n",
    "step = 0\n",
    "total_epoch_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329d10f4-c7a8-4ab8-9f18-cccd39b8c510",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "for step in range(pt_max_steps):\n",
    "    last_step = (step == pt_max_steps - 1)\n",
    "\n",
    "    # once in a while evaluate our validation set loss\n",
    "    if step % 100 == 0 or last_step:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss_accum = 0.0\n",
    "            val_loss_steps = 10\n",
    "            for i in range(val_loss_steps):\n",
    "                x_val, total_val = pt_val_loader.next_batch()\n",
    "\n",
    "                val_loss = model.forward_pretrain(x_val, total_val)\n",
    "                val_loss_accum += val_loss.item()\n",
    "\n",
    "            avg_val_loss = val_loss_accum / val_loss_steps\n",
    "            print(f'val loss: {avg_val_loss:.4f}')\n",
    "            \n",
    "        # with open(log_file, 'a') as f:\n",
    "        #    f.write(f'{step} test {test_loss_accum.item():.4f}\\n')\n",
    "        model.train()\n",
    "\n",
    "\n",
    "    if step > 0 and (step+1) % steps_per_epoch ==0 and not last_step:\n",
    "        # Save Checkpoint\n",
    "        torch.save({\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'step': step,\n",
    "            'config': config \n",
    "        }, checkpoint_dir / f'bio_jepa_pt_ckpt_{step}.pt')\n",
    "\n",
    "    # Pre-Training\n",
    "    x, total = pt_train_loader.next_batch()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss = model.forward_pretrain(x, total)\n",
    "    loss.backward()\n",
    "\n",
    "    # gradient clipping\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Update Teacher (V-JEPA Momentum)\n",
    "    model.update_teacher()\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    pt_lossi.append(loss.item())\n",
    "    total_epoch_loss += loss.item()\n",
    "    \n",
    "    if step % 25 == 0:\n",
    "        print(f'Step {step} | Loss: {loss.item():.5f} | LR: {scheduler.get_last_lr()[0]:.2e}')\n",
    "    \n",
    "    \n",
    "    if step > 0 and (step+1) % steps_per_epoch == 0:   \n",
    "        avg_loss = total_epoch_loss / steps_per_epoch\n",
    "        print(f'=== Step {step} Done. Avg Loss: {avg_loss:.5f} ===')\n",
    "        total_epoch_loss = 0.0\n",
    "    \n",
    "    \n",
    "    if last_step:\n",
    "        # Save Checkpoint\n",
    "        torch.save({\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'step': step,\n",
    "            'config': config \n",
    "        }, checkpoint_dir / f'bio_jepa_pt_ckpt_{step}_final.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd06e53-1e6b-4793-bb4d-9f2c69a6ce0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pt_lossi[:])\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12aea4d-8756-43a0-aee7-d5c3a76b8263",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715e6fd8-ba08-4544-ac66-cf5ffa312e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "LR = 1e-3\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f68d4d-0cbf-44d8-ba34-bd0aed833b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = TrainingLoader(batch_size=batch_size, split='train', data_dir=train_dir, device=DEVICE)\n",
    "val_loader = TrainingLoader(batch_size=batch_size, split='val', data_dir=train_dir, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec115e2d-684d-4d6d-a202-ea84db97a7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = 101682 // batch_size\n",
    "max_steps = epochs * steps_per_epoch\n",
    "steps_per_epoch, max_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad84847-ec01-49f7-bb05-28c49924a716",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6aab6d-046c-4bd1-a1f2-94a0099430ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=LR, total_steps=max_steps, pct_start=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8333d07-ddf0-428b-969d-bb46efdc27dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossi = []\n",
    "step = 0\n",
    "total_epoch_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c6ac49-44b7-4556-89ae-6c087721016a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze_encoders()\n",
    "model.train()\n",
    "\n",
    "for step in range(max_steps):\n",
    "\n",
    "    last_step = (step == pt_max_steps - 1)\n",
    "\n",
    "    # once in a while evaluate our validation set loss\n",
    "    if step % 100 == 0 or last_step:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss_accum = 0.0\n",
    "            val_loss_steps = 10\n",
    "            for i in range(val_loss_steps):\n",
    "                xc, xct, xt, xtt, aid = val_loader.next_batch()\n",
    "\n",
    "                val_loss = model(xc, xct, xt, xtt, aid)\n",
    "                val_loss_accum += val_loss.item()\n",
    "\n",
    "            avg_val_loss = val_loss_accum / val_loss_steps\n",
    "            print(f'val loss: {avg_val_loss:.4f}')\n",
    "\n",
    "        # with open(log_file, \"a\") as f:\n",
    "        #    f.write(f'{step} test {test_loss_accum.item():.4f}\\n')\n",
    "        model.train()\n",
    "\n",
    "\n",
    "    if step > 0 and (step+1) % steps_per_epoch ==0 and not last_step:\n",
    "        # Save Checkpoint\n",
    "        torch.save({\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'step': step,\n",
    "            'config': config \n",
    "        }, checkpoint_dir / f'bio_jepa_ckpt_{step}.pt')\n",
    "\n",
    "\n",
    "    # Get Batch (xc=Control, xt=Treated/Case)\n",
    "    xc, xct, xt, xtt, aid = train_loader.next_batch()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss = model(xc, xct, xt, xtt, aid)\n",
    "    loss.backward()\n",
    "\n",
    "    # gradient clipping\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    lossi.append(loss.item())\n",
    "    total_epoch_loss += loss.item()\n",
    "    \n",
    "    if step % 25 == 0:\n",
    "        print(f\"Step {step} | Loss: {loss.item():.5f} | LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "    \n",
    "    \n",
    "    if step > 0 and (step+1) % steps_per_epoch == 0:   \n",
    "        avg_loss = total_epoch_loss / steps_per_epoch\n",
    "        print(f\"=== Step {step} Done. Avg Loss: {avg_loss:.5f} ===\")\n",
    "        total_epoch_loss = 0\n",
    "    \n",
    "    \n",
    "    if last_step:\n",
    "        # Save Checkpoint\n",
    "        torch.save({\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'step': step,\n",
    "            'config': config \n",
    "        }, checkpoint_dir / f'bio_jepa_ckpt_{step}_final.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e5f01f-441d-44bc-95d2-a8c28eacbd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lossi[:])\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c86039a-d9fa-49f4-9ec2-888bd3ee666c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
