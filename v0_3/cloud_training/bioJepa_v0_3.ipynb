{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01f94ec1-568f-40cc-b663-5c4f4f01c054",
   "metadata": {},
   "source": [
    "# BioJEPA v0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ecde55a-b2ae-48c4-b33b-81e327eb2c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import pearsonr\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "import torch.serialization\n",
    "\n",
    "import biojepa_ac_model as model\n",
    "from bio_dataloader import PretrainLoader, TrainingLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f002c8c-dc2a-45ee-83c4-334648fcf033",
   "metadata": {},
   "source": [
    "## BioJEPA Model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df05ee08-24ef-46b5-be25-9820a8304b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jan  7 19:24:46 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GH200 480GB             On  |   00000000:DD:00.0 Off |                    0 |\n",
      "| N/A   34C    P0             89W /  700W |       6MiB /  97871MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78bcd644-b8eb-413f-8614-28ebda4ecd69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e089507f-69f6-4883-aca7-43b361c9da21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    device = 'cpu'\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(1337)\n",
    "        device = 'cuda'\n",
    "    print(f'using {device}')\n",
    "    return device\n",
    "\n",
    "DEVICE = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4efa224b-1884-4f79-b84c-d1518a1f54d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "random.seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79fa3b1-d191-40bc-90f1-fe2b4a62057d",
   "metadata": {},
   "source": [
    "## Training Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b646069-a51f-48c3-ac6c-d8fda740de7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('/home/ubuntu')\n",
    "train_dir = data_dir / 'training'\n",
    "pretrain_dir = data_dir / 'pretraining'\n",
    "checkpoint_dir = data_dir / 'checkpoint'\n",
    "pert_dir = data_dir / 'pert_embd'\n",
    "pert_embd_path = pert_dir / 'action_embeddings_esm2.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "671d9f97-c31e-412c-a84a-b860ed561be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Action Embedding ...\n",
      "Bank Loaded. Shape: (1087, 320)\n"
     ]
    }
   ],
   "source": [
    "print('Loading Action Embedding ...')\n",
    "pert_embd = np.load(pert_embd_path)\n",
    "print(f'Bank Loaded. Shape: {pert_embd.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d668759-740c-4b86-913f-98817ef29a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "n_embd = 256\n",
    "pt_epochs = 100\n",
    "training_file_chunk = 25000\n",
    "pretraining_file_chunk = 50000\n",
    "n_heads = 4\n",
    "n_layers = 6\n",
    "n_genes = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2e53d0e-fb0c-41a8-82c1-b81c18afb03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high')\n",
    "config = model.BioJepaConfig(\n",
    "    num_genes = n_genes,\n",
    "    n_layer= n_layers,\n",
    "    heads= n_heads,\n",
    "    embed_dim = n_embd,\n",
    "    n_pre_layer= n_layers\n",
    ")\n",
    "model = model.BioJepa(config, pert_embd=pert_embd).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d0160cf-2e8f-4e0a-bda0-2a5fe9c80b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student/Teacher: 6019840\n",
      "ACpredictor: 7881216\n"
     ]
    }
   ],
   "source": [
    "print(f'Student/Teacher: {sum(p.numel() for p in model.student.parameters() if p.requires_grad)}')\n",
    "print(f'ACpredictor: {sum(p.numel() for p in model.predictor.parameters() if p.requires_grad)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978f697d-0c6e-4da1-bf29-5500fa68a893",
   "metadata": {},
   "source": [
    "### Pre-Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73e3082c-515e-428e-94d4-8b281501f8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 2 shards for split train\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "found 1 shards for split val\n",
      "loading /home/ubuntu/pretraining/val/pt_shard_k562e_val_0000.npz\n"
     ]
    }
   ],
   "source": [
    "pt_train_loader = PretrainLoader(batch_size=batch_size, split='train', data_dir=pretrain_dir, device=DEVICE)\n",
    "pt_val_loader = PretrainLoader(batch_size=batch_size, split='val', data_dir=pretrain_dir, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41728e1e-e69e-4fb3-8f34-952e1608a5c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1755, 175500)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_train_total = 112373\n",
    "pt_val_total = 11044\n",
    "steps_per_epoch = pt_train_total // batch_size\n",
    "pt_max_steps = pt_epochs * steps_per_epoch\n",
    "steps_per_epoch, pt_max_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b59997f1-43ca-4945-a881-4222f6f88b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_LR = 1e-4\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=pt_LR, weight_decay=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85e4c635-c8eb-4ad7-8c06-b81677e9df88",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=pt_LR, total_steps=pt_max_steps, pct_start=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f7ca9f7-1586-4978-b562-adfc325aa6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_lossi = []\n",
    "step = 0\n",
    "total_epoch_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "329d10f4-c7a8-4ab8-9f18-cccd39b8c510",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss: 75.0255\n",
      "Step 0 | Loss: 75.02950 | LR: 4.00e-06\n",
      "Step 100 | Loss: 62.89892 | LR: 4.03e-06\n",
      "Step 200 | Loss: 57.78164 | LR: 4.12e-06\n",
      "val loss: 56.4037\n",
      "Step 300 | Loss: 55.36280 | LR: 4.28e-06\n",
      "Step 400 | Loss: 53.85463 | LR: 4.49e-06\n",
      "val loss: 52.7864\n",
      "Step 500 | Loss: 52.79113 | LR: 4.77e-06\n",
      "Step 600 | Loss: 51.90943 | LR: 5.11e-06\n",
      "Step 700 | Loss: 51.16814 | LR: 5.50e-06\n",
      "val loss: 50.8154\n",
      "Step 800 | Loss: 50.51602 | LR: 5.96e-06\n",
      "Step 900 | Loss: 50.04613 | LR: 6.48e-06\n",
      "val loss: 49.7251\n",
      "Step 1000 | Loss: 49.73059 | LR: 7.05e-06\n",
      "Step 1100 | Loss: 49.61456 | LR: 7.68e-06\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 1200 | Loss: 49.57229 | LR: 8.37e-06\n",
      "val loss: 49.5472\n",
      "Step 1300 | Loss: 49.53524 | LR: 9.11e-06\n",
      "Step 1400 | Loss: 49.54256 | LR: 9.91e-06\n",
      "val loss: 49.5475\n",
      "Step 1500 | Loss: 49.55655 | LR: 1.08e-05\n",
      "Step 1600 | Loss: 49.53616 | LR: 1.17e-05\n",
      "Step 1700 | Loss: 49.54992 | LR: 1.26e-05\n",
      "val loss: 49.5373\n",
      "=== Step 1754 Done. Avg Loss: 52.48167 ===\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 1800 | Loss: 49.51552 | LR: 1.36e-05\n",
      "Step 1900 | Loss: 49.52892 | LR: 1.47e-05\n",
      "val loss: 49.5382\n",
      "Step 2000 | Loss: 49.53253 | LR: 1.58e-05\n",
      "Step 2100 | Loss: 49.53967 | LR: 1.70e-05\n",
      "Step 2200 | Loss: 49.55518 | LR: 1.82e-05\n",
      "val loss: 49.5397\n",
      "Step 2300 | Loss: 49.55849 | LR: 1.94e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 2400 | Loss: 49.54751 | LR: 2.07e-05\n",
      "val loss: 49.5453\n",
      "Step 2500 | Loss: 49.52971 | LR: 2.20e-05\n",
      "Step 2600 | Loss: 49.54459 | LR: 2.34e-05\n",
      "Step 2700 | Loss: 49.49787 | LR: 2.48e-05\n",
      "val loss: 49.5343\n",
      "Step 2800 | Loss: 49.55068 | LR: 2.62e-05\n",
      "Step 2900 | Loss: 49.55106 | LR: 2.76e-05\n",
      "val loss: 49.5101\n",
      "Step 3000 | Loss: 49.52066 | LR: 2.91e-05\n",
      "Step 3100 | Loss: 49.55110 | LR: 3.07e-05\n",
      "Step 3200 | Loss: 49.50183 | LR: 3.22e-05\n",
      "val loss: 49.4611\n",
      "Step 3300 | Loss: 49.47309 | LR: 3.38e-05\n",
      "Step 3400 | Loss: 49.40900 | LR: 3.54e-05\n",
      "val loss: 49.2240\n",
      "Step 3500 | Loss: 49.25354 | LR: 3.70e-05\n",
      "=== Step 3509 Done. Avg Loss: 49.50845 ===\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 3600 | Loss: 48.68460 | LR: 3.87e-05\n",
      "Step 3700 | Loss: 44.66074 | LR: 4.03e-05\n",
      "val loss: 44.3064\n",
      "Step 3800 | Loss: 43.72731 | LR: 4.20e-05\n",
      "Step 3900 | Loss: 43.41886 | LR: 4.37e-05\n",
      "val loss: 42.4230\n",
      "Step 4000 | Loss: 42.18338 | LR: 4.54e-05\n",
      "Step 4100 | Loss: 40.95094 | LR: 4.71e-05\n",
      "Step 4200 | Loss: 40.61480 | LR: 4.88e-05\n",
      "loading /home/ubuntu/pretraining/val/pt_shard_k562e_val_0000.npz\n",
      "val loss: 41.3012\n",
      "Step 4300 | Loss: 40.05975 | LR: 5.05e-05\n",
      "Step 4400 | Loss: 40.12069 | LR: 5.22e-05\n",
      "val loss: 40.5156\n",
      "Step 4500 | Loss: 40.34272 | LR: 5.40e-05\n",
      "Step 4600 | Loss: 40.34730 | LR: 5.57e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 4700 | Loss: 39.99712 | LR: 5.74e-05\n",
      "val loss: 41.2743\n",
      "Step 4800 | Loss: 40.43935 | LR: 5.91e-05\n",
      "Step 4900 | Loss: 40.38467 | LR: 6.08e-05\n",
      "val loss: 40.6094\n",
      "Step 5000 | Loss: 40.30523 | LR: 6.25e-05\n",
      "Step 5100 | Loss: 40.45257 | LR: 6.41e-05\n",
      "Step 5200 | Loss: 41.02169 | LR: 6.58e-05\n",
      "val loss: 41.4244\n",
      "=== Step 5264 Done. Avg Loss: 42.11085 ===\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 5300 | Loss: 41.23854 | LR: 6.74e-05\n",
      "Step 5400 | Loss: 40.96412 | LR: 6.90e-05\n",
      "val loss: 40.1544\n",
      "Step 5500 | Loss: 39.85638 | LR: 7.06e-05\n",
      "Step 5600 | Loss: 40.11601 | LR: 7.22e-05\n",
      "Step 5700 | Loss: 39.92253 | LR: 7.38e-05\n",
      "val loss: 40.0371\n",
      "Step 5800 | Loss: 40.34469 | LR: 7.53e-05\n",
      "Step 5900 | Loss: 39.98957 | LR: 7.68e-05\n",
      "val loss: 40.8682\n",
      "Step 6000 | Loss: 40.58356 | LR: 7.82e-05\n",
      "Step 6100 | Loss: 40.01790 | LR: 7.96e-05\n",
      "Step 6200 | Loss: 40.13361 | LR: 8.10e-05\n",
      "val loss: 39.4920\n",
      "Step 6300 | Loss: 39.27776 | LR: 8.24e-05\n",
      "Step 6400 | Loss: 39.02985 | LR: 8.37e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "val loss: 38.6339\n",
      "Step 6500 | Loss: 38.58907 | LR: 8.50e-05\n",
      "Step 6600 | Loss: 38.33074 | LR: 8.62e-05\n",
      "Step 6700 | Loss: 37.99486 | LR: 8.74e-05\n",
      "val loss: 38.8323\n",
      "Step 6800 | Loss: 37.75324 | LR: 8.85e-05\n",
      "Step 6900 | Loss: 37.53211 | LR: 8.96e-05\n",
      "val loss: 36.9292\n",
      "Step 7000 | Loss: 36.63883 | LR: 9.06e-05\n",
      "=== Step 7019 Done. Avg Loss: 39.16112 ===\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 7100 | Loss: 36.75162 | LR: 9.16e-05\n",
      "Step 7200 | Loss: 34.97279 | LR: 9.26e-05\n",
      "val loss: 38.3032\n",
      "Step 7300 | Loss: 34.99388 | LR: 9.35e-05\n",
      "Step 7400 | Loss: 34.30703 | LR: 9.43e-05\n",
      "val loss: 34.2527\n",
      "Step 7500 | Loss: 34.47070 | LR: 9.51e-05\n",
      "Step 7600 | Loss: 34.06709 | LR: 9.58e-05\n",
      "Step 7700 | Loss: 34.08839 | LR: 9.65e-05\n",
      "val loss: 34.3765\n",
      "Step 7800 | Loss: 33.96782 | LR: 9.71e-05\n",
      "Step 7900 | Loss: 34.38102 | LR: 9.77e-05\n",
      "val loss: 33.8631\n",
      "Step 8000 | Loss: 33.72575 | LR: 9.82e-05\n",
      "Step 8100 | Loss: 33.89568 | LR: 9.86e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 8200 | Loss: 37.99606 | LR: 9.90e-05\n",
      "val loss: 33.6338\n",
      "Step 8300 | Loss: 33.89182 | LR: 9.93e-05\n",
      "Step 8400 | Loss: 33.04565 | LR: 9.96e-05\n",
      "loading /home/ubuntu/pretraining/val/pt_shard_k562e_val_0000.npz\n",
      "val loss: 33.0046\n",
      "Step 8500 | Loss: 33.28689 | LR: 9.98e-05\n",
      "Step 8600 | Loss: 33.96343 | LR: 9.99e-05\n",
      "Step 8700 | Loss: 33.78753 | LR: 1.00e-04\n",
      "val loss: 34.5267\n",
      "=== Step 8774 Done. Avg Loss: 34.61132 ===\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 8800 | Loss: 34.21083 | LR: 1.00e-04\n",
      "Step 8900 | Loss: 33.37902 | LR: 1.00e-04\n",
      "val loss: 33.5802\n",
      "Step 9000 | Loss: 32.55028 | LR: 1.00e-04\n",
      "Step 9100 | Loss: 39.68442 | LR: 1.00e-04\n",
      "Step 9200 | Loss: 32.96922 | LR: 1.00e-04\n",
      "val loss: 33.3392\n",
      "Step 9300 | Loss: 32.61991 | LR: 1.00e-04\n",
      "Step 9400 | Loss: 34.59444 | LR: 1.00e-04\n",
      "val loss: 33.1346\n",
      "Step 9500 | Loss: 32.64061 | LR: 1.00e-04\n",
      "Step 9600 | Loss: 35.78777 | LR: 1.00e-04\n",
      "Step 9700 | Loss: 35.31264 | LR: 1.00e-04\n",
      "val loss: 35.7905\n",
      "Step 9800 | Loss: 34.05868 | LR: 1.00e-04\n",
      "Step 9900 | Loss: 34.55902 | LR: 1.00e-04\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "val loss: 37.7353\n",
      "Step 10000 | Loss: 36.88309 | LR: 1.00e-04\n",
      "Step 10100 | Loss: 34.06287 | LR: 1.00e-04\n",
      "Step 10200 | Loss: 33.19763 | LR: 1.00e-04\n",
      "val loss: 37.2742\n",
      "Step 10300 | Loss: 32.87601 | LR: 1.00e-04\n",
      "Step 10400 | Loss: 32.57138 | LR: 1.00e-04\n",
      "val loss: 32.7084\n",
      "Step 10500 | Loss: 32.93921 | LR: 1.00e-04\n",
      "=== Step 10529 Done. Avg Loss: 34.18868 ===\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 10600 | Loss: 32.89525 | LR: 1.00e-04\n",
      "Step 10700 | Loss: 31.04140 | LR: 1.00e-04\n",
      "val loss: 30.5517\n",
      "Step 10800 | Loss: 30.94918 | LR: 1.00e-04\n",
      "Step 10900 | Loss: 30.48664 | LR: 1.00e-04\n",
      "val loss: 30.3109\n",
      "Step 11000 | Loss: 30.79689 | LR: 1.00e-04\n",
      "Step 11100 | Loss: 30.67954 | LR: 1.00e-04\n",
      "Step 11200 | Loss: 30.60160 | LR: 9.99e-05\n",
      "val loss: 30.6504\n",
      "Step 11300 | Loss: 29.92824 | LR: 9.99e-05\n",
      "Step 11400 | Loss: 30.74369 | LR: 9.99e-05\n",
      "val loss: 30.4713\n",
      "Step 11500 | Loss: 31.31753 | LR: 9.99e-05\n",
      "Step 11600 | Loss: 30.49336 | LR: 9.99e-05\n",
      "Step 11700 | Loss: 30.98113 | LR: 9.99e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "val loss: 30.8767\n",
      "Step 11800 | Loss: 30.85625 | LR: 9.99e-05\n",
      "Step 11900 | Loss: 30.59597 | LR: 9.99e-05\n",
      "val loss: 30.8082\n",
      "Step 12000 | Loss: 31.66925 | LR: 9.99e-05\n",
      "Step 12100 | Loss: 30.97205 | LR: 9.99e-05\n",
      "Step 12200 | Loss: 30.68117 | LR: 9.99e-05\n",
      "val loss: 31.1043\n",
      "=== Step 12284 Done. Avg Loss: 31.52423 ===\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 12300 | Loss: 29.94143 | LR: 9.99e-05\n",
      "Step 12400 | Loss: 30.39667 | LR: 9.99e-05\n",
      "val loss: 31.1100\n",
      "Step 12500 | Loss: 30.64877 | LR: 9.99e-05\n",
      "Step 12600 | Loss: 30.67177 | LR: 9.99e-05\n",
      "Step 12700 | Loss: 31.61738 | LR: 9.99e-05\n",
      "loading /home/ubuntu/pretraining/val/pt_shard_k562e_val_0000.npz\n",
      "val loss: 30.7831\n",
      "Step 12800 | Loss: 29.67456 | LR: 9.99e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 12900 | Loss: 31.09067 | LR: 9.98e-05\n",
      "val loss: 30.3519\n",
      "Step 13000 | Loss: 30.49361 | LR: 9.98e-05\n",
      "Step 13100 | Loss: 31.16044 | LR: 9.98e-05\n",
      "Step 13200 | Loss: 31.11993 | LR: 9.98e-05\n",
      "val loss: 30.2845\n",
      "Step 13300 | Loss: 29.99028 | LR: 9.98e-05\n",
      "Step 13400 | Loss: 32.05477 | LR: 9.98e-05\n",
      "val loss: 30.1855\n",
      "Step 13500 | Loss: 30.28291 | LR: 9.98e-05\n",
      "Step 13600 | Loss: 29.32821 | LR: 9.98e-05\n",
      "Step 13700 | Loss: 32.35511 | LR: 9.98e-05\n",
      "val loss: 30.5540\n",
      "Step 13800 | Loss: 30.15983 | LR: 9.98e-05\n",
      "Step 13900 | Loss: 30.31740 | LR: 9.98e-05\n",
      "val loss: 29.9892\n",
      "Step 14000 | Loss: 29.46196 | LR: 9.98e-05\n",
      "=== Step 14039 Done. Avg Loss: 31.26881 ===\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 14100 | Loss: 31.89693 | LR: 9.97e-05\n",
      "Step 14200 | Loss: 29.91331 | LR: 9.97e-05\n",
      "val loss: 30.1971\n",
      "Step 14300 | Loss: 29.50303 | LR: 9.97e-05\n",
      "Step 14400 | Loss: 30.01988 | LR: 9.97e-05\n",
      "val loss: 29.7594\n",
      "Step 14500 | Loss: 30.02951 | LR: 9.97e-05\n",
      "Step 14600 | Loss: 29.72518 | LR: 9.97e-05\n",
      "Step 14700 | Loss: 29.57054 | LR: 9.97e-05\n",
      "val loss: 29.6845\n",
      "Step 14800 | Loss: 30.51323 | LR: 9.97e-05\n",
      "Step 14900 | Loss: 29.08890 | LR: 9.97e-05\n",
      "val loss: 29.7709\n",
      "Step 15000 | Loss: 30.25738 | LR: 9.97e-05\n",
      "Step 15100 | Loss: 28.56737 | LR: 9.96e-05\n",
      "Step 15200 | Loss: 29.69259 | LR: 9.96e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "val loss: 29.7957\n",
      "Step 15300 | Loss: 28.70242 | LR: 9.96e-05\n",
      "Step 15400 | Loss: 29.09149 | LR: 9.96e-05\n",
      "val loss: 29.5669\n",
      "Step 15500 | Loss: 28.79291 | LR: 9.96e-05\n",
      "Step 15600 | Loss: 31.22751 | LR: 9.96e-05\n",
      "Step 15700 | Loss: 28.32055 | LR: 9.96e-05\n",
      "val loss: 29.1685\n",
      "=== Step 15794 Done. Avg Loss: 30.48168 ===\n",
      "Step 15800 | Loss: 28.92550 | LR: 9.96e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 15900 | Loss: 29.94778 | LR: 9.95e-05\n",
      "val loss: 29.3786\n",
      "Step 16000 | Loss: 28.19995 | LR: 9.95e-05\n",
      "Step 16100 | Loss: 28.91243 | LR: 9.95e-05\n",
      "Step 16200 | Loss: 28.83360 | LR: 9.95e-05\n",
      "val loss: 28.9683\n",
      "Step 16300 | Loss: 30.01652 | LR: 9.95e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 16400 | Loss: 30.79702 | LR: 9.95e-05\n",
      "val loss: 29.3711\n",
      "Step 16500 | Loss: 29.11833 | LR: 9.95e-05\n",
      "Step 16600 | Loss: 31.09119 | LR: 9.95e-05\n",
      "Step 16700 | Loss: 29.65737 | LR: 9.94e-05\n",
      "val loss: 29.8050\n",
      "Step 16800 | Loss: 29.71673 | LR: 9.94e-05\n",
      "Step 16900 | Loss: 30.02527 | LR: 9.94e-05\n",
      "val loss: 30.3182\n",
      "Step 17000 | Loss: 29.99591 | LR: 9.94e-05\n",
      "Step 17100 | Loss: 30.07921 | LR: 9.94e-05\n",
      "Step 17200 | Loss: 29.83378 | LR: 9.94e-05\n",
      "loading /home/ubuntu/pretraining/val/pt_shard_k562e_val_0000.npz\n",
      "val loss: 30.6123\n",
      "Step 17300 | Loss: 30.49352 | LR: 9.94e-05\n",
      "Step 17400 | Loss: 31.66602 | LR: 9.93e-05\n",
      "val loss: 30.0923\n",
      "Step 17500 | Loss: 29.15106 | LR: 9.93e-05\n",
      "=== Step 17549 Done. Avg Loss: 30.53048 ===\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 17600 | Loss: 30.22392 | LR: 9.93e-05\n",
      "Step 17700 | Loss: 30.42050 | LR: 9.93e-05\n",
      "val loss: 31.1736\n",
      "Step 17800 | Loss: 30.53041 | LR: 9.93e-05\n",
      "Step 17900 | Loss: 30.48683 | LR: 9.93e-05\n",
      "val loss: 31.8655\n",
      "Step 18000 | Loss: 30.62004 | LR: 9.92e-05\n",
      "Step 18100 | Loss: 31.81461 | LR: 9.92e-05\n",
      "Step 18200 | Loss: 35.23407 | LR: 9.92e-05\n",
      "val loss: 32.8393\n",
      "Step 18300 | Loss: 35.55780 | LR: 9.92e-05\n",
      "Step 18400 | Loss: 32.22009 | LR: 9.92e-05\n",
      "val loss: 33.6615\n",
      "Step 18500 | Loss: 34.38480 | LR: 9.92e-05\n",
      "Step 18600 | Loss: 34.63473 | LR: 9.91e-05\n",
      "Step 18700 | Loss: 35.77594 | LR: 9.91e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "val loss: 34.9942\n",
      "Step 18800 | Loss: 36.47705 | LR: 9.91e-05\n",
      "Step 18900 | Loss: 37.79283 | LR: 9.91e-05\n",
      "val loss: 38.8460\n",
      "Step 19000 | Loss: 39.15373 | LR: 9.91e-05\n",
      "Step 19100 | Loss: 36.66706 | LR: 9.91e-05\n",
      "Step 19200 | Loss: 39.72433 | LR: 9.90e-05\n",
      "val loss: 40.8407\n",
      "Step 19300 | Loss: 41.36087 | LR: 9.90e-05\n",
      "=== Step 19304 Done. Avg Loss: 35.08170 ===\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 19400 | Loss: 41.37296 | LR: 9.90e-05\n",
      "val loss: 41.6847\n",
      "Step 19500 | Loss: 43.05560 | LR: 9.90e-05\n",
      "Step 19600 | Loss: 44.51207 | LR: 9.90e-05\n",
      "Step 19700 | Loss: 39.10375 | LR: 9.89e-05\n",
      "val loss: 40.4307\n",
      "Step 19800 | Loss: 42.41572 | LR: 9.89e-05\n",
      "Step 19900 | Loss: 39.00619 | LR: 9.89e-05\n",
      "val loss: 39.7584\n",
      "Step 20000 | Loss: 43.33100 | LR: 9.89e-05\n",
      "Step 20100 | Loss: 39.82917 | LR: 9.89e-05\n",
      "Step 20200 | Loss: 39.92309 | LR: 9.88e-05\n",
      "val loss: 40.3762\n",
      "Step 20300 | Loss: 40.17086 | LR: 9.88e-05\n",
      "Step 20400 | Loss: 45.48861 | LR: 9.88e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "val loss: 40.2525\n",
      "Step 20500 | Loss: 38.32987 | LR: 9.88e-05\n",
      "Step 20600 | Loss: 40.81081 | LR: 9.88e-05\n",
      "Step 20700 | Loss: 40.66482 | LR: 9.87e-05\n",
      "val loss: 40.1516\n",
      "Step 20800 | Loss: 44.32948 | LR: 9.87e-05\n",
      "Step 20900 | Loss: 38.37643 | LR: 9.87e-05\n",
      "val loss: 39.7980\n",
      "Step 21000 | Loss: 37.46955 | LR: 9.87e-05\n",
      "=== Step 21059 Done. Avg Loss: 41.65164 ===\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 21100 | Loss: 38.70987 | LR: 9.87e-05\n",
      "Step 21200 | Loss: 38.88845 | LR: 9.86e-05\n",
      "val loss: 38.8758\n",
      "Step 21300 | Loss: 37.46444 | LR: 9.86e-05\n",
      "Step 21400 | Loss: 42.90058 | LR: 9.86e-05\n",
      "loading /home/ubuntu/pretraining/val/pt_shard_k562e_val_0000.npz\n",
      "val loss: 37.9736\n",
      "Step 21500 | Loss: 38.11253 | LR: 9.86e-05\n",
      "Step 21600 | Loss: 40.21818 | LR: 9.85e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 21700 | Loss: 42.15710 | LR: 9.85e-05\n",
      "val loss: 38.4748\n",
      "Step 21800 | Loss: 38.77242 | LR: 9.85e-05\n",
      "Step 21900 | Loss: 40.61353 | LR: 9.85e-05\n",
      "val loss: 38.8514\n",
      "Step 22000 | Loss: 41.10677 | LR: 9.85e-05\n",
      "Step 22100 | Loss: 39.43237 | LR: 9.84e-05\n",
      "Step 22200 | Loss: 38.93000 | LR: 9.84e-05\n",
      "val loss: 39.6213\n",
      "Step 22300 | Loss: 37.90674 | LR: 9.84e-05\n",
      "Step 22400 | Loss: 41.92493 | LR: 9.84e-05\n",
      "val loss: 39.8230\n",
      "Step 22500 | Loss: 38.93834 | LR: 9.83e-05\n",
      "Step 22600 | Loss: 38.18140 | LR: 9.83e-05\n",
      "Step 22700 | Loss: 39.27196 | LR: 9.83e-05\n",
      "val loss: 39.6423\n",
      "Step 22800 | Loss: 40.53636 | LR: 9.83e-05\n",
      "=== Step 22814 Done. Avg Loss: 39.79791 ===\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 22900 | Loss: 38.51071 | LR: 9.82e-05\n",
      "val loss: 40.5052\n",
      "Step 23000 | Loss: 40.10055 | LR: 9.82e-05\n",
      "Step 23100 | Loss: 38.98829 | LR: 9.82e-05\n",
      "Step 23200 | Loss: 36.54080 | LR: 9.82e-05\n",
      "val loss: 38.3553\n",
      "Step 23300 | Loss: 39.75961 | LR: 9.81e-05\n",
      "Step 23400 | Loss: 40.41849 | LR: 9.81e-05\n",
      "val loss: 39.4275\n",
      "Step 23500 | Loss: 37.76881 | LR: 9.81e-05\n",
      "Step 23600 | Loss: 40.07435 | LR: 9.81e-05\n",
      "Step 23700 | Loss: 37.67188 | LR: 9.80e-05\n",
      "val loss: 37.3655\n",
      "Step 23800 | Loss: 40.20909 | LR: 9.80e-05\n",
      "Step 23900 | Loss: 37.19225 | LR: 9.80e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "val loss: 37.5688\n",
      "Step 24000 | Loss: 37.59625 | LR: 9.80e-05\n",
      "Step 24100 | Loss: 36.53943 | LR: 9.79e-05\n",
      "Step 24200 | Loss: 36.21339 | LR: 9.79e-05\n",
      "val loss: 36.8595\n",
      "Step 24300 | Loss: 36.49603 | LR: 9.79e-05\n",
      "Step 24400 | Loss: 34.66813 | LR: 9.78e-05\n",
      "val loss: 36.1895\n",
      "Step 24500 | Loss: 35.71814 | LR: 9.78e-05\n",
      "=== Step 24569 Done. Avg Loss: 38.62450 ===\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 24600 | Loss: 33.64837 | LR: 9.78e-05\n",
      "Step 24700 | Loss: 36.60687 | LR: 9.78e-05\n",
      "val loss: 34.3151\n",
      "Step 24800 | Loss: 36.98131 | LR: 9.77e-05\n",
      "Step 24900 | Loss: 34.75412 | LR: 9.77e-05\n",
      "val loss: 35.8447\n",
      "Step 25000 | Loss: 34.70695 | LR: 9.77e-05\n",
      "Step 25100 | Loss: 37.36265 | LR: 9.77e-05\n",
      "Step 25200 | Loss: 34.59696 | LR: 9.76e-05\n",
      "val loss: 34.9455\n",
      "Step 25300 | Loss: 35.69132 | LR: 9.76e-05\n",
      "Step 25400 | Loss: 34.83453 | LR: 9.76e-05\n",
      "val loss: 35.3777\n",
      "Step 25500 | Loss: 35.17692 | LR: 9.75e-05\n",
      "Step 25600 | Loss: 35.64242 | LR: 9.75e-05\n",
      "Step 25700 | Loss: 37.91188 | LR: 9.75e-05\n",
      "loading /home/ubuntu/pretraining/val/pt_shard_k562e_val_0000.npz\n",
      "val loss: 35.2714\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 25800 | Loss: 36.84054 | LR: 9.74e-05\n",
      "Step 25900 | Loss: 35.89368 | LR: 9.74e-05\n",
      "val loss: 35.3886\n",
      "Step 26000 | Loss: 33.98902 | LR: 9.74e-05\n",
      "Step 26100 | Loss: 35.27837 | LR: 9.74e-05\n",
      "Step 26200 | Loss: 36.94222 | LR: 9.73e-05\n",
      "val loss: 35.2043\n",
      "Step 26300 | Loss: 35.32722 | LR: 9.73e-05\n",
      "=== Step 26324 Done. Avg Loss: 35.87298 ===\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 26400 | Loss: 35.97243 | LR: 9.73e-05\n",
      "val loss: 35.3033\n",
      "Step 26500 | Loss: 34.77599 | LR: 9.72e-05\n",
      "Step 26600 | Loss: 35.76490 | LR: 9.72e-05\n",
      "Step 26700 | Loss: 34.86860 | LR: 9.72e-05\n",
      "val loss: 34.5767\n",
      "Step 26800 | Loss: 38.80137 | LR: 9.71e-05\n",
      "Step 26900 | Loss: 33.81481 | LR: 9.71e-05\n",
      "val loss: 34.5231\n",
      "Step 27000 | Loss: 34.44931 | LR: 9.71e-05\n",
      "Step 27100 | Loss: 37.25285 | LR: 9.70e-05\n",
      "Step 27200 | Loss: 34.54121 | LR: 9.70e-05\n",
      "val loss: 34.2273\n",
      "Step 27300 | Loss: 35.55764 | LR: 9.70e-05\n",
      "Step 27400 | Loss: 36.18427 | LR: 9.70e-05\n",
      "val loss: 34.1620\n",
      "Step 27500 | Loss: 34.92352 | LR: 9.69e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 27600 | Loss: 33.21053 | LR: 9.69e-05\n",
      "Step 27700 | Loss: 34.62000 | LR: 9.69e-05\n",
      "val loss: 34.5515\n",
      "Step 27800 | Loss: 38.36787 | LR: 9.68e-05\n",
      "Step 27900 | Loss: 31.86048 | LR: 9.68e-05\n",
      "val loss: 35.1813\n",
      "Step 28000 | Loss: 35.19371 | LR: 9.68e-05\n",
      "=== Step 28079 Done. Avg Loss: 35.60839 ===\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 28100 | Loss: 35.16940 | LR: 9.67e-05\n",
      "Step 28200 | Loss: 31.52572 | LR: 9.67e-05\n",
      "val loss: 35.7223\n",
      "Step 28300 | Loss: 36.03345 | LR: 9.67e-05\n",
      "Step 28400 | Loss: 32.44851 | LR: 9.66e-05\n",
      "val loss: 34.4337\n",
      "Step 28500 | Loss: 33.83307 | LR: 9.66e-05\n",
      "Step 28600 | Loss: 36.01328 | LR: 9.66e-05\n",
      "Step 28700 | Loss: 33.08538 | LR: 9.65e-05\n",
      "val loss: 34.7932\n",
      "Step 28800 | Loss: 35.05614 | LR: 9.65e-05\n",
      "Step 28900 | Loss: 35.13495 | LR: 9.64e-05\n",
      "val loss: 35.5749\n",
      "Step 29000 | Loss: 33.23535 | LR: 9.64e-05\n",
      "Step 29100 | Loss: 37.84267 | LR: 9.64e-05\n",
      "Step 29200 | Loss: 34.18849 | LR: 9.63e-05\n",
      "val loss: 35.5124\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 29300 | Loss: 33.83788 | LR: 9.63e-05\n",
      "Step 29400 | Loss: 37.27611 | LR: 9.63e-05\n",
      "val loss: 35.2915\n",
      "Step 29500 | Loss: 35.11632 | LR: 9.62e-05\n",
      "Step 29600 | Loss: 34.15060 | LR: 9.62e-05\n",
      "Step 29700 | Loss: 32.44106 | LR: 9.62e-05\n",
      "val loss: 34.0250\n",
      "Step 29800 | Loss: 35.17010 | LR: 9.61e-05\n",
      "=== Step 29834 Done. Avg Loss: 35.79281 ===\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 29900 | Loss: 35.00613 | LR: 9.61e-05\n",
      "loading /home/ubuntu/pretraining/val/pt_shard_k562e_val_0000.npz\n",
      "val loss: 34.7452\n",
      "Step 30000 | Loss: 33.51440 | LR: 9.61e-05\n",
      "Step 30100 | Loss: 35.79203 | LR: 9.60e-05\n",
      "Step 30200 | Loss: 36.20831 | LR: 9.60e-05\n",
      "val loss: 34.9652\n",
      "Step 30300 | Loss: 35.88355 | LR: 9.59e-05\n",
      "Step 30400 | Loss: 32.36935 | LR: 9.59e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "val loss: 34.7774\n",
      "Step 30500 | Loss: 34.14996 | LR: 9.59e-05\n",
      "Step 30600 | Loss: 40.55325 | LR: 9.58e-05\n",
      "Step 30700 | Loss: 32.60043 | LR: 9.58e-05\n",
      "val loss: 34.0059\n",
      "Step 30800 | Loss: 33.66313 | LR: 9.58e-05\n",
      "Step 30900 | Loss: 37.51562 | LR: 9.57e-05\n",
      "val loss: 35.3933\n",
      "Step 31000 | Loss: 36.78991 | LR: 9.57e-05\n",
      "Step 31100 | Loss: 34.71108 | LR: 9.56e-05\n",
      "Step 31200 | Loss: 35.97832 | LR: 9.56e-05\n",
      "val loss: 33.8397\n",
      "Step 31300 | Loss: 35.11208 | LR: 9.56e-05\n",
      "Step 31400 | Loss: 32.81300 | LR: 9.55e-05\n",
      "val loss: 33.5234\n",
      "Step 31500 | Loss: 32.08824 | LR: 9.55e-05\n",
      "=== Step 31589 Done. Avg Loss: 35.45433 ===\n",
      "Step 31600 | Loss: 36.11049 | LR: 9.54e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 31700 | Loss: 34.08046 | LR: 9.54e-05\n",
      "val loss: 34.4544\n",
      "Step 31800 | Loss: 33.88382 | LR: 9.54e-05\n",
      "Step 31900 | Loss: 33.35640 | LR: 9.53e-05\n",
      "val loss: 34.0881\n",
      "Step 32000 | Loss: 34.77673 | LR: 9.53e-05\n",
      "Step 32100 | Loss: 37.32443 | LR: 9.52e-05\n",
      "Step 32200 | Loss: 34.54533 | LR: 9.52e-05\n",
      "val loss: 33.0090\n",
      "Step 32300 | Loss: 32.18770 | LR: 9.52e-05\n",
      "Step 32400 | Loss: 31.79203 | LR: 9.51e-05\n",
      "val loss: 33.7012\n",
      "Step 32500 | Loss: 33.03887 | LR: 9.51e-05\n",
      "Step 32600 | Loss: 32.36671 | LR: 9.50e-05\n",
      "Step 32700 | Loss: 33.22767 | LR: 9.50e-05\n",
      "val loss: 33.6383\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 32800 | Loss: 33.72055 | LR: 9.50e-05\n",
      "Step 32900 | Loss: 35.29361 | LR: 9.49e-05\n",
      "val loss: 34.6562\n",
      "Step 33000 | Loss: 34.24849 | LR: 9.49e-05\n",
      "Step 33100 | Loss: 32.86428 | LR: 9.48e-05\n",
      "Step 33200 | Loss: 33.48575 | LR: 9.48e-05\n",
      "val loss: 33.7799\n",
      "Step 33300 | Loss: 32.64726 | LR: 9.48e-05\n",
      "=== Step 33344 Done. Avg Loss: 34.77162 ===\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 33400 | Loss: 34.09479 | LR: 9.47e-05\n",
      "val loss: 32.5557\n",
      "Step 33500 | Loss: 33.93117 | LR: 9.47e-05\n",
      "Step 33600 | Loss: 35.28486 | LR: 9.46e-05\n",
      "Step 33700 | Loss: 34.08712 | LR: 9.46e-05\n",
      "val loss: 33.7141\n",
      "Step 33800 | Loss: 32.27512 | LR: 9.45e-05\n",
      "Step 33900 | Loss: 30.46957 | LR: 9.45e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "val loss: 32.6837\n",
      "Step 34000 | Loss: 32.63698 | LR: 9.45e-05\n",
      "Step 34100 | Loss: 33.73094 | LR: 9.44e-05\n",
      "Step 34200 | Loss: 32.61462 | LR: 9.44e-05\n",
      "val loss: 32.7968\n",
      "Step 34300 | Loss: 33.26019 | LR: 9.43e-05\n",
      "Step 34400 | Loss: 32.95309 | LR: 9.43e-05\n",
      "loading /home/ubuntu/pretraining/val/pt_shard_k562e_val_0000.npz\n",
      "val loss: 32.2976\n",
      "Step 34500 | Loss: 31.95485 | LR: 9.42e-05\n",
      "Step 34600 | Loss: 32.64803 | LR: 9.42e-05\n",
      "Step 34700 | Loss: 31.25972 | LR: 9.42e-05\n",
      "val loss: 31.7984\n",
      "Step 34800 | Loss: 33.41785 | LR: 9.41e-05\n",
      "Step 34900 | Loss: 33.60978 | LR: 9.41e-05\n",
      "val loss: 32.3394\n",
      "Step 35000 | Loss: 34.69979 | LR: 9.40e-05\n",
      "=== Step 35099 Done. Avg Loss: 33.62459 ===\n",
      "Step 35100 | Loss: 30.84473 | LR: 9.40e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 35200 | Loss: 29.99821 | LR: 9.39e-05\n",
      "val loss: 31.3473\n",
      "Step 35300 | Loss: 32.11492 | LR: 9.39e-05\n",
      "Step 35400 | Loss: 32.76611 | LR: 9.38e-05\n",
      "val loss: 32.3370\n",
      "Step 35500 | Loss: 29.03556 | LR: 9.38e-05\n",
      "Step 35600 | Loss: 30.73313 | LR: 9.37e-05\n",
      "Step 35700 | Loss: 34.90192 | LR: 9.37e-05\n",
      "val loss: 30.9843\n",
      "Step 35800 | Loss: 31.36016 | LR: 9.37e-05\n",
      "Step 35900 | Loss: 32.69514 | LR: 9.36e-05\n",
      "val loss: 29.8699\n",
      "Step 36000 | Loss: 32.40455 | LR: 9.36e-05\n",
      "Step 36100 | Loss: 30.25808 | LR: 9.35e-05\n",
      "Step 36200 | Loss: 30.64992 | LR: 9.35e-05\n",
      "val loss: 30.0070\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 36300 | Loss: 30.17002 | LR: 9.34e-05\n",
      "Step 36400 | Loss: 29.90876 | LR: 9.34e-05\n",
      "val loss: 30.0274\n",
      "Step 36500 | Loss: 29.14929 | LR: 9.33e-05\n",
      "Step 36600 | Loss: 29.98804 | LR: 9.33e-05\n",
      "Step 36700 | Loss: 28.99711 | LR: 9.32e-05\n",
      "val loss: 29.0183\n",
      "Step 36800 | Loss: 30.70355 | LR: 9.32e-05\n",
      "=== Step 36854 Done. Avg Loss: 31.17827 ===\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 36900 | Loss: 26.46167 | LR: 9.31e-05\n",
      "val loss: 28.9862\n",
      "Step 37000 | Loss: 29.81773 | LR: 9.31e-05\n",
      "Step 37100 | Loss: 28.34676 | LR: 9.30e-05\n",
      "Step 37200 | Loss: 28.04590 | LR: 9.30e-05\n",
      "val loss: 28.1768\n",
      "Step 37300 | Loss: 27.59988 | LR: 9.29e-05\n",
      "Step 37400 | Loss: 29.20726 | LR: 9.29e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "val loss: 29.0190\n",
      "Step 37500 | Loss: 29.15553 | LR: 9.29e-05\n",
      "Step 37600 | Loss: 28.65458 | LR: 9.28e-05\n",
      "Step 37700 | Loss: 29.22049 | LR: 9.28e-05\n",
      "val loss: 28.2603\n",
      "Step 37800 | Loss: 28.81192 | LR: 9.27e-05\n",
      "Step 37900 | Loss: 26.78515 | LR: 9.27e-05\n",
      "val loss: 27.7340\n",
      "Step 38000 | Loss: 27.83411 | LR: 9.26e-05\n",
      "Step 38100 | Loss: 26.51778 | LR: 9.26e-05\n",
      "Step 38200 | Loss: 27.73527 | LR: 9.25e-05\n",
      "val loss: 26.8220\n",
      "Step 38300 | Loss: 27.51612 | LR: 9.25e-05\n",
      "Step 38400 | Loss: 26.73905 | LR: 9.24e-05\n",
      "val loss: 27.1697\n",
      "Step 38500 | Loss: 26.42306 | LR: 9.24e-05\n",
      "Step 38600 | Loss: 27.28173 | LR: 9.23e-05\n",
      "=== Step 38609 Done. Avg Loss: 28.51106 ===\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 38700 | Loss: 27.99958 | LR: 9.23e-05\n",
      "loading /home/ubuntu/pretraining/val/pt_shard_k562e_val_0000.npz\n",
      "val loss: 26.6526\n",
      "Step 38800 | Loss: 26.47047 | LR: 9.22e-05\n",
      "Step 38900 | Loss: 26.73669 | LR: 9.22e-05\n",
      "val loss: 26.5443\n",
      "Step 39000 | Loss: 25.71653 | LR: 9.21e-05\n",
      "Step 39100 | Loss: 26.51374 | LR: 9.21e-05\n",
      "Step 39200 | Loss: 26.97776 | LR: 9.20e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "val loss: 26.7258\n",
      "Step 39300 | Loss: 25.92205 | LR: 9.20e-05\n",
      "Step 39400 | Loss: 26.69587 | LR: 9.19e-05\n",
      "val loss: 25.8653\n",
      "Step 39500 | Loss: 27.05330 | LR: 9.19e-05\n",
      "Step 39600 | Loss: 26.71727 | LR: 9.18e-05\n",
      "Step 39700 | Loss: 25.62330 | LR: 9.17e-05\n",
      "val loss: 25.7090\n",
      "Step 39800 | Loss: 26.18604 | LR: 9.17e-05\n",
      "Step 39900 | Loss: 25.82113 | LR: 9.16e-05\n",
      "val loss: 26.2689\n",
      "Step 40000 | Loss: 25.88735 | LR: 9.16e-05\n",
      "Step 40100 | Loss: 26.08568 | LR: 9.15e-05\n",
      "Step 40200 | Loss: 25.92054 | LR: 9.15e-05\n",
      "val loss: 25.7684\n",
      "Step 40300 | Loss: 25.36485 | LR: 9.14e-05\n",
      "=== Step 40364 Done. Avg Loss: 26.41555 ===\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 40400 | Loss: 24.54491 | LR: 9.14e-05\n",
      "val loss: 25.7984\n",
      "Step 40500 | Loss: 25.58244 | LR: 9.13e-05\n",
      "Step 40600 | Loss: 24.71949 | LR: 9.13e-05\n",
      "Step 40700 | Loss: 25.14420 | LR: 9.12e-05\n",
      "val loss: 25.1286\n",
      "Step 40800 | Loss: 24.50242 | LR: 9.12e-05\n",
      "Step 40900 | Loss: 24.70026 | LR: 9.11e-05\n",
      "val loss: 25.6233\n",
      "Step 41000 | Loss: 25.45730 | LR: 9.11e-05\n",
      "Step 41100 | Loss: 25.57021 | LR: 9.10e-05\n",
      "Step 41200 | Loss: 25.73434 | LR: 9.10e-05\n",
      "val loss: 25.3227\n",
      "Step 41300 | Loss: 26.78315 | LR: 9.09e-05\n",
      "Step 41400 | Loss: 25.71160 | LR: 9.08e-05\n",
      "val loss: 25.5922\n",
      "Step 41500 | Loss: 25.60570 | LR: 9.08e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 41600 | Loss: 26.01982 | LR: 9.07e-05\n",
      "Step 41700 | Loss: 24.91836 | LR: 9.07e-05\n",
      "val loss: 25.6237\n",
      "Step 41800 | Loss: 25.96349 | LR: 9.06e-05\n",
      "Step 41900 | Loss: 25.96236 | LR: 9.06e-05\n",
      "val loss: 25.1711\n",
      "Step 42000 | Loss: 24.89996 | LR: 9.05e-05\n",
      "Step 42100 | Loss: 24.86056 | LR: 9.05e-05\n",
      "=== Step 42119 Done. Avg Loss: 25.56855 ===\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 42200 | Loss: 24.60124 | LR: 9.04e-05\n",
      "val loss: 25.2429\n",
      "Step 42300 | Loss: 24.73053 | LR: 9.03e-05\n",
      "Step 42400 | Loss: 25.16000 | LR: 9.03e-05\n",
      "val loss: 25.2708\n",
      "Step 42500 | Loss: 24.68221 | LR: 9.02e-05\n",
      "Step 42600 | Loss: 25.40058 | LR: 9.02e-05\n",
      "Step 42700 | Loss: 24.35207 | LR: 9.01e-05\n",
      "val loss: 25.1369\n",
      "Step 42800 | Loss: 25.46264 | LR: 9.01e-05\n",
      "Step 42900 | Loss: 25.27863 | LR: 9.00e-05\n",
      "loading /home/ubuntu/pretraining/val/pt_shard_k562e_val_0000.npz\n",
      "val loss: 25.3908\n",
      "Step 43000 | Loss: 25.66785 | LR: 9.00e-05\n",
      "Step 43100 | Loss: 25.05045 | LR: 8.99e-05\n",
      "Step 43200 | Loss: 25.57516 | LR: 8.98e-05\n",
      "val loss: 25.4055\n",
      "Step 43300 | Loss: 24.86668 | LR: 8.98e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 43400 | Loss: 25.16945 | LR: 8.97e-05\n",
      "val loss: 25.0735\n",
      "Step 43500 | Loss: 26.58647 | LR: 8.97e-05\n",
      "Step 43600 | Loss: 25.25386 | LR: 8.96e-05\n",
      "Step 43700 | Loss: 24.84995 | LR: 8.96e-05\n",
      "val loss: 25.2927\n",
      "Step 43800 | Loss: 24.68177 | LR: 8.95e-05\n",
      "=== Step 43874 Done. Avg Loss: 25.40737 ===\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 43900 | Loss: 25.44588 | LR: 8.94e-05\n",
      "val loss: 25.1646\n",
      "Step 44000 | Loss: 25.05036 | LR: 8.94e-05\n",
      "Step 44100 | Loss: 24.13297 | LR: 8.93e-05\n",
      "Step 44200 | Loss: 25.10666 | LR: 8.93e-05\n",
      "val loss: 25.0508\n",
      "Step 44300 | Loss: 25.08064 | LR: 8.92e-05\n",
      "Step 44400 | Loss: 25.36833 | LR: 8.92e-05\n",
      "val loss: 25.2608\n",
      "Step 44500 | Loss: 24.75036 | LR: 8.91e-05\n",
      "Step 44600 | Loss: 24.76746 | LR: 8.90e-05\n",
      "Step 44700 | Loss: 24.82230 | LR: 8.90e-05\n",
      "val loss: 25.4947\n",
      "Step 44800 | Loss: 24.88868 | LR: 8.89e-05\n",
      "Step 44900 | Loss: 25.40462 | LR: 8.89e-05\n",
      "val loss: 25.2984\n",
      "Step 45000 | Loss: 24.91983 | LR: 8.88e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 45100 | Loss: 26.72170 | LR: 8.87e-05\n",
      "Step 45200 | Loss: 26.12223 | LR: 8.87e-05\n",
      "val loss: 25.0243\n",
      "Step 45300 | Loss: 24.51573 | LR: 8.86e-05\n",
      "Step 45400 | Loss: 23.93263 | LR: 8.86e-05\n",
      "val loss: 25.0467\n",
      "Step 45500 | Loss: 25.20994 | LR: 8.85e-05\n",
      "Step 45600 | Loss: 25.48139 | LR: 8.84e-05\n",
      "=== Step 45629 Done. Avg Loss: 25.38408 ===\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 45700 | Loss: 24.30802 | LR: 8.84e-05\n",
      "val loss: 25.1078\n",
      "Step 45800 | Loss: 24.10451 | LR: 8.83e-05\n",
      "Step 45900 | Loss: 24.84109 | LR: 8.83e-05\n",
      "val loss: 24.9629\n",
      "Step 46000 | Loss: 24.47976 | LR: 8.82e-05\n",
      "Step 46100 | Loss: 23.89526 | LR: 8.81e-05\n",
      "Step 46200 | Loss: 25.63415 | LR: 8.81e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "val loss: 25.2881\n",
      "Step 46300 | Loss: 24.69666 | LR: 8.80e-05\n",
      "Step 46400 | Loss: 24.50924 | LR: 8.80e-05\n",
      "val loss: 25.2637\n",
      "Step 46500 | Loss: 24.50216 | LR: 8.79e-05\n",
      "Step 46600 | Loss: 25.41356 | LR: 8.78e-05\n",
      "Step 46700 | Loss: 24.14583 | LR: 8.78e-05\n",
      "val loss: 24.7891\n",
      "Step 46800 | Loss: 26.03121 | LR: 8.77e-05\n",
      "Step 46900 | Loss: 25.52646 | LR: 8.76e-05\n",
      "val loss: 25.0344\n",
      "Step 47000 | Loss: 24.71570 | LR: 8.76e-05\n",
      "Step 47100 | Loss: 25.40656 | LR: 8.75e-05\n",
      "Step 47200 | Loss: 24.85996 | LR: 8.75e-05\n",
      "loading /home/ubuntu/pretraining/val/pt_shard_k562e_val_0000.npz\n",
      "val loss: 25.3609\n",
      "Step 47300 | Loss: 25.95063 | LR: 8.74e-05\n",
      "=== Step 47384 Done. Avg Loss: 25.38243 ===\n",
      "Step 47400 | Loss: 24.71732 | LR: 8.73e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "val loss: 25.2192\n",
      "Step 47500 | Loss: 24.81256 | LR: 8.73e-05\n",
      "Step 47600 | Loss: 24.28269 | LR: 8.72e-05\n",
      "Step 47700 | Loss: 24.68293 | LR: 8.71e-05\n",
      "val loss: 24.8741\n",
      "Step 47800 | Loss: 26.03275 | LR: 8.71e-05\n",
      "Step 47900 | Loss: 24.94796 | LR: 8.70e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "val loss: 24.9467\n",
      "Step 48000 | Loss: 25.21470 | LR: 8.70e-05\n",
      "Step 48100 | Loss: 24.15380 | LR: 8.69e-05\n",
      "Step 48200 | Loss: 24.20951 | LR: 8.68e-05\n",
      "val loss: 24.7915\n",
      "Step 48300 | Loss: 24.45078 | LR: 8.68e-05\n",
      "Step 48400 | Loss: 24.74489 | LR: 8.67e-05\n",
      "val loss: 25.3099\n",
      "Step 48500 | Loss: 24.15510 | LR: 8.66e-05\n",
      "Step 48600 | Loss: 24.78351 | LR: 8.66e-05\n",
      "Step 48700 | Loss: 23.97707 | LR: 8.65e-05\n",
      "val loss: 25.0844\n",
      "Step 48800 | Loss: 23.97662 | LR: 8.64e-05\n",
      "Step 48900 | Loss: 25.19710 | LR: 8.64e-05\n",
      "val loss: 25.2624\n",
      "Step 49000 | Loss: 25.68655 | LR: 8.63e-05\n",
      "Step 49100 | Loss: 25.32585 | LR: 8.62e-05\n",
      "=== Step 49139 Done. Avg Loss: 25.37068 ===\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 49200 | Loss: 23.83068 | LR: 8.62e-05\n",
      "val loss: 24.6308\n",
      "Step 49300 | Loss: 23.72746 | LR: 8.61e-05\n",
      "Step 49400 | Loss: 24.23233 | LR: 8.61e-05\n",
      "val loss: 25.1456\n",
      "Step 49500 | Loss: 25.81589 | LR: 8.60e-05\n",
      "Step 49600 | Loss: 23.99612 | LR: 8.59e-05\n",
      "Step 49700 | Loss: 25.98911 | LR: 8.59e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "val loss: 25.1622\n",
      "Step 49800 | Loss: 25.57209 | LR: 8.58e-05\n",
      "Step 49900 | Loss: 24.15463 | LR: 8.57e-05\n",
      "val loss: 25.3290\n",
      "Step 50000 | Loss: 26.22348 | LR: 8.57e-05\n",
      "Step 50100 | Loss: 24.64922 | LR: 8.56e-05\n",
      "Step 50200 | Loss: 24.47281 | LR: 8.55e-05\n",
      "val loss: 24.9544\n",
      "Step 50300 | Loss: 24.58047 | LR: 8.55e-05\n",
      "Step 50400 | Loss: 25.08680 | LR: 8.54e-05\n",
      "val loss: 24.9451\n",
      "Step 50500 | Loss: 24.35548 | LR: 8.53e-05\n",
      "Step 50600 | Loss: 25.16206 | LR: 8.53e-05\n",
      "Step 50700 | Loss: 25.35941 | LR: 8.52e-05\n",
      "val loss: 24.9552\n",
      "Step 50800 | Loss: 24.85746 | LR: 8.51e-05\n",
      "=== Step 50894 Done. Avg Loss: 25.29155 ===\n",
      "Step 50900 | Loss: 23.94255 | LR: 8.51e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "val loss: 25.0523\n",
      "Step 51000 | Loss: 24.63223 | LR: 8.50e-05\n",
      "Step 51100 | Loss: 24.67501 | LR: 8.49e-05\n",
      "Step 51200 | Loss: 24.81339 | LR: 8.49e-05\n",
      "val loss: 24.8144\n",
      "Step 51300 | Loss: 25.66737 | LR: 8.48e-05\n",
      "Step 51400 | Loss: 24.04481 | LR: 8.47e-05\n",
      "val loss: 24.9093\n",
      "Step 51500 | Loss: 24.26953 | LR: 8.47e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 51600 | Loss: 25.19659 | LR: 8.46e-05\n",
      "Step 51700 | Loss: 24.26424 | LR: 8.45e-05\n",
      "loading /home/ubuntu/pretraining/val/pt_shard_k562e_val_0000.npz\n",
      "val loss: 24.7511\n",
      "Step 51800 | Loss: 24.54183 | LR: 8.44e-05\n",
      "Step 51900 | Loss: 24.13248 | LR: 8.44e-05\n",
      "val loss: 24.8512\n",
      "Step 52000 | Loss: 25.35315 | LR: 8.43e-05\n",
      "Step 52100 | Loss: 24.88450 | LR: 8.42e-05\n",
      "Step 52200 | Loss: 24.89795 | LR: 8.42e-05\n",
      "val loss: 24.9438\n",
      "Step 52300 | Loss: 24.63430 | LR: 8.41e-05\n",
      "Step 52400 | Loss: 24.99245 | LR: 8.40e-05\n",
      "val loss: 25.3924\n",
      "Step 52500 | Loss: 24.97977 | LR: 8.40e-05\n",
      "Step 52600 | Loss: 24.24840 | LR: 8.39e-05\n",
      "=== Step 52649 Done. Avg Loss: 25.16223 ===\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 52700 | Loss: 25.44068 | LR: 8.38e-05\n",
      "val loss: 25.0431\n",
      "Step 52800 | Loss: 24.64265 | LR: 8.38e-05\n",
      "Step 52900 | Loss: 24.99286 | LR: 8.37e-05\n",
      "val loss: 24.5884\n",
      "Step 53000 | Loss: 24.68760 | LR: 8.36e-05\n",
      "Step 53100 | Loss: 25.93512 | LR: 8.35e-05\n",
      "Step 53200 | Loss: 25.16854 | LR: 8.35e-05\n",
      "val loss: 24.7613\n",
      "Step 53300 | Loss: 24.92556 | LR: 8.34e-05\n",
      "Step 53400 | Loss: 24.27913 | LR: 8.33e-05\n",
      "val loss: 24.6867\n",
      "Step 53500 | Loss: 25.81461 | LR: 8.33e-05\n",
      "Step 53600 | Loss: 24.82059 | LR: 8.32e-05\n",
      "Step 53700 | Loss: 25.26794 | LR: 8.31e-05\n",
      "val loss: 24.7037\n",
      "Step 53800 | Loss: 24.26899 | LR: 8.31e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 53900 | Loss: 24.82631 | LR: 8.30e-05\n",
      "val loss: 24.6250\n",
      "Step 54000 | Loss: 24.73280 | LR: 8.29e-05\n",
      "Step 54100 | Loss: 25.32230 | LR: 8.28e-05\n",
      "Step 54200 | Loss: 24.80976 | LR: 8.28e-05\n",
      "val loss: 25.0104\n",
      "Step 54300 | Loss: 25.01052 | LR: 8.27e-05\n",
      "Step 54400 | Loss: 24.48341 | LR: 8.26e-05\n",
      "=== Step 54404 Done. Avg Loss: 24.98996 ===\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "val loss: 24.7587\n",
      "Step 54500 | Loss: 24.47504 | LR: 8.26e-05\n",
      "Step 54600 | Loss: 24.63594 | LR: 8.25e-05\n",
      "Step 54700 | Loss: 25.85276 | LR: 8.24e-05\n",
      "val loss: 24.3081\n",
      "Step 54800 | Loss: 24.48638 | LR: 8.23e-05\n",
      "Step 54900 | Loss: 24.14787 | LR: 8.23e-05\n",
      "val loss: 24.4265\n",
      "Step 55000 | Loss: 23.72002 | LR: 8.22e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 55100 | Loss: 24.35984 | LR: 8.21e-05\n",
      "Step 55200 | Loss: 25.03521 | LR: 8.21e-05\n",
      "val loss: 24.7032\n",
      "Step 55300 | Loss: 24.47084 | LR: 8.20e-05\n",
      "Step 55400 | Loss: 24.76992 | LR: 8.19e-05\n",
      "val loss: 24.5578\n",
      "Step 55500 | Loss: 23.68270 | LR: 8.18e-05\n",
      "Step 55600 | Loss: 24.83807 | LR: 8.18e-05\n",
      "Step 55700 | Loss: 24.53627 | LR: 8.17e-05\n",
      "val loss: 24.5309\n",
      "Step 55800 | Loss: 24.08351 | LR: 8.16e-05\n",
      "Step 55900 | Loss: 24.32093 | LR: 8.15e-05\n",
      "loading /home/ubuntu/pretraining/val/pt_shard_k562e_val_0000.npz\n",
      "val loss: 24.5582\n",
      "Step 56000 | Loss: 24.25113 | LR: 8.15e-05\n",
      "Step 56100 | Loss: 23.94646 | LR: 8.14e-05\n",
      "=== Step 56159 Done. Avg Loss: 24.77149 ===\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 56200 | Loss: 24.95283 | LR: 8.13e-05\n",
      "val loss: 24.6885\n",
      "Step 56300 | Loss: 24.63133 | LR: 8.13e-05\n",
      "Step 56400 | Loss: 23.81353 | LR: 8.12e-05\n",
      "val loss: 24.4661\n",
      "Step 56500 | Loss: 25.03737 | LR: 8.11e-05\n",
      "Step 56600 | Loss: 24.36982 | LR: 8.10e-05\n",
      "Step 56700 | Loss: 23.97437 | LR: 8.10e-05\n",
      "val loss: 24.5518\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 56800 | Loss: 23.56297 | LR: 8.09e-05\n",
      "Step 56900 | Loss: 24.64520 | LR: 8.08e-05\n",
      "val loss: 24.1274\n",
      "Step 57000 | Loss: 24.84374 | LR: 8.07e-05\n",
      "Step 57100 | Loss: 24.37438 | LR: 8.07e-05\n",
      "Step 57200 | Loss: 24.48971 | LR: 8.06e-05\n",
      "val loss: 24.4079\n",
      "Step 57300 | Loss: 24.65613 | LR: 8.05e-05\n",
      "Step 57400 | Loss: 23.74697 | LR: 8.04e-05\n",
      "val loss: 24.3760\n",
      "Step 57500 | Loss: 24.47538 | LR: 8.04e-05\n",
      "Step 57600 | Loss: 24.15298 | LR: 8.03e-05\n",
      "Step 57700 | Loss: 24.71705 | LR: 8.02e-05\n",
      "val loss: 24.1226\n",
      "Step 57800 | Loss: 23.78721 | LR: 8.01e-05\n",
      "Step 57900 | Loss: 24.51501 | LR: 8.01e-05\n",
      "=== Step 57914 Done. Avg Loss: 24.64155 ===\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "val loss: 24.3947\n",
      "Step 58000 | Loss: 24.19121 | LR: 8.00e-05\n",
      "Step 58100 | Loss: 24.34182 | LR: 7.99e-05\n",
      "Step 58200 | Loss: 23.29602 | LR: 7.98e-05\n",
      "val loss: 23.9333\n",
      "Step 58300 | Loss: 23.87477 | LR: 7.98e-05\n",
      "Step 58400 | Loss: 24.73607 | LR: 7.97e-05\n",
      "val loss: 24.3792\n",
      "Step 58500 | Loss: 24.69718 | LR: 7.96e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 58600 | Loss: 24.51961 | LR: 7.95e-05\n",
      "Step 58700 | Loss: 24.31098 | LR: 7.95e-05\n",
      "val loss: 24.0713\n",
      "Step 58800 | Loss: 24.85719 | LR: 7.94e-05\n",
      "Step 58900 | Loss: 24.02078 | LR: 7.93e-05\n",
      "val loss: 24.4404\n",
      "Step 59000 | Loss: 23.90907 | LR: 7.92e-05\n",
      "Step 59100 | Loss: 25.46029 | LR: 7.92e-05\n",
      "Step 59200 | Loss: 24.68757 | LR: 7.91e-05\n",
      "val loss: 24.2485\n",
      "Step 59300 | Loss: 24.62870 | LR: 7.90e-05\n",
      "Step 59400 | Loss: 23.34575 | LR: 7.89e-05\n",
      "val loss: 24.4051\n",
      "Step 59500 | Loss: 24.61396 | LR: 7.88e-05\n",
      "Step 59600 | Loss: 23.83627 | LR: 7.88e-05\n",
      "=== Step 59669 Done. Avg Loss: 24.42938 ===\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 59700 | Loss: 25.02025 | LR: 7.87e-05\n",
      "val loss: 24.4030\n",
      "Step 59800 | Loss: 23.80105 | LR: 7.86e-05\n",
      "Step 59900 | Loss: 24.40404 | LR: 7.85e-05\n",
      "val loss: 24.3645\n",
      "Step 60000 | Loss: 22.49235 | LR: 7.85e-05\n",
      "Step 60100 | Loss: 23.60944 | LR: 7.84e-05\n",
      "Step 60200 | Loss: 23.38658 | LR: 7.83e-05\n",
      "loading /home/ubuntu/pretraining/val/pt_shard_k562e_val_0000.npz\n",
      "val loss: 24.0202\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 60300 | Loss: 24.82651 | LR: 7.82e-05\n",
      "Step 60400 | Loss: 24.04454 | LR: 7.81e-05\n",
      "val loss: 24.5743\n",
      "Step 60500 | Loss: 25.56730 | LR: 7.81e-05\n",
      "Step 60600 | Loss: 24.34120 | LR: 7.80e-05\n",
      "Step 60700 | Loss: 23.19644 | LR: 7.79e-05\n",
      "val loss: 24.2361\n",
      "Step 60800 | Loss: 23.39667 | LR: 7.78e-05\n",
      "Step 60900 | Loss: 24.47148 | LR: 7.78e-05\n",
      "val loss: 24.1333\n",
      "Step 61000 | Loss: 23.94013 | LR: 7.77e-05\n",
      "Step 61100 | Loss: 23.02208 | LR: 7.76e-05\n",
      "Step 61200 | Loss: 22.69925 | LR: 7.75e-05\n",
      "val loss: 23.8394\n",
      "Step 61300 | Loss: 23.45247 | LR: 7.74e-05\n",
      "Step 61400 | Loss: 24.25027 | LR: 7.74e-05\n",
      "=== Step 61424 Done. Avg Loss: 24.33849 ===\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "val loss: 24.1948\n",
      "Step 61500 | Loss: 23.81439 | LR: 7.73e-05\n",
      "Step 61600 | Loss: 24.85730 | LR: 7.72e-05\n",
      "Step 61700 | Loss: 24.23162 | LR: 7.71e-05\n",
      "val loss: 24.1200\n",
      "Step 61800 | Loss: 24.86786 | LR: 7.70e-05\n",
      "Step 61900 | Loss: 24.44112 | LR: 7.70e-05\n",
      "val loss: 24.1527\n",
      "Step 62000 | Loss: 23.04571 | LR: 7.69e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 62100 | Loss: 24.75910 | LR: 7.68e-05\n",
      "Step 62200 | Loss: 24.35664 | LR: 7.67e-05\n",
      "val loss: 23.7428\n",
      "Step 62300 | Loss: 24.53300 | LR: 7.67e-05\n",
      "Step 62400 | Loss: 24.32478 | LR: 7.66e-05\n",
      "val loss: 23.7206\n",
      "Step 62500 | Loss: 23.57881 | LR: 7.65e-05\n",
      "Step 62600 | Loss: 23.79409 | LR: 7.64e-05\n",
      "Step 62700 | Loss: 24.07708 | LR: 7.63e-05\n",
      "val loss: 24.2000\n",
      "Step 62800 | Loss: 24.16985 | LR: 7.63e-05\n",
      "Step 62900 | Loss: 23.24948 | LR: 7.62e-05\n",
      "val loss: 23.8058\n",
      "Step 63000 | Loss: 24.34513 | LR: 7.61e-05\n",
      "Step 63100 | Loss: 24.32978 | LR: 7.60e-05\n",
      "=== Step 63179 Done. Avg Loss: 24.19753 ===\n",
      "Step 63200 | Loss: 23.95999 | LR: 7.59e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "val loss: 24.1109\n",
      "Step 63300 | Loss: 23.90363 | LR: 7.59e-05\n",
      "Step 63400 | Loss: 23.64784 | LR: 7.58e-05\n",
      "val loss: 23.7992\n",
      "Step 63500 | Loss: 24.45735 | LR: 7.57e-05\n",
      "Step 63600 | Loss: 23.89804 | LR: 7.56e-05\n",
      "Step 63700 | Loss: 23.45994 | LR: 7.55e-05\n",
      "val loss: 24.2486\n",
      "Step 63800 | Loss: 24.73540 | LR: 7.54e-05\n",
      "Step 63900 | Loss: 23.43011 | LR: 7.54e-05\n",
      "val loss: 24.0554\n",
      "Step 64000 | Loss: 23.74660 | LR: 7.53e-05\n",
      "Step 64100 | Loss: 23.95320 | LR: 7.52e-05\n",
      "Step 64200 | Loss: 23.82788 | LR: 7.51e-05\n",
      "val loss: 24.1928\n",
      "Step 64300 | Loss: 23.19427 | LR: 7.50e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 64400 | Loss: 23.38970 | LR: 7.50e-05\n",
      "loading /home/ubuntu/pretraining/val/pt_shard_k562e_val_0000.npz\n",
      "val loss: 24.1156\n",
      "Step 64500 | Loss: 23.52687 | LR: 7.49e-05\n",
      "Step 64600 | Loss: 25.33320 | LR: 7.48e-05\n",
      "Step 64700 | Loss: 23.92690 | LR: 7.47e-05\n",
      "val loss: 24.1581\n",
      "Step 64800 | Loss: 24.07216 | LR: 7.46e-05\n",
      "Step 64900 | Loss: 24.59858 | LR: 7.45e-05\n",
      "=== Step 64934 Done. Avg Loss: 24.14358 ===\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "val loss: 23.9382\n",
      "Step 65000 | Loss: 24.04645 | LR: 7.45e-05\n",
      "Step 65100 | Loss: 23.63050 | LR: 7.44e-05\n",
      "Step 65200 | Loss: 23.86573 | LR: 7.43e-05\n",
      "val loss: 23.9386\n",
      "Step 65300 | Loss: 23.67281 | LR: 7.42e-05\n",
      "Step 65400 | Loss: 24.41097 | LR: 7.41e-05\n",
      "val loss: 23.7450\n",
      "Step 65500 | Loss: 24.18986 | LR: 7.41e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 65600 | Loss: 23.37014 | LR: 7.40e-05\n",
      "Step 65700 | Loss: 24.18384 | LR: 7.39e-05\n",
      "val loss: 24.0437\n",
      "Step 65800 | Loss: 24.10850 | LR: 7.38e-05\n",
      "Step 65900 | Loss: 23.06392 | LR: 7.37e-05\n",
      "val loss: 23.9302\n",
      "Step 66000 | Loss: 24.33666 | LR: 7.36e-05\n",
      "Step 66100 | Loss: 23.96136 | LR: 7.36e-05\n",
      "Step 66200 | Loss: 23.25147 | LR: 7.35e-05\n",
      "val loss: 23.8519\n",
      "Step 66300 | Loss: 23.67073 | LR: 7.34e-05\n",
      "Step 66400 | Loss: 23.97525 | LR: 7.33e-05\n",
      "val loss: 24.1037\n",
      "Step 66500 | Loss: 24.17960 | LR: 7.32e-05\n",
      "Step 66600 | Loss: 23.26914 | LR: 7.31e-05\n",
      "=== Step 66689 Done. Avg Loss: 24.08731 ===\n",
      "Step 66700 | Loss: 23.86509 | LR: 7.31e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "val loss: 24.0277\n",
      "Step 66800 | Loss: 23.42301 | LR: 7.30e-05\n",
      "Step 66900 | Loss: 23.81403 | LR: 7.29e-05\n",
      "val loss: 23.9428\n",
      "Step 67000 | Loss: 23.66272 | LR: 7.28e-05\n",
      "Step 67100 | Loss: 24.96358 | LR: 7.27e-05\n",
      "Step 67200 | Loss: 24.55662 | LR: 7.26e-05\n",
      "val loss: 24.0480\n",
      "Step 67300 | Loss: 24.58146 | LR: 7.26e-05\n",
      "Step 67400 | Loss: 23.65429 | LR: 7.25e-05\n",
      "val loss: 24.1077\n",
      "Step 67500 | Loss: 24.72104 | LR: 7.24e-05\n",
      "Step 67600 | Loss: 23.34516 | LR: 7.23e-05\n",
      "Step 67700 | Loss: 23.81997 | LR: 7.22e-05\n",
      "val loss: 23.8421\n",
      "Step 67800 | Loss: 23.91610 | LR: 7.21e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 67900 | Loss: 23.83228 | LR: 7.20e-05\n",
      "val loss: 23.5295\n",
      "Step 68000 | Loss: 23.34470 | LR: 7.20e-05\n",
      "Step 68100 | Loss: 23.61256 | LR: 7.19e-05\n",
      "Step 68200 | Loss: 23.82872 | LR: 7.18e-05\n",
      "val loss: 23.8280\n",
      "Step 68300 | Loss: 24.46544 | LR: 7.17e-05\n",
      "Step 68400 | Loss: 24.15579 | LR: 7.16e-05\n",
      "=== Step 68444 Done. Avg Loss: 23.93258 ===\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "val loss: 24.0313\n",
      "Step 68500 | Loss: 22.78041 | LR: 7.15e-05\n",
      "Step 68600 | Loss: 23.66947 | LR: 7.15e-05\n",
      "Step 68700 | Loss: 23.14664 | LR: 7.14e-05\n",
      "val loss: 23.9792\n",
      "Step 68800 | Loss: 23.51128 | LR: 7.13e-05\n",
      "Step 68900 | Loss: 23.94729 | LR: 7.12e-05\n",
      "loading /home/ubuntu/pretraining/val/pt_shard_k562e_val_0000.npz\n",
      "val loss: 23.8006\n",
      "Step 69000 | Loss: 24.62925 | LR: 7.11e-05\n",
      "Step 69100 | Loss: 24.18484 | LR: 7.10e-05\n",
      "Step 69200 | Loss: 23.45365 | LR: 7.09e-05\n",
      "val loss: 23.7176\n",
      "Step 69300 | Loss: 23.98596 | LR: 7.09e-05\n",
      "Step 69400 | Loss: 22.75559 | LR: 7.08e-05\n",
      "val loss: 23.7948\n",
      "Step 69500 | Loss: 24.28723 | LR: 7.07e-05\n",
      "Step 69600 | Loss: 23.92532 | LR: 7.06e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 69700 | Loss: 23.77374 | LR: 7.05e-05\n",
      "val loss: 23.8399\n",
      "Step 69800 | Loss: 24.07230 | LR: 7.04e-05\n",
      "Step 69900 | Loss: 23.63762 | LR: 7.03e-05\n",
      "val loss: 23.8864\n",
      "Step 70000 | Loss: 25.28633 | LR: 7.03e-05\n",
      "Step 70100 | Loss: 22.36701 | LR: 7.02e-05\n",
      "=== Step 70199 Done. Avg Loss: 23.88408 ===\n",
      "Step 70200 | Loss: 22.11303 | LR: 7.01e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "val loss: 23.8441\n",
      "Step 70300 | Loss: 23.58117 | LR: 7.00e-05\n",
      "Step 70400 | Loss: 23.29074 | LR: 6.99e-05\n",
      "val loss: 23.4444\n",
      "Step 70500 | Loss: 23.68852 | LR: 6.98e-05\n",
      "Step 70600 | Loss: 23.41733 | LR: 6.97e-05\n",
      "Step 70700 | Loss: 24.40880 | LR: 6.97e-05\n",
      "val loss: 23.4938\n",
      "Step 70800 | Loss: 24.15132 | LR: 6.96e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 70900 | Loss: 24.55914 | LR: 6.95e-05\n",
      "val loss: 23.6665\n",
      "Step 71000 | Loss: 23.90337 | LR: 6.94e-05\n",
      "Step 71100 | Loss: 24.08828 | LR: 6.93e-05\n",
      "Step 71200 | Loss: 24.11742 | LR: 6.92e-05\n",
      "val loss: 23.6813\n",
      "Step 71300 | Loss: 24.58905 | LR: 6.91e-05\n",
      "Step 71400 | Loss: 23.01439 | LR: 6.90e-05\n",
      "val loss: 23.6794\n",
      "Step 71500 | Loss: 23.69538 | LR: 6.90e-05\n",
      "Step 71600 | Loss: 23.93189 | LR: 6.89e-05\n",
      "Step 71700 | Loss: 24.14313 | LR: 6.88e-05\n",
      "val loss: 23.5918\n",
      "Step 71800 | Loss: 24.23133 | LR: 6.87e-05\n",
      "Step 71900 | Loss: 23.28529 | LR: 6.86e-05\n",
      "=== Step 71954 Done. Avg Loss: 23.72436 ===\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "val loss: 23.7273\n",
      "Step 72000 | Loss: 23.50592 | LR: 6.85e-05\n",
      "Step 72100 | Loss: 23.85014 | LR: 6.84e-05\n",
      "Step 72200 | Loss: 22.61940 | LR: 6.83e-05\n",
      "val loss: 23.6465\n",
      "Step 72300 | Loss: 23.55963 | LR: 6.83e-05\n",
      "Step 72400 | Loss: 22.88138 | LR: 6.82e-05\n",
      "val loss: 23.9030\n",
      "Step 72500 | Loss: 24.33720 | LR: 6.81e-05\n",
      "Step 72600 | Loss: 23.29355 | LR: 6.80e-05\n",
      "Step 72700 | Loss: 24.53780 | LR: 6.79e-05\n",
      "val loss: 23.5222\n",
      "Step 72800 | Loss: 23.85761 | LR: 6.78e-05\n",
      "Step 72900 | Loss: 22.46267 | LR: 6.77e-05\n",
      "val loss: 23.5609\n",
      "Step 73000 | Loss: 22.11561 | LR: 6.76e-05\n",
      "Step 73100 | Loss: 22.42493 | LR: 6.76e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 73200 | Loss: 23.71014 | LR: 6.75e-05\n",
      "loading /home/ubuntu/pretraining/val/pt_shard_k562e_val_0000.npz\n",
      "val loss: 23.5592\n",
      "Step 73300 | Loss: 24.08071 | LR: 6.74e-05\n",
      "Step 73400 | Loss: 23.53690 | LR: 6.73e-05\n",
      "val loss: 23.5706\n",
      "Step 73500 | Loss: 24.00913 | LR: 6.72e-05\n",
      "Step 73600 | Loss: 22.76757 | LR: 6.71e-05\n",
      "Step 73700 | Loss: 22.39100 | LR: 6.70e-05\n",
      "=== Step 73709 Done. Avg Loss: 23.68633 ===\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "val loss: 23.2486\n",
      "Step 73800 | Loss: 23.18928 | LR: 6.69e-05\n",
      "Step 73900 | Loss: 23.37454 | LR: 6.68e-05\n",
      "val loss: 23.9544\n",
      "Step 74000 | Loss: 23.54528 | LR: 6.68e-05\n",
      "Step 74100 | Loss: 23.93618 | LR: 6.67e-05\n",
      "Step 74200 | Loss: 24.54165 | LR: 6.66e-05\n",
      "val loss: 23.5600\n",
      "Step 74300 | Loss: 23.19007 | LR: 6.65e-05\n",
      "Step 74400 | Loss: 24.03991 | LR: 6.64e-05\n",
      "val loss: 23.7444\n",
      "Step 74500 | Loss: 23.86586 | LR: 6.63e-05\n",
      "Step 74600 | Loss: 23.50978 | LR: 6.62e-05\n",
      "Step 74700 | Loss: 23.23756 | LR: 6.61e-05\n",
      "val loss: 23.6797\n",
      "Step 74800 | Loss: 23.23244 | LR: 6.60e-05\n",
      "Step 74900 | Loss: 23.07876 | LR: 6.60e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "val loss: 23.5885\n",
      "Step 75000 | Loss: 22.43644 | LR: 6.59e-05\n",
      "Step 75100 | Loss: 22.59444 | LR: 6.58e-05\n",
      "Step 75200 | Loss: 23.41138 | LR: 6.57e-05\n",
      "val loss: 23.5743\n",
      "Step 75300 | Loss: 23.85135 | LR: 6.56e-05\n",
      "Step 75400 | Loss: 23.39455 | LR: 6.55e-05\n",
      "=== Step 75464 Done. Avg Loss: 23.59195 ===\n",
      "val loss: 23.4452\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 75500 | Loss: 23.82841 | LR: 6.54e-05\n",
      "Step 75600 | Loss: 22.67089 | LR: 6.53e-05\n",
      "Step 75700 | Loss: 23.36906 | LR: 6.52e-05\n",
      "val loss: 23.4549\n",
      "Step 75800 | Loss: 23.85949 | LR: 6.51e-05\n",
      "Step 75900 | Loss: 22.94896 | LR: 6.51e-05\n",
      "val loss: 23.6537\n",
      "Step 76000 | Loss: 23.89027 | LR: 6.50e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 76100 | Loss: 22.62908 | LR: 6.49e-05\n",
      "Step 76200 | Loss: 23.72561 | LR: 6.48e-05\n",
      "val loss: 23.6827\n",
      "Step 76300 | Loss: 22.88348 | LR: 6.47e-05\n",
      "Step 76400 | Loss: 22.79499 | LR: 6.46e-05\n",
      "val loss: 23.0574\n",
      "Step 76500 | Loss: 22.91239 | LR: 6.45e-05\n",
      "Step 76600 | Loss: 23.38681 | LR: 6.44e-05\n",
      "Step 76700 | Loss: 23.36502 | LR: 6.43e-05\n",
      "val loss: 23.4305\n",
      "Step 76800 | Loss: 22.09414 | LR: 6.42e-05\n",
      "Step 76900 | Loss: 23.06512 | LR: 6.42e-05\n",
      "val loss: 23.3929\n",
      "Step 77000 | Loss: 23.15584 | LR: 6.41e-05\n",
      "Step 77100 | Loss: 23.96839 | LR: 6.40e-05\n",
      "Step 77200 | Loss: 23.43716 | LR: 6.39e-05\n",
      "=== Step 77219 Done. Avg Loss: 23.46887 ===\n",
      "val loss: 23.1592\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 77300 | Loss: 24.45790 | LR: 6.38e-05\n",
      "Step 77400 | Loss: 23.40517 | LR: 6.37e-05\n",
      "loading /home/ubuntu/pretraining/val/pt_shard_k562e_val_0000.npz\n",
      "val loss: 23.3086\n",
      "Step 77500 | Loss: 23.99367 | LR: 6.36e-05\n",
      "Step 77600 | Loss: 23.87291 | LR: 6.35e-05\n",
      "Step 77700 | Loss: 23.39155 | LR: 6.34e-05\n",
      "val loss: 23.4155\n",
      "Step 77800 | Loss: 23.03644 | LR: 6.33e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 77900 | Loss: 23.41123 | LR: 6.33e-05\n",
      "val loss: 23.3274\n",
      "Step 78000 | Loss: 24.17532 | LR: 6.32e-05\n",
      "Step 78100 | Loss: 22.87534 | LR: 6.31e-05\n",
      "Step 78200 | Loss: 24.32489 | LR: 6.30e-05\n",
      "val loss: 22.9874\n",
      "Step 78300 | Loss: 24.21499 | LR: 6.29e-05\n",
      "Step 78400 | Loss: 24.30093 | LR: 6.28e-05\n",
      "val loss: 23.3417\n",
      "Step 78500 | Loss: 21.96637 | LR: 6.27e-05\n",
      "Step 78600 | Loss: 23.56477 | LR: 6.26e-05\n",
      "Step 78700 | Loss: 23.11039 | LR: 6.25e-05\n",
      "val loss: 23.0391\n",
      "Step 78800 | Loss: 22.84762 | LR: 6.24e-05\n",
      "Step 78900 | Loss: 23.23667 | LR: 6.23e-05\n",
      "=== Step 78974 Done. Avg Loss: 23.40262 ===\n",
      "val loss: 22.9043\n",
      "Step 79000 | Loss: 23.29004 | LR: 6.22e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 79100 | Loss: 23.83970 | LR: 6.22e-05\n",
      "Step 79200 | Loss: 22.36066 | LR: 6.21e-05\n",
      "val loss: 23.2360\n",
      "Step 79300 | Loss: 22.67387 | LR: 6.20e-05\n",
      "Step 79400 | Loss: 22.94338 | LR: 6.19e-05\n",
      "val loss: 23.4785\n",
      "Step 79500 | Loss: 24.38158 | LR: 6.18e-05\n",
      "Step 79600 | Loss: 23.34288 | LR: 6.17e-05\n",
      "Step 79700 | Loss: 24.33880 | LR: 6.16e-05\n",
      "val loss: 23.2644\n",
      "Step 79800 | Loss: 22.77287 | LR: 6.15e-05\n",
      "Step 79900 | Loss: 22.53277 | LR: 6.14e-05\n",
      "val loss: 23.1566\n",
      "Step 80000 | Loss: 23.96688 | LR: 6.13e-05\n",
      "Step 80100 | Loss: 23.65684 | LR: 6.12e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 80200 | Loss: 23.79187 | LR: 6.12e-05\n",
      "val loss: 23.6395\n",
      "Step 80300 | Loss: 23.66455 | LR: 6.11e-05\n",
      "Step 80400 | Loss: 22.67521 | LR: 6.10e-05\n",
      "val loss: 23.5514\n",
      "Step 80500 | Loss: 22.54792 | LR: 6.09e-05\n",
      "Step 80600 | Loss: 24.34518 | LR: 6.08e-05\n",
      "Step 80700 | Loss: 24.09051 | LR: 6.07e-05\n",
      "=== Step 80729 Done. Avg Loss: 23.36241 ===\n",
      "val loss: 23.3950\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 80800 | Loss: 23.33466 | LR: 6.06e-05\n",
      "Step 80900 | Loss: 23.37778 | LR: 6.05e-05\n",
      "val loss: 23.8475\n",
      "Step 81000 | Loss: 22.89082 | LR: 6.04e-05\n",
      "Step 81100 | Loss: 23.42814 | LR: 6.03e-05\n",
      "Step 81200 | Loss: 23.23809 | LR: 6.02e-05\n",
      "val loss: 23.3713\n",
      "Step 81300 | Loss: 23.45805 | LR: 6.01e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 81400 | Loss: 22.81657 | LR: 6.00e-05\n",
      "val loss: 23.4854\n",
      "Step 81500 | Loss: 22.54200 | LR: 6.00e-05\n",
      "Step 81600 | Loss: 23.64697 | LR: 5.99e-05\n",
      "Step 81700 | Loss: 22.41928 | LR: 5.98e-05\n",
      "loading /home/ubuntu/pretraining/val/pt_shard_k562e_val_0000.npz\n",
      "val loss: 23.4091\n",
      "Step 81800 | Loss: 22.85512 | LR: 5.97e-05\n",
      "Step 81900 | Loss: 23.77676 | LR: 5.96e-05\n",
      "val loss: 23.5082\n",
      "Step 82000 | Loss: 22.66907 | LR: 5.95e-05\n",
      "Step 82100 | Loss: 22.30386 | LR: 5.94e-05\n",
      "Step 82200 | Loss: 22.46019 | LR: 5.93e-05\n",
      "val loss: 23.0885\n",
      "Step 82300 | Loss: 23.15701 | LR: 5.92e-05\n",
      "Step 82400 | Loss: 24.06894 | LR: 5.91e-05\n",
      "=== Step 82484 Done. Avg Loss: 23.25443 ===\n",
      "val loss: 22.9233\n",
      "Step 82500 | Loss: 22.93899 | LR: 5.90e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 82600 | Loss: 23.18768 | LR: 5.89e-05\n",
      "Step 82700 | Loss: 22.96164 | LR: 5.88e-05\n",
      "val loss: 23.1728\n",
      "Step 82800 | Loss: 22.89038 | LR: 5.88e-05\n",
      "Step 82900 | Loss: 23.44701 | LR: 5.87e-05\n",
      "val loss: 23.3007\n",
      "Step 83000 | Loss: 22.94176 | LR: 5.86e-05\n",
      "Step 83100 | Loss: 23.55171 | LR: 5.85e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 83200 | Loss: 23.06749 | LR: 5.84e-05\n",
      "val loss: 22.9851\n",
      "Step 83300 | Loss: 22.04497 | LR: 5.83e-05\n",
      "Step 83400 | Loss: 21.50692 | LR: 5.82e-05\n",
      "val loss: 23.2013\n",
      "Step 83500 | Loss: 23.83592 | LR: 5.81e-05\n",
      "Step 83600 | Loss: 23.16482 | LR: 5.80e-05\n",
      "Step 83700 | Loss: 23.47407 | LR: 5.79e-05\n",
      "val loss: 23.1432\n",
      "Step 83800 | Loss: 23.56591 | LR: 5.78e-05\n",
      "Step 83900 | Loss: 23.73224 | LR: 5.77e-05\n",
      "val loss: 23.0508\n",
      "Step 84000 | Loss: 22.40181 | LR: 5.76e-05\n",
      "Step 84100 | Loss: 23.34158 | LR: 5.75e-05\n",
      "Step 84200 | Loss: 22.66797 | LR: 5.74e-05\n",
      "=== Step 84239 Done. Avg Loss: 23.18205 ===\n",
      "val loss: 23.0801\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 84300 | Loss: 22.52307 | LR: 5.74e-05\n",
      "Step 84400 | Loss: 23.01795 | LR: 5.73e-05\n",
      "val loss: 23.1345\n",
      "Step 84500 | Loss: 22.86139 | LR: 5.72e-05\n",
      "Step 84600 | Loss: 23.14649 | LR: 5.71e-05\n",
      "Step 84700 | Loss: 21.96496 | LR: 5.70e-05\n",
      "val loss: 23.0152\n",
      "Step 84800 | Loss: 22.67025 | LR: 5.69e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 84900 | Loss: 22.73072 | LR: 5.68e-05\n",
      "val loss: 23.0112\n",
      "Step 85000 | Loss: 23.23951 | LR: 5.67e-05\n",
      "Step 85100 | Loss: 22.86847 | LR: 5.66e-05\n",
      "Step 85200 | Loss: 22.80499 | LR: 5.65e-05\n",
      "val loss: 23.1238\n",
      "Step 85300 | Loss: 23.44563 | LR: 5.64e-05\n",
      "Step 85400 | Loss: 23.39550 | LR: 5.63e-05\n",
      "val loss: 23.3853\n",
      "Step 85500 | Loss: 22.51815 | LR: 5.62e-05\n",
      "Step 85600 | Loss: 23.07010 | LR: 5.61e-05\n",
      "Step 85700 | Loss: 22.12916 | LR: 5.60e-05\n",
      "val loss: 22.9453\n",
      "Step 85800 | Loss: 23.33562 | LR: 5.60e-05\n",
      "Step 85900 | Loss: 22.92284 | LR: 5.59e-05\n",
      "=== Step 85994 Done. Avg Loss: 23.08021 ===\n",
      "val loss: 22.8946\n",
      "Step 86000 | Loss: 22.51878 | LR: 5.58e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 86100 | Loss: 23.05308 | LR: 5.57e-05\n",
      "Step 86200 | Loss: 23.62569 | LR: 5.56e-05\n",
      "loading /home/ubuntu/pretraining/val/pt_shard_k562e_val_0000.npz\n",
      "val loss: 23.0783\n",
      "Step 86300 | Loss: 22.53902 | LR: 5.55e-05\n",
      "Step 86400 | Loss: 23.62222 | LR: 5.54e-05\n",
      "val loss: 23.0083\n",
      "Step 86500 | Loss: 23.88068 | LR: 5.53e-05\n",
      "Step 86600 | Loss: 22.62445 | LR: 5.52e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 86700 | Loss: 22.65406 | LR: 5.51e-05\n",
      "val loss: 22.7445\n",
      "Step 86800 | Loss: 22.16431 | LR: 5.50e-05\n",
      "Step 86900 | Loss: 23.78450 | LR: 5.49e-05\n",
      "val loss: 23.2546\n",
      "Step 87000 | Loss: 23.49355 | LR: 5.48e-05\n",
      "Step 87100 | Loss: 22.86983 | LR: 5.47e-05\n",
      "Step 87200 | Loss: 22.99133 | LR: 5.46e-05\n",
      "val loss: 23.3588\n",
      "Step 87300 | Loss: 23.95889 | LR: 5.45e-05\n",
      "Step 87400 | Loss: 23.15241 | LR: 5.45e-05\n",
      "val loss: 22.9752\n",
      "Step 87500 | Loss: 24.04398 | LR: 5.44e-05\n",
      "Step 87600 | Loss: 23.40417 | LR: 5.43e-05\n",
      "Step 87700 | Loss: 23.17120 | LR: 5.42e-05\n",
      "=== Step 87749 Done. Avg Loss: 23.03345 ===\n",
      "val loss: 22.9072\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 87800 | Loss: 23.22581 | LR: 5.41e-05\n",
      "Step 87900 | Loss: 22.50509 | LR: 5.40e-05\n",
      "val loss: 23.0775\n",
      "Step 88000 | Loss: 23.67127 | LR: 5.39e-05\n",
      "Step 88100 | Loss: 22.96651 | LR: 5.38e-05\n",
      "Step 88200 | Loss: 23.20967 | LR: 5.37e-05\n",
      "val loss: 22.9168\n",
      "Step 88300 | Loss: 22.91484 | LR: 5.36e-05\n",
      "Step 88400 | Loss: 22.26305 | LR: 5.35e-05\n",
      "val loss: 23.2845\n",
      "Step 88500 | Loss: 23.80606 | LR: 5.34e-05\n",
      "Step 88600 | Loss: 22.29347 | LR: 5.33e-05\n",
      "Step 88700 | Loss: 23.51658 | LR: 5.32e-05\n",
      "val loss: 22.9604\n",
      "Step 88800 | Loss: 22.96902 | LR: 5.31e-05\n",
      "Step 88900 | Loss: 22.73492 | LR: 5.30e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "val loss: 22.9604\n",
      "Step 89000 | Loss: 23.25804 | LR: 5.30e-05\n",
      "Step 89100 | Loss: 23.39459 | LR: 5.29e-05\n",
      "Step 89200 | Loss: 23.03832 | LR: 5.28e-05\n",
      "val loss: 22.9322\n",
      "Step 89300 | Loss: 22.40802 | LR: 5.27e-05\n",
      "Step 89400 | Loss: 23.26262 | LR: 5.26e-05\n",
      "val loss: 23.0494\n",
      "Step 89500 | Loss: 22.61346 | LR: 5.25e-05\n",
      "=== Step 89504 Done. Avg Loss: 23.03688 ===\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 89600 | Loss: 22.75342 | LR: 5.24e-05\n",
      "Step 89700 | Loss: 22.90471 | LR: 5.23e-05\n",
      "val loss: 23.0479\n",
      "Step 89800 | Loss: 22.34751 | LR: 5.22e-05\n",
      "Step 89900 | Loss: 23.38899 | LR: 5.21e-05\n",
      "val loss: 22.9652\n",
      "Step 90000 | Loss: 22.02592 | LR: 5.20e-05\n",
      "Step 90100 | Loss: 23.75963 | LR: 5.19e-05\n",
      "Step 90200 | Loss: 21.87352 | LR: 5.18e-05\n",
      "val loss: 23.0793\n",
      "Step 90300 | Loss: 23.00539 | LR: 5.17e-05\n",
      "Step 90400 | Loss: 22.42157 | LR: 5.16e-05\n",
      "loading /home/ubuntu/pretraining/val/pt_shard_k562e_val_0000.npz\n",
      "val loss: 22.9918\n",
      "Step 90500 | Loss: 23.14284 | LR: 5.15e-05\n",
      "Step 90600 | Loss: 23.07337 | LR: 5.14e-05\n",
      "Step 90700 | Loss: 22.06839 | LR: 5.14e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "val loss: 23.0554\n",
      "Step 90800 | Loss: 22.35004 | LR: 5.13e-05\n",
      "Step 90900 | Loss: 22.77566 | LR: 5.12e-05\n",
      "val loss: 23.1824\n",
      "Step 91000 | Loss: 22.67862 | LR: 5.11e-05\n",
      "Step 91100 | Loss: 22.68506 | LR: 5.10e-05\n",
      "Step 91200 | Loss: 23.90572 | LR: 5.09e-05\n",
      "val loss: 23.1149\n",
      "=== Step 91259 Done. Avg Loss: 22.96919 ===\n",
      "Step 91300 | Loss: 23.41731 | LR: 5.08e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 91400 | Loss: 21.99969 | LR: 5.07e-05\n",
      "val loss: 22.9424\n",
      "Step 91500 | Loss: 22.58756 | LR: 5.06e-05\n",
      "Step 91600 | Loss: 23.65438 | LR: 5.05e-05\n",
      "Step 91700 | Loss: 23.10074 | LR: 5.04e-05\n",
      "val loss: 23.0106\n",
      "Step 91800 | Loss: 22.64153 | LR: 5.03e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 91900 | Loss: 22.03947 | LR: 5.02e-05\n",
      "val loss: 22.9168\n",
      "Step 92000 | Loss: 22.09522 | LR: 5.01e-05\n",
      "Step 92100 | Loss: 22.71608 | LR: 5.00e-05\n",
      "Step 92200 | Loss: 22.99509 | LR: 4.99e-05\n",
      "val loss: 22.8893\n",
      "Step 92300 | Loss: 23.64577 | LR: 4.98e-05\n",
      "Step 92400 | Loss: 23.18558 | LR: 4.98e-05\n",
      "val loss: 22.7215\n",
      "Step 92500 | Loss: 23.42664 | LR: 4.97e-05\n",
      "Step 92600 | Loss: 22.52715 | LR: 4.96e-05\n",
      "Step 92700 | Loss: 23.35142 | LR: 4.95e-05\n",
      "val loss: 23.1754\n",
      "Step 92800 | Loss: 22.90819 | LR: 4.94e-05\n",
      "Step 92900 | Loss: 22.91077 | LR: 4.93e-05\n",
      "val loss: 22.8926\n",
      "Step 93000 | Loss: 21.84881 | LR: 4.92e-05\n",
      "=== Step 93014 Done. Avg Loss: 22.92146 ===\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 93100 | Loss: 22.29154 | LR: 4.91e-05\n",
      "Step 93200 | Loss: 24.16453 | LR: 4.90e-05\n",
      "val loss: 23.3143\n",
      "Step 93300 | Loss: 22.93839 | LR: 4.89e-05\n",
      "Step 93400 | Loss: 24.60089 | LR: 4.88e-05\n",
      "val loss: 24.0482\n",
      "Step 93500 | Loss: 24.16830 | LR: 4.87e-05\n",
      "Step 93600 | Loss: 22.82131 | LR: 4.86e-05\n",
      "Step 93700 | Loss: 22.42841 | LR: 4.85e-05\n",
      "val loss: 22.7255\n",
      "Step 93800 | Loss: 23.59176 | LR: 4.84e-05\n",
      "Step 93900 | Loss: 22.92412 | LR: 4.83e-05\n",
      "val loss: 23.2679\n",
      "Step 94000 | Loss: 23.53585 | LR: 4.82e-05\n",
      "Step 94100 | Loss: 24.27045 | LR: 4.81e-05\n",
      "Step 94200 | Loss: 23.11735 | LR: 4.81e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "val loss: 23.6977\n",
      "Step 94300 | Loss: 22.97993 | LR: 4.80e-05\n",
      "Step 94400 | Loss: 24.01562 | LR: 4.79e-05\n",
      "val loss: 22.8959\n",
      "Step 94500 | Loss: 23.36704 | LR: 4.78e-05\n",
      "Step 94600 | Loss: 22.99204 | LR: 4.77e-05\n",
      "Step 94700 | Loss: 24.55033 | LR: 4.76e-05\n",
      "loading /home/ubuntu/pretraining/val/pt_shard_k562e_val_0000.npz\n",
      "val loss: 22.9988\n",
      "=== Step 94769 Done. Avg Loss: 23.31240 ===\n",
      "Step 94800 | Loss: 23.87942 | LR: 4.75e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 94900 | Loss: 23.95039 | LR: 4.74e-05\n",
      "val loss: 23.8596\n",
      "Step 95000 | Loss: 24.91124 | LR: 4.73e-05\n",
      "Step 95100 | Loss: 22.82423 | LR: 4.72e-05\n",
      "Step 95200 | Loss: 24.13062 | LR: 4.71e-05\n",
      "val loss: 23.2240\n",
      "Step 95300 | Loss: 24.09000 | LR: 4.70e-05\n",
      "Step 95400 | Loss: 23.57819 | LR: 4.69e-05\n",
      "val loss: 23.5268\n",
      "Step 95500 | Loss: 22.58167 | LR: 4.68e-05\n",
      "Step 95600 | Loss: 23.93564 | LR: 4.67e-05\n",
      "Step 95700 | Loss: 22.25579 | LR: 4.66e-05\n",
      "val loss: 23.8412\n",
      "Step 95800 | Loss: 22.40583 | LR: 4.66e-05\n",
      "Step 95900 | Loss: 23.33632 | LR: 4.65e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "val loss: 23.0620\n",
      "Step 96000 | Loss: 22.13681 | LR: 4.64e-05\n",
      "Step 96100 | Loss: 22.35444 | LR: 4.63e-05\n",
      "Step 96200 | Loss: 23.79018 | LR: 4.62e-05\n",
      "val loss: 23.3198\n",
      "Step 96300 | Loss: 22.61169 | LR: 4.61e-05\n",
      "Step 96400 | Loss: 22.66339 | LR: 4.60e-05\n",
      "val loss: 23.4575\n",
      "Step 96500 | Loss: 24.29948 | LR: 4.59e-05\n",
      "=== Step 96524 Done. Avg Loss: 23.39672 ===\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 96600 | Loss: 22.54756 | LR: 4.58e-05\n",
      "Step 96700 | Loss: 22.55846 | LR: 4.57e-05\n",
      "val loss: 23.3963\n",
      "Step 96800 | Loss: 23.59898 | LR: 4.56e-05\n",
      "Step 96900 | Loss: 22.93708 | LR: 4.55e-05\n",
      "val loss: 23.0099\n",
      "Step 97000 | Loss: 22.37123 | LR: 4.54e-05\n",
      "Step 97100 | Loss: 23.82502 | LR: 4.53e-05\n",
      "Step 97200 | Loss: 22.79191 | LR: 4.52e-05\n",
      "val loss: 23.8507\n",
      "Step 97300 | Loss: 22.86077 | LR: 4.51e-05\n",
      "Step 97400 | Loss: 23.77597 | LR: 4.50e-05\n",
      "val loss: 22.9126\n",
      "Step 97500 | Loss: 23.48387 | LR: 4.50e-05\n",
      "Step 97600 | Loss: 23.70682 | LR: 4.49e-05\n",
      "Step 97700 | Loss: 24.80950 | LR: 4.48e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "val loss: 23.2208\n",
      "Step 97800 | Loss: 23.33446 | LR: 4.47e-05\n",
      "Step 97900 | Loss: 23.77391 | LR: 4.46e-05\n",
      "val loss: 23.7955\n",
      "Step 98000 | Loss: 23.47235 | LR: 4.45e-05\n",
      "Step 98100 | Loss: 23.15806 | LR: 4.44e-05\n",
      "Step 98200 | Loss: 23.75472 | LR: 4.43e-05\n",
      "val loss: 23.1545\n",
      "=== Step 98279 Done. Avg Loss: 23.37937 ===\n",
      "Step 98300 | Loss: 24.63408 | LR: 4.42e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 98400 | Loss: 23.45656 | LR: 4.41e-05\n",
      "val loss: 23.2533\n",
      "Step 98500 | Loss: 23.27403 | LR: 4.40e-05\n",
      "Step 98600 | Loss: 24.12512 | LR: 4.39e-05\n",
      "Step 98700 | Loss: 22.99672 | LR: 4.38e-05\n",
      "val loss: 23.9442\n",
      "Step 98800 | Loss: 22.76236 | LR: 4.37e-05\n",
      "Step 98900 | Loss: 24.00219 | LR: 4.36e-05\n",
      "loading /home/ubuntu/pretraining/val/pt_shard_k562e_val_0000.npz\n",
      "val loss: 23.5017\n",
      "Step 99000 | Loss: 23.08125 | LR: 4.36e-05\n",
      "Step 99100 | Loss: 23.44201 | LR: 4.35e-05\n",
      "Step 99200 | Loss: 22.92592 | LR: 4.34e-05\n",
      "val loss: 23.1574\n",
      "Step 99300 | Loss: 24.12683 | LR: 4.33e-05\n",
      "Step 99400 | Loss: 23.96266 | LR: 4.32e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "val loss: 23.7491\n",
      "Step 99500 | Loss: 24.52613 | LR: 4.31e-05\n",
      "Step 99600 | Loss: 22.99585 | LR: 4.30e-05\n",
      "Step 99700 | Loss: 22.97858 | LR: 4.29e-05\n",
      "val loss: 23.4152\n",
      "Step 99800 | Loss: 23.23780 | LR: 4.28e-05\n",
      "Step 99900 | Loss: 22.75002 | LR: 4.27e-05\n",
      "val loss: 22.9091\n",
      "Step 100000 | Loss: 22.94134 | LR: 4.26e-05\n",
      "=== Step 100034 Done. Avg Loss: 23.36033 ===\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 100100 | Loss: 23.59595 | LR: 4.25e-05\n",
      "Step 100200 | Loss: 22.64390 | LR: 4.24e-05\n",
      "val loss: 23.5064\n",
      "Step 100300 | Loss: 23.20766 | LR: 4.23e-05\n",
      "Step 100400 | Loss: 23.74979 | LR: 4.22e-05\n",
      "val loss: 22.9359\n",
      "Step 100500 | Loss: 22.70778 | LR: 4.22e-05\n",
      "Step 100600 | Loss: 21.92572 | LR: 4.21e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 100700 | Loss: 24.03729 | LR: 4.20e-05\n",
      "val loss: 22.8509\n",
      "Step 100800 | Loss: 23.89463 | LR: 4.19e-05\n",
      "Step 100900 | Loss: 22.52578 | LR: 4.18e-05\n",
      "val loss: 23.6395\n",
      "Step 101000 | Loss: 22.96918 | LR: 4.17e-05\n",
      "Step 101100 | Loss: 22.81887 | LR: 4.16e-05\n",
      "Step 101200 | Loss: 23.33813 | LR: 4.15e-05\n",
      "val loss: 22.7418\n",
      "Step 101300 | Loss: 22.89734 | LR: 4.14e-05\n",
      "Step 101400 | Loss: 23.49846 | LR: 4.13e-05\n",
      "val loss: 23.0352\n",
      "Step 101500 | Loss: 23.57669 | LR: 4.12e-05\n",
      "Step 101600 | Loss: 23.43169 | LR: 4.11e-05\n",
      "Step 101700 | Loss: 23.55652 | LR: 4.10e-05\n",
      "val loss: 23.6362\n",
      "=== Step 101789 Done. Avg Loss: 23.26560 ===\n",
      "Step 101800 | Loss: 22.73508 | LR: 4.09e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 101900 | Loss: 24.00998 | LR: 4.09e-05\n",
      "val loss: 23.0674\n",
      "Step 102000 | Loss: 23.87989 | LR: 4.08e-05\n",
      "Step 102100 | Loss: 23.52317 | LR: 4.07e-05\n",
      "Step 102200 | Loss: 23.52726 | LR: 4.06e-05\n",
      "val loss: 23.1331\n",
      "Step 102300 | Loss: 22.37807 | LR: 4.05e-05\n",
      "Step 102400 | Loss: 22.75694 | LR: 4.04e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "val loss: 23.4694\n",
      "Step 102500 | Loss: 23.62384 | LR: 4.03e-05\n",
      "Step 102600 | Loss: 23.42689 | LR: 4.02e-05\n",
      "Step 102700 | Loss: 23.38781 | LR: 4.01e-05\n",
      "val loss: 23.1849\n",
      "Step 102800 | Loss: 23.12498 | LR: 4.00e-05\n",
      "Step 102900 | Loss: 22.70204 | LR: 3.99e-05\n",
      "val loss: 23.1719\n",
      "Step 103000 | Loss: 23.98430 | LR: 3.98e-05\n",
      "Step 103100 | Loss: 23.48416 | LR: 3.97e-05\n",
      "Step 103200 | Loss: 22.97841 | LR: 3.97e-05\n",
      "val loss: 23.6260\n",
      "Step 103300 | Loss: 23.12112 | LR: 3.96e-05\n",
      "Step 103400 | Loss: 24.14941 | LR: 3.95e-05\n",
      "loading /home/ubuntu/pretraining/val/pt_shard_k562e_val_0000.npz\n",
      "val loss: 22.9224\n",
      "Step 103500 | Loss: 23.23538 | LR: 3.94e-05\n",
      "=== Step 103544 Done. Avg Loss: 23.20176 ===\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 103600 | Loss: 22.78266 | LR: 3.93e-05\n",
      "Step 103700 | Loss: 23.26048 | LR: 3.92e-05\n",
      "val loss: 22.9820\n",
      "Step 103800 | Loss: 23.47554 | LR: 3.91e-05\n",
      "Step 103900 | Loss: 22.50317 | LR: 3.90e-05\n",
      "val loss: 23.6101\n",
      "Step 104000 | Loss: 23.34493 | LR: 3.89e-05\n",
      "Step 104100 | Loss: 23.51515 | LR: 3.88e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 104200 | Loss: 22.72083 | LR: 3.87e-05\n",
      "val loss: 23.0278\n",
      "Step 104300 | Loss: 23.47200 | LR: 3.86e-05\n",
      "Step 104400 | Loss: 22.08823 | LR: 3.85e-05\n",
      "val loss: 23.3452\n",
      "Step 104500 | Loss: 22.74466 | LR: 3.85e-05\n",
      "Step 104600 | Loss: 23.80815 | LR: 3.84e-05\n",
      "Step 104700 | Loss: 22.84689 | LR: 3.83e-05\n",
      "val loss: 23.0477\n",
      "Step 104800 | Loss: 22.47414 | LR: 3.82e-05\n",
      "Step 104900 | Loss: 23.18959 | LR: 3.81e-05\n",
      "val loss: 23.2209\n",
      "Step 105000 | Loss: 22.90256 | LR: 3.80e-05\n",
      "Step 105100 | Loss: 21.57698 | LR: 3.79e-05\n",
      "Step 105200 | Loss: 23.39252 | LR: 3.78e-05\n",
      "val loss: 22.8516\n",
      "=== Step 105299 Done. Avg Loss: 23.17106 ===\n",
      "Step 105300 | Loss: 22.76263 | LR: 3.77e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 105400 | Loss: 22.73829 | LR: 3.76e-05\n",
      "val loss: 23.4374\n",
      "Step 105500 | Loss: 24.01911 | LR: 3.75e-05\n",
      "Step 105600 | Loss: 22.95326 | LR: 3.75e-05\n",
      "Step 105700 | Loss: 23.07055 | LR: 3.74e-05\n",
      "val loss: 23.0147\n",
      "Step 105800 | Loss: 24.97519 | LR: 3.73e-05\n",
      "Step 105900 | Loss: 23.35215 | LR: 3.72e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "val loss: 23.0051\n",
      "Step 106000 | Loss: 23.36971 | LR: 3.71e-05\n",
      "Step 106100 | Loss: 23.30416 | LR: 3.70e-05\n",
      "Step 106200 | Loss: 22.49551 | LR: 3.69e-05\n",
      "val loss: 23.7481\n",
      "Step 106300 | Loss: 23.18486 | LR: 3.68e-05\n",
      "Step 106400 | Loss: 23.71149 | LR: 3.67e-05\n",
      "val loss: 23.0507\n",
      "Step 106500 | Loss: 22.17053 | LR: 3.66e-05\n",
      "Step 106600 | Loss: 22.41209 | LR: 3.65e-05\n",
      "Step 106700 | Loss: 23.12062 | LR: 3.64e-05\n",
      "val loss: 23.2874\n",
      "Step 106800 | Loss: 24.94686 | LR: 3.64e-05\n",
      "Step 106900 | Loss: 22.76672 | LR: 3.63e-05\n",
      "val loss: 23.2769\n",
      "Step 107000 | Loss: 23.72630 | LR: 3.62e-05\n",
      "=== Step 107054 Done. Avg Loss: 23.14226 ===\n",
      "Step 107100 | Loss: 22.97037 | LR: 3.61e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 107200 | Loss: 23.30953 | LR: 3.60e-05\n",
      "val loss: 23.1319\n",
      "Step 107300 | Loss: 23.57046 | LR: 3.59e-05\n",
      "Step 107400 | Loss: 22.75300 | LR: 3.58e-05\n",
      "val loss: 23.1979\n",
      "Step 107500 | Loss: 22.36355 | LR: 3.57e-05\n",
      "Step 107600 | Loss: 22.85601 | LR: 3.56e-05\n",
      "Step 107700 | Loss: 22.93056 | LR: 3.55e-05\n",
      "loading /home/ubuntu/pretraining/val/pt_shard_k562e_val_0000.npz\n",
      "val loss: 23.4416\n",
      "Step 107800 | Loss: 22.49790 | LR: 3.55e-05\n",
      "Step 107900 | Loss: 23.70555 | LR: 3.54e-05\n",
      "val loss: 23.0980\n",
      "Step 108000 | Loss: 22.51756 | LR: 3.53e-05\n",
      "Step 108100 | Loss: 24.34117 | LR: 3.52e-05\n",
      "Step 108200 | Loss: 21.92624 | LR: 3.51e-05\n",
      "val loss: 23.1248\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 108300 | Loss: 23.07690 | LR: 3.50e-05\n",
      "Step 108400 | Loss: 24.31882 | LR: 3.49e-05\n",
      "val loss: 23.4237\n",
      "Step 108500 | Loss: 22.60948 | LR: 3.48e-05\n",
      "Step 108600 | Loss: 22.49649 | LR: 3.47e-05\n",
      "Step 108700 | Loss: 23.12914 | LR: 3.46e-05\n",
      "val loss: 23.1350\n",
      "Step 108800 | Loss: 23.41190 | LR: 3.46e-05\n",
      "=== Step 108809 Done. Avg Loss: 23.12063 ===\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 108900 | Loss: 22.79335 | LR: 3.45e-05\n",
      "val loss: 22.9617\n",
      "Step 109000 | Loss: 22.94576 | LR: 3.44e-05\n",
      "Step 109100 | Loss: 23.12573 | LR: 3.43e-05\n",
      "Step 109200 | Loss: 21.90705 | LR: 3.42e-05\n",
      "val loss: 23.2827\n",
      "Step 109300 | Loss: 22.76790 | LR: 3.41e-05\n",
      "Step 109400 | Loss: 22.90141 | LR: 3.40e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "val loss: 22.8572\n",
      "Step 109500 | Loss: 22.91444 | LR: 3.39e-05\n",
      "Step 109600 | Loss: 23.45415 | LR: 3.38e-05\n",
      "Step 109700 | Loss: 23.74974 | LR: 3.38e-05\n",
      "val loss: 22.6213\n",
      "Step 109800 | Loss: 22.06845 | LR: 3.37e-05\n",
      "Step 109900 | Loss: 23.48963 | LR: 3.36e-05\n",
      "val loss: 23.3161\n",
      "Step 110000 | Loss: 22.19146 | LR: 3.35e-05\n",
      "Step 110100 | Loss: 22.88275 | LR: 3.34e-05\n",
      "Step 110200 | Loss: 22.92687 | LR: 3.33e-05\n",
      "val loss: 22.8957\n",
      "Step 110300 | Loss: 24.25712 | LR: 3.32e-05\n",
      "Step 110400 | Loss: 23.30296 | LR: 3.31e-05\n",
      "val loss: 23.0639\n",
      "Step 110500 | Loss: 22.46027 | LR: 3.30e-05\n",
      "=== Step 110564 Done. Avg Loss: 23.07679 ===\n",
      "Step 110600 | Loss: 24.21097 | LR: 3.30e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 110700 | Loss: 23.57772 | LR: 3.29e-05\n",
      "val loss: 23.5244\n",
      "Step 110800 | Loss: 22.91168 | LR: 3.28e-05\n",
      "Step 110900 | Loss: 24.22053 | LR: 3.27e-05\n",
      "val loss: 22.8630\n",
      "Step 111000 | Loss: 22.44282 | LR: 3.26e-05\n",
      "Step 111100 | Loss: 23.15836 | LR: 3.25e-05\n",
      "Step 111200 | Loss: 22.12948 | LR: 3.24e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "val loss: 23.0173\n",
      "Step 111300 | Loss: 22.52985 | LR: 3.23e-05\n",
      "Step 111400 | Loss: 23.51098 | LR: 3.22e-05\n",
      "val loss: 23.0794\n",
      "Step 111500 | Loss: 23.45642 | LR: 3.22e-05\n",
      "Step 111600 | Loss: 22.85645 | LR: 3.21e-05\n",
      "Step 111700 | Loss: 22.00498 | LR: 3.20e-05\n",
      "val loss: 22.8139\n",
      "Step 111800 | Loss: 22.59953 | LR: 3.19e-05\n",
      "Step 111900 | Loss: 22.92647 | LR: 3.18e-05\n",
      "loading /home/ubuntu/pretraining/val/pt_shard_k562e_val_0000.npz\n",
      "val loss: 22.7115\n",
      "Step 112000 | Loss: 23.30873 | LR: 3.17e-05\n",
      "Step 112100 | Loss: 23.72304 | LR: 3.16e-05\n",
      "Step 112200 | Loss: 22.99965 | LR: 3.15e-05\n",
      "val loss: 23.2105\n",
      "Step 112300 | Loss: 23.85883 | LR: 3.15e-05\n",
      "=== Step 112319 Done. Avg Loss: 23.03474 ===\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 112400 | Loss: 23.43027 | LR: 3.14e-05\n",
      "val loss: 23.0506\n",
      "Step 112500 | Loss: 22.32341 | LR: 3.13e-05\n",
      "Step 112600 | Loss: 22.02831 | LR: 3.12e-05\n",
      "Step 112700 | Loss: 24.00862 | LR: 3.11e-05\n",
      "val loss: 23.1736\n",
      "Step 112800 | Loss: 23.90005 | LR: 3.10e-05\n",
      "Step 112900 | Loss: 21.81213 | LR: 3.09e-05\n",
      "val loss: 23.3112\n",
      "Step 113000 | Loss: 23.23346 | LR: 3.08e-05\n",
      "Step 113100 | Loss: 22.90478 | LR: 3.08e-05\n",
      "Step 113200 | Loss: 22.84727 | LR: 3.07e-05\n",
      "val loss: 22.7085\n",
      "Step 113300 | Loss: 23.15076 | LR: 3.06e-05\n",
      "Step 113400 | Loss: 22.87907 | LR: 3.05e-05\n",
      "val loss: 22.8590\n",
      "Step 113500 | Loss: 22.63082 | LR: 3.04e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 113600 | Loss: 22.63995 | LR: 3.03e-05\n",
      "Step 113700 | Loss: 22.72177 | LR: 3.02e-05\n",
      "val loss: 23.0382\n",
      "Step 113800 | Loss: 22.34583 | LR: 3.02e-05\n",
      "Step 113900 | Loss: 23.19386 | LR: 3.01e-05\n",
      "val loss: 22.8634\n",
      "Step 114000 | Loss: 21.67891 | LR: 3.00e-05\n",
      "=== Step 114074 Done. Avg Loss: 23.02469 ===\n",
      "Step 114100 | Loss: 22.58806 | LR: 2.99e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 114200 | Loss: 22.20410 | LR: 2.98e-05\n",
      "val loss: 22.7567\n",
      "Step 114300 | Loss: 23.11222 | LR: 2.97e-05\n",
      "Step 114400 | Loss: 23.10351 | LR: 2.96e-05\n",
      "val loss: 22.9473\n",
      "Step 114500 | Loss: 23.85248 | LR: 2.95e-05\n",
      "Step 114600 | Loss: 23.70156 | LR: 2.95e-05\n",
      "Step 114700 | Loss: 23.57499 | LR: 2.94e-05\n",
      "val loss: 22.9191\n",
      "Step 114800 | Loss: 23.38279 | LR: 2.93e-05\n",
      "Step 114900 | Loss: 22.88823 | LR: 2.92e-05\n",
      "val loss: 22.9356\n",
      "Step 115000 | Loss: 23.47849 | LR: 2.91e-05\n",
      "Step 115100 | Loss: 21.71418 | LR: 2.90e-05\n",
      "Step 115200 | Loss: 23.38363 | LR: 2.89e-05\n",
      "val loss: 23.1538\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 115300 | Loss: 22.71144 | LR: 2.89e-05\n",
      "Step 115400 | Loss: 23.66543 | LR: 2.88e-05\n",
      "val loss: 22.7621\n",
      "Step 115500 | Loss: 23.06485 | LR: 2.87e-05\n",
      "Step 115600 | Loss: 23.34308 | LR: 2.86e-05\n",
      "Step 115700 | Loss: 22.63119 | LR: 2.85e-05\n",
      "val loss: 22.9647\n",
      "Step 115800 | Loss: 22.28647 | LR: 2.84e-05\n",
      "=== Step 115829 Done. Avg Loss: 22.99458 ===\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 115900 | Loss: 22.71648 | LR: 2.84e-05\n",
      "val loss: 23.1895\n",
      "Step 116000 | Loss: 22.88299 | LR: 2.83e-05\n",
      "Step 116100 | Loss: 23.07960 | LR: 2.82e-05\n",
      "Step 116200 | Loss: 23.55760 | LR: 2.81e-05\n",
      "loading /home/ubuntu/pretraining/val/pt_shard_k562e_val_0000.npz\n",
      "val loss: 23.0760\n",
      "Step 116300 | Loss: 23.20493 | LR: 2.80e-05\n",
      "Step 116400 | Loss: 22.89748 | LR: 2.79e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "val loss: 22.8747\n",
      "Step 116500 | Loss: 23.18728 | LR: 2.78e-05\n",
      "Step 116600 | Loss: 23.25432 | LR: 2.78e-05\n",
      "Step 116700 | Loss: 22.13024 | LR: 2.77e-05\n",
      "val loss: 23.1603\n",
      "Step 116800 | Loss: 23.90181 | LR: 2.76e-05\n",
      "Step 116900 | Loss: 22.84564 | LR: 2.75e-05\n",
      "val loss: 22.8502\n",
      "Step 117000 | Loss: 22.63780 | LR: 2.74e-05\n",
      "Step 117100 | Loss: 23.13583 | LR: 2.73e-05\n",
      "Step 117200 | Loss: 23.05855 | LR: 2.73e-05\n",
      "val loss: 23.0367\n",
      "Step 117300 | Loss: 22.85354 | LR: 2.72e-05\n",
      "Step 117400 | Loss: 23.29382 | LR: 2.71e-05\n",
      "val loss: 23.0169\n",
      "Step 117500 | Loss: 22.93228 | LR: 2.70e-05\n",
      "=== Step 117584 Done. Avg Loss: 22.98516 ===\n",
      "Step 117600 | Loss: 21.41507 | LR: 2.69e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 117700 | Loss: 23.73585 | LR: 2.68e-05\n",
      "val loss: 22.8351\n",
      "Step 117800 | Loss: 23.10044 | LR: 2.68e-05\n",
      "Step 117900 | Loss: 23.14550 | LR: 2.67e-05\n",
      "val loss: 22.8281\n",
      "Step 118000 | Loss: 23.84549 | LR: 2.66e-05\n",
      "Step 118100 | Loss: 22.24831 | LR: 2.65e-05\n",
      "Step 118200 | Loss: 22.46710 | LR: 2.64e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "val loss: 22.8908\n",
      "Step 118300 | Loss: 23.78876 | LR: 2.63e-05\n",
      "Step 118400 | Loss: 22.51382 | LR: 2.63e-05\n",
      "val loss: 23.0178\n",
      "Step 118500 | Loss: 22.87475 | LR: 2.62e-05\n",
      "Step 118600 | Loss: 22.94761 | LR: 2.61e-05\n",
      "Step 118700 | Loss: 22.43473 | LR: 2.60e-05\n",
      "val loss: 22.4759\n",
      "Step 118800 | Loss: 22.92855 | LR: 2.59e-05\n",
      "Step 118900 | Loss: 22.23835 | LR: 2.58e-05\n",
      "val loss: 23.0510\n",
      "Step 119000 | Loss: 22.80491 | LR: 2.58e-05\n",
      "Step 119100 | Loss: 23.29780 | LR: 2.57e-05\n",
      "Step 119200 | Loss: 22.53951 | LR: 2.56e-05\n",
      "val loss: 23.1852\n",
      "Step 119300 | Loss: 21.86353 | LR: 2.55e-05\n",
      "=== Step 119339 Done. Avg Loss: 22.92158 ===\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 119400 | Loss: 22.74426 | LR: 2.54e-05\n",
      "val loss: 22.8540\n",
      "Step 119500 | Loss: 22.90452 | LR: 2.53e-05\n",
      "Step 119600 | Loss: 22.39502 | LR: 2.53e-05\n",
      "Step 119700 | Loss: 23.46350 | LR: 2.52e-05\n",
      "val loss: 22.9892\n",
      "Step 119800 | Loss: 23.57145 | LR: 2.51e-05\n",
      "Step 119900 | Loss: 25.07237 | LR: 2.50e-05\n",
      "val loss: 23.0135\n",
      "Step 120000 | Loss: 23.12711 | LR: 2.49e-05\n",
      "Step 120100 | Loss: 23.90375 | LR: 2.49e-05\n",
      "Step 120200 | Loss: 22.48194 | LR: 2.48e-05\n",
      "val loss: 22.8927\n",
      "Step 120300 | Loss: 22.49623 | LR: 2.47e-05\n",
      "Step 120400 | Loss: 22.35937 | LR: 2.46e-05\n",
      "val loss: 23.1802\n",
      "Step 120500 | Loss: 23.25343 | LR: 2.45e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 120600 | Loss: 22.28451 | LR: 2.44e-05\n",
      "Step 120700 | Loss: 23.98928 | LR: 2.44e-05\n",
      "loading /home/ubuntu/pretraining/val/pt_shard_k562e_val_0000.npz\n",
      "val loss: 22.8306\n",
      "Step 120800 | Loss: 23.02560 | LR: 2.43e-05\n",
      "Step 120900 | Loss: 23.69159 | LR: 2.42e-05\n",
      "val loss: 22.9710\n",
      "Step 121000 | Loss: 21.91865 | LR: 2.41e-05\n",
      "=== Step 121094 Done. Avg Loss: 22.93672 ===\n",
      "Step 121100 | Loss: 23.05784 | LR: 2.40e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 121200 | Loss: 24.00906 | LR: 2.40e-05\n",
      "val loss: 23.0834\n",
      "Step 121300 | Loss: 23.14474 | LR: 2.39e-05\n",
      "Step 121400 | Loss: 22.81694 | LR: 2.38e-05\n",
      "val loss: 22.8079\n",
      "Step 121500 | Loss: 22.93167 | LR: 2.37e-05\n",
      "Step 121600 | Loss: 23.10653 | LR: 2.36e-05\n",
      "Step 121700 | Loss: 23.24662 | LR: 2.36e-05\n",
      "val loss: 23.0655\n",
      "Step 121800 | Loss: 22.46349 | LR: 2.35e-05\n",
      "Step 121900 | Loss: 22.70448 | LR: 2.34e-05\n",
      "val loss: 22.8717\n",
      "Step 122000 | Loss: 23.32910 | LR: 2.33e-05\n",
      "Step 122100 | Loss: 23.09336 | LR: 2.32e-05\n",
      "Step 122200 | Loss: 22.86858 | LR: 2.32e-05\n",
      "val loss: 22.7760\n",
      "Step 122300 | Loss: 24.36909 | LR: 2.31e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 122400 | Loss: 23.50959 | LR: 2.30e-05\n",
      "val loss: 22.9377\n",
      "Step 122500 | Loss: 22.81463 | LR: 2.29e-05\n",
      "Step 122600 | Loss: 23.21872 | LR: 2.28e-05\n",
      "Step 122700 | Loss: 23.48619 | LR: 2.28e-05\n",
      "val loss: 23.1630\n",
      "Step 122800 | Loss: 23.34545 | LR: 2.27e-05\n",
      "=== Step 122849 Done. Avg Loss: 22.93901 ===\n",
      "Step 122900 | Loss: 23.15108 | LR: 2.26e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "val loss: 22.9685\n",
      "Step 123000 | Loss: 23.02237 | LR: 2.25e-05\n",
      "Step 123100 | Loss: 23.08217 | LR: 2.25e-05\n",
      "Step 123200 | Loss: 23.27491 | LR: 2.24e-05\n",
      "val loss: 22.6025\n",
      "Step 123300 | Loss: 23.19747 | LR: 2.23e-05\n",
      "Step 123400 | Loss: 23.67377 | LR: 2.22e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "val loss: 22.9771\n",
      "Step 123500 | Loss: 23.21344 | LR: 2.21e-05\n",
      "Step 123600 | Loss: 22.31119 | LR: 2.21e-05\n",
      "Step 123700 | Loss: 22.07592 | LR: 2.20e-05\n",
      "val loss: 22.9088\n",
      "Step 123800 | Loss: 23.04753 | LR: 2.19e-05\n",
      "Step 123900 | Loss: 22.36000 | LR: 2.18e-05\n",
      "val loss: 22.8840\n",
      "Step 124000 | Loss: 23.31935 | LR: 2.18e-05\n",
      "Step 124100 | Loss: 22.93454 | LR: 2.17e-05\n",
      "Step 124200 | Loss: 22.87791 | LR: 2.16e-05\n",
      "val loss: 23.0757\n",
      "Step 124300 | Loss: 23.16966 | LR: 2.15e-05\n",
      "Step 124400 | Loss: 23.03650 | LR: 2.14e-05\n",
      "val loss: 22.9716\n",
      "Step 124500 | Loss: 23.29807 | LR: 2.14e-05\n",
      "Step 124600 | Loss: 23.04654 | LR: 2.13e-05\n",
      "=== Step 124604 Done. Avg Loss: 22.95320 ===\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 124700 | Loss: 22.90959 | LR: 2.12e-05\n",
      "val loss: 23.0752\n",
      "Step 124800 | Loss: 22.85933 | LR: 2.11e-05\n",
      "Step 124900 | Loss: 23.60744 | LR: 2.11e-05\n",
      "loading /home/ubuntu/pretraining/val/pt_shard_k562e_val_0000.npz\n",
      "val loss: 22.9851\n",
      "Step 125000 | Loss: 23.08774 | LR: 2.10e-05\n",
      "Step 125100 | Loss: 21.92944 | LR: 2.09e-05\n",
      "Step 125200 | Loss: 23.05691 | LR: 2.08e-05\n",
      "val loss: 22.9740\n",
      "Step 125300 | Loss: 23.31481 | LR: 2.07e-05\n",
      "Step 125400 | Loss: 23.20629 | LR: 2.07e-05\n",
      "val loss: 23.0035\n",
      "Step 125500 | Loss: 22.26993 | LR: 2.06e-05\n",
      "Step 125600 | Loss: 22.69679 | LR: 2.05e-05\n",
      "Step 125700 | Loss: 22.65411 | LR: 2.04e-05\n",
      "val loss: 23.2037\n",
      "Step 125800 | Loss: 23.41937 | LR: 2.04e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 125900 | Loss: 23.81683 | LR: 2.03e-05\n",
      "val loss: 22.9130\n",
      "Step 126000 | Loss: 23.34618 | LR: 2.02e-05\n",
      "Step 126100 | Loss: 23.28172 | LR: 2.01e-05\n",
      "Step 126200 | Loss: 22.42453 | LR: 2.01e-05\n",
      "val loss: 22.9544\n",
      "Step 126300 | Loss: 23.57622 | LR: 2.00e-05\n",
      "=== Step 126359 Done. Avg Loss: 22.95606 ===\n",
      "Step 126400 | Loss: 23.38865 | LR: 1.99e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "val loss: 23.4522\n",
      "Step 126500 | Loss: 22.85880 | LR: 1.98e-05\n",
      "Step 126600 | Loss: 22.15740 | LR: 1.98e-05\n",
      "Step 126700 | Loss: 22.37444 | LR: 1.97e-05\n",
      "val loss: 22.7540\n",
      "Step 126800 | Loss: 22.75309 | LR: 1.96e-05\n",
      "Step 126900 | Loss: 23.18736 | LR: 1.95e-05\n",
      "val loss: 22.8507\n",
      "Step 127000 | Loss: 23.00309 | LR: 1.95e-05\n",
      "Step 127100 | Loss: 22.53803 | LR: 1.94e-05\n",
      "Step 127200 | Loss: 22.52872 | LR: 1.93e-05\n",
      "val loss: 22.9294\n",
      "Step 127300 | Loss: 23.24394 | LR: 1.92e-05\n",
      "Step 127400 | Loss: 23.92296 | LR: 1.92e-05\n",
      "val loss: 22.9561\n",
      "Step 127500 | Loss: 23.35219 | LR: 1.91e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 127600 | Loss: 23.94429 | LR: 1.90e-05\n",
      "Step 127700 | Loss: 24.16746 | LR: 1.89e-05\n",
      "val loss: 22.8767\n",
      "Step 127800 | Loss: 23.00286 | LR: 1.89e-05\n",
      "Step 127900 | Loss: 22.88973 | LR: 1.88e-05\n",
      "val loss: 23.1558\n",
      "Step 128000 | Loss: 22.77374 | LR: 1.87e-05\n",
      "Step 128100 | Loss: 21.70133 | LR: 1.87e-05\n",
      "=== Step 128114 Done. Avg Loss: 22.98344 ===\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 128200 | Loss: 22.97715 | LR: 1.86e-05\n",
      "val loss: 22.5695\n",
      "Step 128300 | Loss: 22.41928 | LR: 1.85e-05\n",
      "Step 128400 | Loss: 23.92745 | LR: 1.84e-05\n",
      "val loss: 23.0435\n",
      "Step 128500 | Loss: 23.28871 | LR: 1.84e-05\n",
      "Step 128600 | Loss: 22.74506 | LR: 1.83e-05\n",
      "Step 128700 | Loss: 23.73291 | LR: 1.82e-05\n",
      "val loss: 22.9980\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 128800 | Loss: 23.54184 | LR: 1.81e-05\n",
      "Step 128900 | Loss: 23.09390 | LR: 1.81e-05\n",
      "val loss: 22.8734\n",
      "Step 129000 | Loss: 22.98707 | LR: 1.80e-05\n",
      "Step 129100 | Loss: 23.08620 | LR: 1.79e-05\n",
      "Step 129200 | Loss: 23.23614 | LR: 1.79e-05\n",
      "loading /home/ubuntu/pretraining/val/pt_shard_k562e_val_0000.npz\n",
      "val loss: 22.8421\n",
      "Step 129300 | Loss: 21.96401 | LR: 1.78e-05\n",
      "Step 129400 | Loss: 22.40090 | LR: 1.77e-05\n",
      "val loss: 23.1310\n",
      "Step 129500 | Loss: 23.30257 | LR: 1.76e-05\n",
      "Step 129600 | Loss: 22.84192 | LR: 1.76e-05\n",
      "Step 129700 | Loss: 23.26654 | LR: 1.75e-05\n",
      "val loss: 23.2990\n",
      "Step 129800 | Loss: 23.26515 | LR: 1.74e-05\n",
      "=== Step 129869 Done. Avg Loss: 22.97920 ===\n",
      "Step 129900 | Loss: 23.35833 | LR: 1.73e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "val loss: 23.2595\n",
      "Step 130000 | Loss: 22.98172 | LR: 1.73e-05\n",
      "Step 130100 | Loss: 22.27114 | LR: 1.72e-05\n",
      "Step 130200 | Loss: 23.71948 | LR: 1.71e-05\n",
      "val loss: 22.8485\n",
      "Step 130300 | Loss: 23.63470 | LR: 1.71e-05\n",
      "Step 130400 | Loss: 22.78856 | LR: 1.70e-05\n",
      "val loss: 23.1520\n",
      "Step 130500 | Loss: 23.35818 | LR: 1.69e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 130600 | Loss: 23.49173 | LR: 1.69e-05\n",
      "Step 130700 | Loss: 23.00765 | LR: 1.68e-05\n",
      "val loss: 22.8513\n",
      "Step 130800 | Loss: 22.70637 | LR: 1.67e-05\n",
      "Step 130900 | Loss: 22.48103 | LR: 1.66e-05\n",
      "val loss: 22.8622\n",
      "Step 131000 | Loss: 22.48116 | LR: 1.66e-05\n",
      "Step 131100 | Loss: 23.41722 | LR: 1.65e-05\n",
      "Step 131200 | Loss: 23.73326 | LR: 1.64e-05\n",
      "val loss: 22.9665\n",
      "Step 131300 | Loss: 22.98985 | LR: 1.64e-05\n",
      "Step 131400 | Loss: 23.23525 | LR: 1.63e-05\n",
      "val loss: 23.2455\n",
      "Step 131500 | Loss: 22.40708 | LR: 1.62e-05\n",
      "Step 131600 | Loss: 23.10788 | LR: 1.62e-05\n",
      "=== Step 131624 Done. Avg Loss: 23.08799 ===\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 131700 | Loss: 23.86064 | LR: 1.61e-05\n",
      "val loss: 22.9978\n",
      "Step 131800 | Loss: 22.92063 | LR: 1.60e-05\n",
      "Step 131900 | Loss: 22.59296 | LR: 1.59e-05\n",
      "val loss: 23.0372\n",
      "Step 132000 | Loss: 21.91101 | LR: 1.59e-05\n",
      "Step 132100 | Loss: 23.20095 | LR: 1.58e-05\n",
      "Step 132200 | Loss: 23.16068 | LR: 1.57e-05\n",
      "val loss: 23.2545\n",
      "Step 132300 | Loss: 22.56843 | LR: 1.57e-05\n",
      "Step 132400 | Loss: 23.23022 | LR: 1.56e-05\n",
      "val loss: 23.2483\n",
      "Step 132500 | Loss: 22.71180 | LR: 1.55e-05\n",
      "Step 132600 | Loss: 22.91681 | LR: 1.55e-05\n",
      "Step 132700 | Loss: 23.46654 | LR: 1.54e-05\n",
      "val loss: 23.1806\n",
      "Step 132800 | Loss: 23.93863 | LR: 1.53e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 132900 | Loss: 23.03407 | LR: 1.53e-05\n",
      "val loss: 23.1814\n",
      "Step 133000 | Loss: 24.06724 | LR: 1.52e-05\n",
      "Step 133100 | Loss: 23.71518 | LR: 1.51e-05\n",
      "Step 133200 | Loss: 23.88490 | LR: 1.51e-05\n",
      "val loss: 22.8675\n",
      "Step 133300 | Loss: 23.06911 | LR: 1.50e-05\n",
      "=== Step 133379 Done. Avg Loss: 23.11598 ===\n",
      "Step 133400 | Loss: 23.22522 | LR: 1.49e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "loading /home/ubuntu/pretraining/val/pt_shard_k562e_val_0000.npz\n",
      "val loss: 23.0351\n",
      "Step 133500 | Loss: 23.58818 | LR: 1.49e-05\n",
      "Step 133600 | Loss: 23.97293 | LR: 1.48e-05\n",
      "Step 133700 | Loss: 21.75436 | LR: 1.47e-05\n",
      "val loss: 23.1765\n",
      "Step 133800 | Loss: 23.39586 | LR: 1.47e-05\n",
      "Step 133900 | Loss: 22.82357 | LR: 1.46e-05\n",
      "val loss: 23.2128\n",
      "Step 134000 | Loss: 22.55164 | LR: 1.45e-05\n",
      "Step 134100 | Loss: 22.41634 | LR: 1.45e-05\n",
      "Step 134200 | Loss: 23.41236 | LR: 1.44e-05\n",
      "val loss: 23.2776\n",
      "Step 134300 | Loss: 24.25533 | LR: 1.43e-05\n",
      "Step 134400 | Loss: 23.73029 | LR: 1.43e-05\n",
      "val loss: 23.3367\n",
      "Step 134500 | Loss: 21.92426 | LR: 1.42e-05\n",
      "Step 134600 | Loss: 22.55983 | LR: 1.41e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 134700 | Loss: 23.99732 | LR: 1.41e-05\n",
      "val loss: 23.4179\n",
      "Step 134800 | Loss: 22.48405 | LR: 1.40e-05\n",
      "Step 134900 | Loss: 22.11899 | LR: 1.39e-05\n",
      "val loss: 23.0481\n",
      "Step 135000 | Loss: 22.69498 | LR: 1.39e-05\n",
      "Step 135100 | Loss: 22.90668 | LR: 1.38e-05\n",
      "=== Step 135134 Done. Avg Loss: 23.13661 ===\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 135200 | Loss: 23.91875 | LR: 1.37e-05\n",
      "val loss: 23.3029\n",
      "Step 135300 | Loss: 22.68410 | LR: 1.37e-05\n",
      "Step 135400 | Loss: 23.61824 | LR: 1.36e-05\n",
      "val loss: 23.2215\n",
      "Step 135500 | Loss: 23.68239 | LR: 1.35e-05\n",
      "Step 135600 | Loss: 23.00511 | LR: 1.35e-05\n",
      "Step 135700 | Loss: 22.82719 | LR: 1.34e-05\n",
      "val loss: 23.0212\n",
      "Step 135800 | Loss: 23.01659 | LR: 1.33e-05\n",
      "Step 135900 | Loss: 23.64133 | LR: 1.33e-05\n",
      "val loss: 22.9907\n",
      "Step 136000 | Loss: 23.36567 | LR: 1.32e-05\n",
      "Step 136100 | Loss: 23.40802 | LR: 1.32e-05\n",
      "Step 136200 | Loss: 22.55530 | LR: 1.31e-05\n",
      "val loss: 23.3366\n",
      "Step 136300 | Loss: 22.85008 | LR: 1.30e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 136400 | Loss: 22.96977 | LR: 1.30e-05\n",
      "val loss: 23.1123\n",
      "Step 136500 | Loss: 23.35831 | LR: 1.29e-05\n",
      "Step 136600 | Loss: 22.70074 | LR: 1.28e-05\n",
      "Step 136700 | Loss: 24.12573 | LR: 1.28e-05\n",
      "val loss: 23.2419\n",
      "Step 136800 | Loss: 22.95845 | LR: 1.27e-05\n",
      "=== Step 136889 Done. Avg Loss: 23.14867 ===\n",
      "Step 136900 | Loss: 23.67029 | LR: 1.27e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "val loss: 23.1373\n",
      "Step 137000 | Loss: 23.67833 | LR: 1.26e-05\n",
      "Step 137100 | Loss: 22.44187 | LR: 1.25e-05\n",
      "Step 137200 | Loss: 24.91242 | LR: 1.25e-05\n",
      "val loss: 22.9779\n",
      "Step 137300 | Loss: 23.73951 | LR: 1.24e-05\n",
      "Step 137400 | Loss: 22.14393 | LR: 1.23e-05\n",
      "val loss: 23.0630\n",
      "Step 137500 | Loss: 22.68229 | LR: 1.23e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 137600 | Loss: 22.08090 | LR: 1.22e-05\n",
      "Step 137700 | Loss: 22.48175 | LR: 1.22e-05\n",
      "val loss: 22.9249\n",
      "Step 137800 | Loss: 22.71935 | LR: 1.21e-05\n",
      "Step 137900 | Loss: 22.69036 | LR: 1.20e-05\n",
      "loading /home/ubuntu/pretraining/val/pt_shard_k562e_val_0000.npz\n",
      "val loss: 23.1157\n",
      "Step 138000 | Loss: 23.42248 | LR: 1.20e-05\n",
      "Step 138100 | Loss: 22.71977 | LR: 1.19e-05\n",
      "Step 138200 | Loss: 23.85913 | LR: 1.18e-05\n",
      "val loss: 23.0829\n",
      "Step 138300 | Loss: 23.39240 | LR: 1.18e-05\n",
      "Step 138400 | Loss: 23.45551 | LR: 1.17e-05\n",
      "val loss: 23.0972\n",
      "Step 138500 | Loss: 22.92115 | LR: 1.17e-05\n",
      "Step 138600 | Loss: 24.06433 | LR: 1.16e-05\n",
      "=== Step 138644 Done. Avg Loss: 23.15520 ===\n",
      "Step 138700 | Loss: 22.97143 | LR: 1.15e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "val loss: 22.9815\n",
      "Step 138800 | Loss: 23.19555 | LR: 1.15e-05\n",
      "Step 138900 | Loss: 22.08325 | LR: 1.14e-05\n",
      "val loss: 23.4735\n",
      "Step 139000 | Loss: 23.02504 | LR: 1.14e-05\n",
      "Step 139100 | Loss: 24.39779 | LR: 1.13e-05\n",
      "Step 139200 | Loss: 22.59434 | LR: 1.12e-05\n",
      "val loss: 23.4155\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 139300 | Loss: 22.74275 | LR: 1.12e-05\n",
      "Step 139400 | Loss: 23.67664 | LR: 1.11e-05\n",
      "val loss: 23.3496\n",
      "Step 139500 | Loss: 23.17937 | LR: 1.11e-05\n",
      "Step 139600 | Loss: 23.13650 | LR: 1.10e-05\n",
      "Step 139700 | Loss: 22.59236 | LR: 1.10e-05\n",
      "val loss: 22.7886\n",
      "Step 139800 | Loss: 23.65664 | LR: 1.09e-05\n",
      "Step 139900 | Loss: 21.66913 | LR: 1.08e-05\n",
      "val loss: 23.1639\n",
      "Step 140000 | Loss: 23.83098 | LR: 1.08e-05\n",
      "Step 140100 | Loss: 22.16466 | LR: 1.07e-05\n",
      "Step 140200 | Loss: 23.30445 | LR: 1.07e-05\n",
      "val loss: 22.9748\n",
      "Step 140300 | Loss: 23.19151 | LR: 1.06e-05\n",
      "=== Step 140399 Done. Avg Loss: 23.11572 ===\n",
      "Step 140400 | Loss: 22.65203 | LR: 1.05e-05\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "val loss: 22.9814\n",
      "Step 140500 | Loss: 22.98772 | LR: 1.05e-05\n",
      "Step 140600 | Loss: 23.42023 | LR: 1.04e-05\n",
      "Step 140700 | Loss: 23.26996 | LR: 1.04e-05\n",
      "val loss: 23.0533\n",
      "Step 140800 | Loss: 22.78613 | LR: 1.03e-05\n",
      "Step 140900 | Loss: 23.62202 | LR: 1.03e-05\n",
      "val loss: 22.9388\n",
      "Step 141000 | Loss: 23.23842 | LR: 1.02e-05\n",
      "Step 141100 | Loss: 22.35555 | LR: 1.01e-05\n",
      "Step 141200 | Loss: 23.23453 | LR: 1.01e-05\n",
      "val loss: 22.8770\n",
      "Step 141300 | Loss: 24.37962 | LR: 1.00e-05\n",
      "Step 141400 | Loss: 22.70814 | LR: 9.97e-06\n",
      "val loss: 22.9486\n",
      "Step 141500 | Loss: 23.64414 | LR: 9.91e-06\n",
      "Step 141600 | Loss: 22.25002 | LR: 9.86e-06\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 141700 | Loss: 23.13476 | LR: 9.80e-06\n",
      "val loss: 23.1392\n",
      "Step 141800 | Loss: 23.57500 | LR: 9.75e-06\n",
      "Step 141900 | Loss: 23.32502 | LR: 9.69e-06\n",
      "val loss: 22.9932\n",
      "Step 142000 | Loss: 22.10766 | LR: 9.63e-06\n",
      "Step 142100 | Loss: 22.77919 | LR: 9.58e-06\n",
      "=== Step 142154 Done. Avg Loss: 23.01404 ===\n",
      "Step 142200 | Loss: 23.12950 | LR: 9.52e-06\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "loading /home/ubuntu/pretraining/val/pt_shard_k562e_val_0000.npz\n",
      "val loss: 22.7906\n",
      "Step 142300 | Loss: 23.06532 | LR: 9.47e-06\n",
      "Step 142400 | Loss: 22.87206 | LR: 9.41e-06\n",
      "val loss: 23.1397\n",
      "Step 142500 | Loss: 22.79823 | LR: 9.36e-06\n",
      "Step 142600 | Loss: 23.83662 | LR: 9.30e-06\n",
      "Step 142700 | Loss: 22.72929 | LR: 9.25e-06\n",
      "val loss: 23.0956\n",
      "Step 142800 | Loss: 23.29938 | LR: 9.19e-06\n",
      "Step 142900 | Loss: 23.03793 | LR: 9.14e-06\n",
      "val loss: 23.3524\n",
      "Step 143000 | Loss: 23.43176 | LR: 9.09e-06\n",
      "Step 143100 | Loss: 21.98866 | LR: 9.03e-06\n",
      "Step 143200 | Loss: 23.63824 | LR: 8.98e-06\n",
      "val loss: 23.0299\n",
      "Step 143300 | Loss: 23.26544 | LR: 8.92e-06\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 143400 | Loss: 23.11510 | LR: 8.87e-06\n",
      "val loss: 23.1522\n",
      "Step 143500 | Loss: 22.66900 | LR: 8.82e-06\n",
      "Step 143600 | Loss: 23.29461 | LR: 8.76e-06\n",
      "Step 143700 | Loss: 23.93458 | LR: 8.71e-06\n",
      "val loss: 22.7968\n",
      "Step 143800 | Loss: 23.63382 | LR: 8.66e-06\n",
      "Step 143900 | Loss: 23.55295 | LR: 8.60e-06\n",
      "=== Step 143909 Done. Avg Loss: 23.08292 ===\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "val loss: 23.1307\n",
      "Step 144000 | Loss: 23.10524 | LR: 8.55e-06\n",
      "Step 144100 | Loss: 21.59423 | LR: 8.50e-06\n",
      "Step 144200 | Loss: 22.85683 | LR: 8.45e-06\n",
      "val loss: 23.0966\n",
      "Step 144300 | Loss: 24.44010 | LR: 8.39e-06\n",
      "Step 144400 | Loss: 22.76282 | LR: 8.34e-06\n",
      "val loss: 23.0608\n",
      "Step 144500 | Loss: 24.17938 | LR: 8.29e-06\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 144600 | Loss: 23.92336 | LR: 8.24e-06\n",
      "Step 144700 | Loss: 22.88077 | LR: 8.19e-06\n",
      "val loss: 23.1164\n",
      "Step 144800 | Loss: 24.64322 | LR: 8.13e-06\n",
      "Step 144900 | Loss: 22.56256 | LR: 8.08e-06\n",
      "val loss: 23.2390\n",
      "Step 145000 | Loss: 22.64716 | LR: 8.03e-06\n",
      "Step 145100 | Loss: 24.05288 | LR: 7.98e-06\n",
      "Step 145200 | Loss: 24.33249 | LR: 7.93e-06\n",
      "val loss: 23.0948\n",
      "Step 145300 | Loss: 24.70648 | LR: 7.88e-06\n",
      "Step 145400 | Loss: 23.68045 | LR: 7.83e-06\n",
      "val loss: 23.0166\n",
      "Step 145500 | Loss: 22.24096 | LR: 7.78e-06\n",
      "Step 145600 | Loss: 23.18161 | LR: 7.73e-06\n",
      "=== Step 145664 Done. Avg Loss: 23.06887 ===\n",
      "Step 145700 | Loss: 24.05170 | LR: 7.68e-06\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "val loss: 23.0669\n",
      "Step 145800 | Loss: 23.26194 | LR: 7.63e-06\n",
      "Step 145900 | Loss: 23.22717 | LR: 7.58e-06\n",
      "val loss: 22.9916\n",
      "Step 146000 | Loss: 23.00509 | LR: 7.53e-06\n",
      "Step 146100 | Loss: 23.27411 | LR: 7.48e-06\n",
      "Step 146200 | Loss: 23.18444 | LR: 7.43e-06\n",
      "val loss: 22.9191\n",
      "Step 146300 | Loss: 23.90819 | LR: 7.38e-06\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 146400 | Loss: 23.34436 | LR: 7.33e-06\n",
      "loading /home/ubuntu/pretraining/val/pt_shard_k562e_val_0000.npz\n",
      "val loss: 22.8772\n",
      "Step 146500 | Loss: 23.49780 | LR: 7.28e-06\n",
      "Step 146600 | Loss: 21.94724 | LR: 7.23e-06\n",
      "Step 146700 | Loss: 22.28963 | LR: 7.18e-06\n",
      "val loss: 22.9640\n",
      "Step 146800 | Loss: 22.57013 | LR: 7.13e-06\n",
      "Step 146900 | Loss: 23.72919 | LR: 7.09e-06\n",
      "val loss: 23.1548\n",
      "Step 147000 | Loss: 22.74590 | LR: 7.04e-06\n",
      "Step 147100 | Loss: 22.98253 | LR: 6.99e-06\n",
      "Step 147200 | Loss: 21.96660 | LR: 6.94e-06\n",
      "val loss: 22.9792\n",
      "Step 147300 | Loss: 23.48803 | LR: 6.89e-06\n",
      "Step 147400 | Loss: 23.49691 | LR: 6.85e-06\n",
      "=== Step 147419 Done. Avg Loss: 23.03103 ===\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "val loss: 22.8060\n",
      "Step 147500 | Loss: 22.87531 | LR: 6.80e-06\n",
      "Step 147600 | Loss: 22.49999 | LR: 6.75e-06\n",
      "Step 147700 | Loss: 22.82044 | LR: 6.70e-06\n",
      "val loss: 22.5701\n",
      "Step 147800 | Loss: 22.11555 | LR: 6.66e-06\n",
      "Step 147900 | Loss: 21.60765 | LR: 6.61e-06\n",
      "val loss: 22.8932\n",
      "Step 148000 | Loss: 22.65553 | LR: 6.56e-06\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 148100 | Loss: 23.31884 | LR: 6.52e-06\n",
      "Step 148200 | Loss: 22.96093 | LR: 6.47e-06\n",
      "val loss: 23.1156\n",
      "Step 148300 | Loss: 22.03470 | LR: 6.42e-06\n",
      "Step 148400 | Loss: 22.41868 | LR: 6.38e-06\n",
      "val loss: 23.1902\n",
      "Step 148500 | Loss: 23.17376 | LR: 6.33e-06\n",
      "Step 148600 | Loss: 23.09138 | LR: 6.29e-06\n",
      "Step 148700 | Loss: 22.24196 | LR: 6.24e-06\n",
      "val loss: 23.1231\n",
      "Step 148800 | Loss: 22.92797 | LR: 6.20e-06\n",
      "Step 148900 | Loss: 23.93510 | LR: 6.15e-06\n",
      "val loss: 23.0805\n",
      "Step 149000 | Loss: 22.04458 | LR: 6.10e-06\n",
      "Step 149100 | Loss: 23.22548 | LR: 6.06e-06\n",
      "=== Step 149174 Done. Avg Loss: 23.02852 ===\n",
      "Step 149200 | Loss: 23.36591 | LR: 6.01e-06\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "val loss: 22.9631\n",
      "Step 149300 | Loss: 23.53842 | LR: 5.97e-06\n",
      "Step 149400 | Loss: 23.95767 | LR: 5.93e-06\n",
      "val loss: 23.1499\n",
      "Step 149500 | Loss: 22.71421 | LR: 5.88e-06\n",
      "Step 149600 | Loss: 23.04454 | LR: 5.84e-06\n",
      "Step 149700 | Loss: 22.62380 | LR: 5.79e-06\n",
      "val loss: 23.0824\n",
      "Step 149800 | Loss: 22.27908 | LR: 5.75e-06\n",
      "Step 149900 | Loss: 22.80398 | LR: 5.70e-06\n",
      "val loss: 23.4129\n",
      "Step 150000 | Loss: 23.05054 | LR: 5.66e-06\n",
      "Step 150100 | Loss: 23.15936 | LR: 5.62e-06\n",
      "Step 150200 | Loss: 23.40105 | LR: 5.57e-06\n",
      "val loss: 23.1260\n",
      "Step 150300 | Loss: 22.33910 | LR: 5.53e-06\n",
      "Step 150400 | Loss: 23.24672 | LR: 5.49e-06\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "val loss: 23.1728\n",
      "Step 150500 | Loss: 22.93618 | LR: 5.45e-06\n",
      "Step 150600 | Loss: 23.01075 | LR: 5.40e-06\n",
      "Step 150700 | Loss: 22.15493 | LR: 5.36e-06\n",
      "loading /home/ubuntu/pretraining/val/pt_shard_k562e_val_0000.npz\n",
      "val loss: 23.1282\n",
      "Step 150800 | Loss: 23.22331 | LR: 5.32e-06\n",
      "Step 150900 | Loss: 22.92518 | LR: 5.28e-06\n",
      "=== Step 150929 Done. Avg Loss: 23.06566 ===\n",
      "val loss: 22.8200\n",
      "Step 151000 | Loss: 22.25080 | LR: 5.23e-06\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 151100 | Loss: 23.06950 | LR: 5.19e-06\n",
      "Step 151200 | Loss: 23.42117 | LR: 5.15e-06\n",
      "val loss: 23.0114\n",
      "Step 151300 | Loss: 22.93447 | LR: 5.11e-06\n",
      "Step 151400 | Loss: 22.22146 | LR: 5.07e-06\n",
      "val loss: 23.0429\n",
      "Step 151500 | Loss: 23.68108 | LR: 5.03e-06\n",
      "Step 151600 | Loss: 23.71451 | LR: 4.98e-06\n",
      "Step 151700 | Loss: 23.26895 | LR: 4.94e-06\n",
      "val loss: 22.7583\n",
      "Step 151800 | Loss: 22.87015 | LR: 4.90e-06\n",
      "Step 151900 | Loss: 23.39363 | LR: 4.86e-06\n",
      "val loss: 22.6385\n",
      "Step 152000 | Loss: 22.58594 | LR: 4.82e-06\n",
      "Step 152100 | Loss: 22.12055 | LR: 4.78e-06\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 152200 | Loss: 22.08242 | LR: 4.74e-06\n",
      "val loss: 23.1556\n",
      "Step 152300 | Loss: 22.45894 | LR: 4.70e-06\n",
      "Step 152400 | Loss: 22.95228 | LR: 4.66e-06\n",
      "val loss: 23.2728\n",
      "Step 152500 | Loss: 22.79299 | LR: 4.62e-06\n",
      "Step 152600 | Loss: 22.95794 | LR: 4.58e-06\n",
      "=== Step 152684 Done. Avg Loss: 23.02106 ===\n",
      "Step 152700 | Loss: 23.84333 | LR: 4.54e-06\n",
      "val loss: 23.3236\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 152800 | Loss: 23.18127 | LR: 4.50e-06\n",
      "Step 152900 | Loss: 23.06995 | LR: 4.47e-06\n",
      "val loss: 22.6169\n",
      "Step 153000 | Loss: 24.17879 | LR: 4.43e-06\n",
      "Step 153100 | Loss: 23.43886 | LR: 4.39e-06\n",
      "Step 153200 | Loss: 24.00707 | LR: 4.35e-06\n",
      "val loss: 23.2447\n",
      "Step 153300 | Loss: 24.11250 | LR: 4.31e-06\n",
      "Step 153400 | Loss: 23.39371 | LR: 4.27e-06\n",
      "val loss: 22.9127\n",
      "Step 153500 | Loss: 23.29905 | LR: 4.23e-06\n",
      "Step 153600 | Loss: 22.73123 | LR: 4.20e-06\n",
      "Step 153700 | Loss: 23.13488 | LR: 4.16e-06\n",
      "val loss: 22.8617\n",
      "Step 153800 | Loss: 23.43040 | LR: 4.12e-06\n",
      "Step 153900 | Loss: 22.76610 | LR: 4.08e-06\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "val loss: 23.1507\n",
      "Step 154000 | Loss: 21.91723 | LR: 4.05e-06\n",
      "Step 154100 | Loss: 22.56160 | LR: 4.01e-06\n",
      "Step 154200 | Loss: 22.66882 | LR: 3.97e-06\n",
      "val loss: 22.8272\n",
      "Step 154300 | Loss: 22.30167 | LR: 3.94e-06\n",
      "Step 154400 | Loss: 24.02206 | LR: 3.90e-06\n",
      "=== Step 154439 Done. Avg Loss: 22.98590 ===\n",
      "val loss: 22.8983\n",
      "Step 154500 | Loss: 22.72074 | LR: 3.86e-06\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 154600 | Loss: 21.38396 | LR: 3.83e-06\n",
      "Step 154700 | Loss: 23.04288 | LR: 3.79e-06\n",
      "val loss: 22.8333\n",
      "Step 154800 | Loss: 23.28197 | LR: 3.76e-06\n",
      "Step 154900 | Loss: 23.44258 | LR: 3.72e-06\n",
      "val loss: 23.1633\n",
      "Step 155000 | Loss: 23.34563 | LR: 3.68e-06\n",
      "Step 155100 | Loss: 22.37146 | LR: 3.65e-06\n",
      "Step 155200 | Loss: 22.29790 | LR: 3.61e-06\n",
      "loading /home/ubuntu/pretraining/val/pt_shard_k562e_val_0000.npz\n",
      "val loss: 23.1887\n",
      "Step 155300 | Loss: 22.13482 | LR: 3.58e-06\n",
      "Step 155400 | Loss: 23.11094 | LR: 3.54e-06\n",
      "val loss: 23.1466\n",
      "Step 155500 | Loss: 22.14222 | LR: 3.51e-06\n",
      "Step 155600 | Loss: 22.42330 | LR: 3.47e-06\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 155700 | Loss: 23.70949 | LR: 3.44e-06\n",
      "val loss: 22.8520\n",
      "Step 155800 | Loss: 23.32511 | LR: 3.41e-06\n",
      "Step 155900 | Loss: 22.43501 | LR: 3.37e-06\n",
      "val loss: 22.8924\n",
      "Step 156000 | Loss: 22.92584 | LR: 3.34e-06\n",
      "Step 156100 | Loss: 22.10315 | LR: 3.30e-06\n",
      "=== Step 156194 Done. Avg Loss: 22.97824 ===\n",
      "Step 156200 | Loss: 22.70489 | LR: 3.27e-06\n",
      "val loss: 22.5961\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 156300 | Loss: 22.56083 | LR: 3.24e-06\n",
      "Step 156400 | Loss: 22.10775 | LR: 3.20e-06\n",
      "val loss: 22.8575\n",
      "Step 156500 | Loss: 22.88792 | LR: 3.17e-06\n",
      "Step 156600 | Loss: 23.09509 | LR: 3.14e-06\n",
      "Step 156700 | Loss: 22.05338 | LR: 3.10e-06\n",
      "val loss: 23.1164\n",
      "Step 156800 | Loss: 22.66919 | LR: 3.07e-06\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 156900 | Loss: 21.99931 | LR: 3.04e-06\n",
      "val loss: 23.0497\n",
      "Step 157000 | Loss: 22.90747 | LR: 3.01e-06\n",
      "Step 157100 | Loss: 23.09495 | LR: 2.97e-06\n",
      "Step 157200 | Loss: 22.77338 | LR: 2.94e-06\n",
      "val loss: 22.7578\n",
      "Step 157300 | Loss: 22.66568 | LR: 2.91e-06\n",
      "Step 157400 | Loss: 23.30185 | LR: 2.88e-06\n",
      "val loss: 23.2627\n",
      "Step 157500 | Loss: 23.78528 | LR: 2.85e-06\n",
      "Step 157600 | Loss: 22.67403 | LR: 2.82e-06\n",
      "Step 157700 | Loss: 23.32691 | LR: 2.79e-06\n",
      "val loss: 23.1864\n",
      "Step 157800 | Loss: 24.04328 | LR: 2.75e-06\n",
      "Step 157900 | Loss: 22.25945 | LR: 2.72e-06\n",
      "=== Step 157949 Done. Avg Loss: 23.03093 ===\n",
      "val loss: 22.9084\n",
      "Step 158000 | Loss: 23.37485 | LR: 2.69e-06\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 158100 | Loss: 22.76990 | LR: 2.66e-06\n",
      "Step 158200 | Loss: 23.83548 | LR: 2.63e-06\n",
      "val loss: 22.9143\n",
      "Step 158300 | Loss: 22.50939 | LR: 2.60e-06\n",
      "Step 158400 | Loss: 23.63356 | LR: 2.57e-06\n",
      "val loss: 23.1618\n",
      "Step 158500 | Loss: 22.77268 | LR: 2.54e-06\n",
      "Step 158600 | Loss: 22.85011 | LR: 2.51e-06\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 158700 | Loss: 23.15788 | LR: 2.48e-06\n",
      "val loss: 23.1363\n",
      "Step 158800 | Loss: 22.44300 | LR: 2.45e-06\n",
      "Step 158900 | Loss: 23.63398 | LR: 2.43e-06\n",
      "val loss: 23.3450\n",
      "Step 159000 | Loss: 23.12287 | LR: 2.40e-06\n",
      "Step 159100 | Loss: 22.59387 | LR: 2.37e-06\n",
      "Step 159200 | Loss: 23.54095 | LR: 2.34e-06\n",
      "val loss: 23.1324\n",
      "Step 159300 | Loss: 23.16373 | LR: 2.31e-06\n",
      "Step 159400 | Loss: 22.94074 | LR: 2.28e-06\n",
      "loading /home/ubuntu/pretraining/val/pt_shard_k562e_val_0000.npz\n",
      "val loss: 23.3019\n",
      "Step 159500 | Loss: 23.92706 | LR: 2.26e-06\n",
      "Step 159600 | Loss: 23.42673 | LR: 2.23e-06\n",
      "Step 159700 | Loss: 23.38961 | LR: 2.20e-06\n",
      "=== Step 159704 Done. Avg Loss: 23.10281 ===\n",
      "val loss: 23.3560\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 159800 | Loss: 23.45483 | LR: 2.17e-06\n",
      "Step 159900 | Loss: 22.54828 | LR: 2.14e-06\n",
      "val loss: 23.0928\n",
      "Step 160000 | Loss: 22.80784 | LR: 2.12e-06\n",
      "Step 160100 | Loss: 23.09326 | LR: 2.09e-06\n",
      "Step 160200 | Loss: 23.39001 | LR: 2.06e-06\n",
      "val loss: 23.1798\n",
      "Step 160300 | Loss: 23.59419 | LR: 2.04e-06\n",
      "Step 160400 | Loss: 23.21805 | LR: 2.01e-06\n",
      "val loss: 23.0087\n",
      "Step 160500 | Loss: 23.88929 | LR: 1.98e-06\n",
      "Step 160600 | Loss: 23.29871 | LR: 1.96e-06\n",
      "Step 160700 | Loss: 23.31422 | LR: 1.93e-06\n",
      "val loss: 23.3151\n",
      "Step 160800 | Loss: 23.61613 | LR: 1.91e-06\n",
      "Step 160900 | Loss: 22.94537 | LR: 1.88e-06\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "val loss: 23.5873\n",
      "Step 161000 | Loss: 23.45929 | LR: 1.85e-06\n",
      "Step 161100 | Loss: 23.16254 | LR: 1.83e-06\n",
      "Step 161200 | Loss: 23.35628 | LR: 1.80e-06\n",
      "val loss: 23.1025\n",
      "Step 161300 | Loss: 22.85874 | LR: 1.78e-06\n",
      "Step 161400 | Loss: 22.71316 | LR: 1.75e-06\n",
      "=== Step 161459 Done. Avg Loss: 23.24011 ===\n",
      "val loss: 23.0593\n",
      "Step 161500 | Loss: 22.65319 | LR: 1.73e-06\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 161600 | Loss: 24.18911 | LR: 1.71e-06\n",
      "Step 161700 | Loss: 23.21621 | LR: 1.68e-06\n",
      "val loss: 23.0254\n",
      "Step 161800 | Loss: 23.98523 | LR: 1.66e-06\n",
      "Step 161900 | Loss: 23.89382 | LR: 1.63e-06\n",
      "val loss: 22.8638\n",
      "Step 162000 | Loss: 23.35986 | LR: 1.61e-06\n",
      "Step 162100 | Loss: 23.22732 | LR: 1.59e-06\n",
      "Step 162200 | Loss: 22.88725 | LR: 1.56e-06\n",
      "val loss: 23.4080\n",
      "Step 162300 | Loss: 23.59516 | LR: 1.54e-06\n",
      "Step 162400 | Loss: 24.37164 | LR: 1.52e-06\n",
      "val loss: 22.9381\n",
      "Step 162500 | Loss: 23.05178 | LR: 1.49e-06\n",
      "Step 162600 | Loss: 22.99592 | LR: 1.47e-06\n",
      "Step 162700 | Loss: 22.65605 | LR: 1.45e-06\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "val loss: 23.4175\n",
      "Step 162800 | Loss: 23.28720 | LR: 1.42e-06\n",
      "Step 162900 | Loss: 23.23844 | LR: 1.40e-06\n",
      "val loss: 23.3482\n",
      "Step 163000 | Loss: 24.25080 | LR: 1.38e-06\n",
      "Step 163100 | Loss: 23.97977 | LR: 1.36e-06\n",
      "Step 163200 | Loss: 22.26328 | LR: 1.34e-06\n",
      "=== Step 163214 Done. Avg Loss: 23.16044 ===\n",
      "val loss: 22.8630\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 163300 | Loss: 22.52016 | LR: 1.32e-06\n",
      "Step 163400 | Loss: 22.96767 | LR: 1.29e-06\n",
      "val loss: 23.3581\n",
      "Step 163500 | Loss: 22.64702 | LR: 1.27e-06\n",
      "Step 163600 | Loss: 22.79892 | LR: 1.25e-06\n",
      "Step 163700 | Loss: 23.21860 | LR: 1.23e-06\n",
      "loading /home/ubuntu/pretraining/val/pt_shard_k562e_val_0000.npz\n",
      "val loss: 23.2661\n",
      "Step 163800 | Loss: 24.00121 | LR: 1.21e-06\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 163900 | Loss: 23.48725 | LR: 1.19e-06\n",
      "val loss: 23.2451\n",
      "Step 164000 | Loss: 23.12154 | LR: 1.17e-06\n",
      "Step 164100 | Loss: 24.15039 | LR: 1.15e-06\n",
      "Step 164200 | Loss: 25.17068 | LR: 1.13e-06\n",
      "val loss: 23.3791\n",
      "Step 164300 | Loss: 23.48655 | LR: 1.11e-06\n",
      "Step 164400 | Loss: 22.48885 | LR: 1.09e-06\n",
      "val loss: 23.0297\n",
      "Step 164500 | Loss: 22.51380 | LR: 1.07e-06\n",
      "Step 164600 | Loss: 23.27741 | LR: 1.05e-06\n",
      "Step 164700 | Loss: 24.04445 | LR: 1.03e-06\n",
      "val loss: 23.2784\n",
      "Step 164800 | Loss: 23.10007 | LR: 1.01e-06\n",
      "Step 164900 | Loss: 23.29499 | LR: 9.94e-07\n",
      "=== Step 164969 Done. Avg Loss: 23.27192 ===\n",
      "val loss: 23.2918\n",
      "Step 165000 | Loss: 23.05769 | LR: 9.75e-07\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 165100 | Loss: 23.88776 | LR: 9.57e-07\n",
      "Step 165200 | Loss: 23.43452 | LR: 9.39e-07\n",
      "val loss: 23.1856\n",
      "Step 165300 | Loss: 24.05595 | LR: 9.21e-07\n",
      "Step 165400 | Loss: 23.17807 | LR: 9.03e-07\n",
      "val loss: 23.5526\n",
      "Step 165500 | Loss: 22.69584 | LR: 8.85e-07\n",
      "Step 165600 | Loss: 24.36494 | LR: 8.68e-07\n",
      "Step 165700 | Loss: 22.64123 | LR: 8.50e-07\n",
      "val loss: 23.6490\n",
      "Step 165800 | Loss: 23.11776 | LR: 8.33e-07\n",
      "Step 165900 | Loss: 23.14024 | LR: 8.16e-07\n",
      "val loss: 23.6781\n",
      "Step 166000 | Loss: 22.88413 | LR: 7.99e-07\n",
      "Step 166100 | Loss: 22.89511 | LR: 7.82e-07\n",
      "Step 166200 | Loss: 23.88728 | LR: 7.66e-07\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "val loss: 23.3798\n",
      "Step 166300 | Loss: 23.18288 | LR: 7.49e-07\n",
      "Step 166400 | Loss: 23.95407 | LR: 7.33e-07\n",
      "val loss: 23.5044\n",
      "Step 166500 | Loss: 23.29587 | LR: 7.17e-07\n",
      "Step 166600 | Loss: 22.93892 | LR: 7.02e-07\n",
      "Step 166700 | Loss: 22.75804 | LR: 6.86e-07\n",
      "=== Step 166724 Done. Avg Loss: 23.44148 ===\n",
      "val loss: 23.4249\n",
      "Step 166800 | Loss: 22.78777 | LR: 6.70e-07\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 166900 | Loss: 23.62635 | LR: 6.55e-07\n",
      "val loss: 23.5562\n",
      "Step 167000 | Loss: 23.75984 | LR: 6.40e-07\n",
      "Step 167100 | Loss: 23.82463 | LR: 6.25e-07\n",
      "Step 167200 | Loss: 23.44454 | LR: 6.10e-07\n",
      "val loss: 23.8237\n",
      "Step 167300 | Loss: 24.13172 | LR: 5.96e-07\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 167400 | Loss: 24.37521 | LR: 5.81e-07\n",
      "val loss: 23.9369\n",
      "Step 167500 | Loss: 24.32967 | LR: 5.67e-07\n",
      "Step 167600 | Loss: 25.22024 | LR: 5.53e-07\n",
      "Step 167700 | Loss: 24.68078 | LR: 5.39e-07\n",
      "val loss: 24.1772\n",
      "Step 167800 | Loss: 25.34613 | LR: 5.25e-07\n",
      "Step 167900 | Loss: 24.09870 | LR: 5.12e-07\n",
      "loading /home/ubuntu/pretraining/val/pt_shard_k562e_val_0000.npz\n",
      "val loss: 24.6018\n",
      "Step 168000 | Loss: 23.83303 | LR: 4.99e-07\n",
      "Step 168100 | Loss: 23.90420 | LR: 4.85e-07\n",
      "Step 168200 | Loss: 23.71723 | LR: 4.72e-07\n",
      "val loss: 24.4317\n",
      "Step 168300 | Loss: 24.24807 | LR: 4.60e-07\n",
      "Step 168400 | Loss: 24.33979 | LR: 4.47e-07\n",
      "=== Step 168479 Done. Avg Loss: 24.06771 ===\n",
      "val loss: 24.6049\n",
      "Step 168500 | Loss: 24.63911 | LR: 4.34e-07\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 168600 | Loss: 24.72087 | LR: 4.22e-07\n",
      "Step 168700 | Loss: 24.27266 | LR: 4.10e-07\n",
      "val loss: 24.3093\n",
      "Step 168800 | Loss: 23.71864 | LR: 3.98e-07\n",
      "Step 168900 | Loss: 24.29696 | LR: 3.86e-07\n",
      "val loss: 24.1332\n",
      "Step 169000 | Loss: 23.38549 | LR: 3.75e-07\n",
      "Step 169100 | Loss: 24.05939 | LR: 3.63e-07\n",
      "Step 169200 | Loss: 24.69696 | LR: 3.52e-07\n",
      "val loss: 24.0327\n",
      "Step 169300 | Loss: 24.06213 | LR: 3.41e-07\n",
      "Step 169400 | Loss: 24.11882 | LR: 3.30e-07\n",
      "val loss: 24.1080\n",
      "Step 169500 | Loss: 24.08624 | LR: 3.19e-07\n",
      "Step 169600 | Loss: 23.99954 | LR: 3.09e-07\n",
      "Step 169700 | Loss: 23.88327 | LR: 2.98e-07\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "val loss: 24.1923\n",
      "Step 169800 | Loss: 23.65674 | LR: 2.88e-07\n",
      "Step 169900 | Loss: 24.33342 | LR: 2.78e-07\n",
      "val loss: 24.0284\n",
      "Step 170000 | Loss: 24.76541 | LR: 2.68e-07\n",
      "Step 170100 | Loss: 24.18079 | LR: 2.59e-07\n",
      "Step 170200 | Loss: 23.36602 | LR: 2.49e-07\n",
      "=== Step 170234 Done. Avg Loss: 24.18771 ===\n",
      "val loss: 23.9117\n",
      "Step 170300 | Loss: 24.01965 | LR: 2.40e-07\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 170400 | Loss: 24.43223 | LR: 2.31e-07\n",
      "val loss: 24.2669\n",
      "Step 170500 | Loss: 24.47492 | LR: 2.22e-07\n",
      "Step 170600 | Loss: 24.20737 | LR: 2.13e-07\n",
      "Step 170700 | Loss: 24.15973 | LR: 2.05e-07\n",
      "val loss: 24.3919\n",
      "Step 170800 | Loss: 23.89261 | LR: 1.96e-07\n",
      "Step 170900 | Loss: 25.00233 | LR: 1.88e-07\n",
      "val loss: 24.5343\n",
      "Step 171000 | Loss: 24.63031 | LR: 1.80e-07\n",
      "Step 171100 | Loss: 24.93644 | LR: 1.72e-07\n",
      "Step 171200 | Loss: 25.83281 | LR: 1.64e-07\n",
      "val loss: 24.5971\n",
      "Step 171300 | Loss: 24.31880 | LR: 1.57e-07\n",
      "Step 171400 | Loss: 24.86548 | LR: 1.49e-07\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "val loss: 24.7466\n",
      "Step 171500 | Loss: 24.57473 | LR: 1.42e-07\n",
      "Step 171600 | Loss: 24.41191 | LR: 1.35e-07\n",
      "Step 171700 | Loss: 24.09372 | LR: 1.28e-07\n",
      "val loss: 24.8852\n",
      "Step 171800 | Loss: 25.16420 | LR: 1.22e-07\n",
      "Step 171900 | Loss: 25.39488 | LR: 1.15e-07\n",
      "=== Step 171989 Done. Avg Loss: 24.60614 ===\n",
      "val loss: 24.7412\n",
      "Step 172000 | Loss: 24.74693 | LR: 1.09e-07\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 172100 | Loss: 23.92872 | LR: 1.03e-07\n",
      "Step 172200 | Loss: 24.14555 | LR: 9.69e-08\n",
      "val loss: 24.7473\n",
      "Step 172300 | Loss: 25.41706 | LR: 9.12e-08\n",
      "Step 172400 | Loss: 24.87591 | LR: 8.56e-08\n",
      "loading /home/ubuntu/pretraining/val/pt_shard_k562e_val_0000.npz\n",
      "val loss: 24.4782\n",
      "Step 172500 | Loss: 25.11058 | LR: 8.02e-08\n",
      "Step 172600 | Loss: 25.01665 | LR: 7.49e-08\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 172700 | Loss: 24.90373 | LR: 6.99e-08\n",
      "val loss: 24.7317\n",
      "Step 172800 | Loss: 24.49053 | LR: 6.50e-08\n",
      "Step 172900 | Loss: 25.14994 | LR: 6.03e-08\n",
      "val loss: 24.6940\n",
      "Step 173000 | Loss: 24.66385 | LR: 5.58e-08\n",
      "Step 173100 | Loss: 24.63930 | LR: 5.14e-08\n",
      "Step 173200 | Loss: 25.34066 | LR: 4.73e-08\n",
      "val loss: 24.8656\n",
      "Step 173300 | Loss: 24.77898 | LR: 4.33e-08\n",
      "Step 173400 | Loss: 24.17666 | LR: 3.95e-08\n",
      "val loss: 24.7838\n",
      "Step 173500 | Loss: 24.73919 | LR: 3.58e-08\n",
      "Step 173600 | Loss: 24.26257 | LR: 3.24e-08\n",
      "Step 173700 | Loss: 24.91110 | LR: 2.91e-08\n",
      "=== Step 173744 Done. Avg Loss: 24.78452 ===\n",
      "val loss: 24.8313\n",
      "Step 173800 | Loss: 24.70828 | LR: 2.60e-08\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 173900 | Loss: 25.04739 | LR: 2.31e-08\n",
      "val loss: 24.8269\n",
      "Step 174000 | Loss: 25.39912 | LR: 2.03e-08\n",
      "Step 174100 | Loss: 24.27356 | LR: 1.77e-08\n",
      "Step 174200 | Loss: 25.52431 | LR: 1.54e-08\n",
      "val loss: 24.5089\n",
      "Step 174300 | Loss: 24.78234 | LR: 1.31e-08\n",
      "Step 174400 | Loss: 23.95801 | LR: 1.11e-08\n",
      "loading /home/ubuntu/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "val loss: 25.0158\n",
      "Step 174500 | Loss: 25.35208 | LR: 9.24e-09\n",
      "Step 174600 | Loss: 24.97249 | LR: 7.56e-09\n",
      "Step 174700 | Loss: 25.04295 | LR: 6.05e-09\n",
      "val loss: 24.9404\n",
      "Step 174800 | Loss: 25.86615 | LR: 4.72e-09\n",
      "Step 174900 | Loss: 24.81897 | LR: 3.57e-09\n",
      "val loss: 24.8353\n",
      "Step 175000 | Loss: 25.23163 | LR: 2.60e-09\n",
      "Step 175100 | Loss: 25.02735 | LR: 1.81e-09\n",
      "Step 175200 | Loss: 25.23278 | LR: 1.19e-09\n",
      "val loss: 24.8970\n",
      "Step 175300 | Loss: 25.54318 | LR: 7.48e-10\n",
      "Step 175400 | Loss: 24.67237 | LR: 4.85e-10\n",
      "val loss: 24.8876\n",
      "=== Step 175499 Done. Avg Loss: 24.81607 ===\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "for step in range(pt_max_steps):\n",
    "    last_step = (step == pt_max_steps - 1)\n",
    "\n",
    "    # once in a while evaluate our validation set loss\n",
    "    if step % 250 == 0 or last_step:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss_accum = 0.0\n",
    "            val_loss_steps = 10\n",
    "            for i in range(val_loss_steps):\n",
    "                x_val, total_val = pt_val_loader.next_batch()\n",
    "\n",
    "                val_loss = model.forward_pretrain(x_val, total_val)\n",
    "                val_loss_accum += val_loss.item()\n",
    "\n",
    "            avg_val_loss = val_loss_accum / val_loss_steps\n",
    "            print(f'val loss: {avg_val_loss:.4f}')\n",
    "            \n",
    "        # with open(log_file, 'a') as f:\n",
    "        #    f.write(f'{step} test {test_loss_accum.item():.4f}\\n')\n",
    "        model.train()\n",
    "\n",
    "\n",
    "    if step > 0 and (step+1) % steps_per_epoch ==0 and not last_step:\n",
    "        # Save Checkpoint\n",
    "        torch.save({\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'step': step\n",
    "        }, checkpoint_dir / f'bio_jepa_pt_ckpt_{step}.pt')\n",
    "\n",
    "    # Pre-Training\n",
    "    x, total = pt_train_loader.next_batch()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss = model.forward_pretrain(x, total)\n",
    "    loss.backward()\n",
    "\n",
    "    # gradient clipping\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Update Teacher (V-JEPA Momentum)\n",
    "    model.update_teacher()\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    pt_lossi.append(loss.item())\n",
    "    total_epoch_loss += loss.item()\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "        print(f'Step {step} | Loss: {loss.item():.5f} | LR: {scheduler.get_last_lr()[0]:.2e}')\n",
    "    \n",
    "    \n",
    "    if step > 0 and (step+1) % steps_per_epoch == 0:   \n",
    "        avg_loss = total_epoch_loss / steps_per_epoch\n",
    "        print(f'=== Step {step} Done. Avg Loss: {avg_loss:.5f} ===')\n",
    "        total_epoch_loss = 0.0\n",
    "    \n",
    "    \n",
    "    if last_step:\n",
    "        # Save Checkpoint\n",
    "        torch.save({\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'step': step\n",
    "        }, checkpoint_dir / f'bio_jepa_pt_ckpt_{step}_final.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6bd06e53-1e6b-4793-bb4d-9f2c69a6ce0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0xebd12815dbe0>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pt_lossi[:])\n",
    "#plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0a076f-d4ec-4d12-aca1-4cff35ea07e4",
   "metadata": {},
   "source": [
    "## Loading Pretrained Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab296c0d-749b-4692-bc39-e455e9081df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = checkpoint_dir / 'bio_jepa_pt_ckpt_175499_final.pt'\n",
    "checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "\n",
    "keys = model.load_state_dict(checkpoint['model'])\n",
    "keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12aea4d-8756-43a0-aee7-d5c3a76b8263",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "715e6fd8-ba08-4544-ac66-cf5ffa312e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "LR = 1e-4\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1f68d4d-0cbf-44d8-ba34-bd0aed833b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 3 shards for split train\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "found 1 shards for split val\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n"
     ]
    }
   ],
   "source": [
    "train_loader = TrainingLoader(batch_size=batch_size, split='train', data_dir=train_dir, device=DEVICE)\n",
    "val_loader = TrainingLoader(batch_size=batch_size, split='val', data_dir=train_dir, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec115e2d-684d-4d6d-a202-ea84db97a7b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3177, 63540)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch = 101682 // batch_size\n",
    "max_steps = epochs * steps_per_epoch\n",
    "steps_per_epoch, max_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ad84847-ec01-49f7-bb05-28c49924a716",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac6aab6d-046c-4bd1-a1f2-94a0099430ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=LR, total_steps=max_steps, pct_start=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8333d07-ddf0-428b-969d-bb46efdc27dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossi = []\n",
    "step = 0\n",
    "total_epoch_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38c6ac49-44b7-4556-89ae-6c087721016a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss: 76.6528\n",
      "Step 0 | Loss: 78.92419 | LR: 4.00e-06\n",
      "Step 100 | Loss: 48.38370 | LR: 4.24e-06\n",
      "Step 200 | Loss: 43.02344 | LR: 4.95e-06\n",
      "val loss: 44.2075\n",
      "Step 300 | Loss: 42.58173 | LR: 6.11e-06\n",
      "Step 400 | Loss: 41.69090 | LR: 7.73e-06\n",
      "val loss: 41.5879\n",
      "Step 500 | Loss: 40.48545 | LR: 9.77e-06\n",
      "Step 600 | Loss: 40.02102 | LR: 1.22e-05\n",
      "Step 700 | Loss: 40.27300 | LR: 1.51e-05\n",
      "val loss: 39.6276\n",
      "Step 800 | Loss: 40.05817 | LR: 1.83e-05\n",
      "Step 900 | Loss: 38.49232 | LR: 2.18e-05\n",
      "val loss: 39.3469\n",
      "Step 1000 | Loss: 39.11641 | LR: 2.57e-05\n",
      "Step 1100 | Loss: 37.54802 | LR: 2.98e-05\n",
      "Step 1200 | Loss: 38.63822 | LR: 3.41e-05\n",
      "val loss: 39.0259\n",
      "Step 1300 | Loss: 37.82827 | LR: 3.86e-05\n",
      "Step 1400 | Loss: 37.67643 | LR: 4.32e-05\n",
      "val loss: 38.5513\n",
      "Step 1500 | Loss: 38.48582 | LR: 4.79e-05\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 1600 | Loss: 37.25166 | LR: 5.26e-05\n",
      "Step 1700 | Loss: 37.44345 | LR: 5.74e-05\n",
      "val loss: 36.4130\n",
      "Step 1800 | Loss: 36.20288 | LR: 6.20e-05\n",
      "Step 1900 | Loss: 35.33437 | LR: 6.66e-05\n",
      "val loss: 35.4566\n",
      "Step 2000 | Loss: 35.20171 | LR: 7.11e-05\n",
      "Step 2100 | Loss: 34.83713 | LR: 7.53e-05\n",
      "Step 2200 | Loss: 33.72151 | LR: 7.94e-05\n",
      "val loss: 32.9826\n",
      "Step 2300 | Loss: 32.25589 | LR: 8.31e-05\n",
      "Step 2400 | Loss: 31.13486 | LR: 8.66e-05\n",
      "val loss: 30.8599\n",
      "Step 2500 | Loss: 31.72960 | LR: 8.97e-05\n",
      "Step 2600 | Loss: 30.95047 | LR: 9.24e-05\n",
      "Step 2700 | Loss: 29.23158 | LR: 9.48e-05\n",
      "val loss: 29.5072\n",
      "Step 2800 | Loss: 29.38139 | LR: 9.67e-05\n",
      "Step 2900 | Loss: 28.91601 | LR: 9.82e-05\n",
      "val loss: 28.0862\n",
      "Step 3000 | Loss: 27.84656 | LR: 9.93e-05\n",
      "Step 3100 | Loss: 27.65078 | LR: 9.99e-05\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "=== Step 3176 Done. Avg Loss: 36.64478 ===\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 3200 | Loss: 26.82615 | LR: 1.00e-04\n",
      "val loss: 26.6415\n",
      "Step 3300 | Loss: 26.23453 | LR: 1.00e-04\n",
      "Step 3400 | Loss: 26.71152 | LR: 1.00e-04\n",
      "val loss: 25.6149\n",
      "Step 3500 | Loss: 26.06204 | LR: 1.00e-04\n",
      "Step 3600 | Loss: 24.95215 | LR: 1.00e-04\n",
      "Step 3700 | Loss: 25.14328 | LR: 1.00e-04\n",
      "val loss: 24.7786\n",
      "Step 3800 | Loss: 24.36657 | LR: 1.00e-04\n",
      "Step 3900 | Loss: 23.95262 | LR: 1.00e-04\n",
      "val loss: 24.1355\n",
      "Step 4000 | Loss: 23.96893 | LR: 1.00e-04\n",
      "Step 4100 | Loss: 23.76021 | LR: 9.99e-05\n",
      "Step 4200 | Loss: 23.32988 | LR: 9.99e-05\n",
      "val loss: 23.5786\n",
      "Step 4300 | Loss: 23.79944 | LR: 9.99e-05\n",
      "Step 4400 | Loss: 23.36213 | LR: 9.99e-05\n",
      "val loss: 23.1763\n",
      "Step 4500 | Loss: 22.76463 | LR: 9.99e-05\n",
      "Step 4600 | Loss: 22.06935 | LR: 9.99e-05\n",
      "Step 4700 | Loss: 22.81693 | LR: 9.98e-05\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "val loss: 22.6453\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "Step 4800 | Loss: 22.60246 | LR: 9.98e-05\n",
      "Step 4900 | Loss: 22.42385 | LR: 9.98e-05\n",
      "val loss: 22.3088\n",
      "Step 5000 | Loss: 22.54093 | LR: 9.98e-05\n",
      "Step 5100 | Loss: 21.75373 | LR: 9.97e-05\n",
      "Step 5200 | Loss: 22.59053 | LR: 9.97e-05\n",
      "val loss: 21.8680\n",
      "Step 5300 | Loss: 21.23879 | LR: 9.97e-05\n",
      "Step 5400 | Loss: 21.66527 | LR: 9.97e-05\n",
      "val loss: 21.3222\n",
      "Step 5500 | Loss: 21.53338 | LR: 9.96e-05\n",
      "Step 5600 | Loss: 21.42373 | LR: 9.96e-05\n",
      "Step 5700 | Loss: 21.03326 | LR: 9.96e-05\n",
      "val loss: 21.2937\n",
      "Step 5800 | Loss: 20.65598 | LR: 9.95e-05\n",
      "Step 5900 | Loss: 21.16119 | LR: 9.95e-05\n",
      "val loss: 20.9795\n",
      "Step 6000 | Loss: 21.12352 | LR: 9.95e-05\n",
      "Step 6100 | Loss: 20.44519 | LR: 9.94e-05\n",
      "Step 6200 | Loss: 20.48671 | LR: 9.94e-05\n",
      "val loss: 20.6834\n",
      "Step 6300 | Loss: 20.59662 | LR: 9.93e-05\n",
      "=== Step 6353 Done. Avg Loss: 22.96869 ===\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 6400 | Loss: 20.74442 | LR: 9.93e-05\n",
      "val loss: 20.4740\n",
      "Step 6500 | Loss: 20.09928 | LR: 9.93e-05\n",
      "Step 6600 | Loss: 20.14747 | LR: 9.92e-05\n",
      "Step 6700 | Loss: 20.20391 | LR: 9.92e-05\n",
      "val loss: 20.1942\n",
      "Step 6800 | Loss: 19.99002 | LR: 9.91e-05\n",
      "Step 6900 | Loss: 20.11057 | LR: 9.91e-05\n",
      "val loss: 20.0546\n",
      "Step 7000 | Loss: 20.31702 | LR: 9.90e-05\n",
      "Step 7100 | Loss: 19.76978 | LR: 9.90e-05\n",
      "Step 7200 | Loss: 19.77613 | LR: 9.89e-05\n",
      "val loss: 19.7134\n",
      "Step 7300 | Loss: 19.19903 | LR: 9.89e-05\n",
      "Step 7400 | Loss: 19.89219 | LR: 9.88e-05\n",
      "val loss: 19.6572\n",
      "Step 7500 | Loss: 19.93536 | LR: 9.87e-05\n",
      "Step 7600 | Loss: 19.69633 | LR: 9.87e-05\n",
      "Step 7700 | Loss: 19.24044 | LR: 9.86e-05\n",
      "val loss: 19.5553\n",
      "Step 7800 | Loss: 18.92421 | LR: 9.86e-05\n",
      "Step 7900 | Loss: 18.91556 | LR: 9.85e-05\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "val loss: 19.1480\n",
      "Step 8000 | Loss: 18.80912 | LR: 9.84e-05\n",
      "Step 8100 | Loss: 18.58497 | LR: 9.84e-05\n",
      "Step 8200 | Loss: 19.19525 | LR: 9.83e-05\n",
      "val loss: 19.1238\n",
      "Step 8300 | Loss: 18.64884 | LR: 9.82e-05\n",
      "Step 8400 | Loss: 18.99258 | LR: 9.82e-05\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 19.0474\n",
      "Step 8500 | Loss: 19.18024 | LR: 9.81e-05\n",
      "Step 8600 | Loss: 19.21463 | LR: 9.80e-05\n",
      "Step 8700 | Loss: 18.93118 | LR: 9.79e-05\n",
      "val loss: 18.7115\n",
      "Step 8800 | Loss: 19.02842 | LR: 9.79e-05\n",
      "Step 8900 | Loss: 18.65469 | LR: 9.78e-05\n",
      "val loss: 18.6651\n",
      "Step 9000 | Loss: 18.71430 | LR: 9.77e-05\n",
      "Step 9100 | Loss: 18.47531 | LR: 9.76e-05\n",
      "Step 9200 | Loss: 18.69035 | LR: 9.76e-05\n",
      "val loss: 18.5210\n",
      "Step 9300 | Loss: 18.58596 | LR: 9.75e-05\n",
      "Step 9400 | Loss: 18.50311 | LR: 9.74e-05\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "val loss: 18.6899\n",
      "Step 9500 | Loss: 18.17622 | LR: 9.73e-05\n",
      "=== Step 9530 Done. Avg Loss: 19.34643 ===\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 9600 | Loss: 18.54888 | LR: 9.72e-05\n",
      "Step 9700 | Loss: 18.73324 | LR: 9.71e-05\n",
      "val loss: 18.2448\n",
      "Step 9800 | Loss: 19.17750 | LR: 9.71e-05\n",
      "Step 9900 | Loss: 19.06313 | LR: 9.70e-05\n",
      "val loss: 18.2835\n",
      "Step 10000 | Loss: 18.42262 | LR: 9.69e-05\n",
      "Step 10100 | Loss: 18.10269 | LR: 9.68e-05\n",
      "Step 10200 | Loss: 18.25194 | LR: 9.67e-05\n",
      "val loss: 18.2554\n",
      "Step 10300 | Loss: 18.35522 | LR: 9.66e-05\n",
      "Step 10400 | Loss: 17.78867 | LR: 9.65e-05\n",
      "val loss: 18.4015\n",
      "Step 10500 | Loss: 17.58946 | LR: 9.64e-05\n",
      "Step 10600 | Loss: 18.33716 | LR: 9.63e-05\n",
      "Step 10700 | Loss: 17.25196 | LR: 9.62e-05\n",
      "val loss: 18.0006\n",
      "Step 10800 | Loss: 17.98508 | LR: 9.61e-05\n",
      "Step 10900 | Loss: 18.35048 | LR: 9.60e-05\n",
      "val loss: 17.7569\n",
      "Step 11000 | Loss: 18.23605 | LR: 9.59e-05\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "Step 11100 | Loss: 18.01604 | LR: 9.58e-05\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "Step 11200 | Loss: 17.39750 | LR: 9.57e-05\n",
      "val loss: 17.9620\n",
      "Step 11300 | Loss: 17.62946 | LR: 9.56e-05\n",
      "Step 11400 | Loss: 17.79406 | LR: 9.55e-05\n",
      "val loss: 17.9738\n",
      "Step 11500 | Loss: 18.22828 | LR: 9.54e-05\n",
      "Step 11600 | Loss: 18.49970 | LR: 9.53e-05\n",
      "Step 11700 | Loss: 17.73954 | LR: 9.52e-05\n",
      "val loss: 18.0118\n",
      "Step 11800 | Loss: 18.57866 | LR: 9.50e-05\n",
      "Step 11900 | Loss: 17.82681 | LR: 9.49e-05\n",
      "val loss: 17.8862\n",
      "Step 12000 | Loss: 17.83261 | LR: 9.48e-05\n",
      "Step 12100 | Loss: 17.60120 | LR: 9.47e-05\n",
      "Step 12200 | Loss: 17.39445 | LR: 9.46e-05\n",
      "val loss: 17.7334\n",
      "Step 12300 | Loss: 17.25468 | LR: 9.45e-05\n",
      "Step 12400 | Loss: 18.13904 | LR: 9.43e-05\n",
      "val loss: 17.4353\n",
      "Step 12500 | Loss: 17.86726 | LR: 9.42e-05\n",
      "Step 12600 | Loss: 17.71975 | LR: 9.41e-05\n",
      "Step 12700 | Loss: 17.13538 | LR: 9.40e-05\n",
      "=== Step 12707 Done. Avg Loss: 17.97608 ===\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "val loss: 17.5607\n",
      "Step 12800 | Loss: 17.25287 | LR: 9.39e-05\n",
      "Step 12900 | Loss: 18.31638 | LR: 9.37e-05\n",
      "val loss: 17.5828\n",
      "Step 13000 | Loss: 17.44270 | LR: 9.36e-05\n",
      "Step 13100 | Loss: 17.08149 | LR: 9.35e-05\n",
      "Step 13200 | Loss: 16.90976 | LR: 9.33e-05\n",
      "val loss: 17.5542\n",
      "Step 13300 | Loss: 17.44106 | LR: 9.32e-05\n",
      "Step 13400 | Loss: 17.25247 | LR: 9.31e-05\n",
      "val loss: 17.5599\n",
      "Step 13500 | Loss: 17.34048 | LR: 9.30e-05\n",
      "Step 13600 | Loss: 17.83598 | LR: 9.28e-05\n",
      "Step 13700 | Loss: 17.00127 | LR: 9.27e-05\n",
      "val loss: 17.3648\n",
      "Step 13800 | Loss: 17.45444 | LR: 9.25e-05\n",
      "Step 13900 | Loss: 17.72630 | LR: 9.24e-05\n",
      "val loss: 17.4995\n",
      "Step 14000 | Loss: 17.72990 | LR: 9.23e-05\n",
      "Step 14100 | Loss: 17.67845 | LR: 9.21e-05\n",
      "Step 14200 | Loss: 16.97094 | LR: 9.20e-05\n",
      "val loss: 17.5219\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 14300 | Loss: 16.84616 | LR: 9.19e-05\n",
      "Step 14400 | Loss: 17.56048 | LR: 9.17e-05\n",
      "val loss: 17.6966\n",
      "Step 14500 | Loss: 17.31826 | LR: 9.16e-05\n",
      "Step 14600 | Loss: 17.89287 | LR: 9.14e-05\n",
      "Step 14700 | Loss: 16.95166 | LR: 9.13e-05\n",
      "val loss: 17.5081\n",
      "Step 14800 | Loss: 17.63774 | LR: 9.11e-05\n",
      "Step 14900 | Loss: 16.64653 | LR: 9.10e-05\n",
      "val loss: 17.4523\n",
      "Step 15000 | Loss: 17.12228 | LR: 9.08e-05\n",
      "Step 15100 | Loss: 17.22528 | LR: 9.07e-05\n",
      "Step 15200 | Loss: 17.15015 | LR: 9.05e-05\n",
      "val loss: 17.5824\n",
      "Step 15300 | Loss: 16.72517 | LR: 9.04e-05\n",
      "Step 15400 | Loss: 17.84070 | LR: 9.02e-05\n",
      "val loss: 17.5043\n",
      "Step 15500 | Loss: 17.80099 | LR: 9.01e-05\n",
      "Step 15600 | Loss: 17.55386 | LR: 8.99e-05\n",
      "Step 15700 | Loss: 16.90191 | LR: 8.97e-05\n",
      "val loss: 17.5672\n",
      "Step 15800 | Loss: 17.02328 | LR: 8.96e-05\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "=== Step 15884 Done. Avg Loss: 17.46738 ===\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 15900 | Loss: 17.38754 | LR: 8.94e-05\n",
      "val loss: 17.2831\n",
      "Step 16000 | Loss: 17.13972 | LR: 8.93e-05\n",
      "Step 16100 | Loss: 17.96690 | LR: 8.91e-05\n",
      "Step 16200 | Loss: 17.20696 | LR: 8.89e-05\n",
      "val loss: 17.2191\n",
      "Step 16300 | Loss: 17.12934 | LR: 8.88e-05\n",
      "Step 16400 | Loss: 17.24143 | LR: 8.86e-05\n",
      "val loss: 17.2314\n",
      "Step 16500 | Loss: 16.70163 | LR: 8.85e-05\n",
      "Step 16600 | Loss: 17.42913 | LR: 8.83e-05\n",
      "Step 16700 | Loss: 17.70101 | LR: 8.81e-05\n",
      "val loss: 17.2140\n",
      "Step 16800 | Loss: 17.12479 | LR: 8.79e-05\n",
      "Step 16900 | Loss: 17.50743 | LR: 8.78e-05\n",
      "val loss: 17.3730\n",
      "Step 17000 | Loss: 17.43936 | LR: 8.76e-05\n",
      "Step 17100 | Loss: 17.37908 | LR: 8.74e-05\n",
      "Step 17200 | Loss: 17.23645 | LR: 8.73e-05\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 17.1968\n",
      "Step 17300 | Loss: 16.88395 | LR: 8.71e-05\n",
      "Step 17400 | Loss: 16.97028 | LR: 8.69e-05\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "val loss: 17.2521\n",
      "Step 17500 | Loss: 16.62952 | LR: 8.67e-05\n",
      "Step 17600 | Loss: 17.07726 | LR: 8.66e-05\n",
      "Step 17700 | Loss: 16.82191 | LR: 8.64e-05\n",
      "val loss: 17.2620\n",
      "Step 17800 | Loss: 17.24788 | LR: 8.62e-05\n",
      "Step 17900 | Loss: 16.76520 | LR: 8.60e-05\n",
      "val loss: 17.3590\n",
      "Step 18000 | Loss: 17.73938 | LR: 8.58e-05\n",
      "Step 18100 | Loss: 18.22580 | LR: 8.57e-05\n",
      "Step 18200 | Loss: 17.12916 | LR: 8.55e-05\n",
      "val loss: 17.2741\n",
      "Step 18300 | Loss: 17.03693 | LR: 8.53e-05\n",
      "Step 18400 | Loss: 16.86296 | LR: 8.51e-05\n",
      "val loss: 17.0561\n",
      "Step 18500 | Loss: 17.07013 | LR: 8.49e-05\n",
      "Step 18600 | Loss: 16.80600 | LR: 8.47e-05\n",
      "Step 18700 | Loss: 16.97710 | LR: 8.45e-05\n",
      "val loss: 17.2644\n",
      "Step 18800 | Loss: 17.96332 | LR: 8.44e-05\n",
      "Step 18900 | Loss: 17.05986 | LR: 8.42e-05\n",
      "val loss: 17.4445\n",
      "Step 19000 | Loss: 16.80891 | LR: 8.40e-05\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "=== Step 19061 Done. Avg Loss: 17.23894 ===\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "Step 19100 | Loss: 16.99830 | LR: 8.38e-05\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "Step 19200 | Loss: 17.70972 | LR: 8.36e-05\n",
      "val loss: 17.2146\n",
      "Step 19300 | Loss: 18.69396 | LR: 8.34e-05\n",
      "Step 19400 | Loss: 17.50164 | LR: 8.32e-05\n",
      "val loss: 17.1556\n",
      "Step 19500 | Loss: 17.62036 | LR: 8.30e-05\n",
      "Step 19600 | Loss: 16.99056 | LR: 8.28e-05\n",
      "Step 19700 | Loss: 17.25350 | LR: 8.26e-05\n",
      "val loss: 17.3191\n",
      "Step 19800 | Loss: 16.81971 | LR: 8.24e-05\n",
      "Step 19900 | Loss: 16.93342 | LR: 8.22e-05\n",
      "val loss: 17.3705\n",
      "Step 20000 | Loss: 17.70066 | LR: 8.20e-05\n",
      "Step 20100 | Loss: 17.83111 | LR: 8.18e-05\n",
      "Step 20200 | Loss: 16.75244 | LR: 8.16e-05\n",
      "val loss: 17.2148\n",
      "Step 20300 | Loss: 17.58407 | LR: 8.14e-05\n",
      "Step 20400 | Loss: 17.10184 | LR: 8.12e-05\n",
      "val loss: 17.1910\n",
      "Step 20500 | Loss: 16.14664 | LR: 8.10e-05\n",
      "Step 20600 | Loss: 16.96832 | LR: 8.08e-05\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 20700 | Loss: 17.15367 | LR: 8.06e-05\n",
      "val loss: 17.0321\n",
      "Step 20800 | Loss: 16.74147 | LR: 8.04e-05\n",
      "Step 20900 | Loss: 17.19363 | LR: 8.02e-05\n",
      "val loss: 17.2833\n",
      "Step 21000 | Loss: 16.78509 | LR: 8.00e-05\n",
      "Step 21100 | Loss: 16.58875 | LR: 7.98e-05\n",
      "Step 21200 | Loss: 16.95687 | LR: 7.96e-05\n",
      "val loss: 17.1094\n",
      "Step 21300 | Loss: 16.97763 | LR: 7.94e-05\n",
      "Step 21400 | Loss: 16.45696 | LR: 7.91e-05\n",
      "val loss: 17.1421\n",
      "Step 21500 | Loss: 17.22148 | LR: 7.89e-05\n",
      "Step 21600 | Loss: 16.77048 | LR: 7.87e-05\n",
      "Step 21700 | Loss: 17.96942 | LR: 7.85e-05\n",
      "val loss: 17.1958\n",
      "Step 21800 | Loss: 16.47466 | LR: 7.83e-05\n",
      "Step 21900 | Loss: 17.43610 | LR: 7.81e-05\n",
      "val loss: 17.0801\n",
      "Step 22000 | Loss: 17.17634 | LR: 7.79e-05\n",
      "Step 22100 | Loss: 16.67558 | LR: 7.76e-05\n",
      "Step 22200 | Loss: 17.17170 | LR: 7.74e-05\n",
      "=== Step 22238 Done. Avg Loss: 17.10531 ===\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "val loss: 17.0304\n",
      "Step 22300 | Loss: 17.30837 | LR: 7.72e-05\n",
      "Step 22400 | Loss: 17.79823 | LR: 7.70e-05\n",
      "val loss: 17.1238\n",
      "Step 22500 | Loss: 17.24803 | LR: 7.68e-05\n",
      "Step 22600 | Loss: 16.96372 | LR: 7.66e-05\n",
      "Step 22700 | Loss: 16.80428 | LR: 7.63e-05\n",
      "val loss: 17.3631\n",
      "Step 22800 | Loss: 16.63910 | LR: 7.61e-05\n",
      "Step 22900 | Loss: 17.14095 | LR: 7.59e-05\n",
      "val loss: 17.1070\n",
      "Step 23000 | Loss: 17.40065 | LR: 7.57e-05\n",
      "Step 23100 | Loss: 16.65771 | LR: 7.54e-05\n",
      "Step 23200 | Loss: 16.71902 | LR: 7.52e-05\n",
      "val loss: 17.2718\n",
      "Step 23300 | Loss: 17.36353 | LR: 7.50e-05\n",
      "Step 23400 | Loss: 17.13488 | LR: 7.48e-05\n",
      "val loss: 17.0276\n",
      "Step 23500 | Loss: 17.01099 | LR: 7.45e-05\n",
      "Step 23600 | Loss: 16.98231 | LR: 7.43e-05\n",
      "Step 23700 | Loss: 17.53003 | LR: 7.41e-05\n",
      "val loss: 17.1189\n",
      "Step 23800 | Loss: 17.16718 | LR: 7.39e-05\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "Step 23900 | Loss: 17.60168 | LR: 7.36e-05\n",
      "val loss: 17.0368\n",
      "Step 24000 | Loss: 17.76138 | LR: 7.34e-05\n",
      "Step 24100 | Loss: 16.91325 | LR: 7.32e-05\n",
      "Step 24200 | Loss: 16.62528 | LR: 7.29e-05\n",
      "val loss: 17.0754\n",
      "Step 24300 | Loss: 17.16834 | LR: 7.27e-05\n",
      "Step 24400 | Loss: 16.73798 | LR: 7.25e-05\n",
      "val loss: 17.2087\n",
      "Step 24500 | Loss: 16.77033 | LR: 7.22e-05\n",
      "Step 24600 | Loss: 17.05214 | LR: 7.20e-05\n",
      "Step 24700 | Loss: 16.47370 | LR: 7.18e-05\n",
      "val loss: 16.9415\n",
      "Step 24800 | Loss: 16.94622 | LR: 7.15e-05\n",
      "Step 24900 | Loss: 17.16777 | LR: 7.13e-05\n",
      "val loss: 17.1182\n",
      "Step 25000 | Loss: 17.03613 | LR: 7.11e-05\n",
      "Step 25100 | Loss: 17.19665 | LR: 7.08e-05\n",
      "Step 25200 | Loss: 16.79476 | LR: 7.06e-05\n",
      "val loss: 17.0681\n",
      "Step 25300 | Loss: 17.20050 | LR: 7.04e-05\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "Step 25400 | Loss: 17.16327 | LR: 7.01e-05\n",
      "=== Step 25415 Done. Avg Loss: 17.02440 ===\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "val loss: 16.8903\n",
      "Step 25500 | Loss: 16.49014 | LR: 6.99e-05\n",
      "Step 25600 | Loss: 17.59858 | LR: 6.96e-05\n",
      "Step 25700 | Loss: 16.83207 | LR: 6.94e-05\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 17.2182\n",
      "Step 25800 | Loss: 16.64829 | LR: 6.92e-05\n",
      "Step 25900 | Loss: 16.87447 | LR: 6.89e-05\n",
      "val loss: 17.1373\n",
      "Step 26000 | Loss: 17.03949 | LR: 6.87e-05\n",
      "Step 26100 | Loss: 16.82998 | LR: 6.84e-05\n",
      "Step 26200 | Loss: 16.32835 | LR: 6.82e-05\n",
      "val loss: 17.2760\n",
      "Step 26300 | Loss: 17.37583 | LR: 6.80e-05\n",
      "Step 26400 | Loss: 16.98195 | LR: 6.77e-05\n",
      "val loss: 17.1450\n",
      "Step 26500 | Loss: 17.54518 | LR: 6.75e-05\n",
      "Step 26600 | Loss: 17.25503 | LR: 6.72e-05\n",
      "Step 26700 | Loss: 16.64192 | LR: 6.70e-05\n",
      "val loss: 16.9112\n",
      "Step 26800 | Loss: 16.82417 | LR: 6.67e-05\n",
      "Step 26900 | Loss: 17.60002 | LR: 6.65e-05\n",
      "val loss: 17.1289\n",
      "Step 27000 | Loss: 16.84984 | LR: 6.62e-05\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 27100 | Loss: 17.37296 | LR: 6.60e-05\n",
      "Step 27200 | Loss: 16.61295 | LR: 6.57e-05\n",
      "val loss: 17.1741\n",
      "Step 27300 | Loss: 16.74295 | LR: 6.55e-05\n",
      "Step 27400 | Loss: 16.74809 | LR: 6.53e-05\n",
      "val loss: 17.1194\n",
      "Step 27500 | Loss: 17.11915 | LR: 6.50e-05\n",
      "Step 27600 | Loss: 17.23419 | LR: 6.48e-05\n",
      "Step 27700 | Loss: 16.72092 | LR: 6.45e-05\n",
      "val loss: 17.0304\n",
      "Step 27800 | Loss: 16.78242 | LR: 6.43e-05\n",
      "Step 27900 | Loss: 16.73316 | LR: 6.40e-05\n",
      "val loss: 17.0748\n",
      "Step 28000 | Loss: 17.40232 | LR: 6.38e-05\n",
      "Step 28100 | Loss: 17.67414 | LR: 6.35e-05\n",
      "Step 28200 | Loss: 16.78763 | LR: 6.33e-05\n",
      "val loss: 17.3673\n",
      "Step 28300 | Loss: 16.50686 | LR: 6.30e-05\n",
      "Step 28400 | Loss: 17.35279 | LR: 6.28e-05\n",
      "val loss: 16.9928\n",
      "Step 28500 | Loss: 16.80349 | LR: 6.25e-05\n",
      "=== Step 28592 Done. Avg Loss: 16.96026 ===\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 28600 | Loss: 17.39781 | LR: 6.23e-05\n",
      "Step 28700 | Loss: 16.67686 | LR: 6.20e-05\n",
      "val loss: 16.9531\n",
      "Step 28800 | Loss: 16.83584 | LR: 6.17e-05\n",
      "Step 28900 | Loss: 16.66117 | LR: 6.15e-05\n",
      "val loss: 16.8749\n",
      "Step 29000 | Loss: 16.81079 | LR: 6.12e-05\n",
      "Step 29100 | Loss: 17.53397 | LR: 6.10e-05\n",
      "Step 29200 | Loss: 17.18796 | LR: 6.07e-05\n",
      "val loss: 17.0400\n",
      "Step 29300 | Loss: 16.80753 | LR: 6.05e-05\n",
      "Step 29400 | Loss: 16.73663 | LR: 6.02e-05\n",
      "val loss: 17.0059\n",
      "Step 29500 | Loss: 17.11740 | LR: 6.00e-05\n",
      "Step 29600 | Loss: 16.85986 | LR: 5.97e-05\n",
      "Step 29700 | Loss: 16.30535 | LR: 5.95e-05\n",
      "val loss: 17.0930\n",
      "Step 29800 | Loss: 16.80861 | LR: 5.92e-05\n",
      "Step 29900 | Loss: 16.91332 | LR: 5.89e-05\n",
      "val loss: 17.0241\n",
      "Step 30000 | Loss: 16.82850 | LR: 5.87e-05\n",
      "Step 30100 | Loss: 16.69241 | LR: 5.84e-05\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "Step 30200 | Loss: 18.13193 | LR: 5.82e-05\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "val loss: 17.0437\n",
      "Step 30300 | Loss: 17.36076 | LR: 5.79e-05\n",
      "Step 30400 | Loss: 16.47184 | LR: 5.77e-05\n",
      "val loss: 17.0102\n",
      "Step 30500 | Loss: 17.95878 | LR: 5.74e-05\n",
      "Step 30600 | Loss: 17.26459 | LR: 5.71e-05\n",
      "Step 30700 | Loss: 16.68579 | LR: 5.69e-05\n",
      "val loss: 16.8345\n",
      "Step 30800 | Loss: 16.82899 | LR: 5.66e-05\n",
      "Step 30900 | Loss: 17.03196 | LR: 5.64e-05\n",
      "val loss: 17.3173\n",
      "Step 31000 | Loss: 16.35246 | LR: 5.61e-05\n",
      "Step 31100 | Loss: 16.68690 | LR: 5.59e-05\n",
      "Step 31200 | Loss: 16.92113 | LR: 5.56e-05\n",
      "val loss: 17.1162\n",
      "Step 31300 | Loss: 17.11825 | LR: 5.53e-05\n",
      "Step 31400 | Loss: 16.95083 | LR: 5.51e-05\n",
      "val loss: 16.9072\n",
      "Step 31500 | Loss: 17.09805 | LR: 5.48e-05\n",
      "Step 31600 | Loss: 17.11391 | LR: 5.46e-05\n",
      "Step 31700 | Loss: 16.85202 | LR: 5.43e-05\n",
      "val loss: 17.1447\n",
      "=== Step 31769 Done. Avg Loss: 16.90719 ===\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 31800 | Loss: 17.61456 | LR: 5.40e-05\n",
      "Step 31900 | Loss: 17.45201 | LR: 5.38e-05\n",
      "val loss: 17.0618\n",
      "Step 32000 | Loss: 16.53720 | LR: 5.35e-05\n",
      "Step 32100 | Loss: 17.09258 | LR: 5.33e-05\n",
      "Step 32200 | Loss: 16.69262 | LR: 5.30e-05\n",
      "val loss: 16.9512\n",
      "Step 32300 | Loss: 16.92408 | LR: 5.27e-05\n",
      "Step 32400 | Loss: 16.68112 | LR: 5.25e-05\n",
      "val loss: 17.0661\n",
      "Step 32500 | Loss: 17.31591 | LR: 5.22e-05\n",
      "Step 32600 | Loss: 17.00218 | LR: 5.20e-05\n",
      "Step 32700 | Loss: 16.65441 | LR: 5.17e-05\n",
      "val loss: 16.9397\n",
      "Step 32800 | Loss: 16.58091 | LR: 5.14e-05\n",
      "Step 32900 | Loss: 16.42069 | LR: 5.12e-05\n",
      "val loss: 17.1045\n",
      "Step 33000 | Loss: 17.08154 | LR: 5.09e-05\n",
      "Step 33100 | Loss: 16.42171 | LR: 5.07e-05\n",
      "Step 33200 | Loss: 16.80215 | LR: 5.04e-05\n",
      "val loss: 16.9368\n",
      "Step 33300 | Loss: 16.66143 | LR: 5.01e-05\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "Step 33400 | Loss: 17.04602 | LR: 4.99e-05\n",
      "val loss: 16.9759\n",
      "Step 33500 | Loss: 16.83752 | LR: 4.96e-05\n",
      "Step 33600 | Loss: 16.51998 | LR: 4.94e-05\n",
      "Step 33700 | Loss: 16.70150 | LR: 4.91e-05\n",
      "val loss: 17.0096\n",
      "Step 33800 | Loss: 16.49224 | LR: 4.88e-05\n",
      "Step 33900 | Loss: 17.16885 | LR: 4.86e-05\n",
      "val loss: 17.1489\n",
      "Step 34000 | Loss: 17.14458 | LR: 4.83e-05\n",
      "Step 34100 | Loss: 18.02247 | LR: 4.81e-05\n",
      "Step 34200 | Loss: 16.93662 | LR: 4.78e-05\n",
      "val loss: 17.1312\n",
      "Step 34300 | Loss: 17.38166 | LR: 4.75e-05\n",
      "Step 34400 | Loss: 17.39821 | LR: 4.73e-05\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 16.8362\n",
      "Step 34500 | Loss: 17.14007 | LR: 4.70e-05\n",
      "Step 34600 | Loss: 17.00873 | LR: 4.68e-05\n",
      "Step 34700 | Loss: 16.67989 | LR: 4.65e-05\n",
      "val loss: 17.0609\n",
      "Step 34800 | Loss: 16.84389 | LR: 4.62e-05\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "Step 34900 | Loss: 16.43542 | LR: 4.60e-05\n",
      "=== Step 34946 Done. Avg Loss: 16.86962 ===\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "val loss: 17.1877\n",
      "Step 35000 | Loss: 17.64575 | LR: 4.57e-05\n",
      "Step 35100 | Loss: 16.77120 | LR: 4.55e-05\n",
      "Step 35200 | Loss: 16.41276 | LR: 4.52e-05\n",
      "val loss: 17.0149\n",
      "Step 35300 | Loss: 16.49964 | LR: 4.50e-05\n",
      "Step 35400 | Loss: 16.60238 | LR: 4.47e-05\n",
      "val loss: 16.9931\n",
      "Step 35500 | Loss: 16.70070 | LR: 4.44e-05\n",
      "Step 35600 | Loss: 16.67142 | LR: 4.42e-05\n",
      "Step 35700 | Loss: 16.98500 | LR: 4.39e-05\n",
      "val loss: 16.8068\n",
      "Step 35800 | Loss: 17.26504 | LR: 4.37e-05\n",
      "Step 35900 | Loss: 16.82122 | LR: 4.34e-05\n",
      "val loss: 17.0125\n",
      "Step 36000 | Loss: 17.05885 | LR: 4.31e-05\n",
      "Step 36100 | Loss: 16.40140 | LR: 4.29e-05\n",
      "Step 36200 | Loss: 16.39229 | LR: 4.26e-05\n",
      "val loss: 16.9232\n",
      "Step 36300 | Loss: 17.24645 | LR: 4.24e-05\n",
      "Step 36400 | Loss: 16.85413 | LR: 4.21e-05\n",
      "val loss: 17.0357\n",
      "Step 36500 | Loss: 16.77585 | LR: 4.19e-05\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 36600 | Loss: 16.48741 | LR: 4.16e-05\n",
      "Step 36700 | Loss: 17.07443 | LR: 4.13e-05\n",
      "val loss: 17.1067\n",
      "Step 36800 | Loss: 16.32262 | LR: 4.11e-05\n",
      "Step 36900 | Loss: 16.57594 | LR: 4.08e-05\n",
      "val loss: 17.1933\n",
      "Step 37000 | Loss: 17.55158 | LR: 4.06e-05\n",
      "Step 37100 | Loss: 16.92621 | LR: 4.03e-05\n",
      "Step 37200 | Loss: 16.69061 | LR: 4.01e-05\n",
      "val loss: 17.0807\n",
      "Step 37300 | Loss: 16.63398 | LR: 3.98e-05\n",
      "Step 37400 | Loss: 16.80024 | LR: 3.96e-05\n",
      "val loss: 16.9924\n",
      "Step 37500 | Loss: 16.47288 | LR: 3.93e-05\n",
      "Step 37600 | Loss: 16.74166 | LR: 3.90e-05\n",
      "Step 37700 | Loss: 17.35259 | LR: 3.88e-05\n",
      "val loss: 16.9603\n",
      "Step 37800 | Loss: 16.77153 | LR: 3.85e-05\n",
      "Step 37900 | Loss: 16.83085 | LR: 3.83e-05\n",
      "val loss: 17.1972\n",
      "Step 38000 | Loss: 17.55879 | LR: 3.80e-05\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "Step 38100 | Loss: 16.99147 | LR: 3.78e-05\n",
      "=== Step 38123 Done. Avg Loss: 16.83911 ===\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 38200 | Loss: 16.50125 | LR: 3.75e-05\n",
      "val loss: 17.1570\n",
      "Step 38300 | Loss: 16.67119 | LR: 3.73e-05\n",
      "Step 38400 | Loss: 16.84227 | LR: 3.70e-05\n",
      "val loss: 17.2015\n",
      "Step 38500 | Loss: 16.61939 | LR: 3.68e-05\n",
      "Step 38600 | Loss: 16.29117 | LR: 3.65e-05\n",
      "Step 38700 | Loss: 17.78548 | LR: 3.63e-05\n",
      "val loss: 17.1392\n",
      "Step 38800 | Loss: 17.26559 | LR: 3.60e-05\n",
      "Step 38900 | Loss: 17.23619 | LR: 3.58e-05\n",
      "val loss: 16.9249\n",
      "Step 39000 | Loss: 16.20800 | LR: 3.55e-05\n",
      "Step 39100 | Loss: 15.87598 | LR: 3.53e-05\n",
      "Step 39200 | Loss: 16.46655 | LR: 3.50e-05\n",
      "val loss: 16.9358\n",
      "Step 39300 | Loss: 16.69109 | LR: 3.48e-05\n",
      "Step 39400 | Loss: 16.83075 | LR: 3.45e-05\n",
      "val loss: 17.2152\n",
      "Step 39500 | Loss: 16.94916 | LR: 3.43e-05\n",
      "Step 39600 | Loss: 16.63886 | LR: 3.40e-05\n",
      "Step 39700 | Loss: 16.99773 | LR: 3.38e-05\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "val loss: 17.2594\n",
      "Step 39800 | Loss: 16.64266 | LR: 3.35e-05\n",
      "Step 39900 | Loss: 16.47893 | LR: 3.33e-05\n",
      "val loss: 17.0579\n",
      "Step 40000 | Loss: 16.76958 | LR: 3.31e-05\n",
      "Step 40100 | Loss: 16.66288 | LR: 3.28e-05\n",
      "Step 40200 | Loss: 17.54020 | LR: 3.26e-05\n",
      "val loss: 16.8422\n",
      "Step 40300 | Loss: 17.74581 | LR: 3.23e-05\n",
      "Step 40400 | Loss: 17.55706 | LR: 3.21e-05\n",
      "val loss: 16.7868\n",
      "Step 40500 | Loss: 16.53287 | LR: 3.18e-05\n",
      "Step 40600 | Loss: 17.43053 | LR: 3.16e-05\n",
      "Step 40700 | Loss: 16.65364 | LR: 3.14e-05\n",
      "val loss: 17.0815\n",
      "Step 40800 | Loss: 16.58503 | LR: 3.11e-05\n",
      "Step 40900 | Loss: 15.98914 | LR: 3.09e-05\n",
      "val loss: 16.8321\n",
      "Step 41000 | Loss: 16.73904 | LR: 3.06e-05\n",
      "Step 41100 | Loss: 16.84414 | LR: 3.04e-05\n",
      "Step 41200 | Loss: 16.82860 | LR: 3.02e-05\n",
      "val loss: 17.0643\n",
      "Step 41300 | Loss: 16.60503 | LR: 2.99e-05\n",
      "=== Step 41300 Done. Avg Loss: 16.81353 ===\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 41400 | Loss: 16.94462 | LR: 2.97e-05\n",
      "val loss: 17.0309\n",
      "Step 41500 | Loss: 16.81315 | LR: 2.94e-05\n",
      "Step 41600 | Loss: 16.74479 | LR: 2.92e-05\n",
      "Step 41700 | Loss: 17.26515 | LR: 2.90e-05\n",
      "val loss: 16.9014\n",
      "Step 41800 | Loss: 16.74881 | LR: 2.87e-05\n",
      "Step 41900 | Loss: 17.21770 | LR: 2.85e-05\n",
      "val loss: 17.2127\n",
      "Step 42000 | Loss: 16.43142 | LR: 2.83e-05\n",
      "Step 42100 | Loss: 16.55416 | LR: 2.80e-05\n",
      "Step 42200 | Loss: 16.55623 | LR: 2.78e-05\n",
      "val loss: 16.8545\n",
      "Step 42300 | Loss: 16.83218 | LR: 2.76e-05\n",
      "Step 42400 | Loss: 16.32335 | LR: 2.73e-05\n",
      "val loss: 17.2132\n",
      "Step 42500 | Loss: 17.11002 | LR: 2.71e-05\n",
      "Step 42600 | Loss: 16.42930 | LR: 2.69e-05\n",
      "Step 42700 | Loss: 16.44244 | LR: 2.66e-05\n",
      "val loss: 17.0630\n",
      "Step 42800 | Loss: 17.41535 | LR: 2.64e-05\n",
      "Step 42900 | Loss: 17.18515 | LR: 2.62e-05\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 16.9984\n",
      "Step 43000 | Loss: 16.56874 | LR: 2.59e-05\n",
      "Step 43100 | Loss: 16.42710 | LR: 2.57e-05\n",
      "Step 43200 | Loss: 16.45926 | LR: 2.55e-05\n",
      "val loss: 17.0047\n",
      "Step 43300 | Loss: 17.09194 | LR: 2.53e-05\n",
      "Step 43400 | Loss: 17.12090 | LR: 2.50e-05\n",
      "val loss: 17.0257\n",
      "Step 43500 | Loss: 16.56409 | LR: 2.48e-05\n",
      "Step 43600 | Loss: 16.89952 | LR: 2.46e-05\n",
      "Step 43700 | Loss: 16.80845 | LR: 2.44e-05\n",
      "val loss: 16.9992\n",
      "Step 43800 | Loss: 17.27575 | LR: 2.41e-05\n",
      "Step 43900 | Loss: 17.13846 | LR: 2.39e-05\n",
      "val loss: 16.8954\n",
      "Step 44000 | Loss: 17.28671 | LR: 2.37e-05\n",
      "Step 44100 | Loss: 16.69692 | LR: 2.35e-05\n",
      "Step 44200 | Loss: 16.85638 | LR: 2.33e-05\n",
      "val loss: 16.8737\n",
      "Step 44300 | Loss: 16.83892 | LR: 2.30e-05\n",
      "Step 44400 | Loss: 16.51361 | LR: 2.28e-05\n",
      "=== Step 44477 Done. Avg Loss: 16.79289 ===\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "val loss: 17.0414\n",
      "Step 44500 | Loss: 16.82869 | LR: 2.26e-05\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "Step 44600 | Loss: 16.74120 | LR: 2.24e-05\n",
      "Step 44700 | Loss: 17.15483 | LR: 2.22e-05\n",
      "val loss: 17.2467\n",
      "Step 44800 | Loss: 16.12209 | LR: 2.20e-05\n",
      "Step 44900 | Loss: 16.84401 | LR: 2.17e-05\n",
      "val loss: 17.0302\n",
      "Step 45000 | Loss: 16.37749 | LR: 2.15e-05\n",
      "Step 45100 | Loss: 16.68334 | LR: 2.13e-05\n",
      "Step 45200 | Loss: 17.08236 | LR: 2.11e-05\n",
      "val loss: 16.7617\n",
      "Step 45300 | Loss: 16.46566 | LR: 2.09e-05\n",
      "Step 45400 | Loss: 16.62429 | LR: 2.07e-05\n",
      "val loss: 17.1676\n",
      "Step 45500 | Loss: 16.64290 | LR: 2.05e-05\n",
      "Step 45600 | Loss: 17.15437 | LR: 2.03e-05\n",
      "Step 45700 | Loss: 16.95461 | LR: 2.00e-05\n",
      "val loss: 16.8580\n",
      "Step 45800 | Loss: 16.80144 | LR: 1.98e-05\n",
      "Step 45900 | Loss: 16.94198 | LR: 1.96e-05\n",
      "val loss: 17.1398\n",
      "Step 46000 | Loss: 16.51254 | LR: 1.94e-05\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 46100 | Loss: 16.54834 | LR: 1.92e-05\n",
      "Step 46200 | Loss: 16.41799 | LR: 1.90e-05\n",
      "val loss: 17.1867\n",
      "Step 46300 | Loss: 16.70673 | LR: 1.88e-05\n",
      "Step 46400 | Loss: 16.45571 | LR: 1.86e-05\n",
      "val loss: 17.1536\n",
      "Step 46500 | Loss: 16.93760 | LR: 1.84e-05\n",
      "Step 46600 | Loss: 16.80234 | LR: 1.82e-05\n",
      "Step 46700 | Loss: 15.80381 | LR: 1.80e-05\n",
      "val loss: 17.2103\n",
      "Step 46800 | Loss: 16.71577 | LR: 1.78e-05\n",
      "Step 46900 | Loss: 16.61387 | LR: 1.76e-05\n",
      "val loss: 17.0089\n",
      "Step 47000 | Loss: 17.20434 | LR: 1.74e-05\n",
      "Step 47100 | Loss: 16.94131 | LR: 1.72e-05\n",
      "Step 47200 | Loss: 17.68665 | LR: 1.70e-05\n",
      "val loss: 17.1459\n",
      "Step 47300 | Loss: 16.64888 | LR: 1.68e-05\n",
      "Step 47400 | Loss: 16.95029 | LR: 1.66e-05\n",
      "val loss: 17.1541\n",
      "Step 47500 | Loss: 16.55531 | LR: 1.64e-05\n",
      "Step 47600 | Loss: 16.17513 | LR: 1.62e-05\n",
      "=== Step 47654 Done. Avg Loss: 16.77102 ===\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 47700 | Loss: 17.27422 | LR: 1.60e-05\n",
      "val loss: 17.0599\n",
      "Step 47800 | Loss: 16.77352 | LR: 1.59e-05\n",
      "Step 47900 | Loss: 17.25189 | LR: 1.57e-05\n",
      "val loss: 16.8463\n",
      "Step 48000 | Loss: 16.51635 | LR: 1.55e-05\n",
      "Step 48100 | Loss: 16.69036 | LR: 1.53e-05\n",
      "Step 48200 | Loss: 16.89220 | LR: 1.51e-05\n",
      "val loss: 16.8600\n",
      "Step 48300 | Loss: 16.79312 | LR: 1.49e-05\n",
      "Step 48400 | Loss: 17.66933 | LR: 1.47e-05\n",
      "val loss: 16.7914\n",
      "Step 48500 | Loss: 16.98176 | LR: 1.45e-05\n",
      "Step 48600 | Loss: 16.85133 | LR: 1.44e-05\n",
      "Step 48700 | Loss: 16.22295 | LR: 1.42e-05\n",
      "val loss: 16.9039\n",
      "Step 48800 | Loss: 17.03748 | LR: 1.40e-05\n",
      "Step 48900 | Loss: 17.00434 | LR: 1.38e-05\n",
      "val loss: 17.1076\n",
      "Step 49000 | Loss: 16.64080 | LR: 1.36e-05\n",
      "Step 49100 | Loss: 16.99519 | LR: 1.35e-05\n",
      "Step 49200 | Loss: 16.53572 | LR: 1.33e-05\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "val loss: 17.0998\n",
      "Step 49300 | Loss: 17.16846 | LR: 1.31e-05\n",
      "Step 49400 | Loss: 17.41003 | LR: 1.29e-05\n",
      "val loss: 16.9683\n",
      "Step 49500 | Loss: 17.47761 | LR: 1.28e-05\n",
      "Step 49600 | Loss: 16.41707 | LR: 1.26e-05\n",
      "Step 49700 | Loss: 16.23686 | LR: 1.24e-05\n",
      "val loss: 16.9348\n",
      "Step 49800 | Loss: 17.21039 | LR: 1.22e-05\n",
      "Step 49900 | Loss: 16.24913 | LR: 1.21e-05\n",
      "val loss: 17.1152\n",
      "Step 50000 | Loss: 16.81446 | LR: 1.19e-05\n",
      "Step 50100 | Loss: 16.48414 | LR: 1.17e-05\n",
      "Step 50200 | Loss: 17.02229 | LR: 1.16e-05\n",
      "val loss: 17.1155\n",
      "Step 50300 | Loss: 16.42176 | LR: 1.14e-05\n",
      "Step 50400 | Loss: 16.16923 | LR: 1.12e-05\n",
      "val loss: 16.8792\n",
      "Step 50500 | Loss: 17.40550 | LR: 1.11e-05\n",
      "Step 50600 | Loss: 16.80684 | LR: 1.09e-05\n",
      "Step 50700 | Loss: 16.48663 | LR: 1.08e-05\n",
      "val loss: 17.0190\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "Step 50800 | Loss: 16.55655 | LR: 1.06e-05\n",
      "=== Step 50831 Done. Avg Loss: 16.76047 ===\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 50900 | Loss: 16.49137 | LR: 1.04e-05\n",
      "val loss: 17.1914\n",
      "Step 51000 | Loss: 17.05525 | LR: 1.03e-05\n",
      "Step 51100 | Loss: 17.58600 | LR: 1.01e-05\n",
      "Step 51200 | Loss: 16.77507 | LR: 9.96e-06\n",
      "val loss: 17.1580\n",
      "Step 51300 | Loss: 16.51027 | LR: 9.80e-06\n",
      "Step 51400 | Loss: 17.11026 | LR: 9.65e-06\n",
      "val loss: 17.1084\n",
      "Step 51500 | Loss: 16.83289 | LR: 9.50e-06\n",
      "Step 51600 | Loss: 17.19711 | LR: 9.34e-06\n",
      "Step 51700 | Loss: 16.76755 | LR: 9.19e-06\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 17.0504\n",
      "Step 51800 | Loss: 16.49270 | LR: 9.04e-06\n",
      "Step 51900 | Loss: 17.03533 | LR: 8.90e-06\n",
      "val loss: 17.0811\n",
      "Step 52000 | Loss: 16.68531 | LR: 8.75e-06\n",
      "Step 52100 | Loss: 16.55408 | LR: 8.60e-06\n",
      "Step 52200 | Loss: 16.95650 | LR: 8.46e-06\n",
      "val loss: 17.0619\n",
      "Step 52300 | Loss: 16.79439 | LR: 8.31e-06\n",
      "Step 52400 | Loss: 17.19528 | LR: 8.17e-06\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "val loss: 16.9494\n",
      "Step 52500 | Loss: 17.21507 | LR: 8.03e-06\n",
      "Step 52600 | Loss: 16.13430 | LR: 7.89e-06\n",
      "Step 52700 | Loss: 16.35357 | LR: 7.75e-06\n",
      "val loss: 17.0901\n",
      "Step 52800 | Loss: 17.30466 | LR: 7.61e-06\n",
      "Step 52900 | Loss: 16.07425 | LR: 7.47e-06\n",
      "val loss: 16.8946\n",
      "Step 53000 | Loss: 16.65761 | LR: 7.33e-06\n",
      "Step 53100 | Loss: 16.92130 | LR: 7.20e-06\n",
      "Step 53200 | Loss: 16.62272 | LR: 7.06e-06\n",
      "val loss: 16.9289\n",
      "Step 53300 | Loss: 16.34350 | LR: 6.93e-06\n",
      "Step 53400 | Loss: 16.99640 | LR: 6.80e-06\n",
      "val loss: 16.9982\n",
      "Step 53500 | Loss: 16.51350 | LR: 6.67e-06\n",
      "Step 53600 | Loss: 16.51700 | LR: 6.54e-06\n",
      "Step 53700 | Loss: 15.98772 | LR: 6.41e-06\n",
      "val loss: 16.9983\n",
      "Step 53800 | Loss: 17.19887 | LR: 6.29e-06\n",
      "Step 53900 | Loss: 16.71920 | LR: 6.16e-06\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "val loss: 16.9908\n",
      "Step 54000 | Loss: 16.46746 | LR: 6.04e-06\n",
      "=== Step 54008 Done. Avg Loss: 16.75267 ===\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "Step 54100 | Loss: 16.68739 | LR: 5.91e-06\n",
      "Step 54200 | Loss: 16.60689 | LR: 5.79e-06\n",
      "val loss: 17.0071\n",
      "Step 54300 | Loss: 16.55331 | LR: 5.67e-06\n",
      "Step 54400 | Loss: 17.29953 | LR: 5.55e-06\n",
      "val loss: 17.1897\n",
      "Step 54500 | Loss: 16.61032 | LR: 5.43e-06\n",
      "Step 54600 | Loss: 16.37284 | LR: 5.31e-06\n",
      "Step 54700 | Loss: 16.50013 | LR: 5.20e-06\n",
      "val loss: 17.1290\n",
      "Step 54800 | Loss: 16.90550 | LR: 5.08e-06\n",
      "Step 54900 | Loss: 17.17211 | LR: 4.97e-06\n",
      "val loss: 17.0906\n",
      "Step 55000 | Loss: 16.45537 | LR: 4.86e-06\n",
      "Step 55100 | Loss: 16.58312 | LR: 4.74e-06\n",
      "Step 55200 | Loss: 16.54844 | LR: 4.63e-06\n",
      "val loss: 17.0805\n",
      "Step 55300 | Loss: 16.45066 | LR: 4.53e-06\n",
      "Step 55400 | Loss: 16.62354 | LR: 4.42e-06\n",
      "val loss: 17.0445\n",
      "Step 55500 | Loss: 16.56662 | LR: 4.31e-06\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "Step 55600 | Loss: 16.95568 | LR: 4.21e-06\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 55700 | Loss: 16.51634 | LR: 4.10e-06\n",
      "val loss: 16.9630\n",
      "Step 55800 | Loss: 17.16361 | LR: 4.00e-06\n",
      "Step 55900 | Loss: 16.60416 | LR: 3.90e-06\n",
      "val loss: 17.0929\n",
      "Step 56000 | Loss: 16.54859 | LR: 3.80e-06\n",
      "Step 56100 | Loss: 17.06029 | LR: 3.70e-06\n",
      "Step 56200 | Loss: 17.01010 | LR: 3.60e-06\n",
      "val loss: 16.9986\n",
      "Step 56300 | Loss: 16.44108 | LR: 3.51e-06\n",
      "Step 56400 | Loss: 16.91930 | LR: 3.41e-06\n",
      "val loss: 17.0150\n",
      "Step 56500 | Loss: 16.97771 | LR: 3.32e-06\n",
      "Step 56600 | Loss: 16.42821 | LR: 3.22e-06\n",
      "Step 56700 | Loss: 16.30658 | LR: 3.13e-06\n",
      "val loss: 17.0540\n",
      "Step 56800 | Loss: 16.15454 | LR: 3.04e-06\n",
      "Step 56900 | Loss: 17.18301 | LR: 2.95e-06\n",
      "val loss: 16.8659\n",
      "Step 57000 | Loss: 16.41099 | LR: 2.87e-06\n",
      "Step 57100 | Loss: 16.59066 | LR: 2.78e-06\n",
      "=== Step 57185 Done. Avg Loss: 16.73853 ===\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "Step 57200 | Loss: 16.18717 | LR: 2.70e-06\n",
      "val loss: 17.0583\n",
      "Step 57300 | Loss: 16.68880 | LR: 2.61e-06\n",
      "Step 57400 | Loss: 16.39677 | LR: 2.53e-06\n",
      "val loss: 16.9657\n",
      "Step 57500 | Loss: 16.92980 | LR: 2.45e-06\n",
      "Step 57600 | Loss: 16.36772 | LR: 2.37e-06\n",
      "Step 57700 | Loss: 16.59154 | LR: 2.29e-06\n",
      "val loss: 16.9734\n",
      "Step 57800 | Loss: 16.82847 | LR: 2.21e-06\n",
      "Step 57900 | Loss: 16.39394 | LR: 2.14e-06\n",
      "val loss: 17.0849\n",
      "Step 58000 | Loss: 16.62309 | LR: 2.06e-06\n",
      "Step 58100 | Loss: 16.82580 | LR: 1.99e-06\n",
      "Step 58200 | Loss: 17.46317 | LR: 1.92e-06\n",
      "val loss: 16.9995\n",
      "Step 58300 | Loss: 16.42017 | LR: 1.85e-06\n",
      "Step 58400 | Loss: 16.90114 | LR: 1.78e-06\n",
      "val loss: 17.0193\n",
      "Step 58500 | Loss: 16.30282 | LR: 1.71e-06\n",
      "Step 58600 | Loss: 16.44000 | LR: 1.64e-06\n",
      "Step 58700 | Loss: 16.55223 | LR: 1.58e-06\n",
      "val loss: 16.8647\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 58800 | Loss: 16.54187 | LR: 1.51e-06\n",
      "Step 58900 | Loss: 18.11862 | LR: 1.45e-06\n",
      "val loss: 16.8046\n",
      "Step 59000 | Loss: 17.17580 | LR: 1.39e-06\n",
      "Step 59100 | Loss: 17.07303 | LR: 1.33e-06\n",
      "Step 59200 | Loss: 16.28945 | LR: 1.27e-06\n",
      "val loss: 17.0473\n",
      "Step 59300 | Loss: 16.63611 | LR: 1.21e-06\n",
      "Step 59400 | Loss: 16.34391 | LR: 1.16e-06\n",
      "val loss: 16.8031\n",
      "Step 59500 | Loss: 16.70113 | LR: 1.10e-06\n",
      "Step 59600 | Loss: 16.63875 | LR: 1.05e-06\n",
      "Step 59700 | Loss: 16.38533 | LR: 9.95e-07\n",
      "val loss: 17.2612\n",
      "Step 59800 | Loss: 16.51951 | LR: 9.44e-07\n",
      "Step 59900 | Loss: 16.61489 | LR: 8.94e-07\n",
      "val loss: 17.1298\n",
      "Step 60000 | Loss: 16.52552 | LR: 8.46e-07\n",
      "Step 60100 | Loss: 16.67814 | LR: 7.99e-07\n",
      "Step 60200 | Loss: 16.37142 | LR: 7.53e-07\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 17.0726\n",
      "Step 60300 | Loss: 16.10396 | LR: 7.09e-07\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "=== Step 60362 Done. Avg Loss: 16.73709 ===\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "Step 60400 | Loss: 16.41937 | LR: 6.66e-07\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "val loss: 17.0454\n",
      "Step 60500 | Loss: 16.75103 | LR: 6.24e-07\n",
      "Step 60600 | Loss: 16.34589 | LR: 5.84e-07\n",
      "Step 60700 | Loss: 17.44735 | LR: 5.45e-07\n",
      "val loss: 17.0155\n",
      "Step 60800 | Loss: 16.57711 | LR: 5.07e-07\n",
      "Step 60900 | Loss: 17.11370 | LR: 4.71e-07\n",
      "val loss: 16.9565\n",
      "Step 61000 | Loss: 16.69610 | LR: 4.36e-07\n",
      "Step 61100 | Loss: 16.98686 | LR: 4.02e-07\n",
      "Step 61200 | Loss: 16.82661 | LR: 3.70e-07\n",
      "val loss: 17.2005\n",
      "Step 61300 | Loss: 16.99120 | LR: 3.39e-07\n",
      "Step 61400 | Loss: 16.72231 | LR: 3.10e-07\n",
      "val loss: 16.9824\n",
      "Step 61500 | Loss: 16.97720 | LR: 2.81e-07\n",
      "Step 61600 | Loss: 16.77627 | LR: 2.55e-07\n",
      "Step 61700 | Loss: 16.64575 | LR: 2.29e-07\n",
      "val loss: 17.0150\n",
      "Step 61800 | Loss: 16.89970 | LR: 2.05e-07\n",
      "Step 61900 | Loss: 16.97503 | LR: 1.82e-07\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "val loss: 16.9709\n",
      "Step 62000 | Loss: 16.76478 | LR: 1.60e-07\n",
      "Step 62100 | Loss: 16.74297 | LR: 1.40e-07\n",
      "Step 62200 | Loss: 16.80841 | LR: 1.22e-07\n",
      "val loss: 16.9251\n",
      "Step 62300 | Loss: 16.58110 | LR: 1.04e-07\n",
      "Step 62400 | Loss: 16.25978 | LR: 8.81e-08\n",
      "val loss: 17.0859\n",
      "Step 62500 | Loss: 16.32612 | LR: 7.33e-08\n",
      "Step 62600 | Loss: 16.53945 | LR: 6.00e-08\n",
      "Step 62700 | Loss: 16.88279 | LR: 4.79e-08\n",
      "val loss: 17.2607\n",
      "Step 62800 | Loss: 16.26134 | LR: 3.73e-08\n",
      "Step 62900 | Loss: 16.92782 | LR: 2.80e-08\n",
      "val loss: 16.9730\n",
      "Step 63000 | Loss: 16.37288 | LR: 2.00e-08\n",
      "Step 63100 | Loss: 17.56115 | LR: 1.34e-08\n",
      "Step 63200 | Loss: 16.74872 | LR: 8.14e-09\n",
      "val loss: 16.8969\n",
      "Step 63300 | Loss: 16.11943 | LR: 4.24e-09\n",
      "Step 63400 | Loss: 16.10594 | LR: 1.69e-09\n",
      "val loss: 16.9607\n",
      "Step 63500 | Loss: 17.00905 | LR: 4.98e-10\n",
      "val loss: 16.8812\n",
      "=== Step 63539 Done. Avg Loss: 16.73119 ===\n"
     ]
    }
   ],
   "source": [
    "model.freeze_encoders()\n",
    "model.train()\n",
    "\n",
    "for step in range(max_steps):\n",
    "\n",
    "    last_step = (step == max_steps - 1)\n",
    "\n",
    "    # once in a while evaluate our validation set loss\n",
    "    if step % 250 == 0 or last_step:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss_accum = 0.0\n",
    "            val_loss_steps = 10\n",
    "            for i in range(val_loss_steps):\n",
    "                xc, xct, xt, xtt, aid = val_loader.next_batch()\n",
    "\n",
    "                val_loss = model(xc, xct, xt, xtt, aid)\n",
    "                val_loss_accum += val_loss.item()\n",
    "\n",
    "            avg_val_loss = val_loss_accum / val_loss_steps\n",
    "            print(f'val loss: {avg_val_loss:.4f}')\n",
    "\n",
    "        # with open(log_file, \"a\") as f:\n",
    "        #    f.write(f'{step} test {test_loss_accum.item():.4f}\\n')\n",
    "        model.train()\n",
    "\n",
    "\n",
    "    if step > 0 and (step+1) % steps_per_epoch ==0 and not last_step:\n",
    "        # Save Checkpoint\n",
    "        torch.save({\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'step': step\n",
    "        }, checkpoint_dir / f'bio_jepa_ckpt_{step}.pt')\n",
    "\n",
    "\n",
    "    # Get Batch (xc=Control, xt=Treated/Case)\n",
    "    xc, xct, xt, xtt, aid = train_loader.next_batch()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss = model(xc, xct, xt, xtt, aid)\n",
    "    loss.backward()\n",
    "\n",
    "    # gradient clipping\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    lossi.append(loss.item())\n",
    "    total_epoch_loss += loss.item()\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step} | Loss: {loss.item():.5f} | LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "    \n",
    "    \n",
    "    if step > 0 and (step+1) % steps_per_epoch == 0:   \n",
    "        avg_loss = total_epoch_loss / steps_per_epoch\n",
    "        print(f\"=== Step {step} Done. Avg Loss: {avg_loss:.5f} ===\")\n",
    "        total_epoch_loss = 0\n",
    "    \n",
    "    \n",
    "    if last_step:\n",
    "        # Save Checkpoint\n",
    "        torch.save({\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'step': step\n",
    "        }, checkpoint_dir / f'bio_jepa_ckpt_{step}_final.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99e5f01f-441d-44bc-95d2-a8c28eacbd76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0xf84a481f1e80>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGdCAYAAACyzRGfAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjgsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvwVt1zgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAPvVJREFUeJzt3XlcVOX+B/DPDMMM6wyCLJKguOKeW4hbXaXIvG3Soj8rK2+m0aK2XK20ui2Y3bI0teWaejMzvaVlmqaomIaouOKCoigomxszgDAs8/z+QE6MgDowzBk8n/frdV455zxz5jvHaebjc57zHJUQQoCIiIjIQdRyF0BERETKwvBBREREDsXwQURERA7F8EFEREQOxfBBREREDsXwQURERA7F8EFEREQOxfBBREREDqWRu4CrWSwWZGVlwdvbGyqVSu5yiIiI6AYIIVBQUIDg4GCo1dfu23C68JGVlYWQkBC5yyAiIqJ6yMzMRMuWLa/ZxunCh7e3N4DK4vV6vczVEBER0Y0wmUwICQmRfsevxenCR9WpFr1ez/BBRETUxNzIkAkOOCUiIiKHYvggIiIih2L4ICIiIodi+CAiIiKHsil8VFRUYNq0aQgLC4O7uzvatm2Ld999F0IIqY0QAtOnT0eLFi3g7u6OqKgoHD9+3O6FExERUdNkU/j48MMPMX/+fHz++ec4cuQIPvzwQ8ycORNz5syR2sycOROzZ8/GF198gaSkJHh6eiI6OholJSV2L56IiIiaHpWo3m1xHX//+98RGBiIBQsWSOtiYmLg7u6OJUuWQAiB4OBgvPzyy3jllVcAAEajEYGBgVi0aBFGjhx53dcwmUwwGAwwGo281JaIiKiJsOX326aej/79+yM+Ph7Hjh0DAOzfvx/btm3DsGHDAADp6enIyclBVFSU9ByDwYCIiAgkJibWuk+z2QyTyWS1EBER0c3LpknGpkyZApPJhPDwcLi4uKCiogLvv/8+Ro8eDQDIyckBAAQGBlo9LzAwUNp2tbi4OLzzzjv1qZ2IiIiaIJt6PpYvX47vvvsOS5cuxZ49e7B48WL8+9//xuLFi+tdwNSpU2E0GqUlMzOz3vsiIiIi52dTz8err76KKVOmSGM3unXrhtOnTyMuLg5jxoxBUFAQACA3NxctWrSQnpebm4tbb7211n3qdDrodLp6lk9ERERNjU09H5cvX65xm1wXFxdYLBYAQFhYGIKCghAfHy9tN5lMSEpKQmRkpB3KJSIioqbOpp6Pe++9F++//z5CQ0PRpUsX7N27F5988gmefvppAJU3k5k4cSLee+89tG/fHmFhYZg2bRqCg4PxwAMPNEb9N+xcgRlzN6fBzdUFU4aFy1oLERGRktkUPubMmYNp06bhueeeQ15eHoKDg/Hss89i+vTpUpvXXnsNRUVFGDduHPLz8zFw4ECsW7cObm5udi/eFqaSMiz68xT0bhqGDyIiIhnZNM+HIzTWPB8nzhVi6McJ0LtpcODtaLvtl4iIiBpxno+bgVMlLSIiIgVSTPhQyV0AERERAVBQ+JCw64OIiEhWigkfKhX7PoiIiJyBYsIHEREROQfFhQ+edSEiIpKXYsIHT7oQERE5B8WEDyIiInIOigkfVeNNnWxONSIiIsVRTPggIiIi56C48MF+DyIiInkpJnyoOOSUiIjIKSgmfBAREZFzUFz44HhTIiIieSkmfHB2dSIiIuegmPBBREREzkFx4UPwehciIiJZKS58EBERkbwUFz444JSIiEheigkfHHBKRETkHBQTPoiIiMg5KCZ8qK50ffCsCxERkbwUEz6IiIjIOTB8EBERkUMpJnxI40153oWIiEhWigkfRERE5BwYPoiIiMihFBM+qub54PTqRERE8lJM+CAiIiLnoJjwoboy5JTTqxMREclLMeGDiIiInAPDBxERETmUYsLHXwNOiYiISE6KCR9ERETkHBg+iIiIyKEUEz6qplcXvNyFiIhIVooJH0REROQclBM+OOCUiIjIKSgnfBAREZFTYPggIiIih1JM+OD06kRERM5BMeGDiIiInINN4aN169ZQqVQ1ltjYWABASUkJYmNj4efnBy8vL8TExCA3N7dRCrdV1QynREREJC+bwseuXbuQnZ0tLRs2bAAAPPzwwwCASZMmYfXq1VixYgUSEhKQlZWFESNG2L9qIiIiarI0tjT29/e3ejxjxgy0bdsWt99+O4xGIxYsWIClS5diyJAhAICFCxeiU6dO2LFjB/r162e/qomIiKjJqveYj9LSUixZsgRPP/00VCoVkpOTUVZWhqioKKlNeHg4QkNDkZiYWOd+zGYzTCaT1dIYqp914SynRERE8ql3+Fi1ahXy8/Px5JNPAgBycnKg1Wrh4+Nj1S4wMBA5OTl17icuLg4Gg0FaQkJC6lsSERERNQH1Dh8LFizAsGHDEBwc3KACpk6dCqPRKC2ZmZkN2h8RERE5N5vGfFQ5ffo0Nm7ciJ9++klaFxQUhNLSUuTn51v1fuTm5iIoKKjOfel0Ouh0uvqUYRNVtctdhODVL0RERHKpV8/HwoULERAQgOHDh0vrevfuDVdXV8THx0vrUlNTkZGRgcjIyIZXSkRERDcFm3s+LBYLFi5ciDFjxkCj+evpBoMBY8eOxeTJk+Hr6wu9Xo8XXngBkZGRTnGlCzs6iIiInIPN4WPjxo3IyMjA008/XWPbrFmzoFarERMTA7PZjOjoaMybN88uhdoTr3UhIiKSj0o42XWnJpMJBoMBRqMRer3ebvu9VFSKnu9WTop24oN74KJmXwgREZG92PL7rZh7u3CAKRERkXNQTPiozsk6e4iIiBRFMeFDxSGnRERETkEx4aM69nsQERHJR5Hhg4iIiOSjnPDBsy5EREROQTnhoxqONyUiIpKPIsMHERERyUcx4YPzfBARETkHxYSP6gSvdyEiIpKNYsIHOz6IiIicg2LCR3UccEpERCQfRYYPIiIiko9iwoeKI06JiIicgmLCBxERETkHhg8iIiJyKMWED550ISIicg6KCR/V8WoXIiIi+SgmfHC8KRERkXNQTPiojjOcEhERyUeR4YOIiIjko5jwoeKQUyIiIqegmPBRHQecEhERyUcx4YMDTomIiJyDYsIHEREROQdFhg+edSEiIpKPIsMHERERyUeR4UNwxCkREZFsFBk+juUWyl0CERGRYikmfFS/2qW8wiJfIURERAqnmPDBMy1ERETOQTHhozoVJ/0gIiKSjWLCh6Va1wezBxERkXwUEz60Ln+9VR93VxkrISIiUjbFhA9NtfDhqdPIWAkREZGyKSZ8AIBWU/l2OfaUiIhIPooKH1VDPTjJGBERkXyUFT6upA9mDyIiIvkoK3yAl7kQERHJTVHhg4iIiOSnqPDB0y5ERETyU1b4kLsAIiIisj18nD17Fo899hj8/Pzg7u6Obt26Yffu3dJ2IQSmT5+OFi1awN3dHVFRUTh+/Lhdi24owYttiYiIZGNT+Lh06RIGDBgAV1dX/Pbbbzh8+DA+/vhjNGvWTGozc+ZMzJ49G1988QWSkpLg6emJ6OholJSU2L14W1Xd04WnXYiIiORj01SfH374IUJCQrBw4UJpXVhYmPRnIQQ+/fRTvPnmm7j//vsBAP/9738RGBiIVatWYeTIkXYqu36keT5krYKIiEjZbOr5+OWXX9CnTx88/PDDCAgIQM+ePfH1119L29PT05GTk4OoqChpncFgQEREBBITE2vdp9lshslksloaDQd9EBERyc6m8HHy5EnMnz8f7du3x/r16zFhwgS8+OKLWLx4MQAgJycHABAYGGj1vMDAQGnb1eLi4mAwGKQlJCSkPu/DJpzhlIiISD42hQ+LxYJevXrhgw8+QM+ePTFu3Dg888wz+OKLL+pdwNSpU2E0GqUlMzOz3vu6Hp52ISIikp9N4aNFixbo3Lmz1bpOnTohIyMDABAUFAQAyM3NtWqTm5srbbuaTqeDXq+3WhpL1YBTIiIiko9N4WPAgAFITU21Wnfs2DG0atUKQOXg06CgIMTHx0vbTSYTkpKSEBkZaYdy7YNnXYiIiORj09UukyZNQv/+/fHBBx/gkUcewc6dO/HVV1/hq6++AlDZszBx4kS89957aN++PcLCwjBt2jQEBwfjgQceaIz6bfJXxwfTBxERkVxsCh99+/bFypUrMXXqVPzrX/9CWFgYPv30U4wePVpq89prr6GoqAjjxo1Dfn4+Bg4ciHXr1sHNzc3uxdtKGvPB7EFERCQblXCySz9MJhMMBgOMRqPdx3/0encDLhaVYsOkwWgf6G3XfRMRESmZLb/firq3SxWnSltEREQKo6jwwdMuRERE8lNW+OCVtkRERLJTVPiowrvaEhERyUdh4YN3tSUiIpKbosJH1WkXhg8iIiL5KCt8yF0AERERKSt8VOGYDyIiIvkoKnzwtAsREZH8lBU+eOKFiIhIdsoKH8weREREslNU+KjC0y5ERETyUVT4yDaWAOCAUyIiIjkpKnxUSUg9J3cJREREiqXI8JGaWyB3CURERIqlyPARqHeTuwQiIiLFUmT4OHupWO4SiIiIFEuR4aOotFzuEoiIiBRLkeGDl9oSERHJR5Hhw8L0QUREJBtFhg9mDyIiIvkoM3xwkjEiIiLZKDN8MHsQERHJhuGDiIiIHEqR4UPnqsi3TURE5BQU+Ss8sm+o3CUQEREplqLCR48QHwCATqOot01ERORUFPUrrLryXw75ICIiko+iwof6SvoQHHFKREQkG0WFD5WqMn1YmD2IiIhko6zwIf2J6YOIiEguigofFVdOt/CsCxERkXwUFT72ZuQDANYczJa3ECIiIgVTVPio8usBhg8iIiK5KDJ8EBERkXwYPoiIiMihGD6IiIjIoRg+iIiIyKEYPoiIiMihFBk+OgR6yV0CERGRYikyfBzLLZS7BCIiIsVSZPggIiIi+dgUPt5++22oVCqrJTw8XNpeUlKC2NhY+Pn5wcvLCzExMcjNzbV70URERNR02dzz0aVLF2RnZ0vLtm3bpG2TJk3C6tWrsWLFCiQkJCArKwsjRoywa8FERETUtGlsfoJGg6CgoBrrjUYjFixYgKVLl2LIkCEAgIULF6JTp07YsWMH+vXr1/BqiYiIqMmzuefj+PHjCA4ORps2bTB69GhkZGQAAJKTk1FWVoaoqCipbXh4OEJDQ5GYmFjn/sxmM0wmk9VCRERENy+bwkdERAQWLVqEdevWYf78+UhPT8egQYNQUFCAnJwcaLVa+Pj4WD0nMDAQOTk5de4zLi4OBoNBWkJCQur1RoiIiKhpsOm0y7Bhw6Q/d+/eHREREWjVqhWWL18Od3f3ehUwdepUTJ48WXpsMpkYQIiIiG5iDbrU1sfHBx06dEBaWhqCgoJQWlqK/Px8qza5ubm1jhGpotPpoNfrrRYiIiK6eTUofBQWFuLEiRNo0aIFevfuDVdXV8THx0vbU1NTkZGRgcjIyAYXSkRERDcHm067vPLKK7j33nvRqlUrZGVl4a233oKLiwtGjRoFg8GAsWPHYvLkyfD19YVer8cLL7yAyMhIXulCREREEpvCx5kzZzBq1ChcuHAB/v7+GDhwIHbs2AF/f38AwKxZs6BWqxETEwOz2Yzo6GjMmzevUQonIiKipkklhBByF1GdyWSCwWCA0Wi0+/iP1lPWSH8+NWO4XfdNRESkZLb8fvPeLkRERORQDB9ERETkUAwfRERE5FAMH0RERORQDB9ERETkUIoNHxaLU13kQ0REpBjKDR/OdYUxERGRYig2fBAREZE8FBs+VCqV3CUQEREpknLDh9wFEBERKZRiw0dphUXuEoiIiBRJseHjf8ln5C6BiIhIkRQbPozFZXKXQEREpEiKDR+c54OIiEgeigof3jqN9Oez+cUyVkJERKRcigofendX6c/LdmXKWAkREZFyKSp8dAj0krsEIiIixVNU+HDXushdAhERkeIpKnwEeLvJXQIREZHiKSp8dAj0lrsEIiIixVNU+NC4cFJ1IiIiuSkqfLheFT7OXLosUyVERETKpajw0TOkmdXjicv2yVMIERGRgikqfIT6elg93n36kkyVEBERKZeiwoeKQz6IiIhkp7DwwfRBREQkN0WFDyIiIpIfwwcRERE5lOLDR0FJmdwlEBERKYriw8euUxflLoGIiEhRFB8+0s9zojEiIiJHUlz4+MfAMKvHal4AQ0RE5FCKCx+T7uxg9bi03CJTJURERMqkuPDhqdNYPf5pz1mZKiEiIlImxYWPq6XmFshdAhERkaIoPnwQERGRYzF8EBERkUMxfBAREZFDMXwA+PPEeblLICIiUgyGDwD/93UShBByl0FERKQIDB9XHM8rlLsEIiIiRVBk+BjerUWNdZxsjIiIyDEaFD5mzJgBlUqFiRMnSutKSkoQGxsLPz8/eHl5ISYmBrm5uQ2t067e/HunGuv+Pmcbluw4LUM1REREylLv8LFr1y58+eWX6N69u9X6SZMmYfXq1VixYgUSEhKQlZWFESNGNLhQe/Lz1NW6/s1VKQ6uhIiISHnqFT4KCwsxevRofP3112jWrJm03mg0YsGCBfjkk08wZMgQ9O7dGwsXLsSff/6JHTt22K3ohtJq6n7bmRd5l1siIqLGVK/wERsbi+HDhyMqKspqfXJyMsrKyqzWh4eHIzQ0FImJibXuy2w2w2QyWS1yGjRzM9YcyJa1BiIiopuZzeFj2bJl2LNnD+Li4mpsy8nJgVarhY+Pj9X6wMBA5OTk1Lq/uLg4GAwGaQkJCbG1JLuLXbqHPSBERESNxKbwkZmZiZdeegnfffcd3Nzc7FLA1KlTYTQapSUzM9Mu+72eDx7sds3tg2Zuxp9pnHyMiIjI3mwKH8nJycjLy0OvXr2g0Wig0WiQkJCA2bNnQ6PRIDAwEKWlpcjPz7d6Xm5uLoKCgmrdp06ng16vt1ocoW/rZtdt83//SXJAJURERMqisaXx0KFDcfDgQat1Tz31FMLDw/HPf/4TISEhcHV1RXx8PGJiYgAAqampyMjIQGRkpP2qtgONiyKnOCEiIpKdTeHD29sbXbt2tVrn6ekJPz8/af3YsWMxefJk+Pr6Qq/X44UXXkBkZCT69etnv6rtwNdDK3cJREREimRT+LgRs2bNglqtRkxMDMxmM6KjozFv3jx7v0yDGTxc5S6BiIhIkVTCye6oZjKZYDAYYDQaG338x/hvk7HuUO1X4VQ5NWN4o9ZARER0M7Dl91vRAx+m39v5um3KK3jPFyIiIntSdPgI9nGHr+e1x35sPOJc96UhIiJq6hQdPgDg27G3XXP7+CV7HFQJERGRMig+fLRs5iF3CURERIqi+PBhcHfFppdvv2abMo77ICIishvFhw8AaOPvhUHtm9e5vf0bvzmwGiIiopsbw8cVn4/qdc3tvOqFiIjIPhg+rjB4uOLg23fVub3CuaZDISIiarIYPqrxdnPFkX/dXes2tUrl4GqIiIhuTgwfV3HXuuAWH/ca67u9vV6GaoiIiG4+DB+12DB5cI11JWUc80FERGQPDB+18NDWfr89i4XjPoiIiBqK4cMGy3Zlyl0CERFRk8fwYYOlO0/LXQIREVGTx/BRh7T3h9VYl3LWBFNJmQzVEBER3TwYPuqgcan90LyxMsXBlRAREd1cGD5stPXYOblLICIiatIYPmzEucaIiIgahuHjGuaNrnm/F850SkRE1DAMH9dwT7cWNdZdLCqVoRIiIqKbB8MHERERORTDBxERETkUwwcRERE5FMPHdWyYVPMmc0RERFR/DB/X4e+tk7sEIiKimwrDx3X4eGjlLoGIiOimwvBxA/q2bmb1ODWnQKZKiIiImj6Gjxvw7OC2Vo8zL16WqRIiIqKmj+HjBnQI9LZ6/I//7papEiIioqaP4eMGtGzmXmPdJc50SkREVC8MHzdAra55PxdjcZkMlRARETV9DB/15FJLICEiIqLrY/iop4k/7JO7BCIioiaJ4aOekk9fkrsEIiKiJonh4wbd0dFf7hKIiIhuCgwfN+jjh3vIXQIREdFNgeHjBvl58R4vRERE9sDw0QC/H8qRuwQiIqImh+GjAcZ9myx3CURERE0OwwcRERE5FMOHDaI6BchdAhERUZPH8GGDKcPC5S6BiIioybMpfMyfPx/du3eHXq+HXq9HZGQkfvvtN2l7SUkJYmNj4efnBy8vL8TExCA3N9fuRculXYD39RsRERHRNdkUPlq2bIkZM2YgOTkZu3fvxpAhQ3D//ffj0KFDAIBJkyZh9erVWLFiBRISEpCVlYURI0Y0SuHO4nhugdwlEBERNSkqIYRoyA58fX3x0Ucf4aGHHoK/vz+WLl2Khx56CABw9OhRdOrUCYmJiejXr98N7c9kMsFgMMBoNEKv1zektEbResqaGutOzRguQyVERETOw5bf73qP+aioqMCyZctQVFSEyMhIJCcno6ysDFFRUVKb8PBwhIaGIjExsc79mM1mmEwmq8WZRbbxk7sEIiKiJs3m8HHw4EF4eXlBp9Nh/PjxWLlyJTp37oycnBxotVr4+PhYtQ8MDEROTt2TccXFxcFgMEhLSEiIzW/Ckd57sGuNdfsy8x1fCBERURNlc/jo2LEj9u3bh6SkJEyYMAFjxozB4cOH613A1KlTYTQapSUzM7Pe+3KEtv5eNdY9MHe7DJUQERE1TRpbn6DVatGuXTsAQO/evbFr1y589tlnePTRR1FaWor8/Hyr3o/c3FwEBQXVuT+dTgedjvdNISIiUooGz/NhsVhgNpvRu3dvuLq6Ij4+XtqWmpqKjIwMREZGNvRlnN63iafkLoGIiKhJsKnnY+rUqRg2bBhCQ0NRUFCApUuXYsuWLVi/fj0MBgPGjh2LyZMnw9fXF3q9Hi+88AIiIyNv+EqXpuLZwW3w5daTVuum/XwIj0e2lqcgIiKiJsSm8JGXl4cnnngC2dnZMBgM6N69O9avX48777wTADBr1iyo1WrExMTAbDYjOjoa8+bNa5TC5dQ5uPZLiErKKuDm6uLgaoiIiJqWBs/zYW/OPs8HABSXVqDT9HU11i9++jbc3sFfhoqIiIjk5ZB5PpTMXVt774bKwXUQERE1RQwfdnS5tFzuEoiIiJwew4cdLdiWLncJRERETo/ho57+eO1vNdbtOnVJhkqIiIiaFoaPevL35sRoRERE9cHwUU91XVJbUlbh4EqIiIiaFoYPO3v9p4Nyl0BEROTUGD4aoEeIT411P+096/hCiIiImhCGjwaYM7Jnrev/l3zGwZUQERE1HQwfDRDq51Hr+ldW7HdwJURERE0Hw0cDhTX3rHV9Vn6xgyshIiJqGhg+GmjWo7fWur7/jE2OLYSIiKiJYPhooFtrGXRKREREdWP4aEQVFqe6YTAREZFTYPhoRG1fXyt3CURERE6H4aORXSwqlbsEIiIip8LwYQc/Tuhf57Ze726AhadfiIiIJAwfdtC7VbNaZzutcs/sPxxXDBERkZNj+LCTf97dsc5tR3MKMHz2H0jLK3RgRURERM6J4cNO+rdtfs3th7JMiPokwUHVEBEROS+GDzuK7hIodwlEREROj+HDjsYNbnPdNqXlFgdUQkRE5LwYPuyoZ0iz67aZtyWNV78QEZGiMXzYkVqtgruryzXbfLrxOJ5ctMtBFRERETkfhg87WxU74Lptth47hye+2YlPNx5zQEVERETOheHDzjoGeePd+7tct93WY+fw6cbjDqiIiIjIuTB8NILHI1vfcFtjcRnKKzgIlYiIlIPho5G8Gl33pGPV9Xjnd3Sevh6zNvAUDBERKQPDRyN5tG/IDbctrbDgs3iegiEiImVg+Ggkzb10eOWuDnKXQURE5HQ0chdwM+sVev15P6pbvisTzTy1AIBNR3Px9n1doNNc+9JdIiKipobhoxFFtvXD/bcG4+d9WTfU/rUfD1g9DmvuiXGD2zZGaURERLLhaZdGpFKp8NnInoh/+fZ6PT8rv8TOFREREcmP4cMB2vp71et5i/48Zd9CiIiInADDh4OkvBNdr+dxDhAiIrrZMHw4iJdOg8f6hdr8vHZv/IZtx89j/aGcRqiKiIjI8Rg+HOi9B7rV63mPLUjCs98mY9vx83auiIiIyPEYPhxs9fMD6/3cN1YdhPFymR2rISIicjyVEELIXUR1JpMJBoMBRqMRer1e7nIahcUi0Ob1tQ3aR48QH/x8A3fQJSIicgRbfr/Z8yEDtVqFP177W4P2sT8zH8dzC+xUERERkeMwfMgkxNcDJz64p0H7uHPWVpwvNAMAjJfLcCjLaI/SiIiIGhXDh4xc1CrE/q1hM5j2eW8jPtlwDD3+9TuGz96G5bsz7VQdERFR47ApfMTFxaFv377w9vZGQEAAHnjgAaSmplq1KSkpQWxsLPz8/ODl5YWYmBjk5ubateibyavR4Tj23rAG7WN2tTvivva/A1i19yzWpeTg9IUi7Dh5AX8cPweLRcB4uQzrUnJQWs65Q4iISD42DTi9++67MXLkSPTt2xfl5eV4/fXXkZKSgsOHD8PT0xMAMGHCBKxZswaLFi2CwWDA888/D7Vaje3bt9/QayhhwGltWk9Z06j7/9f9XfDpxuO4WFSK8be3xZRh4Y36ekREpCy2/H436GqXc+fOISAgAAkJCRg8eDCMRiP8/f2xdOlSPPTQQwCAo0ePolOnTkhMTES/fv3sWvzN5MylyziaXYB//He3Q17v1IzhDnkdIiJSBodd7WI0Vg5w9PX1BQAkJyejrKwMUVFRUpvw8HCEhoYiMTGx1n2YzWaYTCarRYlaNvNAVOdA/PK8Yy6fnbclzSGvQ0REdLV6hw+LxYKJEydiwIAB6Nq1KwAgJycHWq0WPj4+Vm0DAwORk1P79OBxcXEwGAzSEhISUt+SbgrdW/og7f1hWPvioEZ9nZnrUrHj5AVYLE41zQsRESlAvcNHbGwsUlJSsGzZsgYVMHXqVBiNRmnJzOTVGhoXNToH6xs8EPV6Rn61A21eX4vWU9bg1wNZVtsOZ5lsup+MEAInzxXim23piPokATnGEnuXiwoGJSKim4KmPk96/vnn8euvv2Lr1q1o2bKltD4oKAilpaXIz8+36v3Izc1FUFBQrfvS6XTQ6XT1KeOmp9WoEfu3tpi7+USjv9bzS/ci2McdC7efQrDBDV9uPQkA+Dl2AHqE+KC8woKScguiZ23FuMFtMKZ/awghoFKpAADfJWXgzVUp0v4+Wp+Kjx/pYVMNQgicLyyFv3fNz8OXCSfw8e/H0KuVDx7pE4IRvVrWsgeyB3N5BXQaF7nLIKKbmE0DToUQeOGFF7By5Ups2bIF7du3t9peNeD0+++/R0xMDAAgNTUV4eHhHHDaABsP5zpsIGptAvU65JrMNdZ7aF3ww7hIdGtpQGRcPLKr9Xbcf2swPhvZ06bXeXn5fvy45wxG3RaKDx7sKgWb5bsy8dqPB6zacsBs4/h531m8tGwfZj7UHY/0UfYpUCKyTaMNOI2NjcWSJUuwdOlSeHt7IycnBzk5OSguLgYAGAwGjB07FpMnT8bmzZuRnJyMp556CpGRkTcUPKh2UZ0DkR53DwJq6RFwhNqCBwBcLq3AvZ9vw8Wi0hrbsvNLqrUrl8aWnCsw45f9WSgurajxnB/3nAEAfL8zA89/vxf5l0tx4Ex+jeBxIyosAutScpBnsv/pn5vZS8v2AaicL0YJ9mfm42x+sdxlkMIJIbDpaC5yr/q+Kq+wYH9mPsorbr65mWw67TJ//nwAwB133GG1fuHChXjyyScBALNmzYJarUZMTAzMZjOio6Mxb948uxSrZCqVCjvfiIIQAst3Z+KfPx6UuyRJr3c31Fi389RFzN2cho/W/zUJXVSnAGw8kgcAGNk3BI/0DcH0n1OQnV+Cl6Kse9HWHMjGmgPZdb5m9VM+APDqiv04X2jGN0/2hUqlwtKdGZi2KgXeOg0OvhN9w+9lZ/pFBOndcCjLiOKyijpP75RVWPDO6kMY1N4f0V1qP6UIACVlFTAVlyFA7yatO5tfjGCDm1X9N5PfDmYjJcuIV+7q6JD3ePVn4UadPFeI++dWzj/U0J608goLVCoVXNR115F8+iKWJmVi6j3haO7FU81Kln+5FNvTLmBopwC4ubrg1wPZeOH7vVCpgPS4vz6L7605gkV/nsLj/Vrh3Qe6ylix/dnU8yGEqHWpCh4A4Obmhrlz5+LixYsoKirCTz/9VOd4D7KdSqXCo31Dsf+tu+Qu5bqqBw8AUvAAgGW7MjFi3p9IOWvChaJSTP/5kE37DptaOVA2La8QReZyrEg+g82p55B65WZ7XyZUjpMpMJcjLa8Qu05dREFJ2ZW6juLZb3dbXelTaC7Hgm3peOTLRAz+aDMmfLcHk5fvR56pBOUVFiQcOwfTlecLIRC39iiW7MjAs98m11pfwrFzGPLxFoRPW4fbPohH+vkiAMAXCScwYMYmPLlwFz7ZcAxF5nIAlT01h7NMVjXlXnnt6kwlZfgz7TyKSyvw7Le78cOujBs+ZucKzBi7aBfij9Sccbjq/+XqUs4aceJc4Q3t21xegYtFpfg28RQmfLcHczefwObUvOs/8Srzt5zAh+uO3nD72fHHMWDGpnr1cKVk/XVZf9XfT10OnMnHlB8PIK+g5utUWARu/2gLhn68pcbVY8biMmldzPxE/LjnDN5cmVJjH/Yg5w3KKyyi1t5MexFC1Hrsq1u2MwOtp6xpEjfcfGxBEmKX7sHMdZXfkUt2nAYAXP1XuOjPUwCAb69sv56yCov0Oajt/2lnUq8BpyQ/g7srvvtHBOZuTsOfJy7IXY5soj5JsHp896d/QOuiRmm1H+3qbdZPHCwN4O047TeoVKprTjd/3+fb8WCvWzB/S+Vz5ozqiXMFZnyzPV1q87d/b8EXj/VGuwAvnL1UDBcXFcZ8s9NqP/FHchHdJQgzfqv8YU04dg4Jx86hoKQM0//eGfd9vg2Hsky4t0cwhncLwse/H8PxvL9++O/pFoTREa0w+j9JVvtdfygX4UF6vLxiPyZGtUePlj5YuP0Unh7YGi2beWB/Zj6ST1/Ck/1bY9TXO5CWV4j4o3lW/9IXQuD/vk6qcfrs73O2Aai9V6C03IL8y6UI0LvhbH4xBszYVKPNkewCDGznD62m8t84q/dnYW9GPt4c3gnqq3oIzly6jBYGdyl4DGzXHAPaNbfq1dh2/DzctS7o3aqZ9LxPNhwDAHwafxwfPNitRg3XUr2Cez77A0fevVt6bz/sysCCben41/1dMbiDP+77vLKHZNmuyqvxXhjSDi/f1REpZ40YvyRZOnVTVFoObzdXCCGwLe08Hl+wE808XLF3+l//WDh14dpB50YVmctxrsCM1s09YSwuw7BPtyKqcyD+df+1/4VcWm6R/k4AYHvaeZRbBP48cR4D2jbH4A7+13z+0RwTjuUW4r4ewSgyl2N2/HFpgPqGSYMxafk+hDTzwJDwADxsp3FDH65LxRcJJ/DBg93wfxGhtbaZ8lNlb/Cds7ZKn9mSsgr8uOcMbu/gj81H87B6fzY+fqQHnvhmJ+7uGoR/3n3tmZ7nbzkBT50Lnohsfd0aNxzORVhzT7QL8JLWXd0rd/pCESosAilnK4Pvz/vOYvq9nZGUfrHG/uoKDiVlFbj/8+2IbOuHt+/rIq0vMpcjMi4e3VoasGRsBJ74ZifM5Rb8MK4fLhRVnsL+autJxI3ojrDmntd9P42tQTOcNgYOOLXdUwt3YnPqObnLoGuYfGcH6YeyOpUKuKODf6P8/c18qLs0diNuRDdM/emvU3U/TugPnUaNv8/ZhlZ+Hjh94XKd+znxwT1Qqyp73c5cuox3Vh/GhsOVvSeP9QtFYUk5Vu3LqvW5fp5aJE+7E8BftxCYN7oXCkrKoFGrEdO7JX49kIXnl+7FoPbN8cfx87XuZ/XzA3Hv55Vh6Jsn+2BpUga0GjXWHqy8HFytAsYODMPr93SSvuw/3XgMn248jsl3dkCAtw7nC81o6++FCd/tQXMvHd65rwtil+6RXuPUjOE4c+kyXl1xAIkn/wr0r98Tjg/W1uyNee+BrlZXeAHA3P/rhZQsI77bcRqmknJp/YM9b8HKvWelx7veiKr1qq4ql4pKsTk1DxkXL2NiVAcAwH/+OIn31hzB4qdvw+0d/NHnvQ04X1iKNS8OxNZj56Xg9u3Y29CvjR9cXSoDxse/p2LOpjS8MKQdAvVueHNVCh64NRifjuyJInM5ury13uq1t7xyB1pX+3HanJqHInM5TuQV4emBrdHt7d+lbQPa+WF7Wt3/+Fk/cTBc1EAzDy283DTQaVxwodAMX08tVCoVUnMKsOjPU5h8Zwes3HsGapUK/xjUBpdLy7E97QKyjcXw89RZ/T2lvT8M2cYS/C/5DMb0b425m9OwPe08jub81eOxb/qdOHjWiG1p5/Flwsk666sKKcbiMuw+dREZFys/3+smDoKPuxb94uIBVP4/4KJWocIipFNrezMuQa1SoUeID3amX8QjX1ZOpHlHR3/0a+OHqE6BeOTLREy4vS2eGdwGyacvImZ+zck2/9bR+v//N+7phId6t8Rz3+2x+hyemjEcaw5kWx2LJWMjsHB7OsYNboP/bEuX/r/cN/1O3PqvytPh26cMqfGPg8YasO+w6dUbA8OH7YQQ2J52AY8tSLp+Y6J6erh3S6xIPmPz8yLb+GHyXR3w8BeVX7z+3jqcK6gcxPzcHW0xb4t9LyWfP7oXlu/OtDnQzRvdC899t+f6De3k6yf64N1fD+NcgRkvDm2P8be3wdn8Ygz8cLNVu/880QeeOg1Gfb1DWvdqdEer05qP9gnBD9XuaN3MwxWLn74NczenYf2h2m/s2bKZO85cqn2w7ZjIVph8V0fM3ZyGr7b+9eM96rYQfL/zxudieuDWYKtg2ivUB3sy8uGpdcGBt6PR9vW1NZ6T8k40ul4ViKr7593hWLAtHecLzdC7aaxCXhVvNw0Kall/tUVP9UWb5l64b+425F8us9rm56nFhSu9gS8OaYfZmypnhY7uEois/BIcPFs5w/fvkwZj/JJknDxn3aM1sF1zbEurDNOdW+hxOLths3fHv3w7hn6ccP2GV+kZ6oO9GflW6xg+asHwUX8bD+dCAHhGxstyiYjIuR18+y54u7nafb8MHwp3odAMF7UKXjoNNC5qfLbxOGZtrNnlT0REyhTTq6XNE0Fejy2/3xxwehPyu+oyvgl3tEULgxu63KKH1kWNQ1kmTPxhnzzFERGR7H7cc8bu4cMWDB8KoNWo8Ujfv0adtw/0Roive62Dn4iIiBobw4dC9W7li5XP9UeIrweae+lw3+fbcOCMUe6yiIhIARg+FKxn6F/zJfwwLhJpeYXoeoseKpUKB88YsXTnafyyLwtFpRUY0fMW/FTtUkEiIqL64oBTuiFCCGw8kocP1x1F2pXJr1oY3KxuJkdERE2HvS+55YBTsjuVSoU7Owfizs6ByCsoQYB35b1KRn6ViB0nL2L87W3Rt3Uz5JrMGHVbCMKm1rx+n4iICGD4oHqoCh4A8M2TfXHgjBF9W/ta3VTrp+f6Y8/pS9Bp1Fh3KAdfPNZbuq58zYFseGhdsDjxFLZwZlYiIsXhaReSlbG4DAb3ylCy9dg5vL/mCJ7o3wqjI1pJbfJMJcgylqCswgJPrQb3zP4DQOV16j/uqZxx81qzNQLArSE+2JeZ33hvhIioiZHztAvDBzVpR7JNOFdglm6GVV5hwVd/nIS5zIKXhra3uoFZ/uVS7D9jxJoDWdC4qDGsaxBa+3lib2Y+jmSb8PXWkyivdlfSN4d3wtiBYQCA8UuSa0xT/cY9nfD+2iPSY7UKsNTxf1NYc0+kny/CvT2CsXp/7fdBISJyJIaPahg+SE5X34WySlmFBdvTzsPHQ4tvE0/jxaHt0MrPE/sy83EstwDDugbB280V039OwZ6MS5g6rBMiwnyhcVHX8iqVdpy8AG83DboEG3C5tBz9Z2zC+NvbYmTfELi5umDh9lMIb+GNOzr4w1xuwdGcAvx2MBtfbj0JjVqF1PeGYXvaeSzfnYmyCgumDuuEhdvTsTjxNGaM6Ib1h3IQ1TkQP+/NwgcjuqFdgBfOF5rR572NtdajVgEbJ9+OlXvPYs6V+1jciFt83LH2xUEY9tlWZHEAstN5tE8IuocY8MbKlOs3JsUY1L45vh0bYdd9MnwQ3cT2ZeYjpJl7jZlsq1x9y/Tatp/NL0aRuRwBeh18PbQorbDAQ1s5BMxiEThw1giDuyuy84vh56XD5dJyGNxdEerrgfvnbsehLBMi2/jh+3H9auzfYhEoKCnHmz+nYHi3FhgSHoBPNx6TbiD31eO9sS3tPF6+qyN0GjWGz/4DfVv7YkZMd2kf29PO4+ylYjzSN0S6k+iGw7l45r+7MbBdcyz5h/WX5vpDOdh8NA/dW/ogpvct0GlcAFSGyQXb0tEjxAflFQJajRoJx86h+y0GtPH3RBv/ytuf/5l2HiXlFfDUavDoV5U3cLuvRzAul5aj2y0+GDe4DbQatXQjtH3T78S5AjPunLXVqsfrgVuDEerniQ2Hc3G+0IyPHuqObGMJWjZzR0SYH3aduohQXw9oXFSIjPvrTqPhQd44mlOACXe0ham4DN8lZSA8yBs/PBsJg7srLhWVInJGPErKLHj9nnC4ubpg9f4szBlVeYfgn/aeRZdgPX7YlYlutxgwb8sJtGnuiU2v3CG9xi/7s/Di93ulx6NuC8X3OzOkxzG9WmJAOz/cGuKDIVfdwGzdxEHoEOCNNtVuBLdkbAQsQmDXqYvoEOiNsgoLJi/fb/W8XW9E4fSFIjz7bTKeH9IO76w+LG17/8Gu9Q5EC5/qi791DIC5vAKdp69HRV1djldEdwnE2IFt8PuhHHQM8sar/zuAz0beilkbjqHQXI7zhaVW7auf0r3ai0Pb467OgWhhcEPvakF+YlR7JBw7h1a+Hniod0iNG336eLgi4dW/YU78cezNzMfIviFYsuM09lebX8nPU4vOwXpkXLxc652mlz4TgZY+Hhj8UeXNB+eP7oUJV26GmPT6UIz+T5J0NWKV1+7uCHOZBZ/FH7da3xg3l2P4IKJGI4RAaYVF+oG/UZuO5qLQXIH7egTX2F9tvU21KTSXw0vXuOPkS8oq4OqithpAXaW4tAKl5RYYPCrHKVksAmq1CubyCuw5nY/erZpdM/hVdyTbhH8s3o2X7+qAEb1a2vU9XEvl3WBdodWoIYTAoSwT2gV4wc215t+nEAKF5nJpsHhJWQX+OH4ekW396vx7OJtfjJh5f+LxyFaI/Vs7aT8qlQonzhWitNyCsOae0uudLzTDS6eBi1qFHScvYP6WE4gb0Q0eWg20GjX+l3wGd3T0R1t/Lxw4k49Wvp7S8QeAS0WlOJpTgM4t9FiceAoD2vmhuNSCxJPn8WDPlmjt51FnD2T1z95z3yVj7cEchAd5Y93EwdL7BVDrsbkRR7JNmLPpOCbf2RHtArxqbC+vsCCvwIxTF4rw/c5MTP97Z/h7//WPiqM5Jiz+8xReHNoeLQzutb5G8umL0Lq4oFtLA4pLK/D9zgwM7RSAVn6eNdpaLALpF4rgpdMgUO9Wy94ahuGDiIhkY0ugdBamkjL8vPcs7u7awioA0I3jPB9ERCSbphY8AEDv5orHI1vLXYZi3Fj/IBEREZGdMHwQERGRQzF8EBERkUMxfBAREZFDMXwQERGRQzF8EBERkUMxfBAREZFDMXwQERGRQzF8EBERkUMxfBAREZFDMXwQERGRQzF8EBERkUMxfBAREZFDOd1dbYUQACpvzUtERERNQ9XvdtXv+LU4XfgoKCgAAISEhMhcCREREdmqoKAABoPhmm1U4kYiigNZLBZkZWXB29sbKpXKrvs2mUwICQlBZmYm9Hq9Xffd1PHY1I3Hpm48NnXjsakbj03dmvKxEUKgoKAAwcHBUKuvParD6Xo+1Go1WrZs2aivodfrm9xfqqPw2NSNx6ZuPDZ147GpG49N3Zrqsblej0cVDjglIiIih2L4ICIiIodSVPjQ6XR46623oNPp5C7F6fDY1I3Hpm48NnXjsakbj03dlHJsnG7AKREREd3cFNXzQURERPJj+CAiIiKHYvggIiIih2L4ICIiIodSTPiYO3cuWrduDTc3N0RERGDnzp1yl9QgW7duxb333ovg4GCoVCqsWrXKarsQAtOnT0eLFi3g7u6OqKgoHD9+3KrNxYsXMXr0aOj1evj4+GDs2LEoLCy0anPgwAEMGjQIbm5uCAkJwcyZM2vUsmLFCoSHh8PNzQ3dunXD2rVr7f5+bREXF4e+ffvC29sbAQEBeOCBB5CammrVpqSkBLGxsfDz84OXlxdiYmKQm5tr1SYjIwPDhw+Hh4cHAgIC8Oqrr6K8vNyqzZYtW9CrVy/odDq0a9cOixYtqlGPM3325s+fj+7du0sTGEVGRuK3336Ttiv1uNRmxowZUKlUmDhxorROqcfn7bffhkqlslrCw8Ol7Uo9LlXOnj2Lxx57DH5+fnB3d0e3bt2we/duabuSv4/rJBRg2bJlQqvVim+++UYcOnRIPPPMM8LHx0fk5ubKXVq9rV27Vrzxxhvip59+EgDEypUrrbbPmDFDGAwGsWrVKrF//35x3333ibCwMFFcXCy1ufvuu0WPHj3Ejh07xB9//CHatWsnRo0aJW03Go0iMDBQjB49WqSkpIjvv/9euLu7iy+//FJqs337duHi4iJmzpwpDh8+LN58803h6uoqDh482OjHoC7R0dFi4cKFIiUlRezbt0/cc889IjQ0VBQWFkptxo8fL0JCQkR8fLzYvXu36Nevn+jfv7+0vby8XHTt2lVERUWJvXv3irVr14rmzZuLqVOnSm1OnjwpPDw8xOTJk8Xhw4fFnDlzhIuLi1i3bp3Uxtk+e7/88otYs2aNOHbsmEhNTRWvv/66cHV1FSkpKUII5R6Xq+3cuVO0bt1adO/eXbz00kvSeqUen7feekt06dJFZGdnS8u5c+ek7Uo9LkIIcfHiRdGqVSvx5JNPiqSkJHHy5Emxfv16kZaWJrVR8vdxXRQRPm677TYRGxsrPa6oqBDBwcEiLi5Oxqrs5+rwYbFYRFBQkPjoo4+kdfn5+UKn04nvv/9eCCHE4cOHBQCxa9cuqc1vv/0mVCqVOHv2rBBCiHnz5olmzZoJs9kstfnnP/8pOnbsKD1+5JFHxPDhw63qiYiIEM8++6xd32ND5OXlCQAiISFBCFF5LFxdXcWKFSukNkeOHBEARGJiohCiMtyp1WqRk5MjtZk/f77Q6/XS8XjttddEly5drF7r0UcfFdHR0dLjpvDZa9asmfjPf/7D43JFQUGBaN++vdiwYYO4/fbbpfCh5OPz1ltviR49etS6TcnHRYjK78SBAwfWuZ3fx7W76U+7lJaWIjk5GVFRUdI6tVqNqKgoJCYmylhZ40lPT0dOTo7VezYYDIiIiJDec2JiInx8fNCnTx+pTVRUFNRqNZKSkqQ2gwcPhlarldpER0cjNTUVly5dktpUf52qNs50bI1GIwDA19cXAJCcnIyysjKrusPDwxEaGmp1fLp164bAwECpTXR0NEwmEw4dOiS1udZ7d/bPXkVFBZYtW4aioiJERkbyuFwRGxuL4cOH13gPSj8+x48fR3BwMNq0aYPRo0cjIyMDAI/LL7/8gj59+uDhhx9GQEAAevbsia+//lrazu/j2t304eP8+fOoqKiw+tADQGBgIHJycmSqqnFVva9rveecnBwEBARYbddoNPD19bVqU9s+qr9GXW2c5dhaLBZMnDgRAwYMQNeuXQFU1qzVauHj42PV9urjU9/3bjKZUFxc7LSfvYMHD8LLyws6nQ7jx4/HypUr0blzZ8UfFwBYtmwZ9uzZg7i4uBrblHx8IiIisGjRIqxbtw7z589Heno6Bg0ahIKCAkUfFwA4efIk5s+fj/bt22P9+vWYMGECXnzxRSxevBgAv4/r4nR3tSWyp9jYWKSkpGDbtm1yl+I0OnbsiH379sFoNOJ///sfxowZg4SEBLnLkl1mZiZeeuklbNiwAW5ubnKX41SGDRsm/bl79+6IiIhAq1atsHz5cri7u8tYmfwsFgv69OmDDz74AADQs2dPpKSk4IsvvsCYMWNkrs553fQ9H82bN4eLi0uNkde5ubkICgqSqarGVfW+rvWeg4KCkJeXZ7W9vLwcFy9etGpT2z6qv0ZdbZzh2D7//PP49ddfsXnzZrRs2VJaHxQUhNLSUuTn51u1v/r41Pe96/V6uLu7O+1nT6vVol27dujduzfi4uLQo0cPfPbZZ4o/LsnJycjLy0OvXr2g0Wig0WiQkJCA2bNnQ6PRIDAwUNHHpzofHx906NABaWlpiv/ctGjRAp07d7Za16lTJ+m0FL+Pa3fThw+tVovevXsjPj5eWmexWBAfH4/IyEgZK2s8YWFhCAoKsnrPJpMJSUlJ0nuOjIxEfn4+kpOTpTabNm2CxWJBRESE1Gbr1q0oKyuT2mzYsAEdO3ZEs2bNpDbVX6eqjZzHVgiB559/HitXrsSmTZsQFhZmtb13795wdXW1qjs1NRUZGRlWx+fgwYNWXwgbNmyAXq+Xvmiu996bymfPYrHAbDYr/rgMHToUBw8exL59+6SlT58+GD16tPRnJR+f6goLC3HixAm0aNFC8Z+bAQMG1LiU/9ixY2jVqhUAfh/XSe4Rr46wbNkyodPpxKJFi8Thw4fFuHHjhI+Pj9XI66amoKBA7N27V+zdu1cAEJ988onYu3evOH36tBCi8tIuHx8f8fPPP4sDBw6I+++/v9ZLu3r27CmSkpLEtm3bRPv27a0u7crPzxeBgYHi8ccfFykpKWLZsmXCw8OjxqVdGo1G/Pvf/xZHjhwRb731luyXdk2YMEEYDAaxZcsWq0sDL1++LLUZP368CA0NFZs2bRK7d+8WkZGRIjIyUtpedWngXXfdJfbt2yfWrVsn/P39a7008NVXXxVHjhwRc+fOrfXSQGf67E2ZMkUkJCSI9PR0ceDAATFlyhShUqnE77//LoRQ7nGpS/WrXYRQ7vF5+eWXxZYtW0R6errYvn27iIqKEs2bNxd5eXlCCOUeFyEqL8vWaDTi/fffF8ePHxffffed8PDwEEuWLJHaKPn7uC6KCB9CCDFnzhwRGhoqtFqtuO2228SOHTvkLqlBNm/eLADUWMaMGSOEqLy8a9q0aSIwMFDodDoxdOhQkZqaarWPCxcuiFGjRgkvLy+h1+vFU089JQoKCqza7N+/XwwcOFDodDpxyy23iBkzZtSoZfny5aJDhw5Cq9WKLl26iDVr1jTa+74RtR0XAGLhwoVSm+LiYvHcc8+JZs2aCQ8PD/Hggw+K7Oxsq/2cOnVKDBs2TLi7u4vmzZuLl19+WZSVlVm12bx5s7j11luFVqsVbdq0sXqNKs702Xv66adFq1athFarFf7+/mLo0KFS8BBCucelLleHD6Uen0cffVS0aNFCaLVaccstt4hHH33Uah4LpR6XKqtXrxZdu3YVOp1OhIeHi6+++spqu5K/j+uiEkIIefpciIiISIlu+jEfRERE5FwYPoiIiMihGD6IiIjIoRg+iIiIyKEYPoiIiMihGD6IiIjIoRg+iIiIyKEYPoiIiMihGD6IiIjIoRg+iIiIyKEYPoiIiMihGD6IiIjIof4fAC7mgrlzSQEAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lossi[:])\n",
    "#plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c86039a-d9fa-49f4-9ec2-888bd3ee666c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
