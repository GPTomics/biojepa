{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71522d3b-8805-48e6-bb36-1c9a7bd15482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.serialization\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "import biojepa_ac_model as model\n",
    "from biojepa_ac_model import BioJepaConfig\n",
    "from bio_dataloader import TrainingLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe98e32-54b3-4da6-adf9-8b3e76e3bc0b",
   "metadata": {},
   "source": [
    "## BioJEPA Model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f67335c-63ea-4f80-9c10-96b4d3e09c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    device = 'cpu'\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(1337)\n",
    "        device = 'cuda'\n",
    "    # elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    #     device = 'mps'\n",
    "    print(f'using {device}')\n",
    "    return device\n",
    "\n",
    "DEVICE = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71e663c2-c9c8-4c7d-8629-f71602248487",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5ec64e1-0d7a-4dee-b478-b99f901ce9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "n_embd = 256\n",
    "training_file_chunk = 25000\n",
    "n_heads = 4\n",
    "n_layers = 6\n",
    "n_genes = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "025d6281-f460-46eb-89d9-842d1ca6fa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('/home/ubuntu')\n",
    "train_dir = data_dir / 'training'\n",
    "checkpoint_dir = data_dir / 'checkpoint'\n",
    "pert_dir = data_dir / 'pert_embd'\n",
    "pert_embd_path = pert_dir / 'action_embeddings_esm2.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9671701-5cc4-4016-a354-b71238c98947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Action Embedding ...\n",
      "Bank Loaded. Shape: (1087, 320)\n"
     ]
    }
   ],
   "source": [
    "print('Loading Action Embedding ...')\n",
    "pert_embd = np.load(pert_embd_path)\n",
    "print(f'Bank Loaded. Shape: {pert_embd.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fe46b1b-aff7-4355-909f-9f71761f11c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high')\n",
    "config = model.BioJepaConfig(\n",
    "    num_genes = n_genes,\n",
    "    n_layer= n_layers,\n",
    "    heads= n_heads,\n",
    "    embed_dim = n_embd,\n",
    "    n_pre_layer= n_layers\n",
    ")\n",
    "model = model.BioJepa(config, pert_embd=pert_embd).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a609e628-943e-41c2-a4d7-a71b9075a640",
   "metadata": {},
   "source": [
    "**Load Checkpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4aac628e-6ab7-4768-955a-3f99e79f58ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = checkpoint_dir / 'bio_jepa_ckpt_63539_final.pt'\n",
    "with torch.serialization.safe_globals([BioJepaConfig]):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "\n",
    "keys = model.load_state_dict(checkpoint['model'])\n",
    "keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8287a64f-9d0d-4fbe-8c9b-a03c463a1487",
   "metadata": {},
   "source": [
    "**Freeze Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63812672-368f-483a-be2a-8b65cc849a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze_encoders()\n",
    "model.eval()\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c98b4a-5df8-4894-9c56-ae0dbcbc3acd",
   "metadata": {},
   "source": [
    "## Build Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3dec1f3-d9cc-4849-aaa8-6a7d60199432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights_robust(module):\n",
    "    if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            fan_in = module.embedding_dim\n",
    "        else:\n",
    "            fan_in = module.weight.size(1)\n",
    "        std = 1.0 / math.sqrt(fan_in) if fan_in > 0 else 0.02\n",
    "        nn.init.trunc_normal_(module.weight, mean=0.0, std=std, a=-2*std, b=2*std)\n",
    "        if hasattr(module, 'bias') and module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "    elif isinstance(module, nn.LayerNorm):\n",
    "        nn.init.zeros_(module.bias)\n",
    "        nn.init.ones_(module.weight)\n",
    "\n",
    "@dataclass\n",
    "class BenchmarkDecoderConfig:\n",
    "    embed_dim: int = 256\n",
    "    \n",
    "class BenchmarkDecoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.head = nn.Linear(config.embed_dim, 1)\n",
    "\n",
    "        self.apply(init_weights_robust)\n",
    "        \n",
    "    def forward(self, latents):\n",
    "\n",
    "        gene_preds = self.head(latents)        \n",
    "        gene_preds = gene_preds.squeeze(-1)\n",
    "        \n",
    "        return gene_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c316fe59-e4aa-4d12-9db4-2e38781f53bc",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e1d5000-f278-4f9b-9227-9554f31e9a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 3 shards for split train\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "found 1 shards for split val\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n"
     ]
    }
   ],
   "source": [
    "train_loader = TrainingLoader(batch_size=batch_size, split='train', data_dir=train_dir, device=DEVICE)\n",
    "val_loader = TrainingLoader(batch_size=batch_size, split='val', data_dir=train_dir, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9025bf-05d7-4ff8-ae9a-c1b87471c3e5",
   "metadata": {},
   "source": [
    "## Training Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f484f1f2-2933-413d-80c0-f0c970656453",
   "metadata": {},
   "source": [
    "### Training Config/Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad700ee3-e710-43de-be89-4003c922a41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_decoder = 1e-3\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0494e7de-e96d-4989-b47b-9229c050a263",
   "metadata": {},
   "source": [
    "**Initialize Decoder** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74224c15-ff49-41d9-9552-b8ec9ed83dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BenchmarkDecoderConfig(\n",
    "    embed_dim= n_embd\n",
    ")\n",
    "\n",
    "decoder = BenchmarkDecoder(config).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2da30fa-9c44-40be-921c-5184c764f0e5",
   "metadata": {},
   "source": [
    "**Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b197e689-8b98-4cf9-8382-6116bf2ce440",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(decoder.parameters(), lr=lr_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958f11cf-8c9f-49b9-b526-74be69d2126f",
   "metadata": {},
   "source": [
    "**Training Lenght**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "009526cc-7703-497d-87cb-4a50f9c1c1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_total_examples = 101682"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "766a254e-2fd7-4274-91c4-7ceab21d8d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(397, 7940)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch = train_total_examples // batch_size\n",
    "max_steps = epochs * steps_per_epoch\n",
    "steps_per_epoch, max_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cf4803-b42e-4d1b-89db-c9b43d6e3558",
   "metadata": {},
   "source": [
    "**Scheduler**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c683627d-3da6-45e9-9827-06d8586a1f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=lr_decoder, total_steps=max_steps, pct_start=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15c5c7c-b741-486e-9ce2-99c2e253b05d",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82cb876c-e45d-43f7-94aa-2fa7f44b15c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossi = []\n",
    "step = 0\n",
    "total_epoch_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69468a01-365b-42b9-b041-5d6969e5b4ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss: 2.1570\n",
      "Step 0 | Loss: 2.15060 | LR: 4.00e-05\n",
      "Step 25 | Loss: 2.06919 | LR: 5.02e-05\n",
      "Step 50 | Loss: 1.96171 | LR: 7.88e-05\n",
      "Step 75 | Loss: 1.77763 | LR: 1.25e-04\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 1.5893\n",
      "Step 100 | Loss: 1.60539 | LR: 1.86e-04\n",
      "Step 125 | Loss: 1.39697 | LR: 2.60e-04\n",
      "Step 150 | Loss: 1.21853 | LR: 3.45e-04\n",
      "Step 175 | Loss: 1.07882 | LR: 4.37e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "val loss: 0.9383\n",
      "Step 200 | Loss: 0.93448 | LR: 5.31e-04\n",
      "Step 225 | Loss: 0.83324 | LR: 6.26e-04\n",
      "Step 250 | Loss: 0.73743 | LR: 7.16e-04\n",
      "Step 275 | Loss: 0.69897 | LR: 7.98e-04\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.6361\n",
      "Step 300 | Loss: 0.64309 | LR: 8.70e-04\n",
      "Step 325 | Loss: 0.59780 | LR: 9.28e-04\n",
      "Step 350 | Loss: 0.58052 | LR: 9.70e-04\n",
      "Step 375 | Loss: 0.55654 | LR: 9.94e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "=== Step 396 Done. Avg Loss: 1.13246 ===\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "val loss: 0.5500\n",
      "Step 400 | Loss: 0.57351 | LR: 1.00e-03\n",
      "Step 425 | Loss: 0.55314 | LR: 1.00e-03\n",
      "Step 450 | Loss: 0.54843 | LR: 1.00e-03\n",
      "Step 475 | Loss: 0.53334 | LR: 1.00e-03\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5312\n",
      "Step 500 | Loss: 0.53280 | LR: 1.00e-03\n",
      "Step 525 | Loss: 0.53690 | LR: 9.99e-04\n",
      "Step 550 | Loss: 0.52612 | LR: 9.99e-04\n",
      "Step 575 | Loss: 0.52266 | LR: 9.99e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5245\n",
      "Step 600 | Loss: 0.53055 | LR: 9.98e-04\n",
      "Step 625 | Loss: 0.53041 | LR: 9.98e-04\n",
      "Step 650 | Loss: 0.52870 | LR: 9.97e-04\n",
      "Step 675 | Loss: 0.52165 | LR: 9.97e-04\n",
      "val loss: 0.5239\n",
      "Step 700 | Loss: 0.52680 | LR: 9.96e-04\n",
      "Step 725 | Loss: 0.52317 | LR: 9.95e-04\n",
      "Step 750 | Loss: 0.52819 | LR: 9.95e-04\n",
      "Step 775 | Loss: 0.52499 | LR: 9.94e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "=== Step 793 Done. Avg Loss: 0.52985 ===\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5194\n",
      "Step 800 | Loss: 0.52690 | LR: 9.93e-04\n",
      "Step 825 | Loss: 0.51806 | LR: 9.92e-04\n",
      "Step 850 | Loss: 0.52245 | LR: 9.91e-04\n",
      "Step 875 | Loss: 0.52406 | LR: 9.90e-04\n",
      "val loss: 0.5200\n",
      "Step 900 | Loss: 0.51914 | LR: 9.89e-04\n",
      "Step 925 | Loss: 0.51918 | LR: 9.88e-04\n",
      "Step 950 | Loss: 0.51893 | LR: 9.87e-04\n",
      "Step 975 | Loss: 0.51226 | LR: 9.85e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5189\n",
      "Step 1000 | Loss: 0.51197 | LR: 9.84e-04\n",
      "Step 1025 | Loss: 0.52395 | LR: 9.83e-04\n",
      "Step 1050 | Loss: 0.51395 | LR: 9.82e-04\n",
      "Step 1075 | Loss: 0.52679 | LR: 9.80e-04\n",
      "val loss: 0.5191\n",
      "Step 1100 | Loss: 0.53374 | LR: 9.79e-04\n",
      "Step 1125 | Loss: 0.51502 | LR: 9.77e-04\n",
      "Step 1150 | Loss: 0.51559 | LR: 9.75e-04\n",
      "Step 1175 | Loss: 0.51631 | LR: 9.74e-04\n",
      "=== Step 1190 Done. Avg Loss: 0.52142 ===\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5196\n",
      "Step 1200 | Loss: 0.51408 | LR: 9.72e-04\n",
      "Step 1225 | Loss: 0.52296 | LR: 9.70e-04\n",
      "Step 1250 | Loss: 0.52857 | LR: 9.69e-04\n",
      "Step 1275 | Loss: 0.52747 | LR: 9.67e-04\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5187\n",
      "Step 1300 | Loss: 0.52309 | LR: 9.65e-04\n",
      "Step 1325 | Loss: 0.52312 | LR: 9.63e-04\n",
      "Step 1350 | Loss: 0.51538 | LR: 9.61e-04\n",
      "Step 1375 | Loss: 0.52246 | LR: 9.59e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "val loss: 0.5173\n",
      "Step 1400 | Loss: 0.52330 | LR: 9.57e-04\n",
      "Step 1425 | Loss: 0.52726 | LR: 9.55e-04\n",
      "Step 1450 | Loss: 0.53208 | LR: 9.53e-04\n",
      "Step 1475 | Loss: 0.51573 | LR: 9.50e-04\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5177\n",
      "Step 1500 | Loss: 0.50370 | LR: 9.48e-04\n",
      "Step 1525 | Loss: 0.51443 | LR: 9.46e-04\n",
      "Step 1550 | Loss: 0.52034 | LR: 9.43e-04\n",
      "Step 1575 | Loss: 0.53059 | LR: 9.41e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "=== Step 1587 Done. Avg Loss: 0.52019 ===\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "val loss: 0.5195\n",
      "Step 1600 | Loss: 0.52111 | LR: 9.38e-04\n",
      "Step 1625 | Loss: 0.52184 | LR: 9.36e-04\n",
      "Step 1650 | Loss: 0.52475 | LR: 9.33e-04\n",
      "Step 1675 | Loss: 0.51991 | LR: 9.31e-04\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5157\n",
      "Step 1700 | Loss: 0.52457 | LR: 9.28e-04\n",
      "Step 1725 | Loss: 0.51669 | LR: 9.25e-04\n",
      "Step 1750 | Loss: 0.51913 | LR: 9.22e-04\n",
      "Step 1775 | Loss: 0.52416 | LR: 9.20e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5192\n",
      "Step 1800 | Loss: 0.52004 | LR: 9.17e-04\n",
      "Step 1825 | Loss: 0.50698 | LR: 9.14e-04\n",
      "Step 1850 | Loss: 0.52038 | LR: 9.11e-04\n",
      "Step 1875 | Loss: 0.51826 | LR: 9.08e-04\n",
      "val loss: 0.5180\n",
      "Step 1900 | Loss: 0.52550 | LR: 9.05e-04\n",
      "Step 1925 | Loss: 0.52004 | LR: 9.02e-04\n",
      "Step 1950 | Loss: 0.50949 | LR: 8.99e-04\n",
      "Step 1975 | Loss: 0.51308 | LR: 8.96e-04\n",
      "=== Step 1984 Done. Avg Loss: 0.51987 ===\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5170\n",
      "Step 2000 | Loss: 0.51821 | LR: 8.92e-04\n",
      "Step 2025 | Loss: 0.52227 | LR: 8.89e-04\n",
      "Step 2050 | Loss: 0.50799 | LR: 8.86e-04\n",
      "Step 2075 | Loss: 0.51224 | LR: 8.83e-04\n",
      "val loss: 0.5168\n",
      "Step 2100 | Loss: 0.52429 | LR: 8.79e-04\n",
      "Step 2125 | Loss: 0.51898 | LR: 8.76e-04\n",
      "Step 2150 | Loss: 0.51752 | LR: 8.72e-04\n",
      "Step 2175 | Loss: 0.51739 | LR: 8.69e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5192\n",
      "Step 2200 | Loss: 0.51859 | LR: 8.65e-04\n",
      "Step 2225 | Loss: 0.51109 | LR: 8.62e-04\n",
      "Step 2250 | Loss: 0.52302 | LR: 8.58e-04\n",
      "Step 2275 | Loss: 0.52879 | LR: 8.54e-04\n",
      "val loss: 0.5180\n",
      "Step 2300 | Loss: 0.52003 | LR: 8.51e-04\n",
      "Step 2325 | Loss: 0.52465 | LR: 8.47e-04\n",
      "Step 2350 | Loss: 0.51768 | LR: 8.43e-04\n",
      "Step 2375 | Loss: 0.52029 | LR: 8.39e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "=== Step 2381 Done. Avg Loss: 0.51981 ===\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5176\n",
      "Step 2400 | Loss: 0.51087 | LR: 8.36e-04\n",
      "Step 2425 | Loss: 0.52664 | LR: 8.32e-04\n",
      "Step 2450 | Loss: 0.52260 | LR: 8.28e-04\n",
      "Step 2475 | Loss: 0.51668 | LR: 8.24e-04\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5175\n",
      "Step 2500 | Loss: 0.51582 | LR: 8.20e-04\n",
      "Step 2525 | Loss: 0.52347 | LR: 8.16e-04\n",
      "Step 2550 | Loss: 0.51379 | LR: 8.12e-04\n",
      "Step 2575 | Loss: 0.51672 | LR: 8.08e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "val loss: 0.5187\n",
      "Step 2600 | Loss: 0.52322 | LR: 8.04e-04\n",
      "Step 2625 | Loss: 0.51830 | LR: 7.99e-04\n",
      "Step 2650 | Loss: 0.52571 | LR: 7.95e-04\n",
      "Step 2675 | Loss: 0.52386 | LR: 7.91e-04\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5175\n",
      "Step 2700 | Loss: 0.51615 | LR: 7.87e-04\n",
      "Step 2725 | Loss: 0.51427 | LR: 7.82e-04\n",
      "Step 2750 | Loss: 0.50470 | LR: 7.78e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "Step 2775 | Loss: 0.51811 | LR: 7.74e-04\n",
      "=== Step 2778 Done. Avg Loss: 0.51979 ===\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "val loss: 0.5167\n",
      "Step 2800 | Loss: 0.51858 | LR: 7.69e-04\n",
      "Step 2825 | Loss: 0.51998 | LR: 7.65e-04\n",
      "Step 2850 | Loss: 0.51661 | LR: 7.61e-04\n",
      "Step 2875 | Loss: 0.52120 | LR: 7.56e-04\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5185\n",
      "Step 2900 | Loss: 0.52150 | LR: 7.52e-04\n",
      "Step 2925 | Loss: 0.53746 | LR: 7.47e-04\n",
      "Step 2950 | Loss: 0.51520 | LR: 7.43e-04\n",
      "Step 2975 | Loss: 0.51658 | LR: 7.38e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "val loss: 0.5173\n",
      "Step 3000 | Loss: 0.51178 | LR: 7.33e-04\n",
      "Step 3025 | Loss: 0.51625 | LR: 7.29e-04\n",
      "Step 3050 | Loss: 0.51677 | LR: 7.24e-04\n",
      "Step 3075 | Loss: 0.52627 | LR: 7.20e-04\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5184\n",
      "Step 3100 | Loss: 0.51507 | LR: 7.15e-04\n",
      "Step 3125 | Loss: 0.51200 | LR: 7.10e-04\n",
      "Step 3150 | Loss: 0.51919 | LR: 7.05e-04\n",
      "Step 3175 | Loss: 0.52696 | LR: 7.01e-04\n",
      "=== Step 3175 Done. Avg Loss: 0.51974 ===\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5180\n",
      "Step 3200 | Loss: 0.51641 | LR: 6.96e-04\n",
      "Step 3225 | Loss: 0.51860 | LR: 6.91e-04\n",
      "Step 3250 | Loss: 0.53311 | LR: 6.86e-04\n",
      "Step 3275 | Loss: 0.51765 | LR: 6.81e-04\n",
      "val loss: 0.5175\n",
      "Step 3300 | Loss: 0.52152 | LR: 6.77e-04\n",
      "Step 3325 | Loss: 0.52250 | LR: 6.72e-04\n",
      "Step 3350 | Loss: 0.52499 | LR: 6.67e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "Step 3375 | Loss: 0.51573 | LR: 6.62e-04\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5172\n",
      "Step 3400 | Loss: 0.51201 | LR: 6.57e-04\n",
      "Step 3425 | Loss: 0.52117 | LR: 6.52e-04\n",
      "Step 3450 | Loss: 0.52712 | LR: 6.47e-04\n",
      "Step 3475 | Loss: 0.52264 | LR: 6.42e-04\n",
      "val loss: 0.5184\n",
      "Step 3500 | Loss: 0.51381 | LR: 6.37e-04\n",
      "Step 3525 | Loss: 0.52303 | LR: 6.32e-04\n",
      "Step 3550 | Loss: 0.52232 | LR: 6.27e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "=== Step 3572 Done. Avg Loss: 0.51987 ===\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "Step 3575 | Loss: 0.51052 | LR: 6.22e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5186\n",
      "Step 3600 | Loss: 0.52841 | LR: 6.17e-04\n",
      "Step 3625 | Loss: 0.51352 | LR: 6.12e-04\n",
      "Step 3650 | Loss: 0.52034 | LR: 6.07e-04\n",
      "Step 3675 | Loss: 0.51473 | LR: 6.02e-04\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5171\n",
      "Step 3700 | Loss: 0.52131 | LR: 5.97e-04\n",
      "Step 3725 | Loss: 0.52315 | LR: 5.91e-04\n",
      "Step 3750 | Loss: 0.51465 | LR: 5.86e-04\n",
      "Step 3775 | Loss: 0.53759 | LR: 5.81e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "val loss: 0.5169\n",
      "Step 3800 | Loss: 0.52265 | LR: 5.76e-04\n",
      "Step 3825 | Loss: 0.52412 | LR: 5.71e-04\n",
      "Step 3850 | Loss: 0.51963 | LR: 5.66e-04\n",
      "Step 3875 | Loss: 0.51689 | LR: 5.61e-04\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5188\n",
      "Step 3900 | Loss: 0.53003 | LR: 5.55e-04\n",
      "Step 3925 | Loss: 0.51941 | LR: 5.50e-04\n",
      "Step 3950 | Loss: 0.51088 | LR: 5.45e-04\n",
      "=== Step 3969 Done. Avg Loss: 0.51979 ===\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 3975 | Loss: 0.51470 | LR: 5.40e-04\n",
      "val loss: 0.5174\n",
      "Step 4000 | Loss: 0.51970 | LR: 5.35e-04\n",
      "Step 4025 | Loss: 0.51493 | LR: 5.29e-04\n",
      "Step 4050 | Loss: 0.52826 | LR: 5.24e-04\n",
      "Step 4075 | Loss: 0.52568 | LR: 5.19e-04\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5200\n",
      "Step 4100 | Loss: 0.51193 | LR: 5.14e-04\n",
      "Step 4125 | Loss: 0.52713 | LR: 5.09e-04\n",
      "Step 4150 | Loss: 0.51746 | LR: 5.03e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "Step 4175 | Loss: 0.51750 | LR: 4.98e-04\n",
      "val loss: 0.5167\n",
      "Step 4200 | Loss: 0.52998 | LR: 4.93e-04\n",
      "Step 4225 | Loss: 0.52312 | LR: 4.88e-04\n",
      "Step 4250 | Loss: 0.51997 | LR: 4.83e-04\n",
      "Step 4275 | Loss: 0.51445 | LR: 4.77e-04\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5173\n",
      "Step 4300 | Loss: 0.51296 | LR: 4.72e-04\n",
      "Step 4325 | Loss: 0.52443 | LR: 4.67e-04\n",
      "Step 4350 | Loss: 0.52140 | LR: 4.62e-04\n",
      "=== Step 4366 Done. Avg Loss: 0.51981 ===\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 4375 | Loss: 0.52699 | LR: 4.57e-04\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5179\n",
      "Step 4400 | Loss: 0.52201 | LR: 4.51e-04\n",
      "Step 4425 | Loss: 0.52450 | LR: 4.46e-04\n",
      "Step 4450 | Loss: 0.52992 | LR: 4.41e-04\n",
      "Step 4475 | Loss: 0.52990 | LR: 4.36e-04\n",
      "val loss: 0.5170\n",
      "Step 4500 | Loss: 0.52034 | LR: 4.31e-04\n",
      "Step 4525 | Loss: 0.52387 | LR: 4.26e-04\n",
      "Step 4550 | Loss: 0.51251 | LR: 4.20e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "Step 4575 | Loss: 0.52418 | LR: 4.15e-04\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5178\n",
      "Step 4600 | Loss: 0.51797 | LR: 4.10e-04\n",
      "Step 4625 | Loss: 0.52132 | LR: 4.05e-04\n",
      "Step 4650 | Loss: 0.51445 | LR: 4.00e-04\n",
      "Step 4675 | Loss: 0.51489 | LR: 3.95e-04\n",
      "val loss: 0.5177\n",
      "Step 4700 | Loss: 0.51549 | LR: 3.90e-04\n",
      "Step 4725 | Loss: 0.51016 | LR: 3.85e-04\n",
      "Step 4750 | Loss: 0.51381 | LR: 3.80e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "=== Step 4763 Done. Avg Loss: 0.51976 ===\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "Step 4775 | Loss: 0.51167 | LR: 3.75e-04\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5179\n",
      "Step 4800 | Loss: 0.53451 | LR: 3.70e-04\n",
      "Step 4825 | Loss: 0.52148 | LR: 3.65e-04\n",
      "Step 4850 | Loss: 0.52123 | LR: 3.60e-04\n",
      "Step 4875 | Loss: 0.51808 | LR: 3.55e-04\n",
      "val loss: 0.5182\n",
      "Step 4900 | Loss: 0.51836 | LR: 3.50e-04\n",
      "Step 4925 | Loss: 0.52707 | LR: 3.45e-04\n",
      "Step 4950 | Loss: 0.52190 | LR: 3.40e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 4975 | Loss: 0.51955 | LR: 3.35e-04\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5183\n",
      "Step 5000 | Loss: 0.50930 | LR: 3.30e-04\n",
      "Step 5025 | Loss: 0.51810 | LR: 3.25e-04\n",
      "Step 5050 | Loss: 0.52211 | LR: 3.20e-04\n",
      "Step 5075 | Loss: 0.51792 | LR: 3.15e-04\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5166\n",
      "Step 5100 | Loss: 0.51244 | LR: 3.10e-04\n",
      "Step 5125 | Loss: 0.52971 | LR: 3.06e-04\n",
      "Step 5150 | Loss: 0.52361 | LR: 3.01e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "=== Step 5160 Done. Avg Loss: 0.51980 ===\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 5175 | Loss: 0.52308 | LR: 2.96e-04\n",
      "val loss: 0.5175\n",
      "Step 5200 | Loss: 0.53473 | LR: 2.91e-04\n",
      "Step 5225 | Loss: 0.52869 | LR: 2.87e-04\n",
      "Step 5250 | Loss: 0.53524 | LR: 2.82e-04\n",
      "Step 5275 | Loss: 0.52173 | LR: 2.77e-04\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5190\n",
      "Step 5300 | Loss: 0.52415 | LR: 2.73e-04\n",
      "Step 5325 | Loss: 0.51008 | LR: 2.68e-04\n",
      "Step 5350 | Loss: 0.51634 | LR: 2.63e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "Step 5375 | Loss: 0.50522 | LR: 2.59e-04\n",
      "val loss: 0.5164\n",
      "Step 5400 | Loss: 0.52436 | LR: 2.54e-04\n",
      "Step 5425 | Loss: 0.51120 | LR: 2.50e-04\n",
      "Step 5450 | Loss: 0.53483 | LR: 2.45e-04\n",
      "Step 5475 | Loss: 0.52733 | LR: 2.41e-04\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5183\n",
      "Step 5500 | Loss: 0.53192 | LR: 2.36e-04\n",
      "Step 5525 | Loss: 0.52387 | LR: 2.32e-04\n",
      "Step 5550 | Loss: 0.50460 | LR: 2.28e-04\n",
      "=== Step 5557 Done. Avg Loss: 0.51971 ===\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 5575 | Loss: 0.51971 | LR: 2.23e-04\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5179\n",
      "Step 5600 | Loss: 0.52663 | LR: 2.19e-04\n",
      "Step 5625 | Loss: 0.51040 | LR: 2.15e-04\n",
      "Step 5650 | Loss: 0.52762 | LR: 2.10e-04\n",
      "Step 5675 | Loss: 0.52299 | LR: 2.06e-04\n",
      "val loss: 0.5187\n",
      "Step 5700 | Loss: 0.52169 | LR: 2.02e-04\n",
      "Step 5725 | Loss: 0.51866 | LR: 1.98e-04\n",
      "Step 5750 | Loss: 0.51472 | LR: 1.94e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "Step 5775 | Loss: 0.52860 | LR: 1.90e-04\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5173\n",
      "Step 5800 | Loss: 0.51766 | LR: 1.85e-04\n",
      "Step 5825 | Loss: 0.51694 | LR: 1.81e-04\n",
      "Step 5850 | Loss: 0.51498 | LR: 1.77e-04\n",
      "Step 5875 | Loss: 0.51513 | LR: 1.73e-04\n",
      "val loss: 0.5176\n",
      "Step 5900 | Loss: 0.51793 | LR: 1.70e-04\n",
      "Step 5925 | Loss: 0.52456 | LR: 1.66e-04\n",
      "Step 5950 | Loss: 0.51228 | LR: 1.62e-04\n",
      "=== Step 5954 Done. Avg Loss: 0.51978 ===\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "Step 5975 | Loss: 0.52035 | LR: 1.58e-04\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5174\n",
      "Step 6000 | Loss: 0.51221 | LR: 1.54e-04\n",
      "Step 6025 | Loss: 0.52519 | LR: 1.50e-04\n",
      "Step 6050 | Loss: 0.51476 | LR: 1.47e-04\n",
      "Step 6075 | Loss: 0.53090 | LR: 1.43e-04\n",
      "val loss: 0.5177\n",
      "Step 6100 | Loss: 0.52065 | LR: 1.39e-04\n",
      "Step 6125 | Loss: 0.51114 | LR: 1.36e-04\n",
      "Step 6150 | Loss: 0.52411 | LR: 1.32e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 6175 | Loss: 0.52236 | LR: 1.29e-04\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5189\n",
      "Step 6200 | Loss: 0.52196 | LR: 1.25e-04\n",
      "Step 6225 | Loss: 0.51936 | LR: 1.22e-04\n",
      "Step 6250 | Loss: 0.53299 | LR: 1.19e-04\n",
      "Step 6275 | Loss: 0.53172 | LR: 1.15e-04\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5167\n",
      "Step 6300 | Loss: 0.52021 | LR: 1.12e-04\n",
      "Step 6325 | Loss: 0.51662 | LR: 1.09e-04\n",
      "Step 6350 | Loss: 0.51324 | LR: 1.05e-04\n",
      "=== Step 6351 Done. Avg Loss: 0.51975 ===\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 6375 | Loss: 0.51892 | LR: 1.02e-04\n",
      "val loss: 0.5183\n",
      "Step 6400 | Loss: 0.51349 | LR: 9.91e-05\n",
      "Step 6425 | Loss: 0.52367 | LR: 9.60e-05\n",
      "Step 6450 | Loss: 0.52201 | LR: 9.30e-05\n",
      "Step 6475 | Loss: 0.50924 | LR: 9.00e-05\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5168\n",
      "Step 6500 | Loss: 0.52175 | LR: 8.70e-05\n",
      "Step 6525 | Loss: 0.52455 | LR: 8.41e-05\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "Step 6550 | Loss: 0.52138 | LR: 8.12e-05\n",
      "Step 6575 | Loss: 0.52339 | LR: 7.84e-05\n",
      "val loss: 0.5167\n",
      "Step 6600 | Loss: 0.52258 | LR: 7.57e-05\n",
      "Step 6625 | Loss: 0.52091 | LR: 7.29e-05\n",
      "Step 6650 | Loss: 0.52411 | LR: 7.02e-05\n",
      "Step 6675 | Loss: 0.52175 | LR: 6.76e-05\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5188\n",
      "Step 6700 | Loss: 0.51761 | LR: 6.50e-05\n",
      "Step 6725 | Loss: 0.51900 | LR: 6.25e-05\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "=== Step 6748 Done. Avg Loss: 0.51973 ===\n",
      "Step 6750 | Loss: 0.51504 | LR: 6.00e-05\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 6775 | Loss: 0.51777 | LR: 5.75e-05\n",
      "val loss: 0.5181\n",
      "Step 6800 | Loss: 0.52557 | LR: 5.51e-05\n",
      "Step 6825 | Loss: 0.52188 | LR: 5.28e-05\n",
      "Step 6850 | Loss: 0.52208 | LR: 5.05e-05\n",
      "Step 6875 | Loss: 0.51768 | LR: 4.82e-05\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5201\n",
      "Step 6900 | Loss: 0.51782 | LR: 4.60e-05\n",
      "Step 6925 | Loss: 0.51479 | LR: 4.38e-05\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "Step 6950 | Loss: 0.51566 | LR: 4.17e-05\n",
      "Step 6975 | Loss: 0.50395 | LR: 3.97e-05\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5161\n",
      "Step 7000 | Loss: 0.51988 | LR: 3.77e-05\n",
      "Step 7025 | Loss: 0.51698 | LR: 3.57e-05\n",
      "Step 7050 | Loss: 0.51557 | LR: 3.38e-05\n",
      "Step 7075 | Loss: 0.52942 | LR: 3.20e-05\n",
      "val loss: 0.5172\n",
      "Step 7100 | Loss: 0.52331 | LR: 3.01e-05\n",
      "Step 7125 | Loss: 0.52038 | LR: 2.84e-05\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "=== Step 7145 Done. Avg Loss: 0.51974 ===\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "Step 7150 | Loss: 0.52108 | LR: 2.67e-05\n",
      "Step 7175 | Loss: 0.52404 | LR: 2.50e-05\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5175\n",
      "Step 7200 | Loss: 0.51401 | LR: 2.34e-05\n",
      "Step 7225 | Loss: 0.53125 | LR: 2.19e-05\n",
      "Step 7250 | Loss: 0.51651 | LR: 2.04e-05\n",
      "Step 7275 | Loss: 0.51458 | LR: 1.89e-05\n",
      "val loss: 0.5174\n",
      "Step 7300 | Loss: 0.51571 | LR: 1.76e-05\n",
      "Step 7325 | Loss: 0.51197 | LR: 1.62e-05\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "Step 7350 | Loss: 0.51530 | LR: 1.49e-05\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 7375 | Loss: 0.51692 | LR: 1.37e-05\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5173\n",
      "Step 7400 | Loss: 0.52816 | LR: 1.25e-05\n",
      "Step 7425 | Loss: 0.52167 | LR: 1.14e-05\n",
      "Step 7450 | Loss: 0.53344 | LR: 1.03e-05\n",
      "Step 7475 | Loss: 0.50866 | LR: 9.27e-06\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5183\n",
      "Step 7500 | Loss: 0.52399 | LR: 8.30e-06\n",
      "Step 7525 | Loss: 0.51892 | LR: 7.38e-06\n",
      "=== Step 7542 Done. Avg Loss: 0.51965 ===\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "Step 7550 | Loss: 0.52662 | LR: 6.52e-06\n",
      "Step 7575 | Loss: 0.52007 | LR: 5.71e-06\n",
      "val loss: 0.5187\n",
      "Step 7600 | Loss: 0.51811 | LR: 4.95e-06\n",
      "Step 7625 | Loss: 0.52077 | LR: 4.25e-06\n",
      "Step 7650 | Loss: 0.51965 | LR: 3.60e-06\n",
      "Step 7675 | Loss: 0.51802 | LR: 3.00e-06\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5163\n",
      "Step 7700 | Loss: 0.52135 | LR: 2.46e-06\n",
      "Step 7725 | Loss: 0.51616 | LR: 1.97e-06\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 7750 | Loss: 0.52236 | LR: 1.54e-06\n",
      "Step 7775 | Loss: 0.51785 | LR: 1.16e-06\n",
      "val loss: 0.5185\n",
      "Step 7800 | Loss: 0.50537 | LR: 8.30e-07\n",
      "Step 7825 | Loss: 0.52068 | LR: 5.58e-07\n",
      "Step 7850 | Loss: 0.51950 | LR: 3.40e-07\n",
      "Step 7875 | Loss: 0.51607 | LR: 1.76e-07\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5179\n",
      "Step 7900 | Loss: 0.52704 | LR: 6.66e-08\n",
      "Step 7925 | Loss: 0.51649 | LR: 1.13e-08\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "val loss: 0.5173\n",
      "=== Step 7939 Done. Avg Loss: 0.51972 ===\n"
     ]
    }
   ],
   "source": [
    "decoder.train()\n",
    "\n",
    "for step in range(max_steps):\n",
    "\n",
    "    last_step = (step == max_steps - 1)\n",
    "\n",
    "    # once in a while evaluate our validation set loss\n",
    "    if step % 250 == 0 or last_step:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss_accum = 0.0\n",
    "            val_loss_steps = 25\n",
    "            for i in range(val_loss_steps):\n",
    "                cont_x, cont_tot, case_x, case_tot, act_id = val_loader.next_batch()\n",
    "                B, N = cont_x.shape\n",
    "\n",
    "                # run BioJEPA\n",
    "                z_context = model.student(cont_x, cont_tot, mask_idx=None)\n",
    "                target_indices = torch.arange(N, device=DEVICE).expand(B, N)\n",
    "                z_pred_mu, _ = model.predictor(z_context, act_id, target_indices)\n",
    "\n",
    "                # run new decoder\n",
    "                pred_delta = decoder(z_pred_mu) - decoder(z_context)\n",
    "                real_delta = case_x - cont_x\n",
    "\n",
    "                loss = F.mse_loss(pred_delta, real_delta)\n",
    "                val_loss_accum += loss.item()\n",
    "\n",
    "            avg_val_loss = val_loss_accum / val_loss_steps\n",
    "            print(f'val loss: {avg_val_loss:.4f}')\n",
    "\n",
    "        decoder.train()\n",
    "\n",
    "    # periodically save checkpoint\n",
    "    if step > 0 and  (step+1) % steps_per_epoch ==0 and not last_step:\n",
    "        # Save Checkpoint\n",
    "        torch.save({\n",
    "            'model': decoder.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'step': step\n",
    "        }, checkpoint_dir / f'biojepa_decoder_ckpt_{step}.pt')\n",
    "\n",
    "    # actual training\n",
    "\n",
    "    cont_x, cont_tot, case_x, case_tot, act_id = train_loader.next_batch()\n",
    "    B, N = cont_x.shape\n",
    "\n",
    "    # run frozen BioJEPA\n",
    "    with torch.no_grad():\n",
    "        z_context = model.student(cont_x, cont_tot, mask_idx=None)\n",
    "        target_indices = torch.arange(N, device=DEVICE).expand(B, N)\n",
    "        z_pred_mu, _ = model.predictor(z_context, act_id, target_indices)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # run decoder\n",
    "    pred_case = decoder(z_pred_mu)\n",
    "    pred_control = decoder(z_context)\n",
    "\n",
    "    pred_delta = pred_case - pred_control\n",
    "    real_delta = case_x - cont_x\n",
    "\n",
    "    # loss\n",
    "    loss = F.mse_loss(pred_delta, real_delta)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    # loss caching\n",
    "    lossi.append(loss.item())\n",
    "    total_epoch_loss += loss.item()\n",
    "\n",
    "    if step % 50 == 0:\n",
    "        print(f\"Step {step} | Loss: {loss.item():.5f} | LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "    \n",
    "    if step > 0 and (step+1) % steps_per_epoch == 0:   \n",
    "        avg_loss = total_epoch_loss / steps_per_epoch\n",
    "        print(f\"=== Step {step} Done. Avg Loss: {avg_loss:.5f} ===\")\n",
    "        total_epoch_loss = 0\n",
    "    \n",
    "    if last_step:\n",
    "        # Save Checkpoint\n",
    "        torch.save({\n",
    "            'model': decoder.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'step': step\n",
    "        }, checkpoint_dir / f'biojepa_decoder_ckpt_{step}_final.pt')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c924050-b2d8-4973-a3c1-f9dd0a3d1f03",
   "metadata": {},
   "source": [
    "**Training Loss Plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b11fde37-0cac-429d-be0c-351390bac22d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lossi)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8aaa78-e607-4bc4-8cde-49a529984c1b",
   "metadata": {},
   "source": [
    "## Trained Decoder Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "974730ff-1cda-43e0-a72f-df0986bb9daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from scipy.stats import ConstantInputWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94fd5355-31c8-47fc-a2fd-9b1fa9f91576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkDecoder(\n",
       "  (head): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "240d54bf-e521-433d-9c79-541dbebab4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_preds = defaultdict(list)      \n",
    "bulk_reals = defaultdict(list)     \n",
    "bulk_reals_delta = defaultdict(list)\n",
    "\n",
    "val_r2_all = []\n",
    "val_r2_top50 = []\n",
    "val_correlations = []\n",
    "val_mses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "417a9438-c9f1-4cb0-8da2-e9fe7ec831a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 1 shards for split val\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n"
     ]
    }
   ],
   "source": [
    "val_total_examples = 11044\n",
    "val_steps_per_epoch = val_total_examples // batch_size\n",
    "val_loader = TrainingLoader(batch_size=batch_size, split='val', data_dir=train_dir, device=DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27c89be7-508c-4e06-9dfc-9f4809807077",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Benchmarking: 100%|| 43/43 [00:33<00:00,  1.27it/s]\n"
     ]
    }
   ],
   "source": [
    "for step in tqdm(range(val_steps_per_epoch), desc='Benchmarking'):\n",
    "    \n",
    "    cont_x, cont_tot, case_x, case_tot, act_id = val_loader.next_batch()\n",
    "    B, N = cont_x.shape\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        z_context = model.student(cont_x, cont_tot, mask_idx=None)\n",
    "        target_indices = torch.arange(N, device=DEVICE).expand(B, N)\n",
    "        z_pred_mu, _ = model.predictor(z_context, act_id, target_indices)\n",
    "        \n",
    "        pred_delta = decoder(z_pred_mu) - decoder(z_context)\n",
    "        real_delta = case_x - cont_x\n",
    "\n",
    "        pred_absolute = cont_x + pred_delta\n",
    "    \n",
    "    pred_delta_np = pred_delta.cpu().numpy()\n",
    "    real_delta_np = real_delta.cpu().numpy()\n",
    "\n",
    "    pred_abs_np = pred_absolute.cpu().numpy()\n",
    "    real_abs_np = case_x.cpu().numpy()\n",
    "    \n",
    "    act_id_np = act_id.cpu().numpy().flatten()\n",
    "\n",
    "    for i in range(B):\n",
    "        p_delta = pred_delta_np[i]\n",
    "        t_delta = real_delta_np[i]\n",
    "        pid = act_id_np[i] \n",
    "\n",
    "        val_mses.append(np.mean((p_delta - t_delta)**2))\n",
    "\n",
    "        top_20_idx = np.argsort(np.abs(t_delta))[-20:]\n",
    "        p_top = p_delta[top_20_idx]\n",
    "        t_top = t_delta[top_20_idx]\n",
    "        \n",
    "        if np.std(p_top) > 1e-9 and np.std(t_top) > 1e-9:\n",
    "            corr, _ = pearsonr(p_top, t_top)\n",
    "            val_correlations.append(0.0 if np.isnan(corr) else corr)\n",
    "        else:\n",
    "            val_correlations.append(0.0)\n",
    "\n",
    "        bulk_preds[pid].append(pred_abs_np[i])\n",
    "        bulk_reals[pid].append(real_abs_np[i])\n",
    "        bulk_reals_delta[pid].append(real_delta_np[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c99d0783-9717-4fd2-933b-9b1d42a66cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pid in bulk_preds:\n",
    "    p_mean = np.mean(np.stack(bulk_preds[pid]), axis=0)\n",
    "    t_mean = np.mean(np.stack(bulk_reals[pid]), axis=0)\n",
    "    t_mean_delta = np.mean(np.stack(bulk_reals_delta[pid]), axis=0)\n",
    "\n",
    "    if np.std(t_mean) > 1e-9:\n",
    "        val_r2_all.append(r2_score(t_mean, p_mean))\n",
    "    \n",
    "    top_50_idx = np.argsort(np.abs(t_mean_delta))[-50:] \n",
    "    \n",
    "    val_r2_top50.append(r2_score(t_mean[top_50_idx], p_mean[top_50_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c9fa6eb-5426-42e6-bc77-4a796c2eba4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_mean_mse = np.mean(val_mses)\n",
    "val_mean_corr = np.mean(val_correlations)\n",
    "val_mean_r2_all = np.mean(val_r2_all)\n",
    "val_median_r2_all = np.median(val_r2_all)\n",
    "val_mean_r2_top50 = np.mean(val_r2_top50)\n",
    "val_median_r2_top50 = np.median(val_r2_top50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "750afdb9-ba92-4a7a-874d-0cc221525de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global MSE: 0.5177\n",
      "Top-20 Pearson R: 0.9212\n",
      "R^2 (All Genes): Mean: 0.9030, Median: 0.9088\n",
      "R^2 (Top 50 DEGs): Mean: 0.0832, Median: 0.1892\n"
     ]
    }
   ],
   "source": [
    "print(f'Global MSE: {val_mean_mse:.4f}')\n",
    "print(f'Top-20 Pearson R: {val_mean_corr:.4f}')\n",
    "print(f'R^2 (All Genes): Mean: {val_mean_r2_all:.4f}, Median: {val_median_r2_all:.4f}')\n",
    "print(f'R^2 (Top 50 DEGs): Mean: {val_mean_r2_top50:.4f}, Median: {val_median_r2_top50:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6b2741-9067-4cee-b37f-2ee471d3c7e6",
   "metadata": {},
   "source": [
    "## Trained Decoder Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ecd5cb77-d9a4-43d8-ad77-b6d102d00caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_preds = defaultdict(list)\n",
    "bulk_reals = defaultdict(list)\n",
    "bulk_reals_delta = defaultdict(list)\n",
    "\n",
    "test_r2_all = []\n",
    "test_r2_top50 = []\n",
    "test_correlations = []\n",
    "test_mses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "17c58931-04dc-4c75-a84d-23fbf3f7abc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 1 shards for split test\n",
      "loading /home/ubuntu/training/test/shard_k562e_test_0000.npz\n"
     ]
    }
   ],
   "source": [
    "test_total_examples = 38829\n",
    "test_loader = TrainingLoader(batch_size=batch_size, split='test', data_dir=train_dir, device=DEVICE)\n",
    "test_steps_per_epoch = test_total_examples // batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9022a3d0-68f1-4de8-bb53-33e5bd05e212",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Benchmarking:  62%|                  | 93/151 [01:13<00:45,  1.27it/s]/tmp/ipykernel_24068/4001959697.py:37: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  corr, _ = pearsonr(p_top, t_top)\n",
      "Benchmarking: 100%|| 151/151 [01:58<00:00,  1.27it/s]\n"
     ]
    }
   ],
   "source": [
    "for step in tqdm(range(test_steps_per_epoch), desc='Benchmarking'):\n",
    "    \n",
    "    cont_x, cont_tot, case_x, case_tot, act_id = test_loader.next_batch()\n",
    "    B, N = cont_x.shape\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        z_context = model.student(cont_x, cont_tot, mask_idx=None)\n",
    "        target_indices = torch.arange(N, device=DEVICE).expand(B, N)\n",
    "        z_pred_mu, _ = model.predictor(z_context, act_id, target_indices)\n",
    "        \n",
    "        pred_delta = decoder(z_pred_mu) - decoder(z_context)\n",
    "        real_delta = case_x - cont_x\n",
    "\n",
    "        pred_absolute = cont_x + pred_delta\n",
    "    \n",
    "    pred_delta_np = pred_delta.cpu().numpy()\n",
    "    real_delta_np = real_delta.cpu().numpy()\n",
    "\n",
    "    pred_abs_np = pred_absolute.cpu().numpy()\n",
    "    real_abs_np = case_x.cpu().numpy()\n",
    "    \n",
    "    act_id_np = act_id.cpu().numpy().flatten()\n",
    "\n",
    "\n",
    "    for i in range(B):\n",
    "        p_delta = pred_delta_np[i]\n",
    "        t_delta = real_delta_np[i]\n",
    "        pid = act_id_np[i] \n",
    "\n",
    "        test_mses.append(np.mean((p_delta - t_delta)**2))\n",
    "\n",
    "        top_20_idx = np.argsort(np.abs(t_delta))[-20:]\n",
    "        p_top = p_delta[top_20_idx]\n",
    "        t_top = t_delta[top_20_idx]\n",
    "        \n",
    "        if np.std(p_top) > 1e-9 and np.std(t_top) > 1e-9:\n",
    "            corr, _ = pearsonr(p_top, t_top)\n",
    "            test_correlations.append(0.0 if np.isnan(corr) else corr)\n",
    "        else:\n",
    "            test_correlations.append(0.0)\n",
    "\n",
    "        bulk_preds[pid].append(pred_abs_np[i])\n",
    "        bulk_reals[pid].append(real_abs_np[i])\n",
    "        bulk_reals_delta[pid].append(real_delta_np[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa48799e-604f-4100-bb2b-3e1f5ac5e0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pid in bulk_preds:\n",
    "    p_mean = np.mean(np.stack(bulk_preds[pid]), axis=0)\n",
    "    t_mean = np.mean(np.stack(bulk_reals[pid]), axis=0)\n",
    "    t_mean_delta = np.mean(np.stack(bulk_reals_delta[pid]), axis=0)\n",
    "\n",
    "    if np.std(t_mean) > 1e-9:\n",
    "        test_r2_all.append(r2_score(t_mean, p_mean))\n",
    "    \n",
    "    top_50_idx = np.argsort(np.abs(t_mean_delta))[-50:] \n",
    "    \n",
    "    test_r2_top50.append(r2_score(t_mean[top_50_idx], p_mean[top_50_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "efaae9f8-c745-4044-b9cd-9d3cf0e944e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mean_mse = np.mean(test_mses)\n",
    "test_mean_corr = np.mean(test_correlations)\n",
    "test_mean_r2_all = np.mean(test_r2_all)\n",
    "test_median_r2_all = np.median(test_r2_all)\n",
    "test_mean_r2_top50 = np.mean(test_r2_top50)\n",
    "test_median_r2_top50 = np.median(test_r2_top50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "40ee16bc-f9f8-4ab6-bc12-9e72fafe1b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global MSE: 0.5153\n",
      "Top-20 Pearson R: 0.9209\n",
      "R^2 (All Genes) - Mean: 0.9018, Median: 0.9103\n",
      "R^2 (Top 50 DEGs) - Mean: 0.0602, Median: 0.2552\n"
     ]
    }
   ],
   "source": [
    "print(f'Global MSE: {test_mean_mse:.4f}')\n",
    "print(f'Top-20 Pearson R: {test_mean_corr:.4f}')\n",
    "print(f'R^2 (All Genes) - Mean: {test_mean_r2_all:.4f}, Median: {test_median_r2_all:.4f}')\n",
    "print(f'R^2 (Top 50 DEGs) - Mean: {test_mean_r2_top50:.4f}, Median: {test_median_r2_top50:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4370f0b7-232f-4fe4-8777-3d08ab4519be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Predicted Shift magnitude: 0.5362\n"
     ]
    }
   ],
   "source": [
    "diff = np.abs(pred_absolute.cpu().numpy() - cont_x.cpu().numpy()).mean()\n",
    "print(f\"Average Predicted Shift magnitude: {diff:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0803aec4-f2d9-4557-93df-6fbbca94a90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2138 | 1.3062 | 1.6131 |  -0.3069 | -0.19\n",
      "1011 | 1.1062 | 0.8533 |  0.2528 | 0.30\n",
      "3705 | 0.9970 | 0.4258 |  0.5712 | 1.34\n",
      "521 | 0.7017 | 1.0426 |  -0.3410 | -0.33\n",
      "3053 | 1.4356 | 1.6548 |  -0.2191 | -0.13\n",
      "2351 | 1.0921 | 0.6466 |  0.4455 | 0.69\n",
      "4639 | 0.8800 | 1.2989 |  -0.4189 | -0.32\n",
      "272 | 1.1252 | 1.0410 |  0.0842 | 0.08\n",
      "636 | 1.0773 | 0.6310 |  0.4463 | 0.71\n",
      "4546 | 0.5293 | 0.1574 |  0.3720 | 2.36\n",
      "1793 | 0.7566 | 1.1037 |  -0.3471 | -0.31\n",
      "1689 | 0.2826 | 0.9347 |  -0.6520 | -0.70\n",
      "2858 | 0.9409 | 0.6639 |  0.2770 | 0.42\n",
      "371 | 1.1604 | 1.5837 |  -0.4233 | -0.27\n",
      "3694 | 0.6511 | 0.4115 |  0.2395 | 0.58\n",
      "1863 | 0.7790 | 0.5561 |  0.2230 | 0.40\n",
      "1290 | 1.2554 | 1.1750 |  0.0804 | 0.07\n",
      "3809 | 0.2244 | 0.6317 |  -0.4073 | -0.64\n",
      "2592 | 0.9586 | 1.3353 |  -0.3768 | -0.28\n",
      "431 | 1.6719 | 1.2455 |  0.4264 | 0.34\n",
      "1084 | 1.0640 | 0.7276 |  0.3364 | 0.46\n",
      "2991 | 0.8690 | 0.2808 |  0.5882 | 2.09\n",
      "4806 | 0.6816 | 0.3199 |  0.3617 | 1.13\n",
      "1561 | 0.7661 | 0.4743 |  0.2918 | 0.62\n",
      "1700 | 0.5421 | 1.1439 |  -0.6018 | -0.53\n",
      "4785 | 1.1088 | 1.3133 |  -0.2045 | -0.16\n",
      "3896 | 0.6832 | 1.0253 |  -0.3421 | -0.33\n",
      "1484 | 1.5858 | 1.4172 |  0.1685 | 0.12\n",
      "3192 | 1.5254 | 1.3368 |  0.1886 | 0.14\n",
      "3242 | 1.3644 | 0.8995 |  0.4649 | 0.52\n",
      "2366 | 0.5267 | 1.0437 |  -0.5170 | -0.50\n",
      "4024 | 1.8579 | 1.4475 |  0.4103 | 0.28\n",
      "630 | 1.4517 | 1.0162 |  0.4356 | 0.43\n",
      "800 | 1.2990 | 1.2462 |  0.0528 | 0.04\n",
      "3937 | 0.5718 | 0.6772 |  -0.1055 | -0.16\n",
      "1701 | 0.8482 | 0.3398 |  0.5085 | 1.50\n",
      "4377 | 1.8360 | 1.3299 |  0.5062 | 0.38\n",
      "548 | 1.1564 | 0.6189 |  0.5375 | 0.87\n",
      "1196 | 1.3989 | 1.1666 |  0.2323 | 0.20\n",
      "1037 | 0.5343 | 0.7747 |  -0.2404 | -0.31\n",
      "3433 | 0.5896 | 1.0917 |  -0.5020 | -0.46\n",
      "566 | 1.2447 | 0.6952 |  0.5495 | 0.79\n",
      "1354 | 0.6534 | 1.2012 |  -0.5478 | -0.46\n",
      "4698 | 0.7375 | 1.0691 |  -0.3317 | -0.31\n",
      "1927 | 0.8437 | 0.4889 |  0.3549 | 0.73\n",
      "518 | 0.9366 | 0.3882 |  0.5485 | 1.41\n",
      "2827 | 0.8577 | 1.6018 |  -0.7442 | -0.46\n",
      "283 | 0.8384 | 1.5573 |  -0.7188 | -0.46\n",
      "2806 | 1.4178 | 1.8465 |  -0.4286 | -0.23\n",
      "4734 | 0.5707 | 1.2351 |  -0.6644 | -0.54\n"
     ]
    }
   ],
   "source": [
    "for i in top_50_idx:\n",
    "    print(f'{i} | {p_mean[i]:.4f} | {t_mean[i]:.4f} |  {(p_mean[i] - t_mean[i]):.4f} | {((p_mean[i] - t_mean[i])/(t_mean[i])):.2f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f469bcf9-4fc7-4cd3-b3e2-46bd7b7fa06b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
