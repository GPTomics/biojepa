{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71522d3b-8805-48e6-bb36-1c9a7bd15482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.serialization\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "import biojepa_ac_v0_5 as model\n",
    "from bio_dataloader import TrainingLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe98e32-54b3-4da6-adf9-8b3e76e3bc0b",
   "metadata": {},
   "source": [
    "## BioJEPA Model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f67335c-63ea-4f80-9c10-96b4d3e09c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cpu\n"
     ]
    }
   ],
   "source": [
    "random.seed(1337)\n",
    "def get_device():\n",
    "    device = 'cpu'\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(1337)\n",
    "        device = 'cuda'\n",
    "    print(f'using {device}')\n",
    "    return device\n",
    "\n",
    "DEVICE = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5ec64e1-0d7a-4dee-b478-b99f901ce9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "n_genes = 5000\n",
    "n_layers = 2\n",
    "n_heads = 2\n",
    "n_embd = 8\n",
    "pert_latent_dim = 320\n",
    "pert_mode_dim = 64\n",
    "\n",
    "training_file_chunk = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "025d6281-f460-46eb-89d9-842d1ca6fa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('/Users/djemec/data/jepa/v0_4')\n",
    "train_dir = data_dir / 'training'\n",
    "checkpoint_dir = Path('/Users/djemec/data/jepa/v0_5') / 'checkpoints'\n",
    "pert_dir = data_dir / 'pert_embd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9671701-5cc4-4016-a354-b71238c98947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Banks Loaded. Input(DNA): torch.Size([1250, 1536]), Anchor(Prot): torch.Size([1087, 320])\n"
     ]
    }
   ],
   "source": [
    "input_bank = torch.from_numpy(np.load(pert_dir / 'input_embeddings_dna.npy')).float().to(DEVICE)\n",
    "anchor_bank = torch.from_numpy(np.load(pert_dir / 'action_embeddings_esm2.npy')).float().to(DEVICE)\n",
    "print(f'Banks Loaded. Input(DNA): {input_bank.shape}, Anchor(Prot): {anchor_bank.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fe46b1b-aff7-4355-909f-9f71761f11c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high')\n",
    "config = model.BioJepaConfig(\n",
    "    num_genes = n_genes,\n",
    "    n_layer= n_layers,\n",
    "    heads= n_heads,\n",
    "    embed_dim = n_embd,\n",
    "    n_pre_layer= n_layers,\n",
    "    pert_latent_dim=pert_latent_dim,\n",
    "    pert_mode_dim=pert_mode_dim\n",
    ")\n",
    "model = model.BioJepa(config).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a609e628-943e-41c2-a4d7-a71b9075a640",
   "metadata": {},
   "source": [
    "**Load Checkpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4aac628e-6ab7-4768-955a-3f99e79f58ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = checkpoint_dir / 'bio_jepa_ckpt_31769_final.pt'\n",
    "checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "\n",
    "keys = model.load_state_dict(checkpoint['model'])\n",
    "keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8287a64f-9d0d-4fbe-8c9b-a03c463a1487",
   "metadata": {},
   "source": [
    "**Freeze Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63812672-368f-483a-be2a-8b65cc849a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze_encoders()\n",
    "model.eval()\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c98b4a-5df8-4894-9c56-ae0dbcbc3acd",
   "metadata": {},
   "source": [
    "## Build Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3dec1f3-d9cc-4849-aaa8-6a7d60199432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights_robust(module):\n",
    "    if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            fan_in = module.embedding_dim\n",
    "        else:\n",
    "            fan_in = module.weight.size(1)\n",
    "        std = 1.0 / math.sqrt(fan_in) if fan_in > 0 else 0.02\n",
    "        nn.init.trunc_normal_(module.weight, mean=0.0, std=std, a=-2*std, b=2*std)\n",
    "        if hasattr(module, 'bias') and module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "    elif isinstance(module, nn.LayerNorm):\n",
    "        nn.init.zeros_(module.bias)\n",
    "        nn.init.ones_(module.weight)\n",
    "\n",
    "@dataclass\n",
    "class BenchmarkDecoderConfig:\n",
    "    embed_dim: int = 256\n",
    "    \n",
    "class BenchmarkDecoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.head = nn.Linear(config.embed_dim, 1)\n",
    "\n",
    "        self.apply(init_weights_robust)\n",
    "        \n",
    "    def forward(self, latents):\n",
    "\n",
    "        gene_preds = self.head(latents)        \n",
    "        gene_preds = gene_preds.squeeze(-1)\n",
    "        \n",
    "        return gene_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c316fe59-e4aa-4d12-9db4-2e38781f53bc",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e1d5000-f278-4f9b-9227-9554f31e9a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 11 shards for split train\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0000.npz\n",
      "found 2 shards for split val\n",
      "loading /Users/djemec/data/jepa/v0_4/training/val/shard_k562e_val_0001.npz\n"
     ]
    }
   ],
   "source": [
    "train_loader = TrainingLoader(batch_size=batch_size, split='train', data_dir=train_dir, device=DEVICE)\n",
    "val_loader = TrainingLoader(batch_size=batch_size, split='val', data_dir=train_dir, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9025bf-05d7-4ff8-ae9a-c1b87471c3e5",
   "metadata": {},
   "source": [
    "## Training Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f484f1f2-2933-413d-80c0-f0c970656453",
   "metadata": {},
   "source": [
    "### Training Config/Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad700ee3-e710-43de-be89-4003c922a41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_decoder = 1e-3\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0494e7de-e96d-4989-b47b-9229c050a263",
   "metadata": {},
   "source": [
    "**Initialize Decoder** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74224c15-ff49-41d9-9552-b8ec9ed83dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BenchmarkDecoderConfig(\n",
    "    embed_dim= n_embd\n",
    ")\n",
    "\n",
    "decoder = BenchmarkDecoder(config).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d5141e-064c-4ec0-9584-ef4a87237125",
   "metadata": {},
   "source": [
    "**Load Checkpoint (only Sometimes)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37d3b286-5a21-4bff-aa8e-cd1a5135df08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder_checkpoint_path = checkpoint_dir / 'biojepa_decoder_ckpt_15884_final.pt'\n",
    "# decode_checkpoint = torch.load(decoder_checkpoint_path, map_location=DEVICE)\n",
    "\n",
    "# d_keys = decoder.load_state_dict(decode_checkpoint['model'])\n",
    "# d_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2da30fa-9c44-40be-921c-5184c764f0e5",
   "metadata": {},
   "source": [
    "**Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b197e689-8b98-4cf9-8382-6116bf2ce440",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(decoder.parameters(), lr=lr_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958f11cf-8c9f-49b9-b526-74be69d2126f",
   "metadata": {},
   "source": [
    "**Training Lenght**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "009526cc-7703-497d-87cb-4a50f9c1c1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_total_examples = 101682"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "766a254e-2fd7-4274-91c4-7ceab21d8d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3177, 15885)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch = train_total_examples // batch_size\n",
    "max_steps = epochs * steps_per_epoch\n",
    "steps_per_epoch, max_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cf4803-b42e-4d1b-89db-c9b43d6e3558",
   "metadata": {},
   "source": [
    "**Scheduler**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c683627d-3da6-45e9-9827-06d8586a1f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=lr_decoder, total_steps=max_steps, pct_start=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15c5c7c-b741-486e-9ce2-99c2e253b05d",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82cb876c-e45d-43f7-94aa-2fa7f44b15c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossi = []\n",
    "step = 0\n",
    "total_epoch_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69468a01-365b-42b9-b041-5d6969e5b4ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss: 2.7859\n",
      "Step 0 | Loss: 2.78058 | LR: 4.00e-05\n",
      "Step 25 | Loss: 2.64592 | LR: 4.25e-05\n",
      "Step 50 | Loss: 2.63653 | LR: 4.98e-05\n",
      "Step 75 | Loss: 2.69555 | LR: 6.16e-05\n",
      "val loss: 2.6665\n",
      "Step 100 | Loss: 2.54252 | LR: 7.79e-05\n",
      "Step 125 | Loss: 2.75719 | LR: 9.85e-05\n",
      "Step 150 | Loss: 2.65918 | LR: 1.23e-04\n",
      "Step 175 | Loss: 2.56672 | LR: 1.52e-04\n",
      "val loss: 2.5541\n",
      "Step 200 | Loss: 2.56508 | LR: 1.84e-04\n",
      "Step 225 | Loss: 2.44247 | LR: 2.20e-04\n",
      "Step 250 | Loss: 2.51545 | LR: 2.58e-04\n",
      "Step 275 | Loss: 2.36609 | LR: 2.99e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 2.3034\n",
      "Step 300 | Loss: 2.19376 | LR: 3.43e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0004.npz\n",
      "Step 325 | Loss: 2.16049 | LR: 3.87e-04\n",
      "Step 350 | Loss: 2.06317 | LR: 4.34e-04\n",
      "Step 375 | Loss: 2.04509 | LR: 4.81e-04\n",
      "val loss: 1.9484\n",
      "Step 400 | Loss: 1.97807 | LR: 5.28e-04\n",
      "Step 425 | Loss: 1.75385 | LR: 5.76e-04\n",
      "Step 450 | Loss: 1.85715 | LR: 6.23e-04\n",
      "Step 475 | Loss: 1.65716 | LR: 6.68e-04\n",
      "val loss: 1.5736\n",
      "Step 500 | Loss: 1.54220 | LR: 7.13e-04\n",
      "Step 525 | Loss: 1.52389 | LR: 7.55e-04\n",
      "Step 550 | Loss: 1.39564 | LR: 7.96e-04\n",
      "Step 575 | Loss: 1.40599 | LR: 8.33e-04\n",
      "val loss: 1.2845\n",
      "Step 600 | Loss: 1.30008 | LR: 8.67e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0001.npz\n",
      "Step 625 | Loss: 1.23328 | LR: 8.98e-04\n",
      "Step 650 | Loss: 1.22177 | LR: 9.26e-04\n",
      "Step 675 | Loss: 1.09083 | LR: 9.49e-04\n",
      "val loss: 1.0924\n",
      "Step 700 | Loss: 1.06999 | LR: 9.68e-04\n",
      "Step 725 | Loss: 1.05765 | LR: 9.83e-04\n",
      "Step 750 | Loss: 1.04763 | LR: 9.93e-04\n",
      "Step 775 | Loss: 1.01731 | LR: 9.99e-04\n",
      "val loss: 0.9516\n",
      "Step 800 | Loss: 0.98502 | LR: 1.00e-03\n",
      "Step 825 | Loss: 0.94527 | LR: 1.00e-03\n",
      "Step 850 | Loss: 0.87827 | LR: 1.00e-03\n",
      "Step 875 | Loss: 0.88697 | LR: 1.00e-03\n",
      "val loss: 0.8633\n",
      "Step 900 | Loss: 0.86954 | LR: 1.00e-03\n",
      "Step 925 | Loss: 0.84801 | LR: 1.00e-03\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0010.npz\n",
      "Step 950 | Loss: 0.80816 | LR: 1.00e-03\n",
      "Step 975 | Loss: 0.79645 | LR: 1.00e-03\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0003.npz\n",
      "val loss: 0.8085\n",
      "Step 1000 | Loss: 0.83509 | LR: 1.00e-03\n",
      "Step 1025 | Loss: 0.78150 | LR: 9.99e-04\n",
      "Step 1050 | Loss: 0.73664 | LR: 9.99e-04\n",
      "Step 1075 | Loss: 0.79154 | LR: 9.99e-04\n",
      "val loss: 0.7587\n",
      "Step 1100 | Loss: 0.76439 | LR: 9.99e-04\n",
      "Step 1125 | Loss: 0.77992 | LR: 9.99e-04\n",
      "Step 1150 | Loss: 0.74334 | LR: 9.99e-04\n",
      "Step 1175 | Loss: 0.72845 | LR: 9.98e-04\n",
      "val loss: 0.7361\n",
      "Step 1200 | Loss: 0.74927 | LR: 9.98e-04\n",
      "Step 1225 | Loss: 0.73625 | LR: 9.98e-04\n",
      "Step 1250 | Loss: 0.71838 | LR: 9.98e-04\n",
      "Step 1275 | Loss: 0.76457 | LR: 9.97e-04\n",
      "val loss: 0.7248\n",
      "Step 1300 | Loss: 0.69458 | LR: 9.97e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0007.npz\n",
      "Step 1325 | Loss: 0.71896 | LR: 9.97e-04\n",
      "Step 1350 | Loss: 0.76191 | LR: 9.97e-04\n",
      "Step 1375 | Loss: 0.69225 | LR: 9.96e-04\n",
      "val loss: 0.7045\n",
      "Step 1400 | Loss: 0.64676 | LR: 9.96e-04\n",
      "Step 1425 | Loss: 0.70634 | LR: 9.96e-04\n",
      "Step 1450 | Loss: 0.71207 | LR: 9.95e-04\n",
      "Step 1475 | Loss: 0.68149 | LR: 9.95e-04\n",
      "val loss: 0.6894\n",
      "Step 1500 | Loss: 0.68846 | LR: 9.95e-04\n",
      "Step 1525 | Loss: 0.70180 | LR: 9.94e-04\n",
      "Step 1550 | Loss: 0.63813 | LR: 9.94e-04\n",
      "Step 1575 | Loss: 0.67112 | LR: 9.93e-04\n",
      "val loss: 0.6723\n",
      "Step 1600 | Loss: 0.64091 | LR: 9.93e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0006.npz\n",
      "Step 1625 | Loss: 0.68042 | LR: 9.93e-04\n",
      "Step 1650 | Loss: 0.69715 | LR: 9.92e-04\n",
      "Step 1675 | Loss: 0.66232 | LR: 9.92e-04\n",
      "val loss: 0.6731\n",
      "Step 1700 | Loss: 0.67781 | LR: 9.91e-04\n",
      "Step 1725 | Loss: 0.69635 | LR: 9.91e-04\n",
      "Step 1750 | Loss: 0.64453 | LR: 9.90e-04\n",
      "Step 1775 | Loss: 0.68740 | LR: 9.90e-04\n",
      "val loss: 0.6501\n",
      "Step 1800 | Loss: 0.67846 | LR: 9.89e-04\n",
      "Step 1825 | Loss: 0.64745 | LR: 9.88e-04\n",
      "Step 1850 | Loss: 0.67436 | LR: 9.88e-04\n",
      "Step 1875 | Loss: 0.59732 | LR: 9.87e-04\n",
      "val loss: 0.6611\n",
      "Step 1900 | Loss: 0.63717 | LR: 9.87e-04\n",
      "Step 1925 | Loss: 0.63094 | LR: 9.86e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0002.npz\n",
      "Step 1950 | Loss: 0.58404 | LR: 9.86e-04\n",
      "Step 1975 | Loss: 0.66319 | LR: 9.85e-04\n",
      "val loss: 0.6385\n",
      "Step 2000 | Loss: 0.65013 | LR: 9.84e-04\n",
      "Step 2025 | Loss: 0.70432 | LR: 9.84e-04\n",
      "Step 2050 | Loss: 0.63497 | LR: 9.83e-04\n",
      "Step 2075 | Loss: 0.59217 | LR: 9.82e-04\n",
      "val loss: 0.6394\n",
      "Step 2100 | Loss: 0.66689 | LR: 9.82e-04\n",
      "Step 2125 | Loss: 0.64832 | LR: 9.81e-04\n",
      "Step 2150 | Loss: 0.63541 | LR: 9.80e-04\n",
      "Step 2175 | Loss: 0.64078 | LR: 9.79e-04\n",
      "val loss: 0.6235\n",
      "Step 2200 | Loss: 0.65778 | LR: 9.79e-04\n",
      "Step 2225 | Loss: 0.64946 | LR: 9.78e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0005.npz\n",
      "Step 2250 | Loss: 0.64501 | LR: 9.77e-04\n",
      "Step 2275 | Loss: 0.61639 | LR: 9.76e-04\n",
      "val loss: 0.6497\n",
      "Step 2300 | Loss: 0.64007 | LR: 9.76e-04\n",
      "Step 2325 | Loss: 0.64286 | LR: 9.75e-04\n",
      "Step 2350 | Loss: 0.62713 | LR: 9.74e-04\n",
      "Step 2375 | Loss: 0.62846 | LR: 9.73e-04\n",
      "val loss: 0.6233\n",
      "Step 2400 | Loss: 0.65684 | LR: 9.72e-04\n",
      "Step 2425 | Loss: 0.68945 | LR: 9.71e-04\n",
      "Step 2450 | Loss: 0.67103 | LR: 9.71e-04\n",
      "Step 2475 | Loss: 0.62371 | LR: 9.70e-04\n",
      "val loss: 0.6357\n",
      "Step 2500 | Loss: 0.65620 | LR: 9.69e-04\n",
      "Step 2525 | Loss: 0.61605 | LR: 9.68e-04\n",
      "Step 2550 | Loss: 0.66684 | LR: 9.67e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0008.npz\n",
      "Step 2575 | Loss: 0.58448 | LR: 9.66e-04\n",
      "val loss: 0.6367\n",
      "Step 2600 | Loss: 0.62416 | LR: 9.65e-04\n",
      "Step 2625 | Loss: 0.66308 | LR: 9.64e-04\n",
      "Step 2650 | Loss: 0.63169 | LR: 9.63e-04\n",
      "Step 2675 | Loss: 0.62635 | LR: 9.62e-04\n",
      "val loss: 0.6370\n",
      "Step 2700 | Loss: 0.64869 | LR: 9.61e-04\n",
      "Step 2725 | Loss: 0.62609 | LR: 9.60e-04\n",
      "Step 2750 | Loss: 0.65102 | LR: 9.59e-04\n",
      "Step 2775 | Loss: 0.63391 | LR: 9.58e-04\n",
      "val loss: 0.6348\n",
      "Step 2800 | Loss: 0.64084 | LR: 9.57e-04\n",
      "Step 2825 | Loss: 0.64164 | LR: 9.56e-04\n",
      "Step 2850 | Loss: 0.66445 | LR: 9.55e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0009.npz\n",
      "Step 2875 | Loss: 0.67140 | LR: 9.54e-04\n",
      "val loss: 0.6370\n",
      "Step 2900 | Loss: 0.64598 | LR: 9.53e-04\n",
      "Step 2925 | Loss: 0.67649 | LR: 9.52e-04\n",
      "Step 2950 | Loss: 0.68267 | LR: 9.50e-04\n",
      "Step 2975 | Loss: 0.69738 | LR: 9.49e-04\n",
      "val loss: 0.6303\n",
      "Step 3000 | Loss: 0.63990 | LR: 9.48e-04\n",
      "Step 3025 | Loss: 0.61570 | LR: 9.47e-04\n",
      "Step 3050 | Loss: 0.65034 | LR: 9.46e-04\n",
      "Step 3075 | Loss: 0.64200 | LR: 9.45e-04\n",
      "val loss: 0.6258\n",
      "Step 3100 | Loss: 0.61012 | LR: 9.43e-04\n",
      "Step 3125 | Loss: 0.60769 | LR: 9.42e-04\n",
      "Step 3150 | Loss: 0.64632 | LR: 9.41e-04\n",
      "Step 3175 | Loss: 0.63900 | LR: 9.40e-04\n",
      "=== Step 3176 Done. Avg Loss: 0.99739 ===\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0004.npz\n",
      "val loss: 0.6322\n",
      "Step 3200 | Loss: 0.63570 | LR: 9.38e-04\n",
      "Step 3225 | Loss: 0.62136 | LR: 9.37e-04\n",
      "Step 3250 | Loss: 0.65052 | LR: 9.36e-04\n",
      "Step 3275 | Loss: 0.61293 | LR: 9.35e-04\n",
      "val loss: 0.6267\n",
      "Step 3300 | Loss: 0.61605 | LR: 9.33e-04\n",
      "Step 3325 | Loss: 0.64092 | LR: 9.32e-04\n",
      "Step 3350 | Loss: 0.67544 | LR: 9.31e-04\n",
      "Step 3375 | Loss: 0.65089 | LR: 9.29e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.6315\n",
      "Step 3400 | Loss: 0.61168 | LR: 9.28e-04\n",
      "Step 3425 | Loss: 0.61668 | LR: 9.27e-04\n",
      "Step 3450 | Loss: 0.62435 | LR: 9.25e-04\n",
      "Step 3475 | Loss: 0.65226 | LR: 9.24e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0008.npz\n",
      "val loss: 0.6391\n",
      "Step 3500 | Loss: 0.59021 | LR: 9.23e-04\n",
      "Step 3525 | Loss: 0.63736 | LR: 9.21e-04\n",
      "Step 3550 | Loss: 0.62081 | LR: 9.20e-04\n",
      "Step 3575 | Loss: 0.63434 | LR: 9.18e-04\n",
      "val loss: 0.6263\n",
      "Step 3600 | Loss: 0.64244 | LR: 9.17e-04\n",
      "Step 3625 | Loss: 0.61544 | LR: 9.16e-04\n",
      "Step 3650 | Loss: 0.67174 | LR: 9.14e-04\n",
      "Step 3675 | Loss: 0.62275 | LR: 9.13e-04\n",
      "val loss: 0.6325\n",
      "Step 3700 | Loss: 0.67007 | LR: 9.11e-04\n",
      "Step 3725 | Loss: 0.66271 | LR: 9.10e-04\n",
      "Step 3750 | Loss: 0.67093 | LR: 9.08e-04\n",
      "Step 3775 | Loss: 0.65586 | LR: 9.07e-04\n",
      "val loss: 0.6255\n",
      "Step 3800 | Loss: 0.65711 | LR: 9.05e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0002.npz\n",
      "Step 3825 | Loss: 0.66335 | LR: 9.04e-04\n",
      "Step 3850 | Loss: 0.66206 | LR: 9.02e-04\n",
      "Step 3875 | Loss: 0.67146 | LR: 9.01e-04\n",
      "val loss: 0.6363\n",
      "Step 3900 | Loss: 0.63105 | LR: 8.99e-04\n",
      "Step 3925 | Loss: 0.64173 | LR: 8.97e-04\n",
      "Step 3950 | Loss: 0.64059 | LR: 8.96e-04\n",
      "Step 3975 | Loss: 0.63531 | LR: 8.94e-04\n",
      "val loss: 0.6221\n",
      "Step 4000 | Loss: 0.64103 | LR: 8.93e-04\n",
      "Step 4025 | Loss: 0.68200 | LR: 8.91e-04\n",
      "Step 4050 | Loss: 0.62225 | LR: 8.89e-04\n",
      "Step 4075 | Loss: 0.62539 | LR: 8.88e-04\n",
      "val loss: 0.6369\n",
      "Step 4100 | Loss: 0.60175 | LR: 8.86e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0000.npz\n",
      "Step 4125 | Loss: 0.63975 | LR: 8.84e-04\n",
      "Step 4150 | Loss: 0.64288 | LR: 8.83e-04\n",
      "Step 4175 | Loss: 0.63981 | LR: 8.81e-04\n",
      "val loss: 0.6441\n",
      "Step 4200 | Loss: 0.62735 | LR: 8.79e-04\n",
      "Step 4225 | Loss: 0.69470 | LR: 8.78e-04\n",
      "Step 4250 | Loss: 0.61952 | LR: 8.76e-04\n",
      "Step 4275 | Loss: 0.60516 | LR: 8.74e-04\n",
      "val loss: 0.6310\n",
      "Step 4300 | Loss: 0.66897 | LR: 8.73e-04\n",
      "Step 4325 | Loss: 0.62882 | LR: 8.71e-04\n",
      "Step 4350 | Loss: 0.66366 | LR: 8.69e-04\n",
      "Step 4375 | Loss: 0.62717 | LR: 8.67e-04\n",
      "val loss: 0.6337\n",
      "Step 4400 | Loss: 0.64894 | LR: 8.65e-04\n",
      "Step 4425 | Loss: 0.68667 | LR: 8.64e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0003.npz\n",
      "Step 4450 | Loss: 0.62508 | LR: 8.62e-04\n",
      "Step 4475 | Loss: 0.60986 | LR: 8.60e-04\n",
      "val loss: 0.6196\n",
      "Step 4500 | Loss: 0.61438 | LR: 8.58e-04\n",
      "Step 4525 | Loss: 0.63188 | LR: 8.56e-04\n",
      "Step 4550 | Loss: 0.65328 | LR: 8.55e-04\n",
      "Step 4575 | Loss: 0.62995 | LR: 8.53e-04\n",
      "val loss: 0.6291\n",
      "Step 4600 | Loss: 0.65982 | LR: 8.51e-04\n",
      "Step 4625 | Loss: 0.63009 | LR: 8.49e-04\n",
      "Step 4650 | Loss: 0.62701 | LR: 8.47e-04\n",
      "Step 4675 | Loss: 0.63702 | LR: 8.45e-04\n",
      "val loss: 0.6316\n",
      "Step 4700 | Loss: 0.64121 | LR: 8.43e-04\n",
      "Step 4725 | Loss: 0.63417 | LR: 8.42e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0009.npz\n",
      "Step 4750 | Loss: 0.65268 | LR: 8.40e-04\n",
      "Step 4775 | Loss: 0.61931 | LR: 8.38e-04\n",
      "val loss: 0.6362\n",
      "Step 4800 | Loss: 0.65924 | LR: 8.36e-04\n",
      "Step 4825 | Loss: 0.60219 | LR: 8.34e-04\n",
      "Step 4850 | Loss: 0.67683 | LR: 8.32e-04\n",
      "Step 4875 | Loss: 0.61835 | LR: 8.30e-04\n",
      "val loss: 0.6317\n",
      "Step 4900 | Loss: 0.61135 | LR: 8.28e-04\n",
      "Step 4925 | Loss: 0.58609 | LR: 8.26e-04\n",
      "Step 4950 | Loss: 0.62546 | LR: 8.24e-04\n",
      "Step 4975 | Loss: 0.64497 | LR: 8.22e-04\n",
      "val loss: 0.6186\n",
      "Step 5000 | Loss: 0.64269 | LR: 8.20e-04\n",
      "Step 5025 | Loss: 0.61464 | LR: 8.18e-04\n",
      "Step 5050 | Loss: 0.59370 | LR: 8.16e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0007.npz\n",
      "Step 5075 | Loss: 0.62457 | LR: 8.14e-04\n",
      "val loss: 0.6237\n",
      "Step 5100 | Loss: 0.64826 | LR: 8.12e-04\n",
      "Step 5125 | Loss: 0.62462 | LR: 8.10e-04\n",
      "Step 5150 | Loss: 0.61724 | LR: 8.08e-04\n",
      "Step 5175 | Loss: 0.65557 | LR: 8.06e-04\n",
      "val loss: 0.6404\n",
      "Step 5200 | Loss: 0.63650 | LR: 8.04e-04\n",
      "Step 5225 | Loss: 0.63335 | LR: 8.02e-04\n",
      "Step 5250 | Loss: 0.62031 | LR: 8.00e-04\n",
      "Step 5275 | Loss: 0.60859 | LR: 7.98e-04\n",
      "val loss: 0.6403\n",
      "Step 5300 | Loss: 0.64432 | LR: 7.96e-04\n",
      "Step 5325 | Loss: 0.68521 | LR: 7.93e-04\n",
      "Step 5350 | Loss: 0.64417 | LR: 7.91e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0006.npz\n",
      "Step 5375 | Loss: 0.65452 | LR: 7.89e-04\n",
      "val loss: 0.6341\n",
      "Step 5400 | Loss: 0.61643 | LR: 7.87e-04\n",
      "Step 5425 | Loss: 0.62628 | LR: 7.85e-04\n",
      "Step 5450 | Loss: 0.62837 | LR: 7.83e-04\n",
      "Step 5475 | Loss: 0.64775 | LR: 7.81e-04\n",
      "val loss: 0.6299\n",
      "Step 5500 | Loss: 0.63624 | LR: 7.78e-04\n",
      "Step 5525 | Loss: 0.59824 | LR: 7.76e-04\n",
      "Step 5550 | Loss: 0.66220 | LR: 7.74e-04\n",
      "Step 5575 | Loss: 0.62039 | LR: 7.72e-04\n",
      "val loss: 0.6365\n",
      "Step 5600 | Loss: 0.64615 | LR: 7.70e-04\n",
      "Step 5625 | Loss: 0.59936 | LR: 7.68e-04\n",
      "Step 5650 | Loss: 0.60415 | LR: 7.65e-04\n",
      "Step 5675 | Loss: 0.65940 | LR: 7.63e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0001.npz\n",
      "val loss: 0.6223\n",
      "Step 5700 | Loss: 0.65541 | LR: 7.61e-04\n",
      "Step 5725 | Loss: 0.65678 | LR: 7.59e-04\n",
      "Step 5750 | Loss: 0.61083 | LR: 7.57e-04\n",
      "Step 5775 | Loss: 0.60508 | LR: 7.54e-04\n",
      "val loss: 0.6325\n",
      "Step 5800 | Loss: 0.64359 | LR: 7.52e-04\n",
      "Step 5825 | Loss: 0.60403 | LR: 7.50e-04\n",
      "Step 5850 | Loss: 0.65774 | LR: 7.48e-04\n",
      "Step 5875 | Loss: 0.60265 | LR: 7.45e-04\n",
      "val loss: 0.6413\n",
      "Step 5900 | Loss: 0.60780 | LR: 7.43e-04\n",
      "Step 5925 | Loss: 0.64490 | LR: 7.41e-04\n",
      "Step 5950 | Loss: 0.65076 | LR: 7.38e-04\n",
      "Step 5975 | Loss: 0.61965 | LR: 7.36e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0005.npz\n",
      "val loss: 0.6273\n",
      "Step 6000 | Loss: 0.64409 | LR: 7.34e-04\n",
      "Step 6025 | Loss: 0.66453 | LR: 7.32e-04\n",
      "Step 6050 | Loss: 0.67643 | LR: 7.29e-04\n",
      "Step 6075 | Loss: 0.61423 | LR: 7.27e-04\n",
      "val loss: 0.6346\n",
      "Step 6100 | Loss: 0.64975 | LR: 7.25e-04\n",
      "Step 6125 | Loss: 0.60079 | LR: 7.22e-04\n",
      "Step 6150 | Loss: 0.60738 | LR: 7.20e-04\n",
      "Step 6175 | Loss: 0.68210 | LR: 7.18e-04\n",
      "val loss: 0.6434\n",
      "Step 6200 | Loss: 0.65616 | LR: 7.15e-04\n",
      "Step 6225 | Loss: 0.67709 | LR: 7.13e-04\n",
      "Step 6250 | Loss: 0.62394 | LR: 7.11e-04\n",
      "Step 6275 | Loss: 0.63566 | LR: 7.08e-04\n",
      "val loss: 0.6189\n",
      "Step 6300 | Loss: 0.63317 | LR: 7.06e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0010.npz\n",
      "Step 6325 | Loss: 0.63693 | LR: 7.03e-04\n",
      "Step 6350 | Loss: 0.67683 | LR: 7.01e-04\n",
      "=== Step 6353 Done. Avg Loss: 0.63684 ===\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0002.npz\n",
      "Step 6375 | Loss: 0.63389 | LR: 6.99e-04\n",
      "val loss: 0.6313\n",
      "Step 6400 | Loss: 0.61562 | LR: 6.96e-04\n",
      "Step 6425 | Loss: 0.61931 | LR: 6.94e-04\n",
      "Step 6450 | Loss: 0.64488 | LR: 6.91e-04\n",
      "Step 6475 | Loss: 0.64757 | LR: 6.89e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/val/shard_k562e_val_0001.npz\n",
      "val loss: 0.6327\n",
      "Step 6500 | Loss: 0.61649 | LR: 6.87e-04\n",
      "Step 6525 | Loss: 0.65366 | LR: 6.84e-04\n",
      "Step 6550 | Loss: 0.65173 | LR: 6.82e-04\n",
      "Step 6575 | Loss: 0.66952 | LR: 6.79e-04\n",
      "val loss: 0.6322\n",
      "Step 6600 | Loss: 0.66682 | LR: 6.77e-04\n",
      "Step 6625 | Loss: 0.60241 | LR: 6.75e-04\n",
      "Step 6650 | Loss: 0.66747 | LR: 6.72e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0010.npz\n",
      "Step 6675 | Loss: 0.65948 | LR: 6.70e-04\n",
      "val loss: 0.6288\n",
      "Step 6700 | Loss: 0.67610 | LR: 6.67e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0007.npz\n",
      "Step 6725 | Loss: 0.59661 | LR: 6.65e-04\n",
      "Step 6750 | Loss: 0.64273 | LR: 6.62e-04\n",
      "Step 6775 | Loss: 0.66055 | LR: 6.60e-04\n",
      "val loss: 0.6394\n",
      "Step 6800 | Loss: 0.60605 | LR: 6.57e-04\n",
      "Step 6825 | Loss: 0.62956 | LR: 6.55e-04\n",
      "Step 6850 | Loss: 0.61749 | LR: 6.52e-04\n",
      "Step 6875 | Loss: 0.59293 | LR: 6.50e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.6337\n",
      "Step 6900 | Loss: 0.64687 | LR: 6.47e-04\n",
      "Step 6925 | Loss: 0.64190 | LR: 6.45e-04\n",
      "Step 6950 | Loss: 0.66862 | LR: 6.42e-04\n",
      "Step 6975 | Loss: 0.62371 | LR: 6.40e-04\n",
      "val loss: 0.6319\n",
      "Step 7000 | Loss: 0.60353 | LR: 6.37e-04\n",
      "Step 7025 | Loss: 0.63087 | LR: 6.35e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0001.npz\n",
      "Step 7050 | Loss: 0.62134 | LR: 6.32e-04\n",
      "Step 7075 | Loss: 0.64395 | LR: 6.30e-04\n",
      "val loss: 0.6217\n",
      "Step 7100 | Loss: 0.60088 | LR: 6.27e-04\n",
      "Step 7125 | Loss: 0.68231 | LR: 6.25e-04\n",
      "Step 7150 | Loss: 0.65282 | LR: 6.22e-04\n",
      "Step 7175 | Loss: 0.61120 | LR: 6.20e-04\n",
      "val loss: 0.6328\n",
      "Step 7200 | Loss: 0.62372 | LR: 6.17e-04\n",
      "Step 7225 | Loss: 0.66475 | LR: 6.15e-04\n",
      "Step 7250 | Loss: 0.64814 | LR: 6.12e-04\n",
      "Step 7275 | Loss: 0.62827 | LR: 6.10e-04\n",
      "val loss: 0.6326\n",
      "Step 7300 | Loss: 0.64016 | LR: 6.07e-04\n",
      "Step 7325 | Loss: 0.62154 | LR: 6.05e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0003.npz\n",
      "Step 7350 | Loss: 0.59951 | LR: 6.02e-04\n",
      "Step 7375 | Loss: 0.66904 | LR: 6.00e-04\n",
      "val loss: 0.6242\n",
      "Step 7400 | Loss: 0.59930 | LR: 5.97e-04\n",
      "Step 7425 | Loss: 0.63869 | LR: 5.94e-04\n",
      "Step 7450 | Loss: 0.65229 | LR: 5.92e-04\n",
      "Step 7475 | Loss: 0.61698 | LR: 5.89e-04\n",
      "val loss: 0.6482\n",
      "Step 7500 | Loss: 0.66295 | LR: 5.87e-04\n",
      "Step 7525 | Loss: 0.61137 | LR: 5.84e-04\n",
      "Step 7550 | Loss: 0.64258 | LR: 5.82e-04\n",
      "Step 7575 | Loss: 0.59774 | LR: 5.79e-04\n",
      "val loss: 0.6340\n",
      "Step 7600 | Loss: 0.64598 | LR: 5.76e-04\n",
      "Step 7625 | Loss: 0.68372 | LR: 5.74e-04\n",
      "Step 7650 | Loss: 0.63178 | LR: 5.71e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0008.npz\n",
      "Step 7675 | Loss: 0.61723 | LR: 5.69e-04\n",
      "val loss: 0.6345\n",
      "Step 7700 | Loss: 0.65955 | LR: 5.66e-04\n",
      "Step 7725 | Loss: 0.62525 | LR: 5.64e-04\n",
      "Step 7750 | Loss: 0.62668 | LR: 5.61e-04\n",
      "Step 7775 | Loss: 0.63856 | LR: 5.58e-04\n",
      "val loss: 0.6316\n",
      "Step 7800 | Loss: 0.62055 | LR: 5.56e-04\n",
      "Step 7825 | Loss: 0.69861 | LR: 5.53e-04\n",
      "Step 7850 | Loss: 0.63717 | LR: 5.51e-04\n",
      "Step 7875 | Loss: 0.66160 | LR: 5.48e-04\n",
      "val loss: 0.6316\n",
      "Step 7900 | Loss: 0.64008 | LR: 5.45e-04\n",
      "Step 7925 | Loss: 0.67326 | LR: 5.43e-04\n",
      "Step 7950 | Loss: 0.65845 | LR: 5.40e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0009.npz\n",
      "Step 7975 | Loss: 0.64597 | LR: 5.38e-04\n",
      "val loss: 0.6312\n",
      "Step 8000 | Loss: 0.68274 | LR: 5.35e-04\n",
      "Step 8025 | Loss: 0.63060 | LR: 5.33e-04\n",
      "Step 8050 | Loss: 0.67076 | LR: 5.30e-04\n",
      "Step 8075 | Loss: 0.64338 | LR: 5.27e-04\n",
      "val loss: 0.6378\n",
      "Step 8100 | Loss: 0.61601 | LR: 5.25e-04\n",
      "Step 8125 | Loss: 0.60561 | LR: 5.22e-04\n",
      "Step 8150 | Loss: 0.65609 | LR: 5.20e-04\n",
      "Step 8175 | Loss: 0.65904 | LR: 5.17e-04\n",
      "val loss: 0.6280\n",
      "Step 8200 | Loss: 0.63109 | LR: 5.14e-04\n",
      "Step 8225 | Loss: 0.68062 | LR: 5.12e-04\n",
      "Step 8250 | Loss: 0.62403 | LR: 5.09e-04\n",
      "Step 8275 | Loss: 0.65967 | LR: 5.07e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0005.npz\n",
      "val loss: 0.6257\n",
      "Step 8300 | Loss: 0.61926 | LR: 5.04e-04\n",
      "Step 8325 | Loss: 0.61080 | LR: 5.01e-04\n",
      "Step 8350 | Loss: 0.65686 | LR: 4.99e-04\n",
      "Step 8375 | Loss: 0.64537 | LR: 4.96e-04\n",
      "val loss: 0.6376\n",
      "Step 8400 | Loss: 0.63021 | LR: 4.94e-04\n",
      "Step 8425 | Loss: 0.62489 | LR: 4.91e-04\n",
      "Step 8450 | Loss: 0.68073 | LR: 4.88e-04\n",
      "Step 8475 | Loss: 0.62584 | LR: 4.86e-04\n",
      "val loss: 0.6331\n",
      "Step 8500 | Loss: 0.65572 | LR: 4.83e-04\n",
      "Step 8525 | Loss: 0.64198 | LR: 4.81e-04\n",
      "Step 8550 | Loss: 0.63900 | LR: 4.78e-04\n",
      "Step 8575 | Loss: 0.57177 | LR: 4.75e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0000.npz\n",
      "val loss: 0.6332\n",
      "Step 8600 | Loss: 0.59881 | LR: 4.73e-04\n",
      "Step 8625 | Loss: 0.59170 | LR: 4.70e-04\n",
      "Step 8650 | Loss: 0.63550 | LR: 4.68e-04\n",
      "Step 8675 | Loss: 0.61034 | LR: 4.65e-04\n",
      "val loss: 0.6286\n",
      "Step 8700 | Loss: 0.65979 | LR: 4.62e-04\n",
      "Step 8725 | Loss: 0.60235 | LR: 4.60e-04\n",
      "Step 8750 | Loss: 0.61560 | LR: 4.57e-04\n",
      "Step 8775 | Loss: 0.65276 | LR: 4.55e-04\n",
      "val loss: 0.6404\n",
      "Step 8800 | Loss: 0.62901 | LR: 4.52e-04\n",
      "Step 8825 | Loss: 0.61117 | LR: 4.49e-04\n",
      "Step 8850 | Loss: 0.66852 | LR: 4.47e-04\n",
      "Step 8875 | Loss: 0.65232 | LR: 4.44e-04\n",
      "val loss: 0.6217\n",
      "Step 8900 | Loss: 0.61195 | LR: 4.42e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0004.npz\n",
      "Step 8925 | Loss: 0.64463 | LR: 4.39e-04\n",
      "Step 8950 | Loss: 0.66139 | LR: 4.36e-04\n",
      "Step 8975 | Loss: 0.62790 | LR: 4.34e-04\n",
      "val loss: 0.6295\n",
      "Step 9000 | Loss: 0.62931 | LR: 4.31e-04\n",
      "Step 9025 | Loss: 0.58526 | LR: 4.29e-04\n",
      "Step 9050 | Loss: 0.67200 | LR: 4.26e-04\n",
      "Step 9075 | Loss: 0.65436 | LR: 4.24e-04\n",
      "val loss: 0.6289\n",
      "Step 9100 | Loss: 0.63863 | LR: 4.21e-04\n",
      "Step 9125 | Loss: 0.64163 | LR: 4.18e-04\n",
      "Step 9150 | Loss: 0.59194 | LR: 4.16e-04\n",
      "Step 9175 | Loss: 0.64226 | LR: 4.13e-04\n",
      "val loss: 0.6314\n",
      "Step 9200 | Loss: 0.58467 | LR: 4.11e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0006.npz\n",
      "Step 9225 | Loss: 0.59913 | LR: 4.08e-04\n",
      "Step 9250 | Loss: 0.64029 | LR: 4.06e-04\n",
      "Step 9275 | Loss: 0.64860 | LR: 4.03e-04\n",
      "val loss: 0.6217\n",
      "Step 9300 | Loss: 0.66393 | LR: 4.00e-04\n",
      "Step 9325 | Loss: 0.64032 | LR: 3.98e-04\n",
      "Step 9350 | Loss: 0.59691 | LR: 3.95e-04\n",
      "Step 9375 | Loss: 0.59485 | LR: 3.93e-04\n",
      "val loss: 0.6239\n",
      "Step 9400 | Loss: 0.63716 | LR: 3.90e-04\n",
      "Step 9425 | Loss: 0.61465 | LR: 3.88e-04\n",
      "Step 9450 | Loss: 0.63342 | LR: 3.85e-04\n",
      "Step 9475 | Loss: 0.66718 | LR: 3.83e-04\n",
      "val loss: 0.6401\n",
      "Step 9500 | Loss: 0.60913 | LR: 3.80e-04\n",
      "Step 9525 | Loss: 0.67223 | LR: 3.78e-04\n",
      "=== Step 9530 Done. Avg Loss: 0.63627 ===\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0002.npz\n",
      "Step 9550 | Loss: 0.64243 | LR: 3.75e-04\n",
      "Step 9575 | Loss: 0.65609 | LR: 3.73e-04\n",
      "val loss: 0.6321\n",
      "Step 9600 | Loss: 0.63813 | LR: 3.70e-04\n",
      "Step 9625 | Loss: 0.61716 | LR: 3.68e-04\n",
      "Step 9650 | Loss: 0.64268 | LR: 3.65e-04\n",
      "Step 9675 | Loss: 0.67272 | LR: 3.63e-04\n",
      "val loss: 0.6218\n",
      "Step 9700 | Loss: 0.62224 | LR: 3.60e-04\n",
      "Step 9725 | Loss: 0.65386 | LR: 3.58e-04\n",
      "Step 9750 | Loss: 0.62721 | LR: 3.55e-04\n",
      "Step 9775 | Loss: 0.59334 | LR: 3.53e-04\n",
      "val loss: 0.6406\n",
      "Step 9800 | Loss: 0.62844 | LR: 3.50e-04\n",
      "Step 9825 | Loss: 0.67124 | LR: 3.48e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0007.npz\n",
      "Step 9850 | Loss: 0.67562 | LR: 3.45e-04\n",
      "Step 9875 | Loss: 0.57656 | LR: 3.43e-04\n",
      "val loss: 0.6308\n",
      "Step 9900 | Loss: 0.62104 | LR: 3.40e-04\n",
      "Step 9925 | Loss: 0.63924 | LR: 3.38e-04\n",
      "Step 9950 | Loss: 0.59399 | LR: 3.35e-04\n",
      "Step 9975 | Loss: 0.71015 | LR: 3.33e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/val/shard_k562e_val_0001.npz\n",
      "val loss: 0.6273\n",
      "Step 10000 | Loss: 0.64641 | LR: 3.30e-04\n",
      "Step 10025 | Loss: 0.61416 | LR: 3.28e-04\n",
      "Step 10050 | Loss: 0.60008 | LR: 3.26e-04\n",
      "Step 10075 | Loss: 0.63188 | LR: 3.23e-04\n",
      "val loss: 0.6283\n",
      "Step 10100 | Loss: 0.62471 | LR: 3.21e-04\n",
      "Step 10125 | Loss: 0.66285 | LR: 3.18e-04\n",
      "Step 10150 | Loss: 0.65093 | LR: 3.16e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0008.npz\n",
      "Step 10175 | Loss: 0.63180 | LR: 3.13e-04\n",
      "val loss: 0.6401\n",
      "Step 10200 | Loss: 0.68094 | LR: 3.11e-04\n",
      "Step 10225 | Loss: 0.65288 | LR: 3.09e-04\n",
      "Step 10250 | Loss: 0.62155 | LR: 3.06e-04\n",
      "Step 10275 | Loss: 0.63879 | LR: 3.04e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/val/shard_k562e_val_0001.npz\n",
      "val loss: 0.6254\n",
      "Step 10300 | Loss: 0.67220 | LR: 3.01e-04\n",
      "Step 10325 | Loss: 0.61235 | LR: 2.99e-04\n",
      "Step 10350 | Loss: 0.66181 | LR: 2.97e-04\n",
      "Step 10375 | Loss: 0.61302 | LR: 2.94e-04\n",
      "val loss: 0.6279\n",
      "Step 10400 | Loss: 0.65799 | LR: 2.92e-04\n",
      "Step 10425 | Loss: 0.59413 | LR: 2.90e-04\n",
      "Step 10450 | Loss: 0.65341 | LR: 2.87e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0000.npz\n",
      "Step 10475 | Loss: 0.63181 | LR: 2.85e-04\n",
      "val loss: 0.6395\n",
      "Step 10500 | Loss: 0.64245 | LR: 2.82e-04\n",
      "Step 10525 | Loss: 0.61528 | LR: 2.80e-04\n",
      "Step 10550 | Loss: 0.60700 | LR: 2.78e-04\n",
      "Step 10575 | Loss: 0.67336 | LR: 2.75e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.6377\n",
      "Step 10600 | Loss: 0.57918 | LR: 2.73e-04\n",
      "Step 10625 | Loss: 0.60248 | LR: 2.71e-04\n",
      "Step 10650 | Loss: 0.63432 | LR: 2.68e-04\n",
      "Step 10675 | Loss: 0.65451 | LR: 2.66e-04\n",
      "val loss: 0.6269\n",
      "Step 10700 | Loss: 0.63214 | LR: 2.64e-04\n",
      "Step 10725 | Loss: 0.63587 | LR: 2.62e-04\n",
      "Step 10750 | Loss: 0.61009 | LR: 2.59e-04\n",
      "Step 10775 | Loss: 0.62532 | LR: 2.57e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0006.npz\n",
      "val loss: 0.6349\n",
      "Step 10800 | Loss: 0.59351 | LR: 2.55e-04\n",
      "Step 10825 | Loss: 0.61536 | LR: 2.53e-04\n",
      "Step 10850 | Loss: 0.63753 | LR: 2.50e-04\n",
      "Step 10875 | Loss: 0.63913 | LR: 2.48e-04\n",
      "val loss: 0.6314\n",
      "Step 10900 | Loss: 0.62456 | LR: 2.46e-04\n",
      "Step 10925 | Loss: 0.60283 | LR: 2.44e-04\n",
      "Step 10950 | Loss: 0.64166 | LR: 2.41e-04\n",
      "Step 10975 | Loss: 0.64071 | LR: 2.39e-04\n",
      "val loss: 0.6282\n",
      "Step 11000 | Loss: 0.60677 | LR: 2.37e-04\n",
      "Step 11025 | Loss: 0.66807 | LR: 2.35e-04\n",
      "Step 11050 | Loss: 0.66418 | LR: 2.32e-04\n",
      "Step 11075 | Loss: 0.62613 | LR: 2.30e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0010.npz\n",
      "val loss: 0.6258\n",
      "Step 11100 | Loss: 0.62146 | LR: 2.28e-04\n",
      "Step 11125 | Loss: 0.63990 | LR: 2.26e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0005.npz\n",
      "Step 11150 | Loss: 0.65546 | LR: 2.24e-04\n",
      "Step 11175 | Loss: 0.60544 | LR: 2.22e-04\n",
      "val loss: 0.6157\n",
      "Step 11200 | Loss: 0.66059 | LR: 2.19e-04\n",
      "Step 11225 | Loss: 0.59889 | LR: 2.17e-04\n",
      "Step 11250 | Loss: 0.61974 | LR: 2.15e-04\n",
      "Step 11275 | Loss: 0.57852 | LR: 2.13e-04\n",
      "val loss: 0.6207\n",
      "Step 11300 | Loss: 0.64457 | LR: 2.11e-04\n",
      "Step 11325 | Loss: 0.60946 | LR: 2.09e-04\n",
      "Step 11350 | Loss: 0.60033 | LR: 2.07e-04\n",
      "Step 11375 | Loss: 0.65403 | LR: 2.04e-04\n",
      "val loss: 0.6399\n",
      "Step 11400 | Loss: 0.61769 | LR: 2.02e-04\n",
      "Step 11425 | Loss: 0.64428 | LR: 2.00e-04\n",
      "Step 11450 | Loss: 0.65652 | LR: 1.98e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0001.npz\n",
      "Step 11475 | Loss: 0.63829 | LR: 1.96e-04\n",
      "val loss: 0.6320\n",
      "Step 11500 | Loss: 0.64334 | LR: 1.94e-04\n",
      "Step 11525 | Loss: 0.64026 | LR: 1.92e-04\n",
      "Step 11550 | Loss: 0.65356 | LR: 1.90e-04\n",
      "Step 11575 | Loss: 0.62440 | LR: 1.88e-04\n",
      "val loss: 0.6300\n",
      "Step 11600 | Loss: 0.58801 | LR: 1.86e-04\n",
      "Step 11625 | Loss: 0.65041 | LR: 1.84e-04\n",
      "Step 11650 | Loss: 0.67420 | LR: 1.82e-04\n",
      "Step 11675 | Loss: 0.62542 | LR: 1.80e-04\n",
      "val loss: 0.6406\n",
      "Step 11700 | Loss: 0.64023 | LR: 1.78e-04\n",
      "Step 11725 | Loss: 0.62755 | LR: 1.76e-04\n",
      "Step 11750 | Loss: 0.58847 | LR: 1.74e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0003.npz\n",
      "Step 11775 | Loss: 0.62692 | LR: 1.72e-04\n",
      "val loss: 0.6406\n",
      "Step 11800 | Loss: 0.63166 | LR: 1.70e-04\n",
      "Step 11825 | Loss: 0.61974 | LR: 1.68e-04\n",
      "Step 11850 | Loss: 0.59519 | LR: 1.66e-04\n",
      "Step 11875 | Loss: 0.63446 | LR: 1.64e-04\n",
      "val loss: 0.6523\n",
      "Step 11900 | Loss: 0.62962 | LR: 1.62e-04\n",
      "Step 11925 | Loss: 0.61565 | LR: 1.60e-04\n",
      "Step 11950 | Loss: 0.66592 | LR: 1.58e-04\n",
      "Step 11975 | Loss: 0.62313 | LR: 1.57e-04\n",
      "val loss: 0.6266\n",
      "Step 12000 | Loss: 0.69061 | LR: 1.55e-04\n",
      "Step 12025 | Loss: 0.62889 | LR: 1.53e-04\n",
      "Step 12050 | Loss: 0.64505 | LR: 1.51e-04\n",
      "Step 12075 | Loss: 0.65704 | LR: 1.49e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0004.npz\n",
      "val loss: 0.6224\n",
      "Step 12100 | Loss: 0.62066 | LR: 1.47e-04\n",
      "Step 12125 | Loss: 0.64082 | LR: 1.45e-04\n",
      "Step 12150 | Loss: 0.59778 | LR: 1.44e-04\n",
      "Step 12175 | Loss: 0.62719 | LR: 1.42e-04\n",
      "val loss: 0.6243\n",
      "Step 12200 | Loss: 0.64577 | LR: 1.40e-04\n",
      "Step 12225 | Loss: 0.64529 | LR: 1.38e-04\n",
      "Step 12250 | Loss: 0.63034 | LR: 1.36e-04\n",
      "Step 12275 | Loss: 0.60565 | LR: 1.35e-04\n",
      "val loss: 0.6134\n",
      "Step 12300 | Loss: 0.63817 | LR: 1.33e-04\n",
      "Step 12325 | Loss: 0.64457 | LR: 1.31e-04\n",
      "Step 12350 | Loss: 0.61832 | LR: 1.29e-04\n",
      "Step 12375 | Loss: 0.64802 | LR: 1.28e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0009.npz\n",
      "val loss: 0.6374\n",
      "Step 12400 | Loss: 0.67416 | LR: 1.26e-04\n",
      "Step 12425 | Loss: 0.62606 | LR: 1.24e-04\n",
      "Step 12450 | Loss: 0.67686 | LR: 1.22e-04\n",
      "Step 12475 | Loss: 0.63684 | LR: 1.21e-04\n",
      "val loss: 0.6332\n",
      "Step 12500 | Loss: 0.59734 | LR: 1.19e-04\n",
      "Step 12525 | Loss: 0.65102 | LR: 1.17e-04\n",
      "Step 12550 | Loss: 0.64118 | LR: 1.16e-04\n",
      "Step 12575 | Loss: 0.63323 | LR: 1.14e-04\n",
      "val loss: 0.6378\n",
      "Step 12600 | Loss: 0.60363 | LR: 1.12e-04\n",
      "Step 12625 | Loss: 0.66554 | LR: 1.11e-04\n",
      "Step 12650 | Loss: 0.64383 | LR: 1.09e-04\n",
      "Step 12675 | Loss: 0.62114 | LR: 1.07e-04\n",
      "val loss: 0.6425\n",
      "Step 12700 | Loss: 0.63332 | LR: 1.06e-04\n",
      "=== Step 12707 Done. Avg Loss: 0.63628 ===\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0006.npz\n",
      "Step 12725 | Loss: 0.66867 | LR: 1.04e-04\n",
      "Step 12750 | Loss: 0.60329 | LR: 1.03e-04\n",
      "Step 12775 | Loss: 0.62024 | LR: 1.01e-04\n",
      "val loss: 0.6328\n",
      "Step 12800 | Loss: 0.62718 | LR: 9.95e-05\n",
      "Step 12825 | Loss: 0.64509 | LR: 9.79e-05\n",
      "Step 12850 | Loss: 0.64111 | LR: 9.64e-05\n",
      "Step 12875 | Loss: 0.63310 | LR: 9.49e-05\n",
      "val loss: 0.6304\n",
      "Step 12900 | Loss: 0.66367 | LR: 9.34e-05\n",
      "Step 12925 | Loss: 0.63765 | LR: 9.18e-05\n",
      "Step 12950 | Loss: 0.63759 | LR: 9.03e-05\n",
      "Step 12975 | Loss: 0.62944 | LR: 8.89e-05\n",
      "val loss: 0.6254\n",
      "Step 13000 | Loss: 0.61429 | LR: 8.74e-05\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0004.npz\n",
      "Step 13025 | Loss: 0.65674 | LR: 8.59e-05\n",
      "Step 13050 | Loss: 0.67159 | LR: 8.45e-05\n",
      "Step 13075 | Loss: 0.62910 | LR: 8.30e-05\n",
      "val loss: 0.6310\n",
      "Step 13100 | Loss: 0.66357 | LR: 8.16e-05\n",
      "Step 13125 | Loss: 0.61954 | LR: 8.02e-05\n",
      "Step 13150 | Loss: 0.62841 | LR: 7.88e-05\n",
      "Step 13175 | Loss: 0.59476 | LR: 7.74e-05\n",
      "val loss: 0.6277\n",
      "Step 13200 | Loss: 0.59718 | LR: 7.60e-05\n",
      "Step 13225 | Loss: 0.67335 | LR: 7.46e-05\n",
      "Step 13250 | Loss: 0.64860 | LR: 7.33e-05\n",
      "Step 13275 | Loss: 0.61542 | LR: 7.19e-05\n",
      "val loss: 0.6384\n",
      "Step 13300 | Loss: 0.63718 | LR: 7.06e-05\n",
      "Step 13325 | Loss: 0.64938 | LR: 6.92e-05\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0005.npz\n",
      "Step 13350 | Loss: 0.60954 | LR: 6.79e-05\n",
      "Step 13375 | Loss: 0.64100 | LR: 6.66e-05\n",
      "val loss: 0.6306\n",
      "Step 13400 | Loss: 0.61466 | LR: 6.53e-05\n",
      "Step 13425 | Loss: 0.59743 | LR: 6.40e-05\n",
      "Step 13450 | Loss: 0.67763 | LR: 6.28e-05\n",
      "Step 13475 | Loss: 0.62303 | LR: 6.15e-05\n",
      "val loss: 0.6309\n",
      "Step 13500 | Loss: 0.67891 | LR: 6.03e-05\n",
      "Step 13525 | Loss: 0.62941 | LR: 5.90e-05\n",
      "Step 13550 | Loss: 0.62989 | LR: 5.78e-05\n",
      "Step 13575 | Loss: 0.60900 | LR: 5.66e-05\n",
      "val loss: 0.6265\n",
      "Step 13600 | Loss: 0.59790 | LR: 5.54e-05\n",
      "Step 13625 | Loss: 0.66080 | LR: 5.42e-05\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0007.npz\n",
      "Step 13650 | Loss: 0.62564 | LR: 5.31e-05\n",
      "Step 13675 | Loss: 0.65611 | LR: 5.19e-05\n",
      "val loss: 0.6350\n",
      "Step 13700 | Loss: 0.61446 | LR: 5.08e-05\n",
      "Step 13725 | Loss: 0.67190 | LR: 4.96e-05\n",
      "Step 13750 | Loss: 0.60307 | LR: 4.85e-05\n",
      "Step 13775 | Loss: 0.66394 | LR: 4.74e-05\n",
      "loading /Users/djemec/data/jepa/v0_4/training/val/shard_k562e_val_0001.npz\n",
      "val loss: 0.6282\n",
      "Step 13800 | Loss: 0.64724 | LR: 4.63e-05\n",
      "Step 13825 | Loss: 0.64792 | LR: 4.52e-05\n",
      "Step 13850 | Loss: 0.66298 | LR: 4.41e-05\n",
      "Step 13875 | Loss: 0.66469 | LR: 4.31e-05\n",
      "val loss: 0.6344\n",
      "Step 13900 | Loss: 0.67803 | LR: 4.20e-05\n",
      "Step 13925 | Loss: 0.61614 | LR: 4.10e-05\n",
      "Step 13950 | Loss: 0.59925 | LR: 3.99e-05\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0001.npz\n",
      "Step 13975 | Loss: 0.65621 | LR: 3.89e-05\n",
      "val loss: 0.6332\n",
      "Step 14000 | Loss: 0.64507 | LR: 3.79e-05\n",
      "Step 14025 | Loss: 0.61213 | LR: 3.69e-05\n",
      "Step 14050 | Loss: 0.62654 | LR: 3.60e-05\n",
      "Step 14075 | Loss: 0.63798 | LR: 3.50e-05\n",
      "loading /Users/djemec/data/jepa/v0_4/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.6390\n",
      "Step 14100 | Loss: 0.64816 | LR: 3.41e-05\n",
      "Step 14125 | Loss: 0.63746 | LR: 3.31e-05\n",
      "Step 14150 | Loss: 0.66716 | LR: 3.22e-05\n",
      "Step 14175 | Loss: 0.65692 | LR: 3.13e-05\n",
      "val loss: 0.6416\n",
      "Step 14200 | Loss: 0.63941 | LR: 3.04e-05\n",
      "Step 14225 | Loss: 0.61773 | LR: 2.95e-05\n",
      "Step 14250 | Loss: 0.59721 | LR: 2.86e-05\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0002.npz\n",
      "Step 14275 | Loss: 0.66091 | LR: 2.78e-05\n",
      "val loss: 0.6400\n",
      "Step 14300 | Loss: 0.61096 | LR: 2.69e-05\n",
      "Step 14325 | Loss: 0.62786 | LR: 2.61e-05\n",
      "Step 14350 | Loss: 0.64421 | LR: 2.53e-05\n",
      "Step 14375 | Loss: 0.66595 | LR: 2.44e-05\n",
      "val loss: 0.6253\n",
      "Step 14400 | Loss: 0.66495 | LR: 2.36e-05\n",
      "Step 14425 | Loss: 0.63792 | LR: 2.29e-05\n",
      "Step 14450 | Loss: 0.62782 | LR: 2.21e-05\n",
      "Step 14475 | Loss: 0.65875 | LR: 2.13e-05\n",
      "val loss: 0.6416\n",
      "Step 14500 | Loss: 0.64799 | LR: 2.06e-05\n",
      "Step 14525 | Loss: 0.61453 | LR: 1.99e-05\n",
      "Step 14550 | Loss: 0.65753 | LR: 1.91e-05\n",
      "Step 14575 | Loss: 0.64605 | LR: 1.84e-05\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0009.npz\n",
      "val loss: 0.6370\n",
      "Step 14600 | Loss: 0.62263 | LR: 1.77e-05\n",
      "Step 14625 | Loss: 0.63195 | LR: 1.71e-05\n",
      "Step 14650 | Loss: 0.62811 | LR: 1.64e-05\n",
      "Step 14675 | Loss: 0.63674 | LR: 1.57e-05\n",
      "val loss: 0.6188\n",
      "Step 14700 | Loss: 0.69553 | LR: 1.51e-05\n",
      "Step 14725 | Loss: 0.66513 | LR: 1.45e-05\n",
      "Step 14750 | Loss: 0.64617 | LR: 1.38e-05\n",
      "Step 14775 | Loss: 0.61857 | LR: 1.32e-05\n",
      "val loss: 0.6253\n",
      "Step 14800 | Loss: 0.64446 | LR: 1.27e-05\n",
      "Step 14825 | Loss: 0.67680 | LR: 1.21e-05\n",
      "Step 14850 | Loss: 0.65269 | LR: 1.15e-05\n",
      "Step 14875 | Loss: 0.60783 | LR: 1.10e-05\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0008.npz\n",
      "val loss: 0.6374\n",
      "Step 14900 | Loss: 0.61103 | LR: 1.04e-05\n",
      "Step 14925 | Loss: 0.63322 | LR: 9.91e-06\n",
      "Step 14950 | Loss: 0.64857 | LR: 9.41e-06\n",
      "Step 14975 | Loss: 0.64351 | LR: 8.91e-06\n",
      "val loss: 0.6240\n",
      "Step 15000 | Loss: 0.61030 | LR: 8.43e-06\n",
      "Step 15025 | Loss: 0.65502 | LR: 7.96e-06\n",
      "Step 15050 | Loss: 0.63446 | LR: 7.50e-06\n",
      "Step 15075 | Loss: 0.62589 | LR: 7.06e-06\n",
      "val loss: 0.6344\n",
      "Step 15100 | Loss: 0.66278 | LR: 6.63e-06\n",
      "Step 15125 | Loss: 0.65048 | LR: 6.22e-06\n",
      "Step 15150 | Loss: 0.65837 | LR: 5.81e-06\n",
      "Step 15175 | Loss: 0.64519 | LR: 5.43e-06\n",
      "val loss: 0.6269\n",
      "Step 15200 | Loss: 0.66261 | LR: 5.05e-06\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0003.npz\n",
      "Step 15225 | Loss: 0.59607 | LR: 4.69e-06\n",
      "Step 15250 | Loss: 0.66836 | LR: 4.34e-06\n",
      "Step 15275 | Loss: 0.62162 | LR: 4.00e-06\n",
      "val loss: 0.6432\n",
      "Step 15300 | Loss: 0.59437 | LR: 3.68e-06\n",
      "Step 15325 | Loss: 0.66862 | LR: 3.37e-06\n",
      "Step 15350 | Loss: 0.63505 | LR: 3.08e-06\n",
      "Step 15375 | Loss: 0.62645 | LR: 2.80e-06\n",
      "val loss: 0.6179\n",
      "Step 15400 | Loss: 0.62761 | LR: 2.53e-06\n",
      "Step 15425 | Loss: 0.65405 | LR: 2.28e-06\n",
      "Step 15450 | Loss: 0.63531 | LR: 2.03e-06\n",
      "Step 15475 | Loss: 0.59419 | LR: 1.81e-06\n",
      "val loss: 0.6319\n",
      "Step 15500 | Loss: 0.64616 | LR: 1.59e-06\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0000.npz\n",
      "Step 15525 | Loss: 0.60203 | LR: 1.39e-06\n",
      "Step 15550 | Loss: 0.62750 | LR: 1.20e-06\n",
      "Step 15575 | Loss: 0.64369 | LR: 1.03e-06\n",
      "val loss: 0.6516\n",
      "Step 15600 | Loss: 0.67469 | LR: 8.71e-07\n",
      "Step 15625 | Loss: 0.62457 | LR: 7.25e-07\n",
      "Step 15650 | Loss: 0.61278 | LR: 5.92e-07\n",
      "Step 15675 | Loss: 0.64272 | LR: 4.73e-07\n",
      "val loss: 0.6310\n",
      "Step 15700 | Loss: 0.61719 | LR: 3.67e-07\n",
      "Step 15725 | Loss: 0.60323 | LR: 2.74e-07\n",
      "Step 15750 | Loss: 0.65559 | LR: 1.96e-07\n",
      "Step 15775 | Loss: 0.63917 | LR: 1.30e-07\n",
      "val loss: 0.6207\n",
      "Step 15800 | Loss: 0.65088 | LR: 7.86e-08\n",
      "Step 15825 | Loss: 0.62205 | LR: 4.04e-08\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0010.npz\n",
      "Step 15850 | Loss: 0.63229 | LR: 1.58e-08\n",
      "Step 15875 | Loss: 0.67422 | LR: 4.69e-09\n",
      "val loss: 0.6190\n",
      "=== Step 15884 Done. Avg Loss: 0.63627 ===\n"
     ]
    }
   ],
   "source": [
    "decoder.train()\n",
    "\n",
    "for step in range(max_steps):\n",
    "\n",
    "    last_step = (step == max_steps - 1)\n",
    "\n",
    "    # once in a while evaluate our validation set loss\n",
    "    if step % 100 == 0 or last_step:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss_accum = 0.0\n",
    "            val_loss_steps = 10\n",
    "            for i in range(val_loss_steps):\n",
    "                xc, xct, xt, xtt, p_idx, p_mod, p_mode = val_loader.next_batch()\n",
    "                p_feats = input_bank[p_idx]\n",
    "                B, N = xc.shape\n",
    "\n",
    "                # get perturbation latents\n",
    "                action_latents = model.composer(p_feats, p_mod, p_mode)\n",
    "\n",
    "                # run BioJEPA\n",
    "                z_context = model.student(xc, xct, mask_idx=None)\n",
    "                target_indices = torch.arange(N, device=DEVICE).expand(B, N)\n",
    "                z_pred_mu, _ = model.predictor(z_context, action_latents, target_indices)\n",
    "\n",
    "                # run new decoder\n",
    "                pred_delta = decoder(z_pred_mu) - decoder(z_context)\n",
    "                real_delta = xt - xc\n",
    "\n",
    "                loss = F.mse_loss(pred_delta, real_delta)\n",
    "                val_loss_accum += loss.item()\n",
    "\n",
    "            avg_val_loss = val_loss_accum / val_loss_steps\n",
    "            print(f'val loss: {avg_val_loss:.4f}')\n",
    "\n",
    "        decoder.train()\n",
    "\n",
    "    # periodically save checkpoint\n",
    "    if step > 0 and  (step+1) % steps_per_epoch ==0 and not last_step:\n",
    "        # Save Checkpoint\n",
    "        torch.save({\n",
    "            'model': decoder.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'step': step\n",
    "        }, checkpoint_dir / f'biojepa_decoder_ckpt_{step}.pt')\n",
    "\n",
    "    # actual training\n",
    "\n",
    "    xc, xct, xt, xtt, p_idx, p_mod, p_mode = train_loader.next_batch()\n",
    "    p_feats = input_bank[p_idx]\n",
    "    B, N = xc.shape\n",
    "\n",
    "    # run frozen BioJEPA\n",
    "    with torch.no_grad():\n",
    "        z_context = model.student(xc, xct, mask_idx=None)\n",
    "        action_latents = model.composer(p_feats, p_mod, p_mode)\n",
    "        \n",
    "        target_indices = torch.arange(N, device=DEVICE).expand(B, N)\n",
    "        z_pred_mu, _ = model.predictor(z_context, action_latents, target_indices)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # run decoder\n",
    "    pred_case = decoder(z_pred_mu)\n",
    "    pred_control = decoder(z_context)\n",
    "\n",
    "    pred_delta = pred_case - pred_control\n",
    "    real_delta = xt - xc\n",
    "\n",
    "    # loss\n",
    "    loss = F.mse_loss(pred_delta, real_delta)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    # loss caching\n",
    "    lossi.append(loss.item())\n",
    "    total_epoch_loss += loss.item()\n",
    "\n",
    "    if step % 25 == 0:\n",
    "        print(f\"Step {step} | Loss: {loss.item():.5f} | LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "    \n",
    "    if step > 0 and (step+1) % steps_per_epoch == 0:   \n",
    "        avg_loss = total_epoch_loss / steps_per_epoch\n",
    "        print(f\"=== Step {step} Done. Avg Loss: {avg_loss:.5f} ===\")\n",
    "        total_epoch_loss = 0\n",
    "    \n",
    "    if last_step:\n",
    "        # Save Checkpoint\n",
    "        torch.save({\n",
    "            'model': decoder.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'step': step\n",
    "        }, checkpoint_dir / f'biojepa_decoder_ckpt_{step}_final.pt')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c924050-b2d8-4973-a3c1-f9dd0a3d1f03",
   "metadata": {},
   "source": [
    "**Training Loss Plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b11fde37-0cac-429d-be0c-351390bac22d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lossi)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8aaa78-e607-4bc4-8cde-49a529984c1b",
   "metadata": {},
   "source": [
    "## Trained Decoder Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "974730ff-1cda-43e0-a72f-df0986bb9daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from scipy.stats import ConstantInputWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94fd5355-31c8-47fc-a2fd-9b1fa9f91576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkDecoder(\n",
       "  (head): Linear(in_features=8, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "240d54bf-e521-433d-9c79-541dbebab4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_preds = defaultdict(list)      \n",
    "bulk_reals = defaultdict(list)     \n",
    "bulk_reals_delta = defaultdict(list)\n",
    "\n",
    "val_r2_all = []\n",
    "val_r2_top50 = []\n",
    "val_correlations = []\n",
    "val_mses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "417a9438-c9f1-4cb0-8da2-e9fe7ec831a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 2 shards for split val\n",
      "loading /Users/djemec/data/jepa/v0_4/training/val/shard_k562e_val_0000.npz\n"
     ]
    }
   ],
   "source": [
    "val_total_examples = 11044\n",
    "val_steps_per_epoch = val_total_examples // batch_size\n",
    "val_loader = TrainingLoader(batch_size=batch_size, split='val', data_dir=train_dir, device=DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27c89be7-508c-4e06-9dfc-9f4809807077",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Benchmarking:  90%|     | 312/345 [00:58<00:05,  5.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading /Users/djemec/data/jepa/v0_4/training/val/shard_k562e_val_0001.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Benchmarking: 100%|| 345/345 [01:04<00:00,  5.33it/s]\n"
     ]
    }
   ],
   "source": [
    "for step in tqdm(range(val_steps_per_epoch), desc='Benchmarking'):\n",
    "    \n",
    "    cont_x, cont_tot, case_x, case_tot, p_idx, p_mod, p_mode = val_loader.next_batch()\n",
    "    p_feats = input_bank[p_idx]\n",
    "    B, N = cont_x.shape\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        z_context = model.student(cont_x, cont_tot, mask_idx=None)\n",
    "        action_latents = model.composer(p_feats, p_mod, p_mode)\n",
    "        \n",
    "        target_indices = torch.arange(N, device=DEVICE).expand(B, N)\n",
    "        z_pred_mu, _ = model.predictor(z_context, action_latents, target_indices)\n",
    "        \n",
    "        pred_delta = decoder(z_pred_mu) - decoder(z_context)\n",
    "        real_delta = case_x - cont_x\n",
    "\n",
    "        pred_absolute = cont_x + pred_delta\n",
    "    \n",
    "    pred_delta_np = pred_delta.cpu().numpy()\n",
    "    real_delta_np = real_delta.cpu().numpy()\n",
    "\n",
    "    pred_abs_np = pred_absolute.cpu().numpy()\n",
    "    real_abs_np = case_x.cpu().numpy()\n",
    "    \n",
    "    act_id_np = p_idx.cpu().numpy().flatten()\n",
    "\n",
    "    for i in range(B):\n",
    "        p_delta = pred_delta_np[i]\n",
    "        t_delta = real_delta_np[i]\n",
    "        pid = act_id_np[i] \n",
    "\n",
    "        val_mses.append(np.mean((p_delta - t_delta)**2))\n",
    "\n",
    "        top_20_idx = np.argsort(np.abs(t_delta))[-20:]\n",
    "        p_top = p_delta[top_20_idx]\n",
    "        t_top = t_delta[top_20_idx]\n",
    "        \n",
    "        if np.std(p_top) > 1e-9 and np.std(t_top) > 1e-9:\n",
    "            corr, _ = pearsonr(p_top, t_top)\n",
    "            val_correlations.append(0.0 if np.isnan(corr) else corr)\n",
    "        else:\n",
    "            val_correlations.append(0.0)\n",
    "\n",
    "        bulk_preds[pid].append(pred_abs_np[i])\n",
    "        bulk_reals[pid].append(real_abs_np[i])\n",
    "        bulk_reals_delta[pid].append(real_delta_np[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c99d0783-9717-4fd2-933b-9b1d42a66cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pid in bulk_preds:\n",
    "    p_mean = np.mean(np.stack(bulk_preds[pid]), axis=0)\n",
    "    t_mean = np.mean(np.stack(bulk_reals[pid]), axis=0)\n",
    "    t_mean_delta = np.mean(np.stack(bulk_reals_delta[pid]), axis=0)\n",
    "\n",
    "    if np.std(t_mean) > 1e-9:\n",
    "        val_r2_all.append(r2_score(t_mean, p_mean))\n",
    "    \n",
    "    top_50_idx = np.argsort(np.abs(t_mean_delta))[-50:] \n",
    "    \n",
    "    val_r2_top50.append(r2_score(t_mean[top_50_idx], p_mean[top_50_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c9fa6eb-5426-42e6-bc77-4a796c2eba4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_mean_mse = np.mean(val_mses)\n",
    "val_mean_corr = np.mean(val_correlations)\n",
    "val_mean_r2_all = np.mean(val_r2_all)\n",
    "val_median_r2_all = np.median(val_r2_all)\n",
    "val_mean_r2_top50 = np.mean(val_r2_top50)\n",
    "val_median_r2_top50 = np.median(val_r2_top50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "750afdb9-ba92-4a7a-874d-0cc221525de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global MSE: 0.6314\n",
      "Top-20 Pearson R: 0.7485\n",
      "R^2 (All Genes): Mean: 0.9112, Median: 0.9215\n",
      "R^2 (Top 50 DEGs): Mean: -0.4828, Median: -0.1377\n"
     ]
    }
   ],
   "source": [
    "print(f'Global MSE: {val_mean_mse:.4f}')\n",
    "print(f'Top-20 Pearson R: {val_mean_corr:.4f}')\n",
    "print(f'R^2 (All Genes): Mean: {val_mean_r2_all:.4f}, Median: {val_median_r2_all:.4f}')\n",
    "print(f'R^2 (Top 50 DEGs): Mean: {val_mean_r2_top50:.4f}, Median: {val_median_r2_top50:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6b2741-9067-4cee-b37f-2ee471d3c7e6",
   "metadata": {},
   "source": [
    "## Trained Decoder Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ecd5cb77-d9a4-43d8-ad77-b6d102d00caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_preds = defaultdict(list)\n",
    "bulk_reals = defaultdict(list)\n",
    "bulk_reals_delta = defaultdict(list)\n",
    "\n",
    "test_r2_all = []\n",
    "test_r2_top50 = []\n",
    "test_correlations = []\n",
    "test_mses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "17c58931-04dc-4c75-a84d-23fbf3f7abc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 4 shards for split test\n",
      "loading /Users/djemec/data/jepa/v0_4/training/test/shard_k562e_test_0001.npz\n"
     ]
    }
   ],
   "source": [
    "test_total_examples = 38829\n",
    "test_loader = TrainingLoader(batch_size=batch_size, split='test', data_dir=train_dir, device=DEVICE)\n",
    "test_steps_per_epoch = test_total_examples // batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9022a3d0-68f1-4de8-bb53-33e5bd05e212",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Benchmarking:  26%|                                           | 312/1213 [01:01<02:48,  5.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading /Users/djemec/data/jepa/v0_4/training/test/shard_k562e_test_0000.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Benchmarking:  52%|                            | 625/1213 [02:03<02:05,  4.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading /Users/djemec/data/jepa/v0_4/training/test/shard_k562e_test_0002.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Benchmarking:  77%|             | 937/1213 [03:07<00:53,  5.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading /Users/djemec/data/jepa/v0_4/training/test/shard_k562e_test_0003.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Benchmarking: 100%|| 1213/1213 [04:00<00:00,  5.05it/s]\n"
     ]
    }
   ],
   "source": [
    "for step in tqdm(range(test_steps_per_epoch), desc='Benchmarking'):\n",
    "    \n",
    "    cont_x, cont_tot, case_x, case_tot, p_idx, p_mod, p_mode = test_loader.next_batch()\n",
    "    p_feats = input_bank[p_idx]\n",
    "    B, N = cont_x.shape\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        z_context = model.student(cont_x, cont_tot, mask_idx=None)\n",
    "        action_latents = model.composer(p_feats, p_mod, p_mode)\n",
    "        \n",
    "        target_indices = torch.arange(N, device=DEVICE).expand(B, N)\n",
    "        z_pred_mu, _ = model.predictor(z_context, action_latents, target_indices)\n",
    "        \n",
    "        pred_delta = decoder(z_pred_mu) - decoder(z_context)\n",
    "        real_delta = case_x - cont_x\n",
    "\n",
    "        pred_absolute = cont_x + pred_delta\n",
    "        pred_absolute = torch.clamp(pred_absolute, min=0.0)\n",
    "    \n",
    "    pred_delta_np = pred_delta.cpu().numpy()\n",
    "    real_delta_np = real_delta.cpu().numpy()\n",
    "\n",
    "    pred_abs_np = pred_absolute.cpu().numpy()\n",
    "    real_abs_np = case_x.cpu().numpy()\n",
    "    \n",
    "    act_id_np = p_idx.cpu().numpy().flatten()\n",
    "\n",
    "\n",
    "    for i in range(B):\n",
    "        p_delta = pred_delta_np[i]\n",
    "        t_delta = real_delta_np[i]\n",
    "        pid = act_id_np[i] \n",
    "\n",
    "        test_mses.append(np.mean((p_delta - t_delta)**2))\n",
    "\n",
    "        top_20_idx = np.argsort(np.abs(t_delta))[-20:]\n",
    "        p_top = p_delta[top_20_idx]\n",
    "        t_top = t_delta[top_20_idx]\n",
    "        \n",
    "        if np.std(p_top) > 1e-9 and np.std(t_top) > 1e-9:\n",
    "            corr, _ = pearsonr(p_top, t_top)\n",
    "            test_correlations.append(0.0 if np.isnan(corr) else corr)\n",
    "        else:\n",
    "            test_correlations.append(0.0)\n",
    "\n",
    "        bulk_preds[pid].append(pred_abs_np[i])\n",
    "        bulk_reals[pid].append(real_abs_np[i])\n",
    "        bulk_reals_delta[pid].append(real_delta_np[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa48799e-604f-4100-bb2b-3e1f5ac5e0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pid in bulk_preds:\n",
    "    p_mean = np.mean(np.stack(bulk_preds[pid]), axis=0)\n",
    "    t_mean = np.mean(np.stack(bulk_reals[pid]), axis=0)\n",
    "    t_mean_delta = np.mean(np.stack(bulk_reals_delta[pid]), axis=0)\n",
    "\n",
    "    if np.std(t_mean) > 1e-9:\n",
    "        test_r2_all.append(r2_score(t_mean, p_mean))\n",
    "    \n",
    "    top_50_idx = np.argsort(np.abs(t_mean_delta))[-50:] \n",
    "    \n",
    "    test_r2_top50.append(r2_score(t_mean[top_50_idx], p_mean[top_50_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "efaae9f8-c745-4044-b9cd-9d3cf0e944e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mean_mse = np.mean(test_mses)\n",
    "test_mean_corr = np.mean(test_correlations)\n",
    "test_mean_r2_all = np.mean(test_r2_all)\n",
    "test_median_r2_all = np.median(test_r2_all)\n",
    "test_mean_r2_top50 = np.mean(test_r2_top50)\n",
    "test_median_r2_top50 = np.median(test_r2_top50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "40ee16bc-f9f8-4ab6-bc12-9e72fafe1b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global MSE: 0.6284\n",
      "Top-20 Pearson R: 0.7479\n",
      "R^2 (All Genes) - Mean: 0.9150, Median: 0.9289\n",
      "R^2 (Top 50 DEGs) - Mean: -0.3417, Median: 0.0316\n"
     ]
    }
   ],
   "source": [
    "print(f'Global MSE: {test_mean_mse:.4f}')\n",
    "print(f'Top-20 Pearson R: {test_mean_corr:.4f}')\n",
    "print(f'R^2 (All Genes) - Mean: {test_mean_r2_all:.4f}, Median: {test_median_r2_all:.4f}')\n",
    "print(f'R^2 (Top 50 DEGs) - Mean: {test_mean_r2_top50:.4f}, Median: {test_median_r2_top50:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4370f0b7-232f-4fe4-8777-3d08ab4519be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Predicted Shift magnitude: 0.3887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/3hmfwyf51rx5tzm9phb1b6cc0000gn/T/ipykernel_79029/2102659733.py:1: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  diff = np.abs(pred_absolute - cont_x).mean()\n"
     ]
    }
   ],
   "source": [
    "diff = np.abs(pred_absolute - cont_x).mean()\n",
    "print(f\"Average Predicted Shift magnitude: {diff:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0803aec4-f2d9-4557-93df-6fbbca94a90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Change | Pred | True | Error | How much of change missed\n",
      "2.1740 | 1.2301 | 2.1740 | -0.9440 | -0.43\n",
      "2.1740 | 1.1768 | 2.1740 | -0.9972 | -0.46\n",
      "2.1740 | 1.1667 | 2.1740 | -1.0073 | -0.46\n",
      "2.1740 | 0.5650 | 2.1740 | -1.6090 | -0.74\n",
      "2.1740 | 0.7634 | 2.1740 | -1.4106 | -0.65\n",
      "2.1740 | 0.9530 | 2.1740 | -1.2211 | -0.56\n",
      "2.1740 | 0.2898 | 2.1740 | -1.8843 | -0.87\n",
      "2.1740 | 0.3634 | 2.1740 | -1.8106 | -0.83\n",
      "2.1740 | 1.0575 | 2.1740 | -1.1165 | -0.51\n",
      "2.1740 | 0.7268 | 2.1740 | -1.4472 | -0.67\n",
      "2.1740 | 0.6955 | 2.1740 | -1.4785 | -0.68\n",
      "2.1740 | 0.5476 | 2.1740 | -1.6265 | -0.75\n",
      "2.1740 | 0.3220 | 2.1740 | -1.8520 | -0.85\n",
      "2.1740 | 0.8671 | 2.1740 | -1.3069 | -0.60\n",
      "2.1740 | 0.6492 | 2.1740 | -1.5248 | -0.70\n",
      "2.1740 | 1.0971 | 2.1740 | -1.0769 | -0.50\n",
      "2.1740 | 0.2623 | 2.1740 | -1.9117 | -0.88\n",
      "2.1740 | 0.7349 | 2.1740 | -1.4391 | -0.66\n",
      "2.1740 | 0.4687 | 2.1740 | -1.7053 | -0.78\n",
      "2.1740 | 0.4162 | 2.1740 | -1.7579 | -0.81\n",
      "2.1740 | 0.2574 | 2.1740 | -1.9166 | -0.88\n",
      "2.1740 | 0.4598 | 2.1740 | -1.7142 | -0.79\n",
      "2.1740 | 0.2919 | 2.1740 | -1.8821 | -0.87\n",
      "-2.2395 | 1.4277 | 0.0000 | 1.4277 | -0.64\n",
      "-2.2395 | 1.8143 | 0.0000 | 1.8143 | -0.81\n",
      "-2.2395 | 2.2249 | 0.0000 | 2.2249 | -0.99\n",
      "-2.2395 | 1.6752 | 0.0000 | 1.6752 | -0.75\n",
      "-2.2395 | 1.7011 | 0.0000 | 1.7011 | -0.76\n",
      "-2.2395 | 1.4808 | 0.0000 | 1.4808 | -0.66\n",
      "-2.2395 | 1.5253 | 0.0000 | 1.5253 | -0.68\n",
      "-2.2395 | 1.4765 | 0.0000 | 1.4765 | -0.66\n",
      "-2.2395 | 1.1448 | 0.0000 | 1.1448 | -0.51\n",
      "2.3052 | 0.5513 | 2.3052 | -1.7538 | -0.76\n",
      "2.3052 | 1.1534 | 2.3052 | -1.1518 | -0.50\n",
      "2.3052 | 0.7684 | 2.3052 | -1.5367 | -0.67\n",
      "-2.3135 | 2.1275 | 0.0000 | 2.1275 | -0.92\n",
      "-2.3135 | 1.4869 | 0.0000 | 1.4869 | -0.64\n",
      "-2.3135 | 2.0665 | 0.0000 | 2.0665 | -0.89\n",
      "-2.3135 | 1.8027 | 0.0000 | 1.8027 | -0.78\n",
      "-2.3135 | 1.6852 | 0.0000 | 1.6852 | -0.73\n",
      "-2.3735 | 1.2455 | 0.0000 | 1.2455 | -0.52\n",
      "2.4006 | 1.2344 | 2.4006 | -1.1663 | -0.49\n",
      "2.4006 | 0.5332 | 2.4006 | -1.8675 | -0.78\n",
      "-2.4236 | 2.1187 | 0.0000 | 2.1187 | -0.87\n",
      "-2.4236 | 1.9480 | 0.0000 | 1.9480 | -0.80\n",
      "-2.4664 | 1.8133 | 0.0000 | 1.8133 | -0.74\n",
      "2.4747 | 0.5565 | 2.4747 | -1.9182 | -0.78\n",
      "2.4747 | 1.2702 | 2.4747 | -1.2045 | -0.49\n",
      "-2.5035 | 3.3963 | 0.0000 | 3.3963 | -1.36\n",
      "-2.5035 | 2.8657 | 0.0000 | 2.8657 | -1.14\n"
     ]
    }
   ],
   "source": [
    "## most of these are either turning on (true change == true) or off (true == 0) \n",
    "print(f'True Change | Pred | True | Error | How much of change missed')\n",
    "for i in top_50_idx:\n",
    "    print(f'{t_mean_delta[i]:.4f} | {p_mean[i]:.4f} | {t_mean[i]:.4f} | {(p_mean[i] - t_mean[i]):.4f} | {((p_mean[i] - t_mean[i])/(t_mean_delta[i])):.2f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f469bcf9-4fc7-4cd3-b3e2-46bd7b7fa06b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af74ea8-323e-4a03-ab3a-b7ab042a157d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
