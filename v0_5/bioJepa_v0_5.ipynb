{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ecde55a-b2ae-48c4-b33b-81e327eb2c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "\n",
    "import biojepa_ac_v0_5 as model\n",
    "from bio_dataloader import PretrainLoader, TrainingLoader, AlignmentLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f002c8c-dc2a-45ee-83c4-334648fcf033",
   "metadata": {},
   "source": [
    "## BioJEPA Model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df05ee08-24ef-46b5-be25-9820a8304b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: nvidia-smi: command not found\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78bcd644-b8eb-413f-8614-28ebda4ecd69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e089507f-69f6-4883-aca7-43b361c9da21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cpu\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    device = 'cpu'\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(1337)\n",
    "        device = 'cuda'\n",
    "    print(f'using {device}')\n",
    "    return device\n",
    "\n",
    "DEVICE = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4efa224b-1884-4f79-b84c-d1518a1f54d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "random.seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79fa3b1-d191-40bc-90f1-fe2b4a62057d",
   "metadata": {},
   "source": [
    "## Training Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b646069-a51f-48c3-ac6c-d8fda740de7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('/Users/djemec/data/jepa/v0_4')\n",
    "train_dir = data_dir / 'training'\n",
    "pretrain_dir = data_dir / 'pretraining'\n",
    "checkpoint_dir = Path('/Users/djemec/data/jepa/v0_5') / 'checkpoints'\n",
    "pert_dir = data_dir / 'pert_embd'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107687e9-828b-452f-a027-84224e37df59",
   "metadata": {},
   "source": [
    "**Loading feature banks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "671d9f97-c31e-412c-a84a-b860ed561be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Banks Loaded. Input(DNA): torch.Size([1250, 1536]), Anchor(Prot): torch.Size([1087, 320])\n"
     ]
    }
   ],
   "source": [
    "input_bank = torch.from_numpy(np.load(pert_dir / 'input_embeddings_dna.npy')).float().to(DEVICE)\n",
    "anchor_bank = torch.from_numpy(np.load(pert_dir / 'action_embeddings_esm2.npy')).float().to(DEVICE)\n",
    "print(f'Banks Loaded. Input(DNA): {input_bank.shape}, Anchor(Prot): {anchor_bank.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d668759-740c-4b86-913f-98817ef29a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "n_genes = 5000\n",
    "n_layers = 2\n",
    "n_heads = 2\n",
    "n_embd = 8\n",
    "pert_latent_dim = 320\n",
    "pert_mode_dim = 64\n",
    "\n",
    "training_file_chunk = 10000\n",
    "pretraining_file_chunk = 50000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2e53d0e-fb0c-41a8-82c1-b81c18afb03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high')\n",
    "config = model.BioJepaConfig(\n",
    "    num_genes = n_genes,\n",
    "    n_layer= n_layers,\n",
    "    heads= n_heads,\n",
    "    embed_dim = n_embd,\n",
    "    n_pre_layer= n_layers,\n",
    "    pert_latent_dim=pert_latent_dim,\n",
    "    pert_mode_dim=pert_mode_dim\n",
    ")\n",
    "model = model.BioJepa(config).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d0160cf-2e8f-4e0a-bda0-2a5fe9c80b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student/Teacher: 42002\n",
      "ACpredictor: 45168\n",
      "PerturbationComposer: 882880\n"
     ]
    }
   ],
   "source": [
    "print(f'Student/Teacher: {sum(p.numel() for p in model.student.parameters() if p.requires_grad)}')\n",
    "print(f'ACpredictor: {sum(p.numel() for p in model.predictor.parameters() if p.requires_grad)}')\n",
    "print(f'PerturbationComposer: {sum(p.numel() for p in model.composer.parameters() if p.requires_grad)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978f697d-0c6e-4da1-bf29-5500fa68a893",
   "metadata": {},
   "source": [
    "### Pre-Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73e3082c-515e-428e-94d4-8b281501f8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 3 shards for split train\n",
      "loading /Users/djemec/data/jepa/v0_4/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "found 1 shards for split val\n",
      "loading /Users/djemec/data/jepa/v0_4/pretraining/val/pt_shard_k562e_val_0000.npz\n"
     ]
    }
   ],
   "source": [
    "pt_train_loader = PretrainLoader(batch_size=batch_size, split='train', data_dir=pretrain_dir, device=DEVICE)\n",
    "pt_val_loader = PretrainLoader(batch_size=batch_size, split='val', data_dir=pretrain_dir, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41728e1e-e69e-4fb3-8f34-952e1608a5c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3511, 35110)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_train_total = 112373\n",
    "pt_val_total = 11044\n",
    "steps_per_epoch = pt_train_total // batch_size\n",
    "pt_max_steps = pt_epochs * steps_per_epoch\n",
    "steps_per_epoch, pt_max_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b59997f1-43ca-4945-a881-4222f6f88b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_LR = 1e-3\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=pt_LR, weight_decay=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85e4c635-c8eb-4ad7-8c06-b81677e9df88",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=pt_LR, total_steps=pt_max_steps, pct_start=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f7ca9f7-1586-4978-b562-adfc325aa6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_lossi = []\n",
    "step = 0\n",
    "total_epoch_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329d10f4-c7a8-4ab8-9f18-cccd39b8c510",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss: 73.3001\n",
      "Step 0 | Loss: 73.32006 | LR: 4.00e-05\n",
      "Step 25 | Loss: 71.63674 | LR: 4.05e-05\n",
      "Step 50 | Loss: 70.18835 | LR: 4.20e-05\n",
      "Step 75 | Loss: 68.96690 | LR: 4.44e-05\n",
      "val loss: 67.7945\n",
      "Step 100 | Loss: 67.81098 | LR: 4.78e-05\n",
      "Step 125 | Loss: 66.63290 | LR: 5.22e-05\n",
      "Step 150 | Loss: 65.36736 | LR: 5.74e-05\n",
      "Step 175 | Loss: 63.98825 | LR: 6.36e-05\n",
      "val loss: 62.6645\n",
      "Step 200 | Loss: 62.68073 | LR: 7.08e-05\n",
      "Step 225 | Loss: 61.72447 | LR: 7.88e-05\n",
      "Step 250 | Loss: 61.05285 | LR: 8.77e-05\n",
      "Step 275 | Loss: 60.47821 | LR: 9.74e-05\n",
      "val loss: 59.8765\n",
      "Step 300 | Loss: 59.87124 | LR: 1.08e-04\n",
      "Step 325 | Loss: 59.19168 | LR: 1.19e-04\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "for step in range(pt_max_steps):\n",
    "    last_step = (step == pt_max_steps - 1)\n",
    "\n",
    "    # once in a while evaluate our validation set loss\n",
    "    if step % 100 == 0 or last_step:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss_accum = 0.0\n",
    "            val_loss_steps = 10\n",
    "            for i in range(val_loss_steps):\n",
    "                x_val, total_val = pt_val_loader.next_batch()\n",
    "\n",
    "                val_loss = model.forward_pretrain(x_val, total_val)\n",
    "                val_loss_accum += val_loss.item()\n",
    "\n",
    "            avg_val_loss = val_loss_accum / val_loss_steps\n",
    "            print(f'val loss: {avg_val_loss:.4f}')\n",
    "            \n",
    "        # with open(log_file, 'a') as f:\n",
    "        #    f.write(f'{step} test {test_loss_accum.item():.4f}\\n')\n",
    "        model.train()\n",
    "\n",
    "\n",
    "    if step > 0 and (step+1) % steps_per_epoch ==0 and not last_step:\n",
    "        # Save Checkpoint\n",
    "        torch.save({\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'step': step\n",
    "        }, checkpoint_dir / f'bio_jepa_pt_ckpt_{step}.pt')\n",
    "\n",
    "    # Pre-Training\n",
    "    x, total = pt_train_loader.next_batch()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss = model.forward_pretrain(x, total)\n",
    "    loss.backward()\n",
    "\n",
    "    # gradient clipping\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Update Teacher (V-JEPA Momentum)\n",
    "    model.update_teacher()\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    pt_lossi.append(loss.item())\n",
    "    total_epoch_loss += loss.item()\n",
    "    \n",
    "    if step % 25 == 0:\n",
    "        print(f'Step {step} | Loss: {loss.item():.5f} | LR: {scheduler.get_last_lr()[0]:.2e}')\n",
    "    \n",
    "    \n",
    "    if step > 0 and (step+1) % steps_per_epoch == 0:   \n",
    "        avg_loss = total_epoch_loss / steps_per_epoch\n",
    "        print(f'=== Step {step} Done. Avg Loss: {avg_loss:.5f} ===')\n",
    "        total_epoch_loss = 0.0\n",
    "    \n",
    "    \n",
    "    if last_step:\n",
    "        # Save Checkpoint\n",
    "        torch.save({\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'step': step\n",
    "        }, checkpoint_dir / f'bio_jepa_pt_ckpt_{step}_final.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd06e53-1e6b-4793-bb4d-9f2c69a6ce0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pt_lossi[:])\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bad8d2a-7e12-4c3a-b5b4-2e233bb9c97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "del pt_val_loader\n",
    "del pt_train_loader\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5224bd9-3a62-48d0-b125-129483dff9be",
   "metadata": {},
   "source": [
    "## Perturbation Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7e1d7c-e226-4ed3-9245-308262b8bf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for BioJEPA\n",
    "modality_to_id = {\n",
    "    'protein': 0,\n",
    "    'chemical': 1,\n",
    "    'dna': 2\n",
    "}\n",
    "\n",
    "mode_to_id = {\n",
    "    'crispri': 0,\n",
    "    'crispra': 1,\n",
    "    'overexpression': 2,\n",
    "    'knockout': 3,\n",
    "    'inhibitor': 4,\n",
    "    'agonist': 5,\n",
    "    'degrader': 6,\n",
    "    'binder': 7,\n",
    "    'control': 8,\n",
    "    'unknown': 9\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66400db-cba6-4e70-a4ba-a4e8e9c8458a",
   "metadata": {},
   "outputs": [],
   "source": [
    "align_total = 1250\n",
    "align_batch_size = 32\n",
    "align_epochs = 1000\n",
    "align_steps_per_epoch = align_total // align_batch_size\n",
    "align_max_steps = align_epochs * align_steps_per_epoch\n",
    "align_steps_per_epoch, align_max_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f193cbd-b70e-4c9e-b93a-c095bb01fc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "align_train_loader = AlignmentLoader(align_batch_size, 'train', pert_dir, DEVICE)\n",
    "align_val_loader = AlignmentLoader(align_batch_size, 'val', pert_dir, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918d0ab8-bdbf-47b4-8776-005b3c0cab4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "align_lr = 4e-3\n",
    "align_optimizer = torch.optim.AdamW(model.composer.parameters(), lr=align_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d37b607-a218-4809-a48f-86a6546c5344",
   "metadata": {},
   "outputs": [],
   "source": [
    "align_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    align_optimizer, max_lr=align_lr, total_steps=align_max_steps, pct_start=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1677f19-2083-4201-8078-09d8b236c791",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epoch_loss = 0\n",
    "align_lossi = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89abcd6c-78c6-4427-a4db-113479ef0930",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.train()\n",
    "for step in range(align_max_steps):\n",
    "    last_step = (step == align_max_steps - 1)\n",
    "\n",
    "    if step % 10 == 0 or last_step:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss_accum = 0.0\n",
    "            val_loss_steps = 10\n",
    "            for i in range(val_loss_steps):\n",
    "                inp_idx, anc_idx, inp_mod, inp_mode = align_val_loader.next_batch()\n",
    "                inp_feats = input_bank[inp_idx]\n",
    "                anc_feats = anchor_bank[anc_idx]\n",
    "\n",
    "                B = inp_idx.shape[0]\n",
    "                anc_mod = torch.full((B,), modality_to_id['protein'], device=DEVICE, dtype=torch.long)\n",
    "                anc_mode = torch.full((B,), mode_to_id['control'], device=DEVICE, dtype=torch.long)\n",
    "\n",
    "                val_loss = model.forward_alignment(\n",
    "                    anchor_feats=anc_feats, anchor_mod=anc_mod, anchor_mode=anc_mode,\n",
    "                    positive_feats=inp_feats, positive_mod=inp_mod, positive_mode=inp_mode\n",
    "                    )\n",
    "    \n",
    "                val_loss_accum += val_loss.item()\n",
    "\n",
    "            avg_val_loss = val_loss_accum / val_loss_steps\n",
    "            print(f'val loss: {avg_val_loss:.4f}')\n",
    "            \n",
    "        # with open(log_file, 'a') as f:\n",
    "        #    f.write(f'{step} test {test_loss_accum.item():.4f}\\n')\n",
    "        model.train()\n",
    "\n",
    "\n",
    "    if step > 0 and (step+1) % steps_per_epoch==0 and not last_step:\n",
    "        # Save Checkpoint\n",
    "        torch.save({\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'step': step\n",
    "        }, checkpoint_dir / f'bio_jepa_perturb_ckpt_{step}.pt')\n",
    "\n",
    "    # Perturb Pretrain\n",
    "\n",
    "    inp_idx, anc_idx, inp_mod, inp_mode = align_train_loader.next_batch()\n",
    "    \n",
    "    # Lookup Features\n",
    "    inp_feats = input_bank[inp_idx]\n",
    "    anc_feats = anchor_bank[anc_idx]\n",
    "    \n",
    "    # Construct Anchor Metadata\n",
    "    # Anchors are Proteins (Mod=0) acting as targets (Mode=Control/Identity/8)\n",
    "    B = inp_idx.shape[0]\n",
    "    anc_mod = torch.full((B,), modality_to_id['protein'], device=DEVICE, dtype=torch.long)\n",
    "    anc_mode = torch.full((B,), mode_to_id['control'], device=DEVICE, dtype=torch.long)\n",
    "    \n",
    "    # Forward\n",
    "    align_optimizer.zero_grad()\n",
    "\n",
    "    loss = model.forward_alignment(\n",
    "        anchor_feats=anc_feats, anchor_mod=anc_mod, anchor_mode=anc_mode,\n",
    "        positive_feats=inp_feats, positive_mod=inp_mod, positive_mode=inp_mode\n",
    "    )\n",
    "    \n",
    "    loss.backward()\n",
    "    align_optimizer.step()\n",
    "    align_scheduler.step()\n",
    "\n",
    "    align_lossi.append(loss.item())\n",
    "    total_epoch_loss += loss.item()\n",
    "    \n",
    "    if step % 25 == 0:\n",
    "        print(f'Step {step} | Loss: {loss.item():.5f} | LR: {scheduler.get_last_lr()[0]:.2e}')\n",
    "\n",
    "    if step > 0 and (step+1) % align_steps_per_epoch == 0:   \n",
    "        avg_loss = total_epoch_loss / align_steps_per_epoch\n",
    "        print(f'=== Step {step} Done. Avg Loss: {avg_loss:.5f} ===')\n",
    "        total_epoch_loss = 0.0\n",
    "\n",
    "    if last_step:\n",
    "        # Save Checkpoint\n",
    "        torch.save({\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'step': step\n",
    "        }, checkpoint_dir / f'bio_jepa_perturb_ckpt_{step}_final.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9403e2a-61fb-483d-9368-c146a93874ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(align_lossi[:])\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bc1e13-a647-4bc0-a38b-3fee826f0f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "del align_val_loader\n",
    "del align_train_loader\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12aea4d-8756-43a0-aee7-d5c3a76b8263",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715e6fd8-ba08-4544-ac66-cf5ffa312e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "LR = 1e-3\n",
    "pert_LR = LR * 0.1\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f68d4d-0cbf-44d8-ba34-bd0aed833b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = TrainingLoader(batch_size=batch_size, split='train', data_dir=train_dir, device=DEVICE)\n",
    "val_loader = TrainingLoader(batch_size=batch_size, split='val', data_dir=train_dir, device=DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec115e2d-684d-4d6d-a202-ea84db97a7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = 101682 // batch_size\n",
    "max_steps = epochs * steps_per_epoch\n",
    "steps_per_epoch, max_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad84847-ec01-49f7-bb05-28c49924a716",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': model.predictor.parameters(), 'lr': LR},       \n",
    "    {'params': model.composer.parameters(),  'lr': pert_LR} \n",
    "], weight_decay=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6aab6d-046c-4bd1-a1f2-94a0099430ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=[LR, pert_LR], total_steps=max_steps, pct_start=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8333d07-ddf0-428b-969d-bb46efdc27dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossi = []\n",
    "total_epoch_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c6ac49-44b7-4556-89ae-6c087721016a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.freeze_encoders()\n",
    "model.train()\n",
    "\n",
    "for step in range(max_steps):\n",
    "\n",
    "    last_step = (step == max_steps - 1)\n",
    "\n",
    "    # once in a while evaluate our validation set loss\n",
    "    if step % 100 == 0 or last_step:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss_accum = 0.0\n",
    "            val_loss_steps = 10\n",
    "            for i in range(val_loss_steps):\n",
    "                xc, xct, xt, xtt, p_idx, p_mod, p_mode = val_loader.next_batch()\n",
    "                p_feats = input_bank[p_idx]\n",
    "\n",
    "\n",
    "                val_loss = model(xc, xct,xt, xtt,p_feats, p_mod, p_mode)\n",
    "                val_loss_accum += val_loss.item()\n",
    "\n",
    "            avg_val_loss = val_loss_accum / val_loss_steps\n",
    "            print(f'val loss: {avg_val_loss:.4f}')\n",
    "\n",
    "        # with open(log_file, \"a\") as f:\n",
    "        #    f.write(f'{step} test {test_loss_accum.item():.4f}\\n')\n",
    "        model.train()\n",
    "\n",
    "\n",
    "    if step > 0 and (step+1) % steps_per_epoch ==0 and not last_step:\n",
    "        # Save Checkpoint\n",
    "        torch.save({\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'step': step\n",
    "        }, checkpoint_dir / f'bio_jepa_ckpt_{step}.pt')\n",
    "\n",
    "\n",
    "    # Get Batch (xc=Control, xt=Treated/Case)\n",
    "    xc, xct, xt, xtt, p_idx, p_mod, p_mode = train_loader.next_batch()\n",
    "\n",
    "    p_feats = input_bank[p_idx]\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss = model(xc, xct,xt, xtt,p_feats, p_mod, p_mode)\n",
    "    loss.backward()\n",
    "\n",
    "    # gradient clipping\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    lossi.append(loss.item())\n",
    "    total_epoch_loss += loss.item()\n",
    "    \n",
    "    if step % 25 == 0:\n",
    "        print(f\"Step {step} | Loss: {loss.item():.5f} | LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "    \n",
    "    \n",
    "    if step > 0 and (step+1) % steps_per_epoch == 0:   \n",
    "        avg_loss = total_epoch_loss / steps_per_epoch\n",
    "        print(f\"=== Step {step} Done. Avg Loss: {avg_loss:.5f} ===\")\n",
    "        total_epoch_loss = 0\n",
    "    \n",
    "    \n",
    "    if last_step:\n",
    "        # Save Checkpoint\n",
    "        torch.save({\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'step': step\n",
    "        }, checkpoint_dir / f'bio_jepa_ckpt_{step}_final.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e5f01f-441d-44bc-95d2-a8c28eacbd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lossi[:])\n",
    "plt.yscale('log')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
