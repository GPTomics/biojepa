{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71522d3b-8805-48e6-bb36-1c9a7bd15482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.serialization\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "import biojepa_v0_4 as model\n",
    "from bio_dataloader import TrainingLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe98e32-54b3-4da6-adf9-8b3e76e3bc0b",
   "metadata": {},
   "source": [
    "## BioJEPA Model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f67335c-63ea-4f80-9c10-96b4d3e09c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cpu\n"
     ]
    }
   ],
   "source": [
    "random.seed(1337)\n",
    "def get_device():\n",
    "    device = 'cpu'\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(1337)\n",
    "        device = 'cuda'\n",
    "    print(f'using {device}')\n",
    "    return device\n",
    "\n",
    "DEVICE = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5ec64e1-0d7a-4dee-b478-b99f901ce9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "n_genes = 5000\n",
    "n_layers = 2\n",
    "n_heads = 2\n",
    "n_embd = 8\n",
    "pert_latent_dim = 320\n",
    "pert_mode_dim = 64\n",
    "\n",
    "training_file_chunk = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "025d6281-f460-46eb-89d9-842d1ca6fa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('/Users/djemec/data/jepa/v0_4')\n",
    "train_dir = data_dir / 'training'\n",
    "checkpoint_dir = data_dir / 'checkpoints'\n",
    "pert_dir = data_dir / 'pert_embd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9671701-5cc4-4016-a354-b71238c98947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Banks Loaded. Input(DNA): torch.Size([1250, 1536]), Anchor(Prot): torch.Size([1087, 320])\n"
     ]
    }
   ],
   "source": [
    "input_bank = torch.from_numpy(np.load(pert_dir / 'input_embeddings_dna.npy')).float().to(DEVICE)\n",
    "anchor_bank = torch.from_numpy(np.load(pert_dir / 'action_embeddings_esm2.npy')).float().to(DEVICE)\n",
    "print(f'Banks Loaded. Input(DNA): {input_bank.shape}, Anchor(Prot): {anchor_bank.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fe46b1b-aff7-4355-909f-9f71761f11c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high')\n",
    "config = model.BioJepaConfig(\n",
    "    num_genes = n_genes,\n",
    "    n_layer= n_layers,\n",
    "    heads= n_heads,\n",
    "    embed_dim = n_embd,\n",
    "    n_pre_layer= n_layers,\n",
    "    pert_latent_dim=pert_latent_dim,\n",
    "    pert_mode_dim=pert_mode_dim\n",
    ")\n",
    "model = model.BioJepa(config).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a609e628-943e-41c2-a4d7-a71b9075a640",
   "metadata": {},
   "source": [
    "**Load Checkpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4aac628e-6ab7-4768-955a-3f99e79f58ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = checkpoint_dir / 'bio_jepa_ckpt_31769_final.pt'\n",
    "checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "\n",
    "keys = model.load_state_dict(checkpoint['model'])\n",
    "keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8287a64f-9d0d-4fbe-8c9b-a03c463a1487",
   "metadata": {},
   "source": [
    "**Freeze Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63812672-368f-483a-be2a-8b65cc849a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze_encoders()\n",
    "model.eval()\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c98b4a-5df8-4894-9c56-ae0dbcbc3acd",
   "metadata": {},
   "source": [
    "## Build Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3dec1f3-d9cc-4849-aaa8-6a7d60199432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights_robust(module):\n",
    "    if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            fan_in = module.embedding_dim\n",
    "        else:\n",
    "            fan_in = module.weight.size(1)\n",
    "        std = 1.0 / math.sqrt(fan_in) if fan_in > 0 else 0.02\n",
    "        nn.init.trunc_normal_(module.weight, mean=0.0, std=std, a=-2*std, b=2*std)\n",
    "        if hasattr(module, 'bias') and module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "    elif isinstance(module, nn.LayerNorm):\n",
    "        nn.init.zeros_(module.bias)\n",
    "        nn.init.ones_(module.weight)\n",
    "\n",
    "@dataclass\n",
    "class BenchmarkDecoderConfig:\n",
    "    embed_dim: int = 256\n",
    "    \n",
    "class BenchmarkDecoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.head = nn.Linear(config.embed_dim, 1)\n",
    "\n",
    "        self.apply(init_weights_robust)\n",
    "        \n",
    "    def forward(self, latents):\n",
    "\n",
    "        gene_preds = self.head(latents)        \n",
    "        gene_preds = gene_preds.squeeze(-1)\n",
    "        \n",
    "        return gene_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c316fe59-e4aa-4d12-9db4-2e38781f53bc",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e1d5000-f278-4f9b-9227-9554f31e9a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 11 shards for split train\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0000.npz\n",
      "found 2 shards for split val\n",
      "loading /Users/djemec/data/jepa/v0_4/training/val/shard_k562e_val_0001.npz\n"
     ]
    }
   ],
   "source": [
    "train_loader = TrainingLoader(batch_size=batch_size, split='train', data_dir=train_dir, device=DEVICE)\n",
    "val_loader = TrainingLoader(batch_size=batch_size, split='val', data_dir=train_dir, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9025bf-05d7-4ff8-ae9a-c1b87471c3e5",
   "metadata": {},
   "source": [
    "## Training Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f484f1f2-2933-413d-80c0-f0c970656453",
   "metadata": {},
   "source": [
    "### Training Config/Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad700ee3-e710-43de-be89-4003c922a41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_decoder = 1e-3\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0494e7de-e96d-4989-b47b-9229c050a263",
   "metadata": {},
   "source": [
    "**Initialize Decoder** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74224c15-ff49-41d9-9552-b8ec9ed83dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BenchmarkDecoderConfig(\n",
    "    embed_dim= n_embd\n",
    ")\n",
    "\n",
    "decoder = BenchmarkDecoder(config).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d5141e-064c-4ec0-9584-ef4a87237125",
   "metadata": {},
   "source": [
    "**Load Checkpoint (only Sometimes)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d3b286-5a21-4bff-aa8e-cd1a5135df08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder_checkpoint_path = checkpoint_dir / 'biojepa_decoder_ckpt_15884_final.pt'\n",
    "# decode_checkpoint = torch.load(decoder_checkpoint_path, map_location=DEVICE)\n",
    "\n",
    "# d_keys = decoder.load_state_dict(decode_checkpoint['model'])\n",
    "# d_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2da30fa-9c44-40be-921c-5184c764f0e5",
   "metadata": {},
   "source": [
    "**Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b197e689-8b98-4cf9-8382-6116bf2ce440",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(decoder.parameters(), lr=lr_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958f11cf-8c9f-49b9-b526-74be69d2126f",
   "metadata": {},
   "source": [
    "**Training Lenght**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "009526cc-7703-497d-87cb-4a50f9c1c1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_total_examples = 101682"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "766a254e-2fd7-4274-91c4-7ceab21d8d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3177, 15885)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch = train_total_examples // batch_size\n",
    "max_steps = epochs * steps_per_epoch\n",
    "steps_per_epoch, max_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cf4803-b42e-4d1b-89db-c9b43d6e3558",
   "metadata": {},
   "source": [
    "**Scheduler**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c683627d-3da6-45e9-9827-06d8586a1f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=lr_decoder, total_steps=max_steps, pct_start=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15c5c7c-b741-486e-9ce2-99c2e253b05d",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82cb876c-e45d-43f7-94aa-2fa7f44b15c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossi = []\n",
    "step = 0\n",
    "total_epoch_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69468a01-365b-42b9-b041-5d6969e5b4ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss: 1.8539\n",
      "Step 0 | Loss: 1.84268 | LR: 4.00e-05\n",
      "Step 25 | Loss: 1.92216 | LR: 4.25e-05\n",
      "Step 50 | Loss: 1.92808 | LR: 4.98e-05\n",
      "Step 75 | Loss: 1.79709 | LR: 6.16e-05\n",
      "val loss: 1.8149\n",
      "Step 100 | Loss: 1.81244 | LR: 7.79e-05\n",
      "Step 125 | Loss: 1.84605 | LR: 9.85e-05\n",
      "Step 150 | Loss: 1.76088 | LR: 1.23e-04\n",
      "Step 175 | Loss: 1.93653 | LR: 1.52e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 1.7231\n",
      "Step 200 | Loss: 1.69199 | LR: 1.84e-04\n",
      "Step 225 | Loss: 1.71804 | LR: 2.20e-04\n",
      "Step 250 | Loss: 1.64003 | LR: 2.58e-04\n",
      "Step 275 | Loss: 1.56506 | LR: 2.99e-04\n",
      "val loss: 1.5564\n",
      "Step 300 | Loss: 1.63393 | LR: 3.43e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0004.npz\n",
      "Step 325 | Loss: 1.49058 | LR: 3.87e-04\n",
      "Step 350 | Loss: 1.50516 | LR: 4.34e-04\n",
      "Step 375 | Loss: 1.47375 | LR: 4.81e-04\n",
      "val loss: 1.3796\n",
      "Step 400 | Loss: 1.30358 | LR: 5.28e-04\n",
      "Step 425 | Loss: 1.28838 | LR: 5.76e-04\n",
      "Step 450 | Loss: 1.33900 | LR: 6.23e-04\n",
      "Step 475 | Loss: 1.24049 | LR: 6.68e-04\n",
      "val loss: 1.1588\n",
      "Step 500 | Loss: 1.16126 | LR: 7.13e-04\n",
      "Step 525 | Loss: 1.12841 | LR: 7.55e-04\n",
      "Step 550 | Loss: 1.10623 | LR: 7.96e-04\n",
      "Step 575 | Loss: 1.02857 | LR: 8.33e-04\n",
      "val loss: 0.9620\n",
      "Step 600 | Loss: 0.89063 | LR: 8.67e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0001.npz\n",
      "Step 625 | Loss: 0.89440 | LR: 8.98e-04\n",
      "Step 650 | Loss: 0.83319 | LR: 9.26e-04\n",
      "Step 675 | Loss: 0.81491 | LR: 9.49e-04\n",
      "val loss: 0.8001\n",
      "Step 700 | Loss: 0.84047 | LR: 9.68e-04\n",
      "Step 725 | Loss: 0.71812 | LR: 9.83e-04\n",
      "Step 750 | Loss: 0.68476 | LR: 9.93e-04\n",
      "Step 775 | Loss: 0.79469 | LR: 9.99e-04\n",
      "val loss: 0.6761\n",
      "Step 800 | Loss: 0.65852 | LR: 1.00e-03\n",
      "Step 825 | Loss: 0.73045 | LR: 1.00e-03\n",
      "Step 850 | Loss: 0.63707 | LR: 1.00e-03\n",
      "Step 875 | Loss: 0.61520 | LR: 1.00e-03\n",
      "val loss: 0.6297\n",
      "Step 900 | Loss: 0.70084 | LR: 1.00e-03\n",
      "Step 925 | Loss: 0.57223 | LR: 1.00e-03\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0010.npz\n",
      "Step 950 | Loss: 0.65010 | LR: 1.00e-03\n",
      "Step 975 | Loss: 0.61243 | LR: 1.00e-03\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0003.npz\n",
      "val loss: 0.5856\n",
      "Step 1000 | Loss: 0.57986 | LR: 1.00e-03\n",
      "Step 1025 | Loss: 0.54465 | LR: 9.99e-04\n",
      "Step 1050 | Loss: 0.59961 | LR: 9.99e-04\n",
      "Step 1075 | Loss: 0.57771 | LR: 9.99e-04\n",
      "val loss: 0.5539\n",
      "Step 1100 | Loss: 0.60168 | LR: 9.99e-04\n",
      "Step 1125 | Loss: 0.54326 | LR: 9.99e-04\n",
      "Step 1150 | Loss: 0.57195 | LR: 9.99e-04\n",
      "Step 1175 | Loss: 0.55322 | LR: 9.98e-04\n",
      "val loss: 0.5415\n",
      "Step 1200 | Loss: 0.58434 | LR: 9.98e-04\n",
      "Step 1225 | Loss: 0.56595 | LR: 9.98e-04\n",
      "Step 1250 | Loss: 0.55775 | LR: 9.98e-04\n",
      "Step 1275 | Loss: 0.53788 | LR: 9.97e-04\n",
      "val loss: 0.5358\n",
      "Step 1300 | Loss: 0.55502 | LR: 9.97e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0007.npz\n",
      "Step 1325 | Loss: 0.52469 | LR: 9.97e-04\n",
      "Step 1350 | Loss: 0.55260 | LR: 9.97e-04\n",
      "Step 1375 | Loss: 0.54198 | LR: 9.96e-04\n",
      "val loss: 0.5271\n",
      "Step 1400 | Loss: 0.51682 | LR: 9.96e-04\n",
      "Step 1425 | Loss: 0.54002 | LR: 9.96e-04\n",
      "Step 1450 | Loss: 0.55209 | LR: 9.95e-04\n",
      "Step 1475 | Loss: 0.52081 | LR: 9.95e-04\n",
      "val loss: 0.5214\n",
      "Step 1500 | Loss: 0.54362 | LR: 9.95e-04\n",
      "Step 1525 | Loss: 0.53691 | LR: 9.94e-04\n",
      "Step 1550 | Loss: 0.52599 | LR: 9.94e-04\n",
      "Step 1575 | Loss: 0.52927 | LR: 9.93e-04\n",
      "val loss: 0.5236\n",
      "Step 1600 | Loss: 0.55667 | LR: 9.93e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0006.npz\n",
      "Step 1625 | Loss: 0.53016 | LR: 9.93e-04\n",
      "Step 1650 | Loss: 0.51845 | LR: 9.92e-04\n",
      "Step 1675 | Loss: 0.51756 | LR: 9.92e-04\n",
      "val loss: 0.5198\n",
      "Step 1700 | Loss: 0.52084 | LR: 9.91e-04\n",
      "Step 1725 | Loss: 0.53094 | LR: 9.91e-04\n",
      "Step 1750 | Loss: 0.50490 | LR: 9.90e-04\n",
      "Step 1775 | Loss: 0.51229 | LR: 9.90e-04\n",
      "val loss: 0.5256\n",
      "Step 1800 | Loss: 0.53843 | LR: 9.89e-04\n",
      "Step 1825 | Loss: 0.55193 | LR: 9.88e-04\n",
      "Step 1850 | Loss: 0.53137 | LR: 9.88e-04\n",
      "Step 1875 | Loss: 0.52854 | LR: 9.87e-04\n",
      "val loss: 0.5155\n",
      "Step 1900 | Loss: 0.52405 | LR: 9.87e-04\n",
      "Step 1925 | Loss: 0.53658 | LR: 9.86e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0002.npz\n",
      "Step 1950 | Loss: 0.53933 | LR: 9.86e-04\n",
      "Step 1975 | Loss: 0.50632 | LR: 9.85e-04\n",
      "val loss: 0.5231\n",
      "Step 2000 | Loss: 0.51117 | LR: 9.84e-04\n",
      "Step 2025 | Loss: 0.50965 | LR: 9.84e-04\n",
      "Step 2050 | Loss: 0.51873 | LR: 9.83e-04\n",
      "Step 2075 | Loss: 0.50414 | LR: 9.82e-04\n",
      "val loss: 0.5186\n",
      "Step 2100 | Loss: 0.55898 | LR: 9.82e-04\n",
      "Step 2125 | Loss: 0.50793 | LR: 9.81e-04\n",
      "Step 2150 | Loss: 0.54046 | LR: 9.80e-04\n",
      "Step 2175 | Loss: 0.54484 | LR: 9.79e-04\n",
      "val loss: 0.5199\n",
      "Step 2200 | Loss: 0.53783 | LR: 9.79e-04\n",
      "Step 2225 | Loss: 0.51351 | LR: 9.78e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0005.npz\n",
      "Step 2250 | Loss: 0.50790 | LR: 9.77e-04\n",
      "Step 2275 | Loss: 0.49188 | LR: 9.76e-04\n",
      "val loss: 0.5159\n",
      "Step 2300 | Loss: 0.54065 | LR: 9.76e-04\n",
      "Step 2325 | Loss: 0.51376 | LR: 9.75e-04\n",
      "Step 2350 | Loss: 0.49980 | LR: 9.74e-04\n",
      "Step 2375 | Loss: 0.53552 | LR: 9.73e-04\n",
      "val loss: 0.5189\n",
      "Step 2400 | Loss: 0.50301 | LR: 9.72e-04\n",
      "Step 2425 | Loss: 0.52547 | LR: 9.71e-04\n",
      "Step 2450 | Loss: 0.52458 | LR: 9.71e-04\n",
      "Step 2475 | Loss: 0.49755 | LR: 9.70e-04\n",
      "val loss: 0.5096\n",
      "Step 2500 | Loss: 0.51189 | LR: 9.69e-04\n",
      "Step 2525 | Loss: 0.52360 | LR: 9.68e-04\n",
      "Step 2550 | Loss: 0.53018 | LR: 9.67e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0008.npz\n",
      "Step 2575 | Loss: 0.50018 | LR: 9.66e-04\n",
      "val loss: 0.5148\n",
      "Step 2600 | Loss: 0.51694 | LR: 9.65e-04\n",
      "Step 2625 | Loss: 0.50711 | LR: 9.64e-04\n",
      "Step 2650 | Loss: 0.49384 | LR: 9.63e-04\n",
      "Step 2675 | Loss: 0.55282 | LR: 9.62e-04\n",
      "val loss: 0.5125\n",
      "Step 2700 | Loss: 0.52375 | LR: 9.61e-04\n",
      "Step 2725 | Loss: 0.52215 | LR: 9.60e-04\n",
      "Step 2750 | Loss: 0.53379 | LR: 9.59e-04\n",
      "Step 2775 | Loss: 0.54664 | LR: 9.58e-04\n",
      "val loss: 0.5168\n",
      "Step 2800 | Loss: 0.53544 | LR: 9.57e-04\n",
      "Step 2825 | Loss: 0.53920 | LR: 9.56e-04\n",
      "Step 2850 | Loss: 0.50567 | LR: 9.55e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0009.npz\n",
      "Step 2875 | Loss: 0.49985 | LR: 9.54e-04\n",
      "val loss: 0.5197\n",
      "Step 2900 | Loss: 0.51900 | LR: 9.53e-04\n",
      "Step 2925 | Loss: 0.52254 | LR: 9.52e-04\n",
      "Step 2950 | Loss: 0.50450 | LR: 9.50e-04\n",
      "Step 2975 | Loss: 0.47447 | LR: 9.49e-04\n",
      "val loss: 0.5256\n",
      "Step 3000 | Loss: 0.51857 | LR: 9.48e-04\n",
      "Step 3025 | Loss: 0.51387 | LR: 9.47e-04\n",
      "Step 3050 | Loss: 0.56003 | LR: 9.46e-04\n",
      "Step 3075 | Loss: 0.51048 | LR: 9.45e-04\n",
      "val loss: 0.5159\n",
      "Step 3100 | Loss: 0.51482 | LR: 9.43e-04\n",
      "Step 3125 | Loss: 0.50823 | LR: 9.42e-04\n",
      "Step 3150 | Loss: 0.54810 | LR: 9.41e-04\n",
      "Step 3175 | Loss: 0.52130 | LR: 9.40e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0004.npz\n",
      "=== Step 3176 Done. Avg Loss: 0.74155 ===\n",
      "val loss: 0.5179\n",
      "Step 3200 | Loss: 0.51809 | LR: 9.38e-04\n",
      "Step 3225 | Loss: 0.53136 | LR: 9.37e-04\n",
      "Step 3250 | Loss: 0.51820 | LR: 9.36e-04\n",
      "Step 3275 | Loss: 0.54430 | LR: 9.35e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5083\n",
      "Step 3300 | Loss: 0.53379 | LR: 9.33e-04\n",
      "Step 3325 | Loss: 0.50742 | LR: 9.32e-04\n",
      "Step 3350 | Loss: 0.50289 | LR: 9.31e-04\n",
      "Step 3375 | Loss: 0.50885 | LR: 9.29e-04\n",
      "val loss: 0.5119\n",
      "Step 3400 | Loss: 0.52000 | LR: 9.28e-04\n",
      "Step 3425 | Loss: 0.50126 | LR: 9.27e-04\n",
      "Step 3450 | Loss: 0.53777 | LR: 9.25e-04\n",
      "Step 3475 | Loss: 0.51066 | LR: 9.24e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0008.npz\n",
      "val loss: 0.5144\n",
      "Step 3500 | Loss: 0.49606 | LR: 9.23e-04\n",
      "Step 3525 | Loss: 0.51690 | LR: 9.21e-04\n",
      "Step 3550 | Loss: 0.51357 | LR: 9.20e-04\n",
      "Step 3575 | Loss: 0.52745 | LR: 9.18e-04\n",
      "val loss: 0.5209\n",
      "Step 3600 | Loss: 0.54472 | LR: 9.17e-04\n",
      "Step 3625 | Loss: 0.53779 | LR: 9.16e-04\n",
      "Step 3650 | Loss: 0.52314 | LR: 9.14e-04\n",
      "Step 3675 | Loss: 0.50850 | LR: 9.13e-04\n",
      "val loss: 0.5184\n",
      "Step 3700 | Loss: 0.52949 | LR: 9.11e-04\n",
      "Step 3725 | Loss: 0.51972 | LR: 9.10e-04\n",
      "Step 3750 | Loss: 0.49192 | LR: 9.08e-04\n",
      "Step 3775 | Loss: 0.53068 | LR: 9.07e-04\n",
      "val loss: 0.5166\n",
      "Step 3800 | Loss: 0.52444 | LR: 9.05e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0002.npz\n",
      "Step 3825 | Loss: 0.52479 | LR: 9.04e-04\n",
      "Step 3850 | Loss: 0.54453 | LR: 9.02e-04\n",
      "Step 3875 | Loss: 0.50548 | LR: 9.01e-04\n",
      "val loss: 0.5121\n",
      "Step 3900 | Loss: 0.50300 | LR: 8.99e-04\n",
      "Step 3925 | Loss: 0.51853 | LR: 8.97e-04\n",
      "Step 3950 | Loss: 0.49924 | LR: 8.96e-04\n",
      "Step 3975 | Loss: 0.50932 | LR: 8.94e-04\n",
      "val loss: 0.5159\n",
      "Step 4000 | Loss: 0.53386 | LR: 8.93e-04\n",
      "Step 4025 | Loss: 0.50267 | LR: 8.91e-04\n",
      "Step 4050 | Loss: 0.53283 | LR: 8.89e-04\n",
      "Step 4075 | Loss: 0.48006 | LR: 8.88e-04\n",
      "val loss: 0.5155\n",
      "Step 4100 | Loss: 0.51796 | LR: 8.86e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0000.npz\n",
      "Step 4125 | Loss: 0.50688 | LR: 8.84e-04\n",
      "Step 4150 | Loss: 0.53376 | LR: 8.83e-04\n",
      "Step 4175 | Loss: 0.50820 | LR: 8.81e-04\n",
      "val loss: 0.5074\n",
      "Step 4200 | Loss: 0.54681 | LR: 8.79e-04\n",
      "Step 4225 | Loss: 0.53456 | LR: 8.78e-04\n",
      "Step 4250 | Loss: 0.51917 | LR: 8.76e-04\n",
      "Step 4275 | Loss: 0.53666 | LR: 8.74e-04\n",
      "val loss: 0.5127\n",
      "Step 4300 | Loss: 0.51829 | LR: 8.73e-04\n",
      "Step 4325 | Loss: 0.51656 | LR: 8.71e-04\n",
      "Step 4350 | Loss: 0.52205 | LR: 8.69e-04\n",
      "Step 4375 | Loss: 0.51233 | LR: 8.67e-04\n",
      "val loss: 0.5193\n",
      "Step 4400 | Loss: 0.52731 | LR: 8.65e-04\n",
      "Step 4425 | Loss: 0.51222 | LR: 8.64e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0003.npz\n",
      "Step 4450 | Loss: 0.50159 | LR: 8.62e-04\n",
      "Step 4475 | Loss: 0.51956 | LR: 8.60e-04\n",
      "val loss: 0.5190\n",
      "Step 4500 | Loss: 0.50659 | LR: 8.58e-04\n",
      "Step 4525 | Loss: 0.48813 | LR: 8.56e-04\n",
      "Step 4550 | Loss: 0.51966 | LR: 8.55e-04\n",
      "Step 4575 | Loss: 0.50343 | LR: 8.53e-04\n",
      "val loss: 0.5191\n",
      "Step 4600 | Loss: 0.51008 | LR: 8.51e-04\n",
      "Step 4625 | Loss: 0.51109 | LR: 8.49e-04\n",
      "Step 4650 | Loss: 0.48373 | LR: 8.47e-04\n",
      "Step 4675 | Loss: 0.55393 | LR: 8.45e-04\n",
      "val loss: 0.5175\n",
      "Step 4700 | Loss: 0.52209 | LR: 8.43e-04\n",
      "Step 4725 | Loss: 0.55812 | LR: 8.42e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0009.npz\n",
      "Step 4750 | Loss: 0.52596 | LR: 8.40e-04\n",
      "Step 4775 | Loss: 0.54083 | LR: 8.38e-04\n",
      "val loss: 0.5082\n",
      "Step 4800 | Loss: 0.53671 | LR: 8.36e-04\n",
      "Step 4825 | Loss: 0.51283 | LR: 8.34e-04\n",
      "Step 4850 | Loss: 0.54293 | LR: 8.32e-04\n",
      "Step 4875 | Loss: 0.52081 | LR: 8.30e-04\n",
      "val loss: 0.5231\n",
      "Step 4900 | Loss: 0.54804 | LR: 8.28e-04\n",
      "Step 4925 | Loss: 0.49979 | LR: 8.26e-04\n",
      "Step 4950 | Loss: 0.50307 | LR: 8.24e-04\n",
      "Step 4975 | Loss: 0.52703 | LR: 8.22e-04\n",
      "val loss: 0.5107\n",
      "Step 5000 | Loss: 0.51415 | LR: 8.20e-04\n",
      "Step 5025 | Loss: 0.51153 | LR: 8.18e-04\n",
      "Step 5050 | Loss: 0.50964 | LR: 8.16e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0007.npz\n",
      "Step 5075 | Loss: 0.51513 | LR: 8.14e-04\n",
      "val loss: 0.5128\n",
      "Step 5100 | Loss: 0.50263 | LR: 8.12e-04\n",
      "Step 5125 | Loss: 0.51441 | LR: 8.10e-04\n",
      "Step 5150 | Loss: 0.53934 | LR: 8.08e-04\n",
      "Step 5175 | Loss: 0.51867 | LR: 8.06e-04\n",
      "val loss: 0.5160\n",
      "Step 5200 | Loss: 0.48746 | LR: 8.04e-04\n",
      "Step 5225 | Loss: 0.52822 | LR: 8.02e-04\n",
      "Step 5250 | Loss: 0.52795 | LR: 8.00e-04\n",
      "Step 5275 | Loss: 0.54688 | LR: 7.98e-04\n",
      "val loss: 0.5164\n",
      "Step 5300 | Loss: 0.53713 | LR: 7.96e-04\n",
      "Step 5325 | Loss: 0.48069 | LR: 7.93e-04\n",
      "Step 5350 | Loss: 0.52981 | LR: 7.91e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0006.npz\n",
      "Step 5375 | Loss: 0.53233 | LR: 7.89e-04\n",
      "val loss: 0.5245\n",
      "Step 5400 | Loss: 0.53232 | LR: 7.87e-04\n",
      "Step 5425 | Loss: 0.51453 | LR: 7.85e-04\n",
      "Step 5450 | Loss: 0.51590 | LR: 7.83e-04\n",
      "Step 5475 | Loss: 0.52472 | LR: 7.81e-04\n",
      "val loss: 0.5169\n",
      "Step 5500 | Loss: 0.51301 | LR: 7.78e-04\n",
      "Step 5525 | Loss: 0.50124 | LR: 7.76e-04\n",
      "Step 5550 | Loss: 0.51895 | LR: 7.74e-04\n",
      "Step 5575 | Loss: 0.50184 | LR: 7.72e-04\n",
      "val loss: 0.5201\n",
      "Step 5600 | Loss: 0.52992 | LR: 7.70e-04\n",
      "Step 5625 | Loss: 0.53614 | LR: 7.68e-04\n",
      "Step 5650 | Loss: 0.52928 | LR: 7.65e-04\n",
      "Step 5675 | Loss: 0.52468 | LR: 7.63e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0001.npz\n",
      "val loss: 0.5221\n",
      "Step 5700 | Loss: 0.52620 | LR: 7.61e-04\n",
      "Step 5725 | Loss: 0.50826 | LR: 7.59e-04\n",
      "Step 5750 | Loss: 0.52853 | LR: 7.57e-04\n",
      "Step 5775 | Loss: 0.52684 | LR: 7.54e-04\n",
      "val loss: 0.5131\n",
      "Step 5800 | Loss: 0.53804 | LR: 7.52e-04\n",
      "Step 5825 | Loss: 0.52648 | LR: 7.50e-04\n",
      "Step 5850 | Loss: 0.51868 | LR: 7.48e-04\n",
      "Step 5875 | Loss: 0.52546 | LR: 7.45e-04\n",
      "val loss: 0.5232\n",
      "Step 5900 | Loss: 0.52337 | LR: 7.43e-04\n",
      "Step 5925 | Loss: 0.52611 | LR: 7.41e-04\n",
      "Step 5950 | Loss: 0.53200 | LR: 7.38e-04\n",
      "Step 5975 | Loss: 0.51456 | LR: 7.36e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0005.npz\n",
      "val loss: 0.5159\n",
      "Step 6000 | Loss: 0.53819 | LR: 7.34e-04\n",
      "Step 6025 | Loss: 0.54895 | LR: 7.32e-04\n",
      "Step 6050 | Loss: 0.49503 | LR: 7.29e-04\n",
      "Step 6075 | Loss: 0.52857 | LR: 7.27e-04\n",
      "val loss: 0.5225\n",
      "Step 6100 | Loss: 0.51848 | LR: 7.25e-04\n",
      "Step 6125 | Loss: 0.51696 | LR: 7.22e-04\n",
      "Step 6150 | Loss: 0.52780 | LR: 7.20e-04\n",
      "Step 6175 | Loss: 0.51373 | LR: 7.18e-04\n",
      "val loss: 0.5149\n",
      "Step 6200 | Loss: 0.53490 | LR: 7.15e-04\n",
      "Step 6225 | Loss: 0.53071 | LR: 7.13e-04\n",
      "Step 6250 | Loss: 0.52478 | LR: 7.11e-04\n",
      "Step 6275 | Loss: 0.49148 | LR: 7.08e-04\n",
      "val loss: 0.5108\n",
      "Step 6300 | Loss: 0.53674 | LR: 7.06e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0010.npz\n",
      "Step 6325 | Loss: 0.50605 | LR: 7.03e-04\n",
      "Step 6350 | Loss: 0.51633 | LR: 7.01e-04\n",
      "=== Step 6353 Done. Avg Loss: 0.52035 ===\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0002.npz\n",
      "Step 6375 | Loss: 0.50792 | LR: 6.99e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/val/shard_k562e_val_0001.npz\n",
      "val loss: 0.5215\n",
      "Step 6400 | Loss: 0.52721 | LR: 6.96e-04\n",
      "Step 6425 | Loss: 0.56276 | LR: 6.94e-04\n",
      "Step 6450 | Loss: 0.52343 | LR: 6.91e-04\n",
      "Step 6475 | Loss: 0.52523 | LR: 6.89e-04\n",
      "val loss: 0.5212\n",
      "Step 6500 | Loss: 0.53311 | LR: 6.87e-04\n",
      "Step 6525 | Loss: 0.51916 | LR: 6.84e-04\n",
      "Step 6550 | Loss: 0.52577 | LR: 6.82e-04\n",
      "Step 6575 | Loss: 0.52830 | LR: 6.79e-04\n",
      "val loss: 0.5110\n",
      "Step 6600 | Loss: 0.49665 | LR: 6.77e-04\n",
      "Step 6625 | Loss: 0.53108 | LR: 6.75e-04\n",
      "Step 6650 | Loss: 0.51563 | LR: 6.72e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0010.npz\n",
      "Step 6675 | Loss: 0.51767 | LR: 6.70e-04\n",
      "val loss: 0.5152\n",
      "Step 6700 | Loss: 0.53701 | LR: 6.67e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0007.npz\n",
      "Step 6725 | Loss: 0.51872 | LR: 6.65e-04\n",
      "Step 6750 | Loss: 0.52996 | LR: 6.62e-04\n",
      "Step 6775 | Loss: 0.51140 | LR: 6.60e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5113\n",
      "Step 6800 | Loss: 0.52562 | LR: 6.57e-04\n",
      "Step 6825 | Loss: 0.52083 | LR: 6.55e-04\n",
      "Step 6850 | Loss: 0.52799 | LR: 6.52e-04\n",
      "Step 6875 | Loss: 0.54197 | LR: 6.50e-04\n",
      "val loss: 0.5274\n",
      "Step 6900 | Loss: 0.53257 | LR: 6.47e-04\n",
      "Step 6925 | Loss: 0.52318 | LR: 6.45e-04\n",
      "Step 6950 | Loss: 0.52182 | LR: 6.42e-04\n",
      "Step 6975 | Loss: 0.51526 | LR: 6.40e-04\n",
      "val loss: 0.5205\n",
      "Step 7000 | Loss: 0.52169 | LR: 6.37e-04\n",
      "Step 7025 | Loss: 0.51695 | LR: 6.35e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0001.npz\n",
      "Step 7050 | Loss: 0.49247 | LR: 6.32e-04\n",
      "Step 7075 | Loss: 0.51592 | LR: 6.30e-04\n",
      "val loss: 0.5187\n",
      "Step 7100 | Loss: 0.52124 | LR: 6.27e-04\n",
      "Step 7125 | Loss: 0.54147 | LR: 6.25e-04\n",
      "Step 7150 | Loss: 0.48660 | LR: 6.22e-04\n",
      "Step 7175 | Loss: 0.55117 | LR: 6.20e-04\n",
      "val loss: 0.5148\n",
      "Step 7200 | Loss: 0.52441 | LR: 6.17e-04\n",
      "Step 7225 | Loss: 0.52342 | LR: 6.15e-04\n",
      "Step 7250 | Loss: 0.52367 | LR: 6.12e-04\n",
      "Step 7275 | Loss: 0.53006 | LR: 6.10e-04\n",
      "val loss: 0.5210\n",
      "Step 7300 | Loss: 0.52813 | LR: 6.07e-04\n",
      "Step 7325 | Loss: 0.49844 | LR: 6.05e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0003.npz\n",
      "Step 7350 | Loss: 0.48932 | LR: 6.02e-04\n",
      "Step 7375 | Loss: 0.51017 | LR: 6.00e-04\n",
      "val loss: 0.5146\n",
      "Step 7400 | Loss: 0.54005 | LR: 5.97e-04\n",
      "Step 7425 | Loss: 0.51769 | LR: 5.94e-04\n",
      "Step 7450 | Loss: 0.51909 | LR: 5.92e-04\n",
      "Step 7475 | Loss: 0.52145 | LR: 5.89e-04\n",
      "val loss: 0.5127\n",
      "Step 7500 | Loss: 0.50920 | LR: 5.87e-04\n",
      "Step 7525 | Loss: 0.51058 | LR: 5.84e-04\n",
      "Step 7550 | Loss: 0.52905 | LR: 5.82e-04\n",
      "Step 7575 | Loss: 0.51062 | LR: 5.79e-04\n",
      "val loss: 0.5155\n",
      "Step 7600 | Loss: 0.52398 | LR: 5.76e-04\n",
      "Step 7625 | Loss: 0.51966 | LR: 5.74e-04\n",
      "Step 7650 | Loss: 0.51231 | LR: 5.71e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0008.npz\n",
      "Step 7675 | Loss: 0.55012 | LR: 5.69e-04\n",
      "val loss: 0.5136\n",
      "Step 7700 | Loss: 0.53189 | LR: 5.66e-04\n",
      "Step 7725 | Loss: 0.53530 | LR: 5.64e-04\n",
      "Step 7750 | Loss: 0.53634 | LR: 5.61e-04\n",
      "Step 7775 | Loss: 0.54442 | LR: 5.58e-04\n",
      "val loss: 0.5176\n",
      "Step 7800 | Loss: 0.52173 | LR: 5.56e-04\n",
      "Step 7825 | Loss: 0.51345 | LR: 5.53e-04\n",
      "Step 7850 | Loss: 0.52864 | LR: 5.51e-04\n",
      "Step 7875 | Loss: 0.52736 | LR: 5.48e-04\n",
      "val loss: 0.5129\n",
      "Step 7900 | Loss: 0.51493 | LR: 5.45e-04\n",
      "Step 7925 | Loss: 0.54106 | LR: 5.43e-04\n",
      "Step 7950 | Loss: 0.53708 | LR: 5.40e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0009.npz\n",
      "Step 7975 | Loss: 0.50348 | LR: 5.38e-04\n",
      "val loss: 0.5170\n",
      "Step 8000 | Loss: 0.50512 | LR: 5.35e-04\n",
      "Step 8025 | Loss: 0.51856 | LR: 5.33e-04\n",
      "Step 8050 | Loss: 0.54113 | LR: 5.30e-04\n",
      "Step 8075 | Loss: 0.52531 | LR: 5.27e-04\n",
      "val loss: 0.5177\n",
      "Step 8100 | Loss: 0.50403 | LR: 5.25e-04\n",
      "Step 8125 | Loss: 0.52762 | LR: 5.22e-04\n",
      "Step 8150 | Loss: 0.50455 | LR: 5.20e-04\n",
      "Step 8175 | Loss: 0.48553 | LR: 5.17e-04\n",
      "val loss: 0.5172\n",
      "Step 8200 | Loss: 0.50358 | LR: 5.14e-04\n",
      "Step 8225 | Loss: 0.51519 | LR: 5.12e-04\n",
      "Step 8250 | Loss: 0.52372 | LR: 5.09e-04\n",
      "Step 8275 | Loss: 0.51376 | LR: 5.07e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0005.npz\n",
      "val loss: 0.5081\n",
      "Step 8300 | Loss: 0.51638 | LR: 5.04e-04\n",
      "Step 8325 | Loss: 0.51131 | LR: 5.01e-04\n",
      "Step 8350 | Loss: 0.51841 | LR: 4.99e-04\n",
      "Step 8375 | Loss: 0.52500 | LR: 4.96e-04\n",
      "val loss: 0.5094\n",
      "Step 8400 | Loss: 0.52467 | LR: 4.94e-04\n",
      "Step 8425 | Loss: 0.53778 | LR: 4.91e-04\n",
      "Step 8450 | Loss: 0.52635 | LR: 4.88e-04\n",
      "Step 8475 | Loss: 0.50412 | LR: 4.86e-04\n",
      "val loss: 0.5106\n",
      "Step 8500 | Loss: 0.51040 | LR: 4.83e-04\n",
      "Step 8525 | Loss: 0.49647 | LR: 4.81e-04\n",
      "Step 8550 | Loss: 0.53105 | LR: 4.78e-04\n",
      "Step 8575 | Loss: 0.52477 | LR: 4.75e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0000.npz\n",
      "val loss: 0.5261\n",
      "Step 8600 | Loss: 0.53964 | LR: 4.73e-04\n",
      "Step 8625 | Loss: 0.53858 | LR: 4.70e-04\n",
      "Step 8650 | Loss: 0.50725 | LR: 4.68e-04\n",
      "Step 8675 | Loss: 0.53061 | LR: 4.65e-04\n",
      "val loss: 0.5124\n",
      "Step 8700 | Loss: 0.49229 | LR: 4.62e-04\n",
      "Step 8725 | Loss: 0.51841 | LR: 4.60e-04\n",
      "Step 8750 | Loss: 0.53065 | LR: 4.57e-04\n",
      "Step 8775 | Loss: 0.49401 | LR: 4.55e-04\n",
      "val loss: 0.5090\n",
      "Step 8800 | Loss: 0.53800 | LR: 4.52e-04\n",
      "Step 8825 | Loss: 0.48902 | LR: 4.49e-04\n",
      "Step 8850 | Loss: 0.53885 | LR: 4.47e-04\n",
      "Step 8875 | Loss: 0.53131 | LR: 4.44e-04\n",
      "val loss: 0.5268\n",
      "Step 8900 | Loss: 0.52735 | LR: 4.42e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0004.npz\n",
      "Step 8925 | Loss: 0.52271 | LR: 4.39e-04\n",
      "Step 8950 | Loss: 0.55775 | LR: 4.36e-04\n",
      "Step 8975 | Loss: 0.52173 | LR: 4.34e-04\n",
      "val loss: 0.5264\n",
      "Step 9000 | Loss: 0.52049 | LR: 4.31e-04\n",
      "Step 9025 | Loss: 0.52556 | LR: 4.29e-04\n",
      "Step 9050 | Loss: 0.49827 | LR: 4.26e-04\n",
      "Step 9075 | Loss: 0.53772 | LR: 4.24e-04\n",
      "val loss: 0.5127\n",
      "Step 9100 | Loss: 0.52320 | LR: 4.21e-04\n",
      "Step 9125 | Loss: 0.57252 | LR: 4.18e-04\n",
      "Step 9150 | Loss: 0.49746 | LR: 4.16e-04\n",
      "Step 9175 | Loss: 0.53186 | LR: 4.13e-04\n",
      "val loss: 0.5199\n",
      "Step 9200 | Loss: 0.54839 | LR: 4.11e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0006.npz\n",
      "Step 9225 | Loss: 0.52502 | LR: 4.08e-04\n",
      "Step 9250 | Loss: 0.48948 | LR: 4.06e-04\n",
      "Step 9275 | Loss: 0.51859 | LR: 4.03e-04\n",
      "val loss: 0.5197\n",
      "Step 9300 | Loss: 0.50809 | LR: 4.00e-04\n",
      "Step 9325 | Loss: 0.53883 | LR: 3.98e-04\n",
      "Step 9350 | Loss: 0.52129 | LR: 3.95e-04\n",
      "Step 9375 | Loss: 0.53017 | LR: 3.93e-04\n",
      "val loss: 0.5166\n",
      "Step 9400 | Loss: 0.50337 | LR: 3.90e-04\n",
      "Step 9425 | Loss: 0.52766 | LR: 3.88e-04\n",
      "Step 9450 | Loss: 0.52298 | LR: 3.85e-04\n",
      "Step 9475 | Loss: 0.53748 | LR: 3.83e-04\n",
      "val loss: 0.5089\n",
      "Step 9500 | Loss: 0.51722 | LR: 3.80e-04\n",
      "Step 9525 | Loss: 0.51683 | LR: 3.78e-04\n",
      "=== Step 9530 Done. Avg Loss: 0.52031 ===\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0002.npz\n",
      "Step 9550 | Loss: 0.52612 | LR: 3.75e-04\n",
      "Step 9575 | Loss: 0.51718 | LR: 3.73e-04\n",
      "val loss: 0.5209\n",
      "Step 9600 | Loss: 0.49836 | LR: 3.70e-04\n",
      "Step 9625 | Loss: 0.52912 | LR: 3.68e-04\n",
      "Step 9650 | Loss: 0.52366 | LR: 3.65e-04\n",
      "Step 9675 | Loss: 0.52331 | LR: 3.63e-04\n",
      "val loss: 0.5110\n",
      "Step 9700 | Loss: 0.51089 | LR: 3.60e-04\n",
      "Step 9725 | Loss: 0.50265 | LR: 3.58e-04\n",
      "Step 9750 | Loss: 0.54030 | LR: 3.55e-04\n",
      "Step 9775 | Loss: 0.53886 | LR: 3.53e-04\n",
      "val loss: 0.5159\n",
      "Step 9800 | Loss: 0.53418 | LR: 3.50e-04\n",
      "Step 9825 | Loss: 0.51524 | LR: 3.48e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0007.npz\n",
      "Step 9850 | Loss: 0.52009 | LR: 3.45e-04\n",
      "Step 9875 | Loss: 0.54203 | LR: 3.43e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/val/shard_k562e_val_0001.npz\n",
      "val loss: 0.5103\n",
      "Step 9900 | Loss: 0.53961 | LR: 3.40e-04\n",
      "Step 9925 | Loss: 0.48184 | LR: 3.38e-04\n",
      "Step 9950 | Loss: 0.50995 | LR: 3.35e-04\n",
      "Step 9975 | Loss: 0.51855 | LR: 3.33e-04\n",
      "val loss: 0.5224\n",
      "Step 10000 | Loss: 0.51969 | LR: 3.30e-04\n",
      "Step 10025 | Loss: 0.50287 | LR: 3.28e-04\n",
      "Step 10050 | Loss: 0.51403 | LR: 3.26e-04\n",
      "Step 10075 | Loss: 0.54183 | LR: 3.23e-04\n",
      "val loss: 0.5200\n",
      "Step 10100 | Loss: 0.52103 | LR: 3.21e-04\n",
      "Step 10125 | Loss: 0.52161 | LR: 3.18e-04\n",
      "Step 10150 | Loss: 0.53343 | LR: 3.16e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0008.npz\n",
      "Step 10175 | Loss: 0.53500 | LR: 3.13e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/val/shard_k562e_val_0001.npz\n",
      "val loss: 0.5109\n",
      "Step 10200 | Loss: 0.50887 | LR: 3.11e-04\n",
      "Step 10225 | Loss: 0.51158 | LR: 3.09e-04\n",
      "Step 10250 | Loss: 0.52522 | LR: 3.06e-04\n",
      "Step 10275 | Loss: 0.52732 | LR: 3.04e-04\n",
      "val loss: 0.5166\n",
      "Step 10300 | Loss: 0.51680 | LR: 3.01e-04\n",
      "Step 10325 | Loss: 0.54455 | LR: 2.99e-04\n",
      "Step 10350 | Loss: 0.51730 | LR: 2.97e-04\n",
      "Step 10375 | Loss: 0.51659 | LR: 2.94e-04\n",
      "val loss: 0.5173\n",
      "Step 10400 | Loss: 0.53388 | LR: 2.92e-04\n",
      "Step 10425 | Loss: 0.51916 | LR: 2.90e-04\n",
      "Step 10450 | Loss: 0.50414 | LR: 2.87e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0000.npz\n",
      "Step 10475 | Loss: 0.51111 | LR: 2.85e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5166\n",
      "Step 10500 | Loss: 0.53095 | LR: 2.82e-04\n",
      "Step 10525 | Loss: 0.53766 | LR: 2.80e-04\n",
      "Step 10550 | Loss: 0.50841 | LR: 2.78e-04\n",
      "Step 10575 | Loss: 0.50705 | LR: 2.75e-04\n",
      "val loss: 0.5248\n",
      "Step 10600 | Loss: 0.49507 | LR: 2.73e-04\n",
      "Step 10625 | Loss: 0.49279 | LR: 2.71e-04\n",
      "Step 10650 | Loss: 0.51541 | LR: 2.68e-04\n",
      "Step 10675 | Loss: 0.52769 | LR: 2.66e-04\n",
      "val loss: 0.5175\n",
      "Step 10700 | Loss: 0.53291 | LR: 2.64e-04\n",
      "Step 10725 | Loss: 0.54430 | LR: 2.62e-04\n",
      "Step 10750 | Loss: 0.53260 | LR: 2.59e-04\n",
      "Step 10775 | Loss: 0.51212 | LR: 2.57e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0006.npz\n",
      "val loss: 0.5193\n",
      "Step 10800 | Loss: 0.53770 | LR: 2.55e-04\n",
      "Step 10825 | Loss: 0.51526 | LR: 2.53e-04\n",
      "Step 10850 | Loss: 0.49261 | LR: 2.50e-04\n",
      "Step 10875 | Loss: 0.50616 | LR: 2.48e-04\n",
      "val loss: 0.5111\n",
      "Step 10900 | Loss: 0.54009 | LR: 2.46e-04\n",
      "Step 10925 | Loss: 0.49259 | LR: 2.44e-04\n",
      "Step 10950 | Loss: 0.49563 | LR: 2.41e-04\n",
      "Step 10975 | Loss: 0.52172 | LR: 2.39e-04\n",
      "val loss: 0.5185\n",
      "Step 11000 | Loss: 0.53103 | LR: 2.37e-04\n",
      "Step 11025 | Loss: 0.48316 | LR: 2.35e-04\n",
      "Step 11050 | Loss: 0.52519 | LR: 2.32e-04\n",
      "Step 11075 | Loss: 0.53968 | LR: 2.30e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0010.npz\n",
      "val loss: 0.5184\n",
      "Step 11100 | Loss: 0.51961 | LR: 2.28e-04\n",
      "Step 11125 | Loss: 0.52373 | LR: 2.26e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0005.npz\n",
      "Step 11150 | Loss: 0.52576 | LR: 2.24e-04\n",
      "Step 11175 | Loss: 0.51066 | LR: 2.22e-04\n",
      "val loss: 0.5200\n",
      "Step 11200 | Loss: 0.52264 | LR: 2.19e-04\n",
      "Step 11225 | Loss: 0.53411 | LR: 2.17e-04\n",
      "Step 11250 | Loss: 0.51088 | LR: 2.15e-04\n",
      "Step 11275 | Loss: 0.49640 | LR: 2.13e-04\n",
      "val loss: 0.5178\n",
      "Step 11300 | Loss: 0.51077 | LR: 2.11e-04\n",
      "Step 11325 | Loss: 0.54525 | LR: 2.09e-04\n",
      "Step 11350 | Loss: 0.51861 | LR: 2.07e-04\n",
      "Step 11375 | Loss: 0.51059 | LR: 2.04e-04\n",
      "val loss: 0.5148\n",
      "Step 11400 | Loss: 0.51974 | LR: 2.02e-04\n",
      "Step 11425 | Loss: 0.47553 | LR: 2.00e-04\n",
      "Step 11450 | Loss: 0.51838 | LR: 1.98e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0001.npz\n",
      "Step 11475 | Loss: 0.53495 | LR: 1.96e-04\n",
      "val loss: 0.5107\n",
      "Step 11500 | Loss: 0.53290 | LR: 1.94e-04\n",
      "Step 11525 | Loss: 0.51976 | LR: 1.92e-04\n",
      "Step 11550 | Loss: 0.49465 | LR: 1.90e-04\n",
      "Step 11575 | Loss: 0.53257 | LR: 1.88e-04\n",
      "val loss: 0.5189\n",
      "Step 11600 | Loss: 0.50232 | LR: 1.86e-04\n",
      "Step 11625 | Loss: 0.51647 | LR: 1.84e-04\n",
      "Step 11650 | Loss: 0.50979 | LR: 1.82e-04\n",
      "Step 11675 | Loss: 0.52564 | LR: 1.80e-04\n",
      "val loss: 0.5230\n",
      "Step 11700 | Loss: 0.50191 | LR: 1.78e-04\n",
      "Step 11725 | Loss: 0.52617 | LR: 1.76e-04\n",
      "Step 11750 | Loss: 0.51704 | LR: 1.74e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0003.npz\n",
      "Step 11775 | Loss: 0.52158 | LR: 1.72e-04\n",
      "val loss: 0.5148\n",
      "Step 11800 | Loss: 0.49668 | LR: 1.70e-04\n",
      "Step 11825 | Loss: 0.51579 | LR: 1.68e-04\n",
      "Step 11850 | Loss: 0.52160 | LR: 1.66e-04\n",
      "Step 11875 | Loss: 0.52251 | LR: 1.64e-04\n",
      "val loss: 0.5134\n",
      "Step 11900 | Loss: 0.52620 | LR: 1.62e-04\n",
      "Step 11925 | Loss: 0.50890 | LR: 1.60e-04\n",
      "Step 11950 | Loss: 0.52720 | LR: 1.58e-04\n",
      "Step 11975 | Loss: 0.50156 | LR: 1.57e-04\n",
      "val loss: 0.5214\n",
      "Step 12000 | Loss: 0.52799 | LR: 1.55e-04\n",
      "Step 12025 | Loss: 0.51410 | LR: 1.53e-04\n",
      "Step 12050 | Loss: 0.51869 | LR: 1.51e-04\n",
      "Step 12075 | Loss: 0.53642 | LR: 1.49e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0004.npz\n",
      "val loss: 0.5206\n",
      "Step 12100 | Loss: 0.53158 | LR: 1.47e-04\n",
      "Step 12125 | Loss: 0.53202 | LR: 1.45e-04\n",
      "Step 12150 | Loss: 0.52400 | LR: 1.44e-04\n",
      "Step 12175 | Loss: 0.52418 | LR: 1.42e-04\n",
      "val loss: 0.5124\n",
      "Step 12200 | Loss: 0.50710 | LR: 1.40e-04\n",
      "Step 12225 | Loss: 0.54504 | LR: 1.38e-04\n",
      "Step 12250 | Loss: 0.53832 | LR: 1.36e-04\n",
      "Step 12275 | Loss: 0.51312 | LR: 1.35e-04\n",
      "val loss: 0.5134\n",
      "Step 12300 | Loss: 0.50601 | LR: 1.33e-04\n",
      "Step 12325 | Loss: 0.52580 | LR: 1.31e-04\n",
      "Step 12350 | Loss: 0.54021 | LR: 1.29e-04\n",
      "Step 12375 | Loss: 0.54247 | LR: 1.28e-04\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0009.npz\n",
      "val loss: 0.5157\n",
      "Step 12400 | Loss: 0.53226 | LR: 1.26e-04\n",
      "Step 12425 | Loss: 0.52758 | LR: 1.24e-04\n",
      "Step 12450 | Loss: 0.49936 | LR: 1.22e-04\n",
      "Step 12475 | Loss: 0.50636 | LR: 1.21e-04\n",
      "val loss: 0.5182\n",
      "Step 12500 | Loss: 0.53340 | LR: 1.19e-04\n",
      "Step 12525 | Loss: 0.52082 | LR: 1.17e-04\n",
      "Step 12550 | Loss: 0.48934 | LR: 1.16e-04\n",
      "Step 12575 | Loss: 0.51251 | LR: 1.14e-04\n",
      "val loss: 0.5269\n",
      "Step 12600 | Loss: 0.50998 | LR: 1.12e-04\n",
      "Step 12625 | Loss: 0.51868 | LR: 1.11e-04\n",
      "Step 12650 | Loss: 0.55246 | LR: 1.09e-04\n",
      "Step 12675 | Loss: 0.50239 | LR: 1.07e-04\n",
      "val loss: 0.5076\n",
      "Step 12700 | Loss: 0.53260 | LR: 1.06e-04\n",
      "=== Step 12707 Done. Avg Loss: 0.52032 ===\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0006.npz\n",
      "Step 12725 | Loss: 0.50760 | LR: 1.04e-04\n",
      "Step 12750 | Loss: 0.47966 | LR: 1.03e-04\n",
      "Step 12775 | Loss: 0.51616 | LR: 1.01e-04\n",
      "val loss: 0.5270\n",
      "Step 12800 | Loss: 0.50517 | LR: 9.95e-05\n",
      "Step 12825 | Loss: 0.51429 | LR: 9.79e-05\n",
      "Step 12850 | Loss: 0.52752 | LR: 9.64e-05\n",
      "Step 12875 | Loss: 0.48182 | LR: 9.49e-05\n",
      "val loss: 0.5252\n",
      "Step 12900 | Loss: 0.51705 | LR: 9.34e-05\n",
      "Step 12925 | Loss: 0.52290 | LR: 9.18e-05\n",
      "Step 12950 | Loss: 0.52365 | LR: 9.03e-05\n",
      "Step 12975 | Loss: 0.54632 | LR: 8.89e-05\n",
      "val loss: 0.5024\n",
      "Step 13000 | Loss: 0.52373 | LR: 8.74e-05\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0004.npz\n",
      "Step 13025 | Loss: 0.51984 | LR: 8.59e-05\n",
      "Step 13050 | Loss: 0.52536 | LR: 8.45e-05\n",
      "Step 13075 | Loss: 0.53447 | LR: 8.30e-05\n",
      "val loss: 0.5086\n",
      "Step 13100 | Loss: 0.54292 | LR: 8.16e-05\n",
      "Step 13125 | Loss: 0.52383 | LR: 8.02e-05\n",
      "Step 13150 | Loss: 0.52462 | LR: 7.88e-05\n",
      "Step 13175 | Loss: 0.52533 | LR: 7.74e-05\n",
      "val loss: 0.5122\n",
      "Step 13200 | Loss: 0.51212 | LR: 7.60e-05\n",
      "Step 13225 | Loss: 0.50526 | LR: 7.46e-05\n",
      "Step 13250 | Loss: 0.53251 | LR: 7.33e-05\n",
      "Step 13275 | Loss: 0.54367 | LR: 7.19e-05\n",
      "val loss: 0.5118\n",
      "Step 13300 | Loss: 0.53151 | LR: 7.06e-05\n",
      "Step 13325 | Loss: 0.52544 | LR: 6.92e-05\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0005.npz\n",
      "Step 13350 | Loss: 0.52238 | LR: 6.79e-05\n",
      "Step 13375 | Loss: 0.54006 | LR: 6.66e-05\n",
      "val loss: 0.5092\n",
      "Step 13400 | Loss: 0.50635 | LR: 6.53e-05\n",
      "Step 13425 | Loss: 0.52272 | LR: 6.40e-05\n",
      "Step 13450 | Loss: 0.51455 | LR: 6.28e-05\n",
      "Step 13475 | Loss: 0.52503 | LR: 6.15e-05\n",
      "val loss: 0.5205\n",
      "Step 13500 | Loss: 0.50754 | LR: 6.03e-05\n",
      "Step 13525 | Loss: 0.50825 | LR: 5.90e-05\n",
      "Step 13550 | Loss: 0.55926 | LR: 5.78e-05\n",
      "Step 13575 | Loss: 0.50404 | LR: 5.66e-05\n",
      "val loss: 0.5080\n",
      "Step 13600 | Loss: 0.51888 | LR: 5.54e-05\n",
      "Step 13625 | Loss: 0.53440 | LR: 5.42e-05\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0007.npz\n",
      "Step 13650 | Loss: 0.50509 | LR: 5.31e-05\n",
      "Step 13675 | Loss: 0.51397 | LR: 5.19e-05\n",
      "loading /Users/djemec/data/jepa/v0_4/training/val/shard_k562e_val_0001.npz\n",
      "val loss: 0.5104\n",
      "Step 13700 | Loss: 0.51535 | LR: 5.08e-05\n",
      "Step 13725 | Loss: 0.52827 | LR: 4.96e-05\n",
      "Step 13750 | Loss: 0.53777 | LR: 4.85e-05\n",
      "Step 13775 | Loss: 0.50536 | LR: 4.74e-05\n",
      "val loss: 0.5176\n",
      "Step 13800 | Loss: 0.51167 | LR: 4.63e-05\n",
      "Step 13825 | Loss: 0.54950 | LR: 4.52e-05\n",
      "Step 13850 | Loss: 0.49490 | LR: 4.41e-05\n",
      "Step 13875 | Loss: 0.51197 | LR: 4.31e-05\n",
      "val loss: 0.5196\n",
      "Step 13900 | Loss: 0.52460 | LR: 4.20e-05\n",
      "Step 13925 | Loss: 0.50664 | LR: 4.10e-05\n",
      "Step 13950 | Loss: 0.50900 | LR: 3.99e-05\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0001.npz\n",
      "Step 13975 | Loss: 0.45978 | LR: 3.89e-05\n",
      "loading /Users/djemec/data/jepa/v0_4/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.5235\n",
      "Step 14000 | Loss: 0.53504 | LR: 3.79e-05\n",
      "Step 14025 | Loss: 0.53837 | LR: 3.69e-05\n",
      "Step 14050 | Loss: 0.56748 | LR: 3.60e-05\n",
      "Step 14075 | Loss: 0.49772 | LR: 3.50e-05\n",
      "val loss: 0.5224\n",
      "Step 14100 | Loss: 0.51570 | LR: 3.41e-05\n",
      "Step 14125 | Loss: 0.49159 | LR: 3.31e-05\n",
      "Step 14150 | Loss: 0.48907 | LR: 3.22e-05\n",
      "Step 14175 | Loss: 0.51621 | LR: 3.13e-05\n",
      "val loss: 0.5253\n",
      "Step 14200 | Loss: 0.55663 | LR: 3.04e-05\n",
      "Step 14225 | Loss: 0.51593 | LR: 2.95e-05\n",
      "Step 14250 | Loss: 0.51787 | LR: 2.86e-05\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0002.npz\n",
      "Step 14275 | Loss: 0.50087 | LR: 2.78e-05\n",
      "val loss: 0.5136\n",
      "Step 14300 | Loss: 0.52206 | LR: 2.69e-05\n",
      "Step 14325 | Loss: 0.53643 | LR: 2.61e-05\n",
      "Step 14350 | Loss: 0.47976 | LR: 2.53e-05\n",
      "Step 14375 | Loss: 0.52837 | LR: 2.44e-05\n",
      "val loss: 0.5174\n",
      "Step 14400 | Loss: 0.53423 | LR: 2.36e-05\n",
      "Step 14425 | Loss: 0.52255 | LR: 2.29e-05\n",
      "Step 14450 | Loss: 0.52976 | LR: 2.21e-05\n",
      "Step 14475 | Loss: 0.52609 | LR: 2.13e-05\n",
      "val loss: 0.5157\n",
      "Step 14500 | Loss: 0.53890 | LR: 2.06e-05\n",
      "Step 14525 | Loss: 0.51369 | LR: 1.99e-05\n",
      "Step 14550 | Loss: 0.50573 | LR: 1.91e-05\n",
      "Step 14575 | Loss: 0.51335 | LR: 1.84e-05\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0009.npz\n",
      "val loss: 0.5163\n",
      "Step 14600 | Loss: 0.51455 | LR: 1.77e-05\n",
      "Step 14625 | Loss: 0.52060 | LR: 1.71e-05\n",
      "Step 14650 | Loss: 0.51055 | LR: 1.64e-05\n",
      "Step 14675 | Loss: 0.52518 | LR: 1.57e-05\n",
      "val loss: 0.5092\n",
      "Step 14700 | Loss: 0.53816 | LR: 1.51e-05\n",
      "Step 14725 | Loss: 0.54368 | LR: 1.45e-05\n",
      "Step 14750 | Loss: 0.52978 | LR: 1.38e-05\n",
      "Step 14775 | Loss: 0.50597 | LR: 1.32e-05\n",
      "val loss: 0.5164\n",
      "Step 14800 | Loss: 0.52632 | LR: 1.27e-05\n",
      "Step 14825 | Loss: 0.54832 | LR: 1.21e-05\n",
      "Step 14850 | Loss: 0.53611 | LR: 1.15e-05\n",
      "Step 14875 | Loss: 0.53161 | LR: 1.10e-05\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0008.npz\n",
      "val loss: 0.5216\n",
      "Step 14900 | Loss: 0.50020 | LR: 1.04e-05\n",
      "Step 14925 | Loss: 0.51513 | LR: 9.91e-06\n",
      "Step 14950 | Loss: 0.53578 | LR: 9.41e-06\n",
      "Step 14975 | Loss: 0.52126 | LR: 8.91e-06\n",
      "val loss: 0.5139\n",
      "Step 15000 | Loss: 0.53351 | LR: 8.43e-06\n",
      "Step 15025 | Loss: 0.51597 | LR: 7.96e-06\n",
      "Step 15050 | Loss: 0.53435 | LR: 7.50e-06\n",
      "Step 15075 | Loss: 0.50468 | LR: 7.06e-06\n",
      "val loss: 0.5172\n",
      "Step 15100 | Loss: 0.51465 | LR: 6.63e-06\n",
      "Step 15125 | Loss: 0.51944 | LR: 6.22e-06\n",
      "Step 15150 | Loss: 0.52105 | LR: 5.81e-06\n",
      "Step 15175 | Loss: 0.54607 | LR: 5.43e-06\n",
      "val loss: 0.5069\n",
      "Step 15200 | Loss: 0.51800 | LR: 5.05e-06\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0003.npz\n",
      "Step 15225 | Loss: 0.49814 | LR: 4.69e-06\n",
      "Step 15250 | Loss: 0.54626 | LR: 4.34e-06\n",
      "Step 15275 | Loss: 0.52727 | LR: 4.00e-06\n",
      "val loss: 0.5186\n",
      "Step 15300 | Loss: 0.52837 | LR: 3.68e-06\n",
      "Step 15325 | Loss: 0.52241 | LR: 3.37e-06\n",
      "Step 15350 | Loss: 0.53123 | LR: 3.08e-06\n",
      "Step 15375 | Loss: 0.53314 | LR: 2.80e-06\n",
      "val loss: 0.5213\n",
      "Step 15400 | Loss: 0.52604 | LR: 2.53e-06\n",
      "Step 15425 | Loss: 0.52395 | LR: 2.28e-06\n",
      "Step 15450 | Loss: 0.49779 | LR: 2.03e-06\n",
      "Step 15475 | Loss: 0.49667 | LR: 1.81e-06\n",
      "val loss: 0.5118\n",
      "Step 15500 | Loss: 0.49199 | LR: 1.59e-06\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0000.npz\n",
      "Step 15525 | Loss: 0.50734 | LR: 1.39e-06\n",
      "Step 15550 | Loss: 0.52990 | LR: 1.20e-06\n",
      "Step 15575 | Loss: 0.52849 | LR: 1.03e-06\n",
      "val loss: 0.5116\n",
      "Step 15600 | Loss: 0.51970 | LR: 8.71e-07\n",
      "Step 15625 | Loss: 0.51294 | LR: 7.25e-07\n",
      "Step 15650 | Loss: 0.55732 | LR: 5.92e-07\n",
      "Step 15675 | Loss: 0.52069 | LR: 4.73e-07\n",
      "val loss: 0.5150\n",
      "Step 15700 | Loss: 0.53098 | LR: 3.67e-07\n",
      "Step 15725 | Loss: 0.53290 | LR: 2.74e-07\n",
      "Step 15750 | Loss: 0.52664 | LR: 1.96e-07\n",
      "Step 15775 | Loss: 0.51235 | LR: 1.30e-07\n",
      "val loss: 0.5160\n",
      "Step 15800 | Loss: 0.53582 | LR: 7.86e-08\n",
      "Step 15825 | Loss: 0.52532 | LR: 4.04e-08\n",
      "loading /Users/djemec/data/jepa/v0_4/training/train/shard_k562e_train_0010.npz\n",
      "Step 15850 | Loss: 0.52539 | LR: 1.58e-08\n",
      "Step 15875 | Loss: 0.51392 | LR: 4.69e-09\n",
      "val loss: 0.5147\n",
      "=== Step 15884 Done. Avg Loss: 0.52029 ===\n"
     ]
    }
   ],
   "source": [
    "decoder.train()\n",
    "\n",
    "for step in range(max_steps):\n",
    "\n",
    "    last_step = (step == max_steps - 1)\n",
    "\n",
    "    # once in a while evaluate our validation set loss\n",
    "    if step % 100 == 0 or last_step:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss_accum = 0.0\n",
    "            val_loss_steps = 10\n",
    "            for i in range(val_loss_steps):\n",
    "                xc, xct, xt, xtt, p_idx, p_mod, p_mode = val_loader.next_batch()\n",
    "                p_feats = input_bank[p_idx]\n",
    "                B, N = xc.shape\n",
    "\n",
    "                # get perturbation latents\n",
    "                action_latents = model.composer(p_feats, p_mod, p_mode)\n",
    "\n",
    "                # run BioJEPA\n",
    "                z_context = model.student(xc, xct, mask_idx=None)\n",
    "                target_indices = torch.arange(N, device=DEVICE).expand(B, N)\n",
    "                z_pred_mu, _ = model.predictor(z_context, action_latents, target_indices)\n",
    "\n",
    "                # run new decoder\n",
    "                pred_delta = decoder(z_pred_mu) - decoder(z_context)\n",
    "                real_delta = xt - xc\n",
    "\n",
    "                loss = F.mse_loss(pred_delta, real_delta)\n",
    "                val_loss_accum += loss.item()\n",
    "\n",
    "            avg_val_loss = val_loss_accum / val_loss_steps\n",
    "            print(f'val loss: {avg_val_loss:.4f}')\n",
    "\n",
    "        decoder.train()\n",
    "\n",
    "    # periodically save checkpoint\n",
    "    if step > 0 and  (step+1) % steps_per_epoch ==0 and not last_step:\n",
    "        # Save Checkpoint\n",
    "        torch.save({\n",
    "            'model': decoder.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'step': step\n",
    "        }, checkpoint_dir / f'biojepa_decoder_ckpt_{step}.pt')\n",
    "\n",
    "    # actual training\n",
    "\n",
    "    xc, xct, xt, xtt, p_idx, p_mod, p_mode = train_loader.next_batch()\n",
    "    p_feats = input_bank[p_idx]\n",
    "    B, N = xc.shape\n",
    "\n",
    "    # run frozen BioJEPA\n",
    "    with torch.no_grad():\n",
    "        z_context = model.student(xc, xct, mask_idx=None)\n",
    "        action_latents = model.composer(p_feats, p_mod, p_mode)\n",
    "        \n",
    "        target_indices = torch.arange(N, device=DEVICE).expand(B, N)\n",
    "        z_pred_mu, _ = model.predictor(z_context, action_latents, target_indices)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # run decoder\n",
    "    pred_case = decoder(z_pred_mu)\n",
    "    pred_control = decoder(z_context)\n",
    "\n",
    "    pred_delta = pred_case - pred_control\n",
    "    real_delta = xt - xc\n",
    "\n",
    "    # loss\n",
    "    loss = F.mse_loss(pred_delta, real_delta)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    # loss caching\n",
    "    lossi.append(loss.item())\n",
    "    total_epoch_loss += loss.item()\n",
    "\n",
    "    if step % 25 == 0:\n",
    "        print(f\"Step {step} | Loss: {loss.item():.5f} | LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "    \n",
    "    if step > 0 and (step+1) % steps_per_epoch == 0:   \n",
    "        avg_loss = total_epoch_loss / steps_per_epoch\n",
    "        print(f\"=== Step {step} Done. Avg Loss: {avg_loss:.5f} ===\")\n",
    "        total_epoch_loss = 0\n",
    "    \n",
    "    if last_step:\n",
    "        # Save Checkpoint\n",
    "        torch.save({\n",
    "            'model': decoder.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'step': step\n",
    "        }, checkpoint_dir / f'biojepa_decoder_ckpt_{step}_final.pt')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c924050-b2d8-4973-a3c1-f9dd0a3d1f03",
   "metadata": {},
   "source": [
    "**Training Loss Plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b11fde37-0cac-429d-be0c-351390bac22d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lossi)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8aaa78-e607-4bc4-8cde-49a529984c1b",
   "metadata": {},
   "source": [
    "## Trained Decoder Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "974730ff-1cda-43e0-a72f-df0986bb9daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from scipy.stats import ConstantInputWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94fd5355-31c8-47fc-a2fd-9b1fa9f91576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkDecoder(\n",
       "  (head): Linear(in_features=8, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "240d54bf-e521-433d-9c79-541dbebab4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_preds = defaultdict(list)      \n",
    "bulk_reals = defaultdict(list)     \n",
    "bulk_reals_delta = defaultdict(list)\n",
    "\n",
    "val_r2_all = []\n",
    "val_r2_top50 = []\n",
    "val_correlations = []\n",
    "val_mses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "417a9438-c9f1-4cb0-8da2-e9fe7ec831a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 2 shards for split val\n",
      "loading /Users/djemec/data/jepa/v0_4/training/val/shard_k562e_val_0001.npz\n"
     ]
    }
   ],
   "source": [
    "val_total_examples = 11044\n",
    "val_steps_per_epoch = val_total_examples // batch_size\n",
    "val_loader = TrainingLoader(batch_size=batch_size, split='val', data_dir=train_dir, device=DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27c89be7-508c-4e06-9dfc-9f4809807077",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Benchmarking:   9%|                                                      | 31/345 [00:05<00:56,  5.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading /Users/djemec/data/jepa/v0_4/training/val/shard_k562e_val_0000.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Benchmarking: 100%|| 344/345 [00:58<00:00,  5.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading /Users/djemec/data/jepa/v0_4/training/val/shard_k562e_val_0001.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Benchmarking: 100%|| 345/345 [00:59<00:00,  5.85it/s]\n"
     ]
    }
   ],
   "source": [
    "for step in tqdm(range(val_steps_per_epoch), desc='Benchmarking'):\n",
    "    \n",
    "    cont_x, cont_tot, case_x, case_tot, p_idx, p_mod, p_mode = val_loader.next_batch()\n",
    "    p_feats = input_bank[p_idx]\n",
    "    B, N = cont_x.shape\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        z_context = model.student(cont_x, cont_tot, mask_idx=None)\n",
    "        action_latents = model.composer(p_feats, p_mod, p_mode)\n",
    "        \n",
    "        target_indices = torch.arange(N, device=DEVICE).expand(B, N)\n",
    "        z_pred_mu, _ = model.predictor(z_context, action_latents, target_indices)\n",
    "        \n",
    "        pred_delta = decoder(z_pred_mu) - decoder(z_context)\n",
    "        real_delta = case_x - cont_x\n",
    "\n",
    "        pred_absolute = cont_x + pred_delta\n",
    "    \n",
    "    pred_delta_np = pred_delta.cpu().numpy()\n",
    "    real_delta_np = real_delta.cpu().numpy()\n",
    "\n",
    "    pred_abs_np = pred_absolute.cpu().numpy()\n",
    "    real_abs_np = case_x.cpu().numpy()\n",
    "    \n",
    "    act_id_np = p_idx.cpu().numpy().flatten()\n",
    "\n",
    "    for i in range(B):\n",
    "        p_delta = pred_delta_np[i]\n",
    "        t_delta = real_delta_np[i]\n",
    "        pid = act_id_np[i] \n",
    "\n",
    "        val_mses.append(np.mean((p_delta - t_delta)**2))\n",
    "\n",
    "        top_20_idx = np.argsort(np.abs(t_delta))[-20:]\n",
    "        p_top = p_delta[top_20_idx]\n",
    "        t_top = t_delta[top_20_idx]\n",
    "        \n",
    "        if np.std(p_top) > 1e-9 and np.std(t_top) > 1e-9:\n",
    "            corr, _ = pearsonr(p_top, t_top)\n",
    "            val_correlations.append(0.0 if np.isnan(corr) else corr)\n",
    "        else:\n",
    "            val_correlations.append(0.0)\n",
    "\n",
    "        bulk_preds[pid].append(pred_abs_np[i])\n",
    "        bulk_reals[pid].append(real_abs_np[i])\n",
    "        bulk_reals_delta[pid].append(real_delta_np[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c99d0783-9717-4fd2-933b-9b1d42a66cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pid in bulk_preds:\n",
    "    p_mean = np.mean(np.stack(bulk_preds[pid]), axis=0)\n",
    "    t_mean = np.mean(np.stack(bulk_reals[pid]), axis=0)\n",
    "    t_mean_delta = np.mean(np.stack(bulk_reals_delta[pid]), axis=0)\n",
    "\n",
    "    if np.std(t_mean) > 1e-9:\n",
    "        val_r2_all.append(r2_score(t_mean, p_mean))\n",
    "    \n",
    "    top_50_idx = np.argsort(np.abs(t_mean_delta))[-50:] \n",
    "    \n",
    "    val_r2_top50.append(r2_score(t_mean[top_50_idx], p_mean[top_50_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c9fa6eb-5426-42e6-bc77-4a796c2eba4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_mean_mse = np.mean(val_mses)\n",
    "val_mean_corr = np.mean(val_correlations)\n",
    "val_mean_r2_all = np.mean(val_r2_all)\n",
    "val_median_r2_all = np.median(val_r2_all)\n",
    "val_mean_r2_top50 = np.mean(val_r2_top50)\n",
    "val_median_r2_top50 = np.median(val_r2_top50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "750afdb9-ba92-4a7a-874d-0cc221525de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global MSE: 0.5162\n",
      "Top-20 Pearson R: 0.8712\n",
      "R^2 (All Genes): Mean: 0.8663, Median: 0.8741\n",
      "R^2 (Top 50 DEGs): Mean: -0.1333, Median: 0.0345\n"
     ]
    }
   ],
   "source": [
    "print(f'Global MSE: {val_mean_mse:.4f}')\n",
    "print(f'Top-20 Pearson R: {val_mean_corr:.4f}')\n",
    "print(f'R^2 (All Genes): Mean: {val_mean_r2_all:.4f}, Median: {val_median_r2_all:.4f}')\n",
    "print(f'R^2 (Top 50 DEGs): Mean: {val_mean_r2_top50:.4f}, Median: {val_median_r2_top50:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6b2741-9067-4cee-b37f-2ee471d3c7e6",
   "metadata": {},
   "source": [
    "## Trained Decoder Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ecd5cb77-d9a4-43d8-ad77-b6d102d00caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_preds = defaultdict(list)\n",
    "bulk_reals = defaultdict(list)\n",
    "bulk_reals_delta = defaultdict(list)\n",
    "\n",
    "test_r2_all = []\n",
    "test_r2_top50 = []\n",
    "test_correlations = []\n",
    "test_mses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "17c58931-04dc-4c75-a84d-23fbf3f7abc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 4 shards for split test\n",
      "loading /Users/djemec/data/jepa/v0_4/training/test/shard_k562e_test_0003.npz\n"
     ]
    }
   ],
   "source": [
    "test_total_examples = 38829\n",
    "test_loader = TrainingLoader(batch_size=batch_size, split='test', data_dir=train_dir, device=DEVICE)\n",
    "test_steps_per_epoch = test_total_examples // batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9022a3d0-68f1-4de8-bb53-33e5bd05e212",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Benchmarking:  23%|                                            | 275/1213 [00:47<02:36,  6.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading /Users/djemec/data/jepa/v0_4/training/test/shard_k562e_test_0000.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Benchmarking:  48%|                              | 588/1213 [01:40<01:42,  6.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading /Users/djemec/data/jepa/v0_4/training/test/shard_k562e_test_0002.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Benchmarking:  74%|               | 900/1213 [02:34<00:52,  5.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading /Users/djemec/data/jepa/v0_4/training/test/shard_k562e_test_0001.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Benchmarking: 100%|| 1213/1213 [03:25<00:00,  5.90it/s]\n"
     ]
    }
   ],
   "source": [
    "for step in tqdm(range(test_steps_per_epoch), desc='Benchmarking'):\n",
    "    \n",
    "    cont_x, cont_tot, case_x, case_tot, p_idx, p_mod, p_mode = test_loader.next_batch()\n",
    "    p_feats = input_bank[p_idx]\n",
    "    B, N = cont_x.shape\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        z_context = model.student(cont_x, cont_tot, mask_idx=None)\n",
    "        action_latents = model.composer(p_feats, p_mod, p_mode)\n",
    "        \n",
    "        target_indices = torch.arange(N, device=DEVICE).expand(B, N)\n",
    "        z_pred_mu, _ = model.predictor(z_context, action_latents, target_indices)\n",
    "        \n",
    "        pred_delta = decoder(z_pred_mu) - decoder(z_context)\n",
    "        real_delta = case_x - cont_x\n",
    "\n",
    "        pred_absolute = cont_x + pred_delta\n",
    "        pred_absolute = torch.clamp(pred_absolute, min=0.0)\n",
    "    \n",
    "    pred_delta_np = pred_delta.cpu().numpy()\n",
    "    real_delta_np = real_delta.cpu().numpy()\n",
    "\n",
    "    pred_abs_np = pred_absolute.cpu().numpy()\n",
    "    real_abs_np = case_x.cpu().numpy()\n",
    "    \n",
    "    act_id_np = p_idx.cpu().numpy().flatten()\n",
    "\n",
    "\n",
    "    for i in range(B):\n",
    "        p_delta = pred_delta_np[i]\n",
    "        t_delta = real_delta_np[i]\n",
    "        pid = act_id_np[i] \n",
    "\n",
    "        test_mses.append(np.mean((p_delta - t_delta)**2))\n",
    "\n",
    "        top_20_idx = np.argsort(np.abs(t_delta))[-20:]\n",
    "        p_top = p_delta[top_20_idx]\n",
    "        t_top = t_delta[top_20_idx]\n",
    "        \n",
    "        if np.std(p_top) > 1e-9 and np.std(t_top) > 1e-9:\n",
    "            corr, _ = pearsonr(p_top, t_top)\n",
    "            test_correlations.append(0.0 if np.isnan(corr) else corr)\n",
    "        else:\n",
    "            test_correlations.append(0.0)\n",
    "\n",
    "        bulk_preds[pid].append(pred_abs_np[i])\n",
    "        bulk_reals[pid].append(real_abs_np[i])\n",
    "        bulk_reals_delta[pid].append(real_delta_np[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fa48799e-604f-4100-bb2b-3e1f5ac5e0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pid in bulk_preds:\n",
    "    p_mean = np.mean(np.stack(bulk_preds[pid]), axis=0)\n",
    "    t_mean = np.mean(np.stack(bulk_reals[pid]), axis=0)\n",
    "    t_mean_delta = np.mean(np.stack(bulk_reals_delta[pid]), axis=0)\n",
    "\n",
    "    if np.std(t_mean) > 1e-9:\n",
    "        test_r2_all.append(r2_score(t_mean, p_mean))\n",
    "    \n",
    "    top_50_idx = np.argsort(np.abs(t_mean_delta))[-50:] \n",
    "    \n",
    "    test_r2_top50.append(r2_score(t_mean[top_50_idx], p_mean[top_50_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "efaae9f8-c745-4044-b9cd-9d3cf0e944e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mean_mse = np.mean(test_mses)\n",
    "test_mean_corr = np.mean(test_correlations)\n",
    "test_mean_r2_all = np.mean(test_r2_all)\n",
    "test_median_r2_all = np.median(test_r2_all)\n",
    "test_mean_r2_top50 = np.mean(test_r2_top50)\n",
    "test_median_r2_top50 = np.median(test_r2_top50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "40ee16bc-f9f8-4ab6-bc12-9e72fafe1b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global MSE: 0.5140\n",
      "Top-20 Pearson R: 0.8704\n",
      "R^2 (All Genes) - Mean: 0.8665, Median: 0.8749\n",
      "R^2 (Top 50 DEGs) - Mean: -0.0848, Median: 0.1454\n"
     ]
    }
   ],
   "source": [
    "print(f'Global MSE: {test_mean_mse:.4f}')\n",
    "print(f'Top-20 Pearson R: {test_mean_corr:.4f}')\n",
    "print(f'R^2 (All Genes) - Mean: {test_mean_r2_all:.4f}, Median: {test_median_r2_all:.4f}')\n",
    "print(f'R^2 (Top 50 DEGs) - Mean: {test_mean_r2_top50:.4f}, Median: {test_median_r2_top50:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4370f0b7-232f-4fe4-8777-3d08ab4519be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Predicted Shift magnitude: 0.5359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/3hmfwyf51rx5tzm9phb1b6cc0000gn/T/ipykernel_42765/2102659733.py:1: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  diff = np.abs(pred_absolute - cont_x).mean()\n"
     ]
    }
   ],
   "source": [
    "diff = np.abs(pred_absolute - cont_x).mean()\n",
    "print(f\"Average Predicted Shift magnitude: {diff:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0803aec4-f2d9-4557-93df-6fbbca94a90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Change | Pred | True | Error | How much of change missed\n",
      "2.1740 | 1.1561 | 2.1740 | -1.0180 | -0.47\n",
      "2.1740 | 1.1098 | 2.1740 | -1.0642 | -0.49\n",
      "2.1740 | 1.2500 | 2.1740 | -0.9240 | -0.43\n",
      "2.1740 | 0.6356 | 2.1740 | -1.5384 | -0.71\n",
      "2.1740 | 1.3742 | 2.1740 | -0.7999 | -0.37\n",
      "2.1740 | 1.6198 | 2.1740 | -0.5543 | -0.25\n",
      "2.1740 | 0.5321 | 2.1740 | -1.6419 | -0.76\n",
      "2.1740 | 0.2342 | 2.1740 | -1.9398 | -0.89\n",
      "2.1740 | 0.8737 | 2.1740 | -1.3003 | -0.60\n",
      "2.1740 | 1.1172 | 2.1740 | -1.0568 | -0.49\n",
      "2.1740 | 0.9371 | 2.1740 | -1.2369 | -0.57\n",
      "2.1740 | 0.5915 | 2.1740 | -1.5826 | -0.73\n",
      "2.1740 | 0.3481 | 2.1740 | -1.8260 | -0.84\n",
      "2.1740 | 1.4188 | 2.1740 | -0.7552 | -0.35\n",
      "2.1740 | 0.5103 | 2.1740 | -1.6637 | -0.77\n",
      "2.1740 | 1.0666 | 2.1740 | -1.1074 | -0.51\n",
      "2.1740 | 0.3110 | 2.1740 | -1.8630 | -0.86\n",
      "2.1740 | 0.8885 | 2.1740 | -1.2855 | -0.59\n",
      "2.1740 | 0.5415 | 2.1740 | -1.6326 | -0.75\n",
      "2.1740 | 0.5716 | 2.1740 | -1.6025 | -0.74\n",
      "2.1740 | 0.3640 | 2.1740 | -1.8100 | -0.83\n",
      "2.1740 | 0.4736 | 2.1740 | -1.7005 | -0.78\n",
      "2.1740 | 0.3873 | 2.1740 | -1.7867 | -0.82\n",
      "-2.2395 | 1.2091 | 0.0000 | 1.2091 | -0.54\n",
      "-2.2395 | 1.3002 | 0.0000 | 1.3002 | -0.58\n",
      "-2.2395 | 1.8266 | 0.0000 | 1.8266 | -0.82\n",
      "-2.2395 | 1.3152 | 0.0000 | 1.3152 | -0.59\n",
      "-2.2395 | 1.5518 | 0.0000 | 1.5518 | -0.69\n",
      "-2.2395 | 1.1932 | 0.0000 | 1.1932 | -0.53\n",
      "-2.2395 | 1.3498 | 0.0000 | 1.3498 | -0.60\n",
      "-2.2395 | 0.9002 | 0.0000 | 0.9002 | -0.40\n",
      "-2.2395 | 1.2288 | 0.0000 | 1.2288 | -0.55\n",
      "2.3052 | 0.9734 | 2.3052 | -1.3318 | -0.58\n",
      "2.3052 | 0.8370 | 2.3052 | -1.4682 | -0.64\n",
      "2.3052 | 0.6862 | 2.3052 | -1.6189 | -0.70\n",
      "-2.3135 | 1.9523 | 0.0000 | 1.9523 | -0.84\n",
      "-2.3135 | 0.3000 | 0.0000 | 0.3000 | -0.13\n",
      "-2.3135 | 1.9842 | 0.0000 | 1.9842 | -0.86\n",
      "-2.3135 | 1.3649 | 0.0000 | 1.3649 | -0.59\n",
      "-2.3135 | 1.1521 | 0.0000 | 1.1521 | -0.50\n",
      "-2.3735 | 1.6679 | 0.0000 | 1.6679 | -0.70\n",
      "2.4006 | 1.1923 | 2.4006 | -1.2083 | -0.50\n",
      "2.4006 | 0.1095 | 2.4006 | -2.2912 | -0.95\n",
      "-2.4236 | 1.5567 | 0.0000 | 1.5567 | -0.64\n",
      "-2.4236 | 1.9085 | 0.0000 | 1.9085 | -0.79\n",
      "-2.4664 | 1.2513 | 0.0000 | 1.2513 | -0.51\n",
      "2.4747 | 0.4551 | 2.4747 | -2.0196 | -0.82\n",
      "2.4747 | 1.6269 | 2.4747 | -0.8478 | -0.34\n",
      "-2.5035 | 2.6727 | 0.0000 | 2.6727 | -1.07\n",
      "-2.5035 | 1.5190 | 0.0000 | 1.5190 | -0.61\n"
     ]
    }
   ],
   "source": [
    "## most of these are either turning on (true change == true) or off (true == 0) \n",
    "print(f'True Change | Pred | True | Error | How much of change missed')\n",
    "for i in top_50_idx:\n",
    "    print(f'{t_mean_delta[i]:.4f} | {p_mean[i]:.4f} | {t_mean[i]:.4f} | {(p_mean[i] - t_mean[i]):.4f} | {((p_mean[i] - t_mean[i])/(t_mean_delta[i])):.2f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f469bcf9-4fc7-4cd3-b3e2-46bd7b7fa06b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
