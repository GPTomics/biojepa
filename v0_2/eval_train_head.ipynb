{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71522d3b-8805-48e6-bb36-1c9a7bd15482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import pearsonr\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "\n",
    "import biojepa_ac_model as model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe98e32-54b3-4da6-adf9-8b3e76e3bc0b",
   "metadata": {},
   "source": [
    "## BioJEPA Model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bae5c1c-d7c4-4216-ab99-42a26b7aee75",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f67335c-63ea-4f80-9c10-96b4d3e09c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cpu\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    device = 'cpu'\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(1337)\n",
    "        device = 'cuda'\n",
    "    # elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    #     device = 'mps'\n",
    "    print(f'using {device}')\n",
    "    return device\n",
    "\n",
    "DEVICE = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5ec64e1-0d7a-4dee-b478-b99f901ce9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "n_embd = 8\n",
    "n_pathways = 1024\n",
    "training_file_chunk = 25000\n",
    "pretraining_file_chunk = 50000\n",
    "n_heads = 1\n",
    "n_layers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "025d6281-f460-46eb-89d9-842d1ca6fa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('/Users/djemec/data/jepa/v0_2')\n",
    "train_dir = data_dir / 'training' \n",
    "mask_path = data_dir / 'binary_pathway_mask.npy'\n",
    "metadata_path = data_dir / 'perturbation_map.json'\n",
    "checkpoint_dir = data_dir / 'checkpoint'\n",
    "gene_names_path = data_dir / 'gene_names.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9671701-5cc4-4016-a354-b71238c98947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Pathway Mask...\n",
      "Mask Loaded: 5000 Genes -> 1024 Pathways\n",
      "Loaded 1088 perturbations\n",
      "Loaded 5000 genes\n"
     ]
    }
   ],
   "source": [
    "print('Loading Pathway Mask...')\n",
    "binary_mask = np.load(mask_path)\n",
    "n_genes, n_pathways = binary_mask.shape\n",
    "print(f'Mask Loaded: {n_genes} Genes -> {n_pathways} Pathways')\n",
    "\n",
    "with open(metadata_path, 'r') as f:\n",
    "    pert_map = json.load(f)\n",
    "id_to_pert = {v: k for k, v in pert_map.items()}\n",
    "print(f'Loaded {len(id_to_pert.keys())} perturbations')\n",
    "\n",
    "with open(gene_names_path, 'r') as f:\n",
    "    gene_names = json.load(f)\n",
    "print(f'Loaded {len(gene_names)} genes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fe46b1b-aff7-4355-909f-9f71761f11c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = model.BioJepaConfig(\n",
    "    mask_matrix=binary_mask, \n",
    "    num_genes=n_genes,\n",
    "    num_pathways=n_pathways,\n",
    "    embed_dim=n_embd,\n",
    "    n_layer=n_layers,\n",
    "    heads=n_heads,\n",
    "    n_pre_layer = n_layers\n",
    ")\n",
    "model = model.BioJepa(config).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a609e628-943e-41c2-a4d7-a71b9075a640",
   "metadata": {},
   "source": [
    "**Load Checkpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4aac628e-6ab7-4768-955a-3f99e79f58ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = checkpoint_dir / 'bio_jepa_ckpt_6353_final.pt'\n",
    "checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "\n",
    "keys = model.load_state_dict(checkpoint['model'])\n",
    "keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8287a64f-9d0d-4fbe-8c9b-a03c463a1487",
   "metadata": {},
   "source": [
    "**Freeze Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63812672-368f-483a-be2a-8b65cc849a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c98b4a-5df8-4894-9c56-ae0dbcbc3acd",
   "metadata": {},
   "source": [
    "## Build Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3dec1f3-d9cc-4849-aaa8-6a7d60199432",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BenchmarkDecoderConfig:\n",
    "    embed_dim: int = 384\n",
    "    num_pathways: int = 1024\n",
    "    num_genes: int = 4096\n",
    "    \n",
    "class BenchmarkDecoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # Step 1: Collapse the embedding dimension (384 -> 1)\n",
    "        # This asks: \"How active is this pathway overall?\"\n",
    "        self.pool = nn.Linear(config.embed_dim, 1) \n",
    "        \n",
    "        # Step 2: Decode Pathway Activity -> Gene Expression\n",
    "        # This learns the specific contribution of each pathway to each gene\n",
    "        self.decode = nn.Linear(config.num_pathways, config.num_genes) \n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None: \n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        \n",
    "    def forward(self, latents):\n",
    "        # latents: [Batch, 1024, 384]\n",
    "        \n",
    "        # 1. Calculate Pathway Scores\n",
    "        # [B, 1024, 384] -> [B, 1024, 1] -> [B, 1024]\n",
    "        scores = self.pool(latents).squeeze(-1)\n",
    "        \n",
    "        # 2. Project to Genes\n",
    "        # [B, 1024] @ [1024, 2000] -> [B, 2000]\n",
    "        gene_preds = self.decode(scores)\n",
    "        \n",
    "        return gene_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c316fe59-e4aa-4d12-9db4-2e38781f53bc",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bfad6fe-82ef-4019-b0ae-0338089f11c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_shard(filename):\n",
    "    print(f'loading {filename}') # Optional: reduce noise\n",
    "    with np.load(filename) as data:\n",
    "        # Load all arrays into memory\n",
    "        # We convert to correct types immediately to save hassle later\n",
    "        control_x = data['control'].astype(np.float32)\n",
    "        control_tot = data['control_total'].astype(np.float32)\n",
    "        case_x = data['case'].astype(np.float32)\n",
    "        case_tot = data['case_total'].astype(np.float32)\n",
    "        action_ids = data['action_ids'].astype(np.int64)\n",
    "        \n",
    "    return control_x, control_tot, case_x, case_tot, action_ids\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, batch, split, device, tok_dir):\n",
    "        self.batch = batch\n",
    "        self.split = split\n",
    "        self.device = device\n",
    "        \n",
    "        # 1. Find Shards\n",
    "        data_root = tok_dir / f'{split}'\n",
    "        shards = list(data_root.glob('*.npz'))\n",
    "\n",
    "        self.total_files = len(shards)\n",
    "        self.shards = sorted(shards)\n",
    "\n",
    "        assert len(shards) > 0, f'no shards found for split {split}'\n",
    "        print(f'found {len(shards)} shards for split {split}')\n",
    "        \n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # Create a randomized queue of shards\n",
    "        self.remaining_shards = list(self.shards)\n",
    "        random.shuffle(self.remaining_shards)\n",
    "        \n",
    "        self.current_shard_idx = -1\n",
    "        self.load_next_shard()\n",
    "\n",
    "    def load_next_shard(self):\n",
    "        self.current_shard_idx += 1\n",
    "        \n",
    "        # If we ran out of shards, reset (Epoch done)\n",
    "        if self.current_shard_idx >= len(self.remaining_shards):\n",
    "            self.reset() # This resets shard_idx to -1 and reshuffles\n",
    "            return \n",
    "\n",
    "        # Load the file\n",
    "        filename = self.remaining_shards[self.current_shard_idx]\n",
    "        self.data_tuple = load_shard(filename)\n",
    "        \n",
    "        # Shuffle the items INSIDE the shard\n",
    "        # This is critical so we don't just memorize the sorted order of the shard\n",
    "        n_samples = len(self.data_tuple[0])\n",
    "        self.perm = np.random.permutation(n_samples)\n",
    "        self.current_position = 0\n",
    "        self.total_samples_in_shard = n_samples\n",
    "\n",
    "    def next_batch(self):\n",
    "        batch = self.batch\n",
    "        \n",
    "        # Check if we have enough data left in current shard\n",
    "        if self.current_position + batch > self.total_samples_in_shard:\n",
    "            self.load_next_shard()\n",
    "            # Recursively call to get batch from the new shard\n",
    "            return self.next_batch()\n",
    "            \n",
    "        # Get indices for this batch\n",
    "        indices = self.perm[self.current_position : self.current_position + batch]\n",
    "        self.current_position += batch\n",
    "        \n",
    "        # Slice data using the shuffled indices\n",
    "        # data_tuple structure: (xc, xct, xt, xtt, aid)\n",
    "        batch_cont_x  = torch.from_numpy(self.data_tuple[0][indices]).to(self.device)\n",
    "        batch_cont_tot = torch.from_numpy(self.data_tuple[1][indices]).to(self.device)\n",
    "        batch_case_x  = torch.from_numpy(self.data_tuple[2][indices]).to(self.device)\n",
    "        batch_case_t = torch.from_numpy(self.data_tuple[3][indices]).to(self.device)\n",
    "        batch_aid = torch.from_numpy(self.data_tuple[4][indices]).to(self.device)\n",
    "        \n",
    "        return batch_cont_x, batch_cont_tot, batch_case_x, batch_case_t, batch_aid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7f7811-5727-45e3-8194-505072bb16eb",
   "metadata": {},
   "source": [
    "**Data Loader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e1d5000-f278-4f9b-9227-9554f31e9a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 5 shards for split train\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0001.npz\n",
      "found 1 shards for split val\n",
      "loading /Users/djemec/data/jepa/v0_2/training/val/shard_k562e_val_0000.npz\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoaderLite(batch=BATCH_SIZE, split='train', device=DEVICE, tok_dir=train_dir)\n",
    "val_loader = DataLoaderLite(batch=BATCH_SIZE, split='val', device=DEVICE, tok_dir=train_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9025bf-05d7-4ff8-ae9a-c1b87471c3e5",
   "metadata": {},
   "source": [
    "## Training Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f484f1f2-2933-413d-80c0-f0c970656453",
   "metadata": {},
   "source": [
    "### Training Config/Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad700ee3-e710-43de-be89-4003c922a41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_decoder = 1e-2\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0494e7de-e96d-4989-b47b-9229c050a263",
   "metadata": {},
   "source": [
    "**Initialize Decoder** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74224c15-ff49-41d9-9552-b8ec9ed83dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BenchmarkDecoderConfig(\n",
    "    embed_dim= n_embd,\n",
    "    num_pathways= n_pathways,\n",
    "    num_genes= n_genes\n",
    ")\n",
    "\n",
    "decoder = BenchmarkDecoder(config).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2da30fa-9c44-40be-921c-5184c764f0e5",
   "metadata": {},
   "source": [
    "**Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b197e689-8b98-4cf9-8382-6116bf2ce440",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(decoder.parameters(), lr=lr_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958f11cf-8c9f-49b9-b526-74be69d2126f",
   "metadata": {},
   "source": [
    "**Training Lenght**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "009526cc-7703-497d-87cb-4a50f9c1c1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_total_examples = 101682\n",
    "val_total_examples = 11044\n",
    "test_total_examples = 38829"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "766a254e-2fd7-4274-91c4-7ceab21d8d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3177, 15885)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch = train_total_examples // BATCH_SIZE\n",
    "max_steps = epochs * steps_per_epoch\n",
    "steps_per_epoch, max_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cf4803-b42e-4d1b-89db-c9b43d6e3558",
   "metadata": {},
   "source": [
    "**Scheduler**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c683627d-3da6-45e9-9827-06d8586a1f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=lr_decoder, total_steps=max_steps, pct_start=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15c5c7c-b741-486e-9ce2-99c2e253b05d",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82cb876c-e45d-43f7-94aa-2fa7f44b15c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossi = []\n",
    "step = 0\n",
    "total_epoch_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69468a01-365b-42b9-b041-5d6969e5b4ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.9118\n",
      "Step 0 | Loss: 0.89442 | LR: 4.00e-04\n",
      "Step 25 | Loss: 0.95606 | LR: 4.25e-04\n",
      "Step 50 | Loss: 0.91990 | LR: 4.98e-04\n",
      "Step 75 | Loss: 0.93011 | LR: 6.16e-04\n",
      "test loss: 0.9239\n",
      "Step 100 | Loss: 0.91284 | LR: 7.79e-04\n",
      "Step 125 | Loss: 0.93259 | LR: 9.85e-04\n",
      "Step 150 | Loss: 0.89304 | LR: 1.23e-03\n",
      "Step 175 | Loss: 0.91852 | LR: 1.52e-03\n",
      "test loss: 0.9569\n",
      "Step 200 | Loss: 0.92217 | LR: 1.84e-03\n",
      "Step 225 | Loss: 0.89670 | LR: 2.20e-03\n",
      "Step 250 | Loss: 0.91626 | LR: 2.58e-03\n",
      "Step 275 | Loss: 0.91368 | LR: 2.99e-03\n",
      "test loss: 0.9492\n",
      "Step 300 | Loss: 0.93870 | LR: 3.43e-03\n",
      "Step 325 | Loss: 0.91577 | LR: 3.87e-03\n",
      "Step 350 | Loss: 0.89768 | LR: 4.34e-03\n",
      "Step 375 | Loss: 0.93204 | LR: 4.81e-03\n",
      "test loss: 0.9593\n",
      "Step 400 | Loss: 0.90107 | LR: 5.28e-03\n",
      "Step 425 | Loss: 0.88044 | LR: 5.76e-03\n",
      "Step 450 | Loss: 0.89869 | LR: 6.23e-03\n",
      "Step 475 | Loss: 0.90471 | LR: 6.68e-03\n",
      "test loss: 1.0188\n",
      "Step 500 | Loss: 0.85434 | LR: 7.13e-03\n",
      "Step 525 | Loss: 0.85794 | LR: 7.55e-03\n",
      "Step 550 | Loss: 0.88095 | LR: 7.96e-03\n",
      "Step 575 | Loss: 0.90384 | LR: 8.33e-03\n",
      "test loss: 1.2553\n",
      "Step 600 | Loss: 0.92521 | LR: 8.67e-03\n",
      "Step 625 | Loss: 0.85850 | LR: 8.98e-03\n",
      "Step 650 | Loss: 0.89144 | LR: 9.26e-03\n",
      "Step 675 | Loss: 0.92169 | LR: 9.49e-03\n",
      "test loss: 1.6550\n",
      "Step 700 | Loss: 0.88718 | LR: 9.68e-03\n",
      "Step 725 | Loss: 0.89191 | LR: 9.83e-03\n",
      "Step 750 | Loss: 0.92855 | LR: 9.93e-03\n",
      "Step 775 | Loss: 0.92781 | LR: 9.99e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0000.npz\n",
      "test loss: 2.7161\n",
      "Step 800 | Loss: 0.88871 | LR: 1.00e-02\n",
      "Step 825 | Loss: 0.92149 | LR: 1.00e-02\n",
      "Step 850 | Loss: 0.90890 | LR: 1.00e-02\n",
      "Step 875 | Loss: 0.88440 | LR: 1.00e-02\n",
      "test loss: 3.0391\n",
      "Step 900 | Loss: 0.84198 | LR: 1.00e-02\n",
      "Step 925 | Loss: 0.89307 | LR: 1.00e-02\n",
      "Step 950 | Loss: 0.88393 | LR: 1.00e-02\n",
      "Step 975 | Loss: 0.87688 | LR: 1.00e-02\n",
      "test loss: 3.3776\n",
      "Step 1000 | Loss: 0.84851 | LR: 1.00e-02\n",
      "Step 1025 | Loss: 0.89669 | LR: 9.99e-03\n",
      "Step 1050 | Loss: 0.85824 | LR: 9.99e-03\n",
      "Step 1075 | Loss: 0.90136 | LR: 9.99e-03\n",
      "test loss: 2.9785\n",
      "Step 1100 | Loss: 0.88313 | LR: 9.99e-03\n",
      "Step 1125 | Loss: 0.92992 | LR: 9.99e-03\n",
      "Step 1150 | Loss: 0.88491 | LR: 9.99e-03\n",
      "Step 1175 | Loss: 0.87076 | LR: 9.98e-03\n",
      "test loss: 2.6095\n",
      "Step 1200 | Loss: 0.87075 | LR: 9.98e-03\n",
      "Step 1225 | Loss: 0.87467 | LR: 9.98e-03\n",
      "Step 1250 | Loss: 0.90421 | LR: 9.98e-03\n",
      "Step 1275 | Loss: 0.86433 | LR: 9.97e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/training/val/shard_k562e_val_0000.npz\n",
      "test loss: 2.6842\n",
      "Step 1300 | Loss: 0.90517 | LR: 9.97e-03\n",
      "Step 1325 | Loss: 0.90158 | LR: 9.97e-03\n",
      "Step 1350 | Loss: 0.93411 | LR: 9.97e-03\n",
      "Step 1375 | Loss: 0.90197 | LR: 9.96e-03\n",
      "test loss: 2.4623\n",
      "Step 1400 | Loss: 0.88118 | LR: 9.96e-03\n",
      "Step 1425 | Loss: 0.87454 | LR: 9.96e-03\n",
      "Step 1450 | Loss: 0.84116 | LR: 9.95e-03\n",
      "Step 1475 | Loss: 0.87758 | LR: 9.95e-03\n",
      "test loss: 1.9387\n",
      "Step 1500 | Loss: 0.90084 | LR: 9.95e-03\n",
      "Step 1525 | Loss: 0.86184 | LR: 9.94e-03\n",
      "Step 1550 | Loss: 0.82210 | LR: 9.94e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0003.npz\n",
      "Step 1575 | Loss: 0.88250 | LR: 9.93e-03\n",
      "test loss: 1.7325\n",
      "Step 1600 | Loss: 0.85857 | LR: 9.93e-03\n",
      "Step 1625 | Loss: 0.83702 | LR: 9.93e-03\n",
      "Step 1650 | Loss: 0.85571 | LR: 9.92e-03\n",
      "Step 1675 | Loss: 0.86313 | LR: 9.92e-03\n",
      "test loss: 1.5631\n",
      "Step 1700 | Loss: 0.87440 | LR: 9.91e-03\n",
      "Step 1725 | Loss: 0.84458 | LR: 9.91e-03\n",
      "Step 1750 | Loss: 0.84445 | LR: 9.90e-03\n",
      "Step 1775 | Loss: 0.88324 | LR: 9.90e-03\n",
      "test loss: 1.5191\n",
      "Step 1800 | Loss: 0.81998 | LR: 9.89e-03\n",
      "Step 1825 | Loss: 0.91279 | LR: 9.88e-03\n",
      "Step 1850 | Loss: 0.85642 | LR: 9.88e-03\n",
      "Step 1875 | Loss: 0.87464 | LR: 9.87e-03\n",
      "test loss: 1.3367\n",
      "Step 1900 | Loss: 0.83454 | LR: 9.87e-03\n",
      "Step 1925 | Loss: 0.89309 | LR: 9.86e-03\n",
      "Step 1950 | Loss: 0.88578 | LR: 9.86e-03\n",
      "Step 1975 | Loss: 0.86922 | LR: 9.85e-03\n",
      "test loss: 1.2218\n",
      "Step 2000 | Loss: 0.86816 | LR: 9.84e-03\n",
      "Step 2025 | Loss: 0.87877 | LR: 9.84e-03\n",
      "Step 2050 | Loss: 0.85152 | LR: 9.83e-03\n",
      "Step 2075 | Loss: 0.84239 | LR: 9.82e-03\n",
      "test loss: 1.1310\n",
      "Step 2100 | Loss: 0.84038 | LR: 9.82e-03\n",
      "Step 2125 | Loss: 0.87728 | LR: 9.81e-03\n",
      "Step 2150 | Loss: 0.86039 | LR: 9.80e-03\n",
      "Step 2175 | Loss: 0.83481 | LR: 9.79e-03\n",
      "test loss: 1.1488\n",
      "Step 2200 | Loss: 0.85089 | LR: 9.79e-03\n",
      "Step 2225 | Loss: 0.84696 | LR: 9.78e-03\n",
      "Step 2250 | Loss: 0.88216 | LR: 9.77e-03\n",
      "Step 2275 | Loss: 0.89559 | LR: 9.76e-03\n",
      "test loss: 1.0999\n",
      "Step 2300 | Loss: 0.86082 | LR: 9.76e-03\n",
      "Step 2325 | Loss: 0.85156 | LR: 9.75e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0002.npz\n",
      "Step 2350 | Loss: 0.82713 | LR: 9.74e-03\n",
      "Step 2375 | Loss: 0.84972 | LR: 9.73e-03\n",
      "test loss: 1.1123\n",
      "Step 2400 | Loss: 0.84795 | LR: 9.72e-03\n",
      "Step 2425 | Loss: 0.84273 | LR: 9.71e-03\n",
      "Step 2450 | Loss: 0.80529 | LR: 9.71e-03\n",
      "Step 2475 | Loss: 0.85124 | LR: 9.70e-03\n",
      "test loss: 1.0534\n",
      "Step 2500 | Loss: 0.84646 | LR: 9.69e-03\n",
      "Step 2525 | Loss: 0.82122 | LR: 9.68e-03\n",
      "Step 2550 | Loss: 0.83644 | LR: 9.67e-03\n",
      "Step 2575 | Loss: 0.81361 | LR: 9.66e-03\n",
      "test loss: 1.0478\n",
      "Step 2600 | Loss: 0.83371 | LR: 9.65e-03\n",
      "Step 2625 | Loss: 0.85767 | LR: 9.64e-03\n",
      "Step 2650 | Loss: 0.87576 | LR: 9.63e-03\n",
      "Step 2675 | Loss: 0.80080 | LR: 9.62e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/training/val/shard_k562e_val_0000.npz\n",
      "test loss: 1.0790\n",
      "Step 2700 | Loss: 0.83139 | LR: 9.61e-03\n",
      "Step 2725 | Loss: 0.82885 | LR: 9.60e-03\n",
      "Step 2750 | Loss: 0.86093 | LR: 9.59e-03\n",
      "Step 2775 | Loss: 0.80673 | LR: 9.58e-03\n",
      "test loss: 1.0806\n",
      "Step 2800 | Loss: 0.83644 | LR: 9.57e-03\n",
      "Step 2825 | Loss: 0.82312 | LR: 9.56e-03\n",
      "Step 2850 | Loss: 0.83110 | LR: 9.55e-03\n",
      "Step 2875 | Loss: 0.87574 | LR: 9.54e-03\n",
      "test loss: 1.0880\n",
      "Step 2900 | Loss: 0.81578 | LR: 9.53e-03\n",
      "Step 2925 | Loss: 0.82640 | LR: 9.52e-03\n",
      "Step 2950 | Loss: 0.84723 | LR: 9.50e-03\n",
      "Step 2975 | Loss: 0.83734 | LR: 9.49e-03\n",
      "test loss: 1.0823\n",
      "Step 3000 | Loss: 0.82878 | LR: 9.48e-03\n",
      "Step 3025 | Loss: 0.81727 | LR: 9.47e-03\n",
      "Step 3050 | Loss: 0.83586 | LR: 9.46e-03\n",
      "Step 3075 | Loss: 0.84471 | LR: 9.45e-03\n",
      "test loss: 1.1303\n",
      "Step 3100 | Loss: 0.81620 | LR: 9.43e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0004.npz\n",
      "Step 3125 | Loss: 0.78880 | LR: 9.42e-03\n",
      "Step 3150 | Loss: 0.81848 | LR: 9.41e-03\n",
      "Step 3175 | Loss: 0.79516 | LR: 9.40e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0000.npz\n",
      "=== Step 3176 Done. Avg Loss: 0.87541 ===\n",
      "test loss: 1.1854\n",
      "Step 3200 | Loss: 0.83620 | LR: 9.38e-03\n",
      "Step 3225 | Loss: 0.82353 | LR: 9.37e-03\n",
      "Step 3250 | Loss: 0.83649 | LR: 9.36e-03\n",
      "Step 3275 | Loss: 0.81294 | LR: 9.35e-03\n",
      "test loss: 1.2282\n",
      "Step 3300 | Loss: 0.80562 | LR: 9.33e-03\n",
      "Step 3325 | Loss: 0.84297 | LR: 9.32e-03\n",
      "Step 3350 | Loss: 0.82827 | LR: 9.31e-03\n",
      "Step 3375 | Loss: 0.80797 | LR: 9.29e-03\n",
      "test loss: 1.2394\n",
      "Step 3400 | Loss: 0.81865 | LR: 9.28e-03\n",
      "Step 3425 | Loss: 0.87329 | LR: 9.27e-03\n",
      "Step 3450 | Loss: 0.79668 | LR: 9.25e-03\n",
      "Step 3475 | Loss: 0.83524 | LR: 9.24e-03\n",
      "test loss: 1.2537\n",
      "Step 3500 | Loss: 0.82201 | LR: 9.23e-03\n",
      "Step 3525 | Loss: 0.78483 | LR: 9.21e-03\n",
      "Step 3550 | Loss: 0.85839 | LR: 9.20e-03\n",
      "Step 3575 | Loss: 0.80690 | LR: 9.18e-03\n",
      "test loss: 1.3497\n",
      "Step 3600 | Loss: 0.82079 | LR: 9.17e-03\n",
      "Step 3625 | Loss: 0.82583 | LR: 9.16e-03\n",
      "Step 3650 | Loss: 0.79097 | LR: 9.14e-03\n",
      "Step 3675 | Loss: 0.83888 | LR: 9.13e-03\n",
      "test loss: 1.4138\n",
      "Step 3700 | Loss: 0.82548 | LR: 9.11e-03\n",
      "Step 3725 | Loss: 0.80803 | LR: 9.10e-03\n",
      "Step 3750 | Loss: 0.81263 | LR: 9.08e-03\n",
      "Step 3775 | Loss: 0.80784 | LR: 9.07e-03\n",
      "test loss: 1.4822\n",
      "Step 3800 | Loss: 0.79780 | LR: 9.05e-03\n",
      "Step 3825 | Loss: 0.82145 | LR: 9.04e-03\n",
      "Step 3850 | Loss: 0.83514 | LR: 9.02e-03\n",
      "Step 3875 | Loss: 0.85409 | LR: 9.01e-03\n",
      "test loss: 1.3219\n",
      "Step 3900 | Loss: 0.81123 | LR: 8.99e-03\n",
      "Step 3925 | Loss: 0.81239 | LR: 8.97e-03\n",
      "Step 3950 | Loss: 0.80612 | LR: 8.96e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0001.npz\n",
      "Step 3975 | Loss: 0.78889 | LR: 8.94e-03\n",
      "test loss: 1.4200\n",
      "Step 4000 | Loss: 0.83884 | LR: 8.93e-03\n",
      "Step 4025 | Loss: 0.83907 | LR: 8.91e-03\n",
      "Step 4050 | Loss: 0.83602 | LR: 8.89e-03\n",
      "Step 4075 | Loss: 0.83279 | LR: 8.88e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/training/val/shard_k562e_val_0000.npz\n",
      "test loss: 1.4621\n",
      "Step 4100 | Loss: 0.82624 | LR: 8.86e-03\n",
      "Step 4125 | Loss: 0.83263 | LR: 8.84e-03\n",
      "Step 4150 | Loss: 0.84340 | LR: 8.83e-03\n",
      "Step 4175 | Loss: 0.85182 | LR: 8.81e-03\n",
      "test loss: 1.4837\n",
      "Step 4200 | Loss: 0.86225 | LR: 8.79e-03\n",
      "Step 4225 | Loss: 0.81850 | LR: 8.78e-03\n",
      "Step 4250 | Loss: 0.82081 | LR: 8.76e-03\n",
      "Step 4275 | Loss: 0.78328 | LR: 8.74e-03\n",
      "test loss: 1.5609\n",
      "Step 4300 | Loss: 0.86344 | LR: 8.73e-03\n",
      "Step 4325 | Loss: 0.81170 | LR: 8.71e-03\n",
      "Step 4350 | Loss: 0.81147 | LR: 8.69e-03\n",
      "Step 4375 | Loss: 0.81478 | LR: 8.67e-03\n",
      "test loss: 1.5186\n",
      "Step 4400 | Loss: 0.83465 | LR: 8.65e-03\n",
      "Step 4425 | Loss: 0.80557 | LR: 8.64e-03\n",
      "Step 4450 | Loss: 0.79799 | LR: 8.62e-03\n",
      "Step 4475 | Loss: 0.82452 | LR: 8.60e-03\n",
      "test loss: 1.6382\n",
      "Step 4500 | Loss: 0.82280 | LR: 8.58e-03\n",
      "Step 4525 | Loss: 0.83310 | LR: 8.56e-03\n",
      "Step 4550 | Loss: 0.83188 | LR: 8.55e-03\n",
      "Step 4575 | Loss: 0.82212 | LR: 8.53e-03\n",
      "test loss: 1.6542\n",
      "Step 4600 | Loss: 0.83812 | LR: 8.51e-03\n",
      "Step 4625 | Loss: 0.80330 | LR: 8.49e-03\n",
      "Step 4650 | Loss: 0.84228 | LR: 8.47e-03\n",
      "Step 4675 | Loss: 0.82895 | LR: 8.45e-03\n",
      "test loss: 1.5948\n",
      "Step 4700 | Loss: 0.82836 | LR: 8.43e-03\n",
      "Step 4725 | Loss: 0.79161 | LR: 8.42e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0004.npz\n",
      "Step 4750 | Loss: 0.79551 | LR: 8.40e-03\n",
      "Step 4775 | Loss: 0.82198 | LR: 8.38e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0003.npz\n",
      "test loss: 1.6422\n",
      "Step 4800 | Loss: 0.83111 | LR: 8.36e-03\n",
      "Step 4825 | Loss: 0.81493 | LR: 8.34e-03\n",
      "Step 4850 | Loss: 0.80338 | LR: 8.32e-03\n",
      "Step 4875 | Loss: 0.82408 | LR: 8.30e-03\n",
      "test loss: 1.8296\n",
      "Step 4900 | Loss: 0.80206 | LR: 8.28e-03\n",
      "Step 4925 | Loss: 0.81498 | LR: 8.26e-03\n",
      "Step 4950 | Loss: 0.82309 | LR: 8.24e-03\n",
      "Step 4975 | Loss: 0.86535 | LR: 8.22e-03\n",
      "test loss: 1.7115\n",
      "Step 5000 | Loss: 0.79804 | LR: 8.20e-03\n",
      "Step 5025 | Loss: 0.80954 | LR: 8.18e-03\n",
      "Step 5050 | Loss: 0.80203 | LR: 8.16e-03\n",
      "Step 5075 | Loss: 0.84994 | LR: 8.14e-03\n",
      "test loss: 1.7805\n",
      "Step 5100 | Loss: 0.81347 | LR: 8.12e-03\n",
      "Step 5125 | Loss: 0.83440 | LR: 8.10e-03\n",
      "Step 5150 | Loss: 0.83180 | LR: 8.08e-03\n",
      "Step 5175 | Loss: 0.78639 | LR: 8.06e-03\n",
      "test loss: 1.7876\n",
      "Step 5200 | Loss: 0.80294 | LR: 8.04e-03\n",
      "Step 5225 | Loss: 0.81012 | LR: 8.02e-03\n",
      "Step 5250 | Loss: 0.81291 | LR: 8.00e-03\n",
      "Step 5275 | Loss: 0.79523 | LR: 7.98e-03\n",
      "test loss: 1.7620\n",
      "Step 5300 | Loss: 0.81260 | LR: 7.96e-03\n",
      "Step 5325 | Loss: 0.82380 | LR: 7.93e-03\n",
      "Step 5350 | Loss: 0.82416 | LR: 7.91e-03\n",
      "Step 5375 | Loss: 0.81554 | LR: 7.89e-03\n",
      "test loss: 1.7935\n",
      "Step 5400 | Loss: 0.81382 | LR: 7.87e-03\n",
      "Step 5425 | Loss: 0.80896 | LR: 7.85e-03\n",
      "Step 5450 | Loss: 0.78800 | LR: 7.83e-03\n",
      "Step 5475 | Loss: 0.81164 | LR: 7.81e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/training/val/shard_k562e_val_0000.npz\n",
      "test loss: 1.7589\n",
      "Step 5500 | Loss: 0.80337 | LR: 7.78e-03\n",
      "Step 5525 | Loss: 0.78278 | LR: 7.76e-03\n",
      "Step 5550 | Loss: 0.81009 | LR: 7.74e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0002.npz\n",
      "Step 5575 | Loss: 0.82081 | LR: 7.72e-03\n",
      "test loss: 1.8764\n",
      "Step 5600 | Loss: 0.80677 | LR: 7.70e-03\n",
      "Step 5625 | Loss: 0.85637 | LR: 7.68e-03\n",
      "Step 5650 | Loss: 0.79790 | LR: 7.65e-03\n",
      "Step 5675 | Loss: 0.82637 | LR: 7.63e-03\n",
      "test loss: 1.9623\n",
      "Step 5700 | Loss: 0.81227 | LR: 7.61e-03\n",
      "Step 5725 | Loss: 0.79129 | LR: 7.59e-03\n",
      "Step 5750 | Loss: 0.79305 | LR: 7.57e-03\n",
      "Step 5775 | Loss: 0.80758 | LR: 7.54e-03\n",
      "test loss: 1.8966\n",
      "Step 5800 | Loss: 0.83222 | LR: 7.52e-03\n",
      "Step 5825 | Loss: 0.82077 | LR: 7.50e-03\n",
      "Step 5850 | Loss: 0.77425 | LR: 7.48e-03\n",
      "Step 5875 | Loss: 0.81059 | LR: 7.45e-03\n",
      "test loss: 1.9009\n",
      "Step 5900 | Loss: 0.80643 | LR: 7.43e-03\n",
      "Step 5925 | Loss: 0.81867 | LR: 7.41e-03\n",
      "Step 5950 | Loss: 0.81221 | LR: 7.38e-03\n",
      "Step 5975 | Loss: 0.84269 | LR: 7.36e-03\n",
      "test loss: 1.8551\n",
      "Step 6000 | Loss: 0.80985 | LR: 7.34e-03\n",
      "Step 6025 | Loss: 0.84764 | LR: 7.32e-03\n",
      "Step 6050 | Loss: 0.81776 | LR: 7.29e-03\n",
      "Step 6075 | Loss: 0.78618 | LR: 7.27e-03\n",
      "test loss: 1.8577\n",
      "Step 6100 | Loss: 0.85794 | LR: 7.25e-03\n",
      "Step 6125 | Loss: 0.81061 | LR: 7.22e-03\n",
      "Step 6150 | Loss: 0.85114 | LR: 7.20e-03\n",
      "Step 6175 | Loss: 0.83449 | LR: 7.18e-03\n",
      "test loss: 1.8640\n",
      "Step 6200 | Loss: 0.81396 | LR: 7.15e-03\n",
      "Step 6225 | Loss: 0.80044 | LR: 7.13e-03\n",
      "Step 6250 | Loss: 0.82027 | LR: 7.11e-03\n",
      "Step 6275 | Loss: 0.80959 | LR: 7.08e-03\n",
      "test loss: 2.0948\n",
      "Step 6300 | Loss: 0.82320 | LR: 7.06e-03\n",
      "Step 6325 | Loss: 0.80278 | LR: 7.03e-03\n",
      "Step 6350 | Loss: 0.78039 | LR: 7.01e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0001.npz\n",
      "=== Step 6353 Done. Avg Loss: 0.81950 ===\n",
      "Step 6375 | Loss: 0.81073 | LR: 6.99e-03\n",
      "test loss: 1.9419\n",
      "Step 6400 | Loss: 0.81579 | LR: 6.96e-03\n",
      "Step 6425 | Loss: 0.83536 | LR: 6.94e-03\n",
      "Step 6450 | Loss: 0.82769 | LR: 6.91e-03\n",
      "Step 6475 | Loss: 0.81665 | LR: 6.89e-03\n",
      "test loss: 1.9833\n",
      "Step 6500 | Loss: 0.80223 | LR: 6.87e-03\n",
      "Step 6525 | Loss: 0.78965 | LR: 6.84e-03\n",
      "Step 6550 | Loss: 0.77274 | LR: 6.82e-03\n",
      "Step 6575 | Loss: 0.80325 | LR: 6.79e-03\n",
      "test loss: 2.0231\n",
      "Step 6600 | Loss: 0.82705 | LR: 6.77e-03\n",
      "Step 6625 | Loss: 0.79962 | LR: 6.75e-03\n",
      "Step 6650 | Loss: 0.79457 | LR: 6.72e-03\n",
      "Step 6675 | Loss: 0.85076 | LR: 6.70e-03\n",
      "test loss: 2.0063\n",
      "Step 6700 | Loss: 0.83224 | LR: 6.67e-03\n",
      "Step 6725 | Loss: 0.81146 | LR: 6.65e-03\n",
      "Step 6750 | Loss: 0.83534 | LR: 6.62e-03\n",
      "Step 6775 | Loss: 0.82752 | LR: 6.60e-03\n",
      "test loss: 1.8971\n",
      "Step 6800 | Loss: 0.79386 | LR: 6.57e-03\n",
      "Step 6825 | Loss: 0.79261 | LR: 6.55e-03\n",
      "Step 6850 | Loss: 0.83434 | LR: 6.52e-03\n",
      "Step 6875 | Loss: 0.80012 | LR: 6.50e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/training/val/shard_k562e_val_0000.npz\n",
      "test loss: 1.8446\n",
      "Step 6900 | Loss: 0.81793 | LR: 6.47e-03\n",
      "Step 6925 | Loss: 0.81317 | LR: 6.45e-03\n",
      "Step 6950 | Loss: 0.77190 | LR: 6.42e-03\n",
      "Step 6975 | Loss: 0.81089 | LR: 6.40e-03\n",
      "test loss: 1.8688\n",
      "Step 7000 | Loss: 0.85111 | LR: 6.37e-03\n",
      "Step 7025 | Loss: 0.83163 | LR: 6.35e-03\n",
      "Step 7050 | Loss: 0.79994 | LR: 6.32e-03\n",
      "Step 7075 | Loss: 0.84751 | LR: 6.30e-03\n",
      "test loss: 2.0807\n",
      "Step 7100 | Loss: 0.79587 | LR: 6.27e-03\n",
      "Step 7125 | Loss: 0.82020 | LR: 6.25e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0000.npz\n",
      "Step 7150 | Loss: 0.80784 | LR: 6.22e-03\n",
      "Step 7175 | Loss: 0.82298 | LR: 6.20e-03\n",
      "test loss: 1.9863\n",
      "Step 7200 | Loss: 0.82871 | LR: 6.17e-03\n",
      "Step 7225 | Loss: 0.80386 | LR: 6.15e-03\n",
      "Step 7250 | Loss: 0.78448 | LR: 6.12e-03\n",
      "Step 7275 | Loss: 0.82891 | LR: 6.10e-03\n",
      "test loss: 1.7864\n",
      "Step 7300 | Loss: 0.78163 | LR: 6.07e-03\n",
      "Step 7325 | Loss: 0.81990 | LR: 6.05e-03\n",
      "Step 7350 | Loss: 0.79730 | LR: 6.02e-03\n",
      "Step 7375 | Loss: 0.80729 | LR: 6.00e-03\n",
      "test loss: 1.9442\n",
      "Step 7400 | Loss: 0.80506 | LR: 5.97e-03\n",
      "Step 7425 | Loss: 0.80870 | LR: 5.94e-03\n",
      "Step 7450 | Loss: 0.79949 | LR: 5.92e-03\n",
      "Step 7475 | Loss: 0.78626 | LR: 5.89e-03\n",
      "test loss: 2.1672\n",
      "Step 7500 | Loss: 0.83872 | LR: 5.87e-03\n",
      "Step 7525 | Loss: 0.78301 | LR: 5.84e-03\n",
      "Step 7550 | Loss: 0.78902 | LR: 5.82e-03\n",
      "Step 7575 | Loss: 0.79830 | LR: 5.79e-03\n",
      "test loss: 1.8449\n",
      "Step 7600 | Loss: 0.81253 | LR: 5.76e-03\n",
      "Step 7625 | Loss: 0.77890 | LR: 5.74e-03\n",
      "Step 7650 | Loss: 0.81966 | LR: 5.71e-03\n",
      "Step 7675 | Loss: 0.82623 | LR: 5.69e-03\n",
      "test loss: 2.0359\n",
      "Step 7700 | Loss: 0.78308 | LR: 5.66e-03\n",
      "Step 7725 | Loss: 0.81676 | LR: 5.64e-03\n",
      "Step 7750 | Loss: 0.80847 | LR: 5.61e-03\n",
      "Step 7775 | Loss: 0.80778 | LR: 5.58e-03\n",
      "test loss: 2.0534\n",
      "Step 7800 | Loss: 0.82014 | LR: 5.56e-03\n",
      "Step 7825 | Loss: 0.79498 | LR: 5.53e-03\n",
      "Step 7850 | Loss: 0.84671 | LR: 5.51e-03\n",
      "Step 7875 | Loss: 0.81008 | LR: 5.48e-03\n",
      "test loss: 1.9413\n",
      "Step 7900 | Loss: 0.74328 | LR: 5.45e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0004.npz\n",
      "Step 7925 | Loss: 0.78606 | LR: 5.43e-03\n",
      "Step 7950 | Loss: 0.83558 | LR: 5.40e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0003.npz\n",
      "Step 7975 | Loss: 0.79503 | LR: 5.38e-03\n",
      "test loss: 1.7229\n",
      "Step 8000 | Loss: 0.83776 | LR: 5.35e-03\n",
      "Step 8025 | Loss: 0.84614 | LR: 5.33e-03\n",
      "Step 8050 | Loss: 0.80633 | LR: 5.30e-03\n",
      "Step 8075 | Loss: 0.78897 | LR: 5.27e-03\n",
      "test loss: 1.8104\n",
      "Step 8100 | Loss: 0.78856 | LR: 5.25e-03\n",
      "Step 8125 | Loss: 0.77307 | LR: 5.22e-03\n",
      "Step 8150 | Loss: 0.78560 | LR: 5.20e-03\n",
      "Step 8175 | Loss: 0.79776 | LR: 5.17e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/training/val/shard_k562e_val_0000.npz\n",
      "test loss: 1.7980\n",
      "Step 8200 | Loss: 0.78409 | LR: 5.14e-03\n",
      "Step 8225 | Loss: 0.77363 | LR: 5.12e-03\n",
      "Step 8250 | Loss: 0.78673 | LR: 5.09e-03\n",
      "Step 8275 | Loss: 0.82879 | LR: 5.07e-03\n",
      "test loss: 1.9267\n",
      "Step 8300 | Loss: 0.79367 | LR: 5.04e-03\n",
      "Step 8325 | Loss: 0.77811 | LR: 5.01e-03\n",
      "Step 8350 | Loss: 0.78471 | LR: 4.99e-03\n",
      "Step 8375 | Loss: 0.77642 | LR: 4.96e-03\n",
      "test loss: 1.9769\n",
      "Step 8400 | Loss: 0.80040 | LR: 4.94e-03\n",
      "Step 8425 | Loss: 0.75425 | LR: 4.91e-03\n",
      "Step 8450 | Loss: 0.79188 | LR: 4.88e-03\n",
      "Step 8475 | Loss: 0.79625 | LR: 4.86e-03\n",
      "test loss: 1.8620\n",
      "Step 8500 | Loss: 0.76348 | LR: 4.83e-03\n",
      "Step 8525 | Loss: 0.84749 | LR: 4.81e-03\n",
      "Step 8550 | Loss: 0.81107 | LR: 4.78e-03\n",
      "Step 8575 | Loss: 0.80456 | LR: 4.75e-03\n",
      "test loss: 1.7997\n",
      "Step 8600 | Loss: 0.74899 | LR: 4.73e-03\n",
      "Step 8625 | Loss: 0.82878 | LR: 4.70e-03\n",
      "Step 8650 | Loss: 0.79743 | LR: 4.68e-03\n",
      "Step 8675 | Loss: 0.78248 | LR: 4.65e-03\n",
      "test loss: 1.8941\n",
      "Step 8700 | Loss: 0.76932 | LR: 4.62e-03\n",
      "Step 8725 | Loss: 0.80425 | LR: 4.60e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0002.npz\n",
      "Step 8750 | Loss: 0.76916 | LR: 4.57e-03\n",
      "Step 8775 | Loss: 0.78999 | LR: 4.55e-03\n",
      "test loss: 1.8464\n",
      "Step 8800 | Loss: 0.77358 | LR: 4.52e-03\n",
      "Step 8825 | Loss: 0.80309 | LR: 4.49e-03\n",
      "Step 8850 | Loss: 0.79004 | LR: 4.47e-03\n",
      "Step 8875 | Loss: 0.75809 | LR: 4.44e-03\n",
      "test loss: 1.7711\n",
      "Step 8900 | Loss: 0.81014 | LR: 4.42e-03\n",
      "Step 8925 | Loss: 0.77223 | LR: 4.39e-03\n",
      "Step 8950 | Loss: 0.74954 | LR: 4.36e-03\n",
      "Step 8975 | Loss: 0.81421 | LR: 4.34e-03\n",
      "test loss: 1.8076\n",
      "Step 9000 | Loss: 0.78752 | LR: 4.31e-03\n",
      "Step 9025 | Loss: 0.82782 | LR: 4.29e-03\n",
      "Step 9050 | Loss: 0.80736 | LR: 4.26e-03\n",
      "Step 9075 | Loss: 0.80349 | LR: 4.24e-03\n",
      "test loss: 1.7895\n",
      "Step 9100 | Loss: 0.77582 | LR: 4.21e-03\n",
      "Step 9125 | Loss: 0.80666 | LR: 4.18e-03\n",
      "Step 9150 | Loss: 0.81556 | LR: 4.16e-03\n",
      "Step 9175 | Loss: 0.82175 | LR: 4.13e-03\n",
      "test loss: 1.9432\n",
      "Step 9200 | Loss: 0.78658 | LR: 4.11e-03\n",
      "Step 9225 | Loss: 0.81726 | LR: 4.08e-03\n",
      "Step 9250 | Loss: 0.78879 | LR: 4.06e-03\n",
      "Step 9275 | Loss: 0.82906 | LR: 4.03e-03\n",
      "test loss: 1.7333\n",
      "Step 9300 | Loss: 0.76093 | LR: 4.00e-03\n",
      "Step 9325 | Loss: 0.80186 | LR: 3.98e-03\n",
      "Step 9350 | Loss: 0.80548 | LR: 3.95e-03\n",
      "Step 9375 | Loss: 0.80344 | LR: 3.93e-03\n",
      "test loss: 1.7394\n",
      "Step 9400 | Loss: 0.81963 | LR: 3.90e-03\n",
      "Step 9425 | Loss: 0.79786 | LR: 3.88e-03\n",
      "Step 9450 | Loss: 0.79465 | LR: 3.85e-03\n",
      "Step 9475 | Loss: 0.78759 | LR: 3.83e-03\n",
      "test loss: 1.7299\n",
      "Step 9500 | Loss: 0.77491 | LR: 3.80e-03\n",
      "Step 9525 | Loss: 0.78779 | LR: 3.78e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0003.npz\n",
      "=== Step 9530 Done. Avg Loss: 0.80127 ===\n",
      "Step 9550 | Loss: 0.80859 | LR: 3.75e-03\n",
      "Step 9575 | Loss: 0.79223 | LR: 3.73e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/training/val/shard_k562e_val_0000.npz\n",
      "test loss: 1.6435\n",
      "Step 9600 | Loss: 0.77343 | LR: 3.70e-03\n",
      "Step 9625 | Loss: 0.79740 | LR: 3.68e-03\n",
      "Step 9650 | Loss: 0.78950 | LR: 3.65e-03\n",
      "Step 9675 | Loss: 0.79911 | LR: 3.63e-03\n",
      "test loss: 1.8010\n",
      "Step 9700 | Loss: 0.81024 | LR: 3.60e-03\n",
      "Step 9725 | Loss: 0.80586 | LR: 3.58e-03\n",
      "Step 9750 | Loss: 0.80118 | LR: 3.55e-03\n",
      "Step 9775 | Loss: 0.80664 | LR: 3.53e-03\n",
      "test loss: 1.6281\n",
      "Step 9800 | Loss: 0.77701 | LR: 3.50e-03\n",
      "Step 9825 | Loss: 0.76111 | LR: 3.48e-03\n",
      "Step 9850 | Loss: 0.79236 | LR: 3.45e-03\n",
      "Step 9875 | Loss: 0.79845 | LR: 3.43e-03\n",
      "test loss: 1.6975\n",
      "Step 9900 | Loss: 0.78998 | LR: 3.40e-03\n",
      "Step 9925 | Loss: 0.78349 | LR: 3.38e-03\n",
      "Step 9950 | Loss: 0.79752 | LR: 3.35e-03\n",
      "Step 9975 | Loss: 0.81607 | LR: 3.33e-03\n",
      "test loss: 1.7539\n",
      "Step 10000 | Loss: 0.82796 | LR: 3.30e-03\n",
      "Step 10025 | Loss: 0.83058 | LR: 3.28e-03\n",
      "Step 10050 | Loss: 0.75505 | LR: 3.26e-03\n",
      "Step 10075 | Loss: 0.79557 | LR: 3.23e-03\n",
      "test loss: 1.5671\n",
      "Step 10100 | Loss: 0.78541 | LR: 3.21e-03\n",
      "Step 10125 | Loss: 0.79841 | LR: 3.18e-03\n",
      "Step 10150 | Loss: 0.78715 | LR: 3.16e-03\n",
      "Step 10175 | Loss: 0.78058 | LR: 3.13e-03\n",
      "test loss: 1.7183\n",
      "Step 10200 | Loss: 0.78930 | LR: 3.11e-03\n",
      "Step 10225 | Loss: 0.76999 | LR: 3.09e-03\n",
      "Step 10250 | Loss: 0.76751 | LR: 3.06e-03\n",
      "Step 10275 | Loss: 0.75659 | LR: 3.04e-03\n",
      "test loss: 1.6564\n",
      "Step 10300 | Loss: 0.77796 | LR: 3.01e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0004.npz\n",
      "Step 10325 | Loss: 0.78081 | LR: 2.99e-03\n",
      "Step 10350 | Loss: 0.79008 | LR: 2.97e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0001.npz\n",
      "Step 10375 | Loss: 0.77058 | LR: 2.94e-03\n",
      "test loss: 1.5866\n",
      "Step 10400 | Loss: 0.81401 | LR: 2.92e-03\n",
      "Step 10425 | Loss: 0.78004 | LR: 2.90e-03\n",
      "Step 10450 | Loss: 0.76294 | LR: 2.87e-03\n",
      "Step 10475 | Loss: 0.79061 | LR: 2.85e-03\n",
      "test loss: 1.5613\n",
      "Step 10500 | Loss: 0.80204 | LR: 2.82e-03\n",
      "Step 10525 | Loss: 0.78791 | LR: 2.80e-03\n",
      "Step 10550 | Loss: 0.77866 | LR: 2.78e-03\n",
      "Step 10575 | Loss: 0.80194 | LR: 2.75e-03\n",
      "test loss: 1.5016\n",
      "Step 10600 | Loss: 0.79417 | LR: 2.73e-03\n",
      "Step 10625 | Loss: 0.80280 | LR: 2.71e-03\n",
      "Step 10650 | Loss: 0.76989 | LR: 2.68e-03\n",
      "Step 10675 | Loss: 0.78947 | LR: 2.66e-03\n",
      "test loss: 1.5634\n",
      "Step 10700 | Loss: 0.79160 | LR: 2.64e-03\n",
      "Step 10725 | Loss: 0.80837 | LR: 2.62e-03\n",
      "Step 10750 | Loss: 0.81640 | LR: 2.59e-03\n",
      "Step 10775 | Loss: 0.79913 | LR: 2.57e-03\n",
      "test loss: 1.5612\n",
      "Step 10800 | Loss: 0.79592 | LR: 2.55e-03\n",
      "Step 10825 | Loss: 0.80360 | LR: 2.53e-03\n",
      "Step 10850 | Loss: 0.79128 | LR: 2.50e-03\n",
      "Step 10875 | Loss: 0.79799 | LR: 2.48e-03\n",
      "test loss: 1.5171\n",
      "Step 10900 | Loss: 0.82077 | LR: 2.46e-03\n",
      "Step 10925 | Loss: 0.80797 | LR: 2.44e-03\n",
      "Step 10950 | Loss: 0.83538 | LR: 2.41e-03\n",
      "Step 10975 | Loss: 0.76779 | LR: 2.39e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/training/val/shard_k562e_val_0000.npz\n",
      "test loss: 1.4616\n",
      "Step 11000 | Loss: 0.79606 | LR: 2.37e-03\n",
      "Step 11025 | Loss: 0.79868 | LR: 2.35e-03\n",
      "Step 11050 | Loss: 0.79186 | LR: 2.32e-03\n",
      "Step 11075 | Loss: 0.81428 | LR: 2.30e-03\n",
      "test loss: 1.5116\n",
      "Step 11100 | Loss: 0.78150 | LR: 2.28e-03\n",
      "Step 11125 | Loss: 0.79690 | LR: 2.26e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0000.npz\n",
      "Step 11150 | Loss: 0.78828 | LR: 2.24e-03\n",
      "Step 11175 | Loss: 0.80370 | LR: 2.22e-03\n",
      "test loss: 1.5271\n",
      "Step 11200 | Loss: 0.75087 | LR: 2.19e-03\n",
      "Step 11225 | Loss: 0.77887 | LR: 2.17e-03\n",
      "Step 11250 | Loss: 0.79349 | LR: 2.15e-03\n",
      "Step 11275 | Loss: 0.80693 | LR: 2.13e-03\n",
      "test loss: 1.5009\n",
      "Step 11300 | Loss: 0.79036 | LR: 2.11e-03\n",
      "Step 11325 | Loss: 0.76276 | LR: 2.09e-03\n",
      "Step 11350 | Loss: 0.77774 | LR: 2.07e-03\n",
      "Step 11375 | Loss: 0.77991 | LR: 2.04e-03\n",
      "test loss: 1.3740\n",
      "Step 11400 | Loss: 0.77323 | LR: 2.02e-03\n",
      "Step 11425 | Loss: 0.76041 | LR: 2.00e-03\n",
      "Step 11450 | Loss: 0.80604 | LR: 1.98e-03\n",
      "Step 11475 | Loss: 0.80853 | LR: 1.96e-03\n",
      "test loss: 1.4221\n",
      "Step 11500 | Loss: 0.78280 | LR: 1.94e-03\n",
      "Step 11525 | Loss: 0.79655 | LR: 1.92e-03\n",
      "Step 11550 | Loss: 0.78701 | LR: 1.90e-03\n",
      "Step 11575 | Loss: 0.82484 | LR: 1.88e-03\n",
      "test loss: 1.4166\n",
      "Step 11600 | Loss: 0.76568 | LR: 1.86e-03\n",
      "Step 11625 | Loss: 0.82750 | LR: 1.84e-03\n",
      "Step 11650 | Loss: 0.76654 | LR: 1.82e-03\n",
      "Step 11675 | Loss: 0.80361 | LR: 1.80e-03\n",
      "test loss: 1.4232\n",
      "Step 11700 | Loss: 0.78190 | LR: 1.78e-03\n",
      "Step 11725 | Loss: 0.76854 | LR: 1.76e-03\n",
      "Step 11750 | Loss: 0.76998 | LR: 1.74e-03\n",
      "Step 11775 | Loss: 0.82549 | LR: 1.72e-03\n",
      "test loss: 1.4096\n",
      "Step 11800 | Loss: 0.76096 | LR: 1.70e-03\n",
      "Step 11825 | Loss: 0.75667 | LR: 1.68e-03\n",
      "Step 11850 | Loss: 0.77500 | LR: 1.66e-03\n",
      "Step 11875 | Loss: 0.78245 | LR: 1.64e-03\n",
      "test loss: 1.3153\n",
      "Step 11900 | Loss: 0.78588 | LR: 1.62e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0002.npz\n",
      "Step 11925 | Loss: 0.79094 | LR: 1.60e-03\n",
      "Step 11950 | Loss: 0.82433 | LR: 1.58e-03\n",
      "Step 11975 | Loss: 0.78812 | LR: 1.57e-03\n",
      "test loss: 1.3742\n",
      "Step 12000 | Loss: 0.76389 | LR: 1.55e-03\n",
      "Step 12025 | Loss: 0.81020 | LR: 1.53e-03\n",
      "Step 12050 | Loss: 0.79023 | LR: 1.51e-03\n",
      "Step 12075 | Loss: 0.78514 | LR: 1.49e-03\n",
      "test loss: 1.3019\n",
      "Step 12100 | Loss: 0.78799 | LR: 1.47e-03\n",
      "Step 12125 | Loss: 0.79237 | LR: 1.45e-03\n",
      "Step 12150 | Loss: 0.78617 | LR: 1.44e-03\n",
      "Step 12175 | Loss: 0.77364 | LR: 1.42e-03\n",
      "test loss: 1.3405\n",
      "Step 12200 | Loss: 0.76975 | LR: 1.40e-03\n",
      "Step 12225 | Loss: 0.76855 | LR: 1.38e-03\n",
      "Step 12250 | Loss: 0.80024 | LR: 1.36e-03\n",
      "Step 12275 | Loss: 0.79430 | LR: 1.35e-03\n",
      "test loss: 1.3024\n",
      "Step 12300 | Loss: 0.77568 | LR: 1.33e-03\n",
      "Step 12325 | Loss: 0.79763 | LR: 1.31e-03\n",
      "Step 12350 | Loss: 0.78652 | LR: 1.29e-03\n",
      "Step 12375 | Loss: 0.76714 | LR: 1.28e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/training/val/shard_k562e_val_0000.npz\n",
      "test loss: 1.3632\n",
      "Step 12400 | Loss: 0.78165 | LR: 1.26e-03\n",
      "Step 12425 | Loss: 0.79278 | LR: 1.24e-03\n",
      "Step 12450 | Loss: 0.76798 | LR: 1.22e-03\n",
      "Step 12475 | Loss: 0.75122 | LR: 1.21e-03\n",
      "test loss: 1.2944\n",
      "Step 12500 | Loss: 0.75424 | LR: 1.19e-03\n",
      "Step 12525 | Loss: 0.80393 | LR: 1.17e-03\n",
      "Step 12550 | Loss: 0.75031 | LR: 1.16e-03\n",
      "Step 12575 | Loss: 0.74854 | LR: 1.14e-03\n",
      "test loss: 1.2499\n",
      "Step 12600 | Loss: 0.81850 | LR: 1.12e-03\n",
      "Step 12625 | Loss: 0.79494 | LR: 1.11e-03\n",
      "Step 12650 | Loss: 0.77142 | LR: 1.09e-03\n",
      "Step 12675 | Loss: 0.79533 | LR: 1.07e-03\n",
      "test loss: 1.2455\n",
      "Step 12700 | Loss: 0.79256 | LR: 1.06e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0004.npz\n",
      "=== Step 12707 Done. Avg Loss: 0.78736 ===\n",
      "Step 12725 | Loss: 0.79495 | LR: 1.04e-03\n",
      "Step 12750 | Loss: 0.80471 | LR: 1.03e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0003.npz\n",
      "Step 12775 | Loss: 0.76598 | LR: 1.01e-03\n",
      "test loss: 1.2744\n",
      "Step 12800 | Loss: 0.73989 | LR: 9.95e-04\n",
      "Step 12825 | Loss: 0.78136 | LR: 9.79e-04\n",
      "Step 12850 | Loss: 0.76884 | LR: 9.64e-04\n",
      "Step 12875 | Loss: 0.75573 | LR: 9.49e-04\n",
      "test loss: 1.2623\n",
      "Step 12900 | Loss: 0.78157 | LR: 9.34e-04\n",
      "Step 12925 | Loss: 0.80190 | LR: 9.18e-04\n",
      "Step 12950 | Loss: 0.76887 | LR: 9.03e-04\n",
      "Step 12975 | Loss: 0.74976 | LR: 8.89e-04\n",
      "test loss: 1.2477\n",
      "Step 13000 | Loss: 0.78006 | LR: 8.74e-04\n",
      "Step 13025 | Loss: 0.79357 | LR: 8.59e-04\n",
      "Step 13050 | Loss: 0.75282 | LR: 8.45e-04\n",
      "Step 13075 | Loss: 0.74646 | LR: 8.30e-04\n",
      "test loss: 1.2176\n",
      "Step 13100 | Loss: 0.78955 | LR: 8.16e-04\n",
      "Step 13125 | Loss: 0.76061 | LR: 8.02e-04\n",
      "Step 13150 | Loss: 0.78304 | LR: 7.88e-04\n",
      "Step 13175 | Loss: 0.77978 | LR: 7.74e-04\n",
      "test loss: 1.1828\n",
      "Step 13200 | Loss: 0.76536 | LR: 7.60e-04\n",
      "Step 13225 | Loss: 0.78874 | LR: 7.46e-04\n",
      "Step 13250 | Loss: 0.81535 | LR: 7.33e-04\n",
      "Step 13275 | Loss: 0.80286 | LR: 7.19e-04\n",
      "test loss: 1.2063\n",
      "Step 13300 | Loss: 0.74789 | LR: 7.06e-04\n",
      "Step 13325 | Loss: 0.77008 | LR: 6.92e-04\n",
      "Step 13350 | Loss: 0.77638 | LR: 6.79e-04\n",
      "Step 13375 | Loss: 0.79209 | LR: 6.66e-04\n",
      "test loss: 1.1584\n",
      "Step 13400 | Loss: 0.78291 | LR: 6.53e-04\n",
      "Step 13425 | Loss: 0.76459 | LR: 6.40e-04\n",
      "Step 13450 | Loss: 0.78708 | LR: 6.28e-04\n",
      "Step 13475 | Loss: 0.79511 | LR: 6.15e-04\n",
      "test loss: 1.1892\n",
      "Step 13500 | Loss: 0.76507 | LR: 6.03e-04\n",
      "Step 13525 | Loss: 0.75231 | LR: 5.90e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0001.npz\n",
      "Step 13550 | Loss: 0.76267 | LR: 5.78e-04\n",
      "Step 13575 | Loss: 0.81288 | LR: 5.66e-04\n",
      "test loss: 1.1799\n",
      "Step 13600 | Loss: 0.81983 | LR: 5.54e-04\n",
      "Step 13625 | Loss: 0.81171 | LR: 5.42e-04\n",
      "Step 13650 | Loss: 0.76743 | LR: 5.31e-04\n",
      "Step 13675 | Loss: 0.78596 | LR: 5.19e-04\n",
      "test loss: 1.1710\n",
      "Step 13700 | Loss: 0.78796 | LR: 5.08e-04\n",
      "Step 13725 | Loss: 0.78331 | LR: 4.96e-04\n",
      "Step 13750 | Loss: 0.77600 | LR: 4.85e-04\n",
      "Step 13775 | Loss: 0.79625 | LR: 4.74e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/training/val/shard_k562e_val_0000.npz\n",
      "test loss: 1.1427\n",
      "Step 13800 | Loss: 0.77279 | LR: 4.63e-04\n",
      "Step 13825 | Loss: 0.76108 | LR: 4.52e-04\n",
      "Step 13850 | Loss: 0.76992 | LR: 4.41e-04\n",
      "Step 13875 | Loss: 0.81072 | LR: 4.31e-04\n",
      "test loss: 1.1865\n",
      "Step 13900 | Loss: 0.79239 | LR: 4.20e-04\n",
      "Step 13925 | Loss: 0.75575 | LR: 4.10e-04\n",
      "Step 13950 | Loss: 0.77360 | LR: 3.99e-04\n",
      "Step 13975 | Loss: 0.78312 | LR: 3.89e-04\n",
      "test loss: 1.1444\n",
      "Step 14000 | Loss: 0.79698 | LR: 3.79e-04\n",
      "Step 14025 | Loss: 0.76792 | LR: 3.69e-04\n",
      "Step 14050 | Loss: 0.80493 | LR: 3.60e-04\n",
      "Step 14075 | Loss: 0.79324 | LR: 3.50e-04\n",
      "test loss: 1.1210\n",
      "Step 14100 | Loss: 0.76527 | LR: 3.41e-04\n",
      "Step 14125 | Loss: 0.77043 | LR: 3.31e-04\n",
      "Step 14150 | Loss: 0.78940 | LR: 3.22e-04\n",
      "Step 14175 | Loss: 0.72894 | LR: 3.13e-04\n",
      "test loss: 1.1446\n",
      "Step 14200 | Loss: 0.81226 | LR: 3.04e-04\n",
      "Step 14225 | Loss: 0.78112 | LR: 2.95e-04\n",
      "Step 14250 | Loss: 0.79519 | LR: 2.86e-04\n",
      "Step 14275 | Loss: 0.75749 | LR: 2.78e-04\n",
      "test loss: 1.1421\n",
      "Step 14300 | Loss: 0.77726 | LR: 2.69e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0002.npz\n",
      "Step 14325 | Loss: 0.78153 | LR: 2.61e-04\n",
      "Step 14350 | Loss: 0.75100 | LR: 2.53e-04\n",
      "Step 14375 | Loss: 0.76677 | LR: 2.44e-04\n",
      "test loss: 1.1184\n",
      "Step 14400 | Loss: 0.77157 | LR: 2.36e-04\n",
      "Step 14425 | Loss: 0.77879 | LR: 2.29e-04\n",
      "Step 14450 | Loss: 0.78619 | LR: 2.21e-04\n",
      "Step 14475 | Loss: 0.77379 | LR: 2.13e-04\n",
      "test loss: 1.1308\n",
      "Step 14500 | Loss: 0.74659 | LR: 2.06e-04\n",
      "Step 14525 | Loss: 0.73408 | LR: 1.99e-04\n",
      "Step 14550 | Loss: 0.76289 | LR: 1.91e-04\n",
      "Step 14575 | Loss: 0.77927 | LR: 1.84e-04\n",
      "test loss: 1.1147\n",
      "Step 14600 | Loss: 0.80193 | LR: 1.77e-04\n",
      "Step 14625 | Loss: 0.77881 | LR: 1.71e-04\n",
      "Step 14650 | Loss: 0.77008 | LR: 1.64e-04\n",
      "Step 14675 | Loss: 0.80167 | LR: 1.57e-04\n",
      "test loss: 1.1298\n",
      "Step 14700 | Loss: 0.80337 | LR: 1.51e-04\n",
      "Step 14725 | Loss: 0.78265 | LR: 1.45e-04\n",
      "Step 14750 | Loss: 0.79467 | LR: 1.38e-04\n",
      "Step 14775 | Loss: 0.77254 | LR: 1.32e-04\n",
      "test loss: 1.1240\n",
      "Step 14800 | Loss: 0.78382 | LR: 1.27e-04\n",
      "Step 14825 | Loss: 0.77427 | LR: 1.21e-04\n",
      "Step 14850 | Loss: 0.78358 | LR: 1.15e-04\n",
      "Step 14875 | Loss: 0.79523 | LR: 1.10e-04\n",
      "test loss: 1.1086\n",
      "Step 14900 | Loss: 0.76363 | LR: 1.04e-04\n",
      "Step 14925 | Loss: 0.78038 | LR: 9.91e-05\n",
      "Step 14950 | Loss: 0.80274 | LR: 9.41e-05\n",
      "Step 14975 | Loss: 0.79620 | LR: 8.91e-05\n",
      "test loss: 1.0975\n",
      "Step 15000 | Loss: 0.76856 | LR: 8.43e-05\n",
      "Step 15025 | Loss: 0.79212 | LR: 7.96e-05\n",
      "Step 15050 | Loss: 0.76025 | LR: 7.50e-05\n",
      "Step 15075 | Loss: 0.81387 | LR: 7.06e-05\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0000.npz\n",
      "loading /Users/djemec/data/jepa/v0_2/training/val/shard_k562e_val_0000.npz\n",
      "test loss: 1.0973\n",
      "Step 15100 | Loss: 0.76198 | LR: 6.63e-05\n",
      "Step 15125 | Loss: 0.77976 | LR: 6.22e-05\n",
      "Step 15150 | Loss: 0.75985 | LR: 5.81e-05\n",
      "Step 15175 | Loss: 0.81290 | LR: 5.43e-05\n",
      "test loss: 1.0975\n",
      "Step 15200 | Loss: 0.79715 | LR: 5.05e-05\n",
      "Step 15225 | Loss: 0.78503 | LR: 4.69e-05\n",
      "Step 15250 | Loss: 0.76459 | LR: 4.34e-05\n",
      "Step 15275 | Loss: 0.75373 | LR: 4.00e-05\n",
      "test loss: 1.1114\n",
      "Step 15300 | Loss: 0.78385 | LR: 3.68e-05\n",
      "Step 15325 | Loss: 0.79356 | LR: 3.37e-05\n",
      "Step 15350 | Loss: 0.76636 | LR: 3.08e-05\n",
      "Step 15375 | Loss: 0.78437 | LR: 2.80e-05\n",
      "test loss: 1.1053\n",
      "Step 15400 | Loss: 0.77748 | LR: 2.53e-05\n",
      "Step 15425 | Loss: 0.77390 | LR: 2.28e-05\n",
      "Step 15450 | Loss: 0.77374 | LR: 2.03e-05\n",
      "Step 15475 | Loss: 0.77685 | LR: 1.81e-05\n",
      "test loss: 1.1002\n",
      "Step 15500 | Loss: 0.78367 | LR: 1.59e-05\n",
      "Step 15525 | Loss: 0.75034 | LR: 1.39e-05\n",
      "Step 15550 | Loss: 0.74607 | LR: 1.20e-05\n",
      "Step 15575 | Loss: 0.77660 | LR: 1.03e-05\n",
      "test loss: 1.0938\n",
      "Step 15600 | Loss: 0.78214 | LR: 8.71e-06\n",
      "Step 15625 | Loss: 0.76695 | LR: 7.25e-06\n",
      "Step 15650 | Loss: 0.77262 | LR: 5.92e-06\n",
      "Step 15675 | Loss: 0.80025 | LR: 4.73e-06\n",
      "test loss: 1.1013\n",
      "Step 15700 | Loss: 0.73479 | LR: 3.67e-06\n",
      "Step 15725 | Loss: 0.76039 | LR: 2.74e-06\n",
      "Step 15750 | Loss: 0.80757 | LR: 1.96e-06\n",
      "Step 15775 | Loss: 0.78255 | LR: 1.30e-06\n",
      "test loss: 1.0951\n",
      "Step 15800 | Loss: 0.80467 | LR: 7.86e-07\n",
      "Step 15825 | Loss: 0.76222 | LR: 4.04e-07\n",
      "Step 15850 | Loss: 0.80610 | LR: 1.58e-07\n",
      "Step 15875 | Loss: 0.82892 | LR: 4.69e-08\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0004.npz\n",
      "test loss: 1.1033\n",
      "=== Step 15884 Done. Avg Loss: 0.77750 ===\n"
     ]
    }
   ],
   "source": [
    "for step in range(max_steps):\n",
    "\n",
    "    last_step = (step == max_steps - 1)\n",
    "\n",
    "    # once in a while evaluate our validation set loss\n",
    "    if step % 100 == 0 or last_step:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss_accum = 0.0\n",
    "            val_loss_steps = 25\n",
    "            for i in range(val_loss_steps):\n",
    "                cont_x, cont_tot, case_x, case_tot, act_id = val_loader.next_batch()\n",
    "\n",
    "                # run BioJEPA\n",
    "                with torch.no_grad():\n",
    "                    z_context = model.student(cont_x, cont_tot)\n",
    "                    z_pred = model.predictor(z_context, act_id)\n",
    "\n",
    "                # run new decoder\n",
    "                pred_delta = decoder(z_pred) - decoder(z_context)\n",
    "                real_delta = case_x - cont_x\n",
    "\n",
    "                loss = F.mse_loss(pred_delta, real_delta)\n",
    "                loss = loss / val_loss_steps\n",
    "                val_loss_accum += loss.detach()\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "        print(f'test loss: {val_loss_accum.item():.4f}')\n",
    "\n",
    "\n",
    "    # periodically save checkpoint\n",
    "    if step > 0 and  (step+1) % steps_per_epoch ==0 and not last_step:\n",
    "        # Save Checkpoint\n",
    "        torch.save({\n",
    "            'model': decoder.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'step': step\n",
    "        }, checkpoint_dir / f'biojepa_decoder_ckpt_{step}.pt')\n",
    "\n",
    "    # actual training\n",
    "    decoder.train\n",
    "    cont_x, cont_tot, case_x, case_tot, act_id = train_loader.next_batch()\n",
    "\n",
    "    # run frozen BioJEPA\n",
    "    with torch.no_grad():\n",
    "        z_context = model.student(cont_x, cont_tot)\n",
    "        z_pred = model.predictor(z_context, act_id)\n",
    "\n",
    "    # run decoder\n",
    "    pred_delta = decoder(z_pred) - decoder(z_context)\n",
    "    real_delta = case_x - cont_x\n",
    "    real_delta = case_x - cont_x\n",
    "\n",
    "    # loss\n",
    "    pred_delta = decoder(z_pred) - decoder(z_context)\n",
    "    real_delta = case_x - cont_x\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss = F.mse_loss(pred_delta, real_delta)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    # loss caching\n",
    "    lossi.append(loss.item())\n",
    "    total_epoch_loss += loss.item()\n",
    "\n",
    "    if step % 25 == 0:\n",
    "        print(f\"Step {step} | Loss: {loss.item():.5f} | LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "    \n",
    "    \n",
    "    if step > 0 and (step+1) % steps_per_epoch == 0:   \n",
    "        avg_loss = total_epoch_loss / steps_per_epoch\n",
    "        print(f\"=== Step {step} Done. Avg Loss: {avg_loss:.5f} ===\")\n",
    "        total_epoch_loss = 0\n",
    "    \n",
    "    if last_step:\n",
    "        # Save Checkpoint\n",
    "        torch.save({\n",
    "            'model': decoder.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'step': step\n",
    "        }, checkpoint_dir / f'biojepa_decoder_ckpt_{step}_final.pt')\n",
    "\n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c924050-b2d8-4973-a3c1-f9dd0a3d1f03",
   "metadata": {},
   "source": [
    "**Training Loss Plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b11fde37-0cac-429d-be0c-351390bac22d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkkAAAGeCAYAAACAU5U+AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjYsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvq6yFwwAAAAlwSFlzAAAPYQAAD2EBqD+naQAAUQdJREFUeJzt3QeYE9XawPF3K0tZ6tJ7770JKF2KXFTsDbCL4ieKghXQa0O9drGLFesV8VoAqVKl9957bwsssOxuvufMkiW7O0kmySSZSf6/51mySSYzZ0I2eXPOe94T43A4HAIAAIBcYnNfBQAAgEKQBAAAoIMgCQAAQAdBEgAAgA6CJAAAAB0ESQAAADoIkgAAAHQQJAEAAOggSAIAANARr3cj3MvKypK9e/dKcnKyxMTEhLs5AADAALXAyMmTJ6VChQoSG2usj4ggyUcqQKpcuXK4mwEAAPywa9cuqVSpkqFtozJI+v333+XRRx/VeoUef/xxufvuuw0/VvUgOZ/kokWLBrGVAADALKmpqVonh/Nz3IiYaFvgNiMjQxo0aCAzZsyQYsWKScuWLWXevHlSqlQpw0+yetyJEycIkgAAsAl/Pr+jLnF74cKF0rBhQ6lYsaIUKVJEevfuLX/99Ve4mwUAACzGdkHSrFmzpG/fvlrilUqcnjBhQr5txowZI9WqVZOkpCRp27atFhi55hSpAMlJ/b5nz56QtR8AANiD7YKk06dPS9OmTbVASM8PP/wgQ4cOlVGjRsnSpUu1bXv27CkHDx7063jnzp3TuuhcfwAAQOSzXZCkhsdeeOEF6devn+79b7zxhtxzzz1yxx13aLlHH374oRQqVEjGjh2r3a96oFx7jtTv6jZ3Xn75ZW0M0/nDzDYAAKKD7YIkT9LT02XJkiXSvXv3nNtULQR1ff78+dr1Nm3ayOrVq7Xg6NSpUzJx4kStp8mdJ598Ukvycv6oWW0AACDyRVQJgMOHD0tmZqaULVs21+3q+vr167Xf4+Pj5fXXX5cuXbpoJQCGDx/ucWZbgQIFtB8AABBdIipIMurKK6/UfgAAAKJiuC0lJUXi4uLkwIEDuW5X18uVKxe2dgEAAPuJqCApMTFRKw45bdq0nNvUkJq63q5du7C2DQAA2IvthttUsvXmzZtzrm/btk2WL18uJUuWlCpVqmjT/wcOHCitWrXSkrTfeustrWyAmu0GAAAQsUHS4sWLtaRrJxUUKSow+uKLL+TGG2+UQ4cOyciRI2X//v3SrFkzmTRpUr5kbgAAAE+ibu22QLF2GwAA9sPabVFCxbVfz98uS3YcC3dTAACIWLYbboPI1HUHZcSva7Tft4/uE+7mAAAQkehJsqHNB0+FuwkAAEQ8giQAAAAdBEkAAAA6CJKiwNHT6fLmlI2y62hauJsCAIBtECRFgUd/XC5vT9sk13wwL9xNAQDANgiSLCwrS7+EVUyMb/v5Z+tR7fLQyXNmNAsAgKhAkGThGWxN//2XjJlxcQkWAAAQOgRJFvX872vl5NkMeW3yhnz3USMdAIDgI0gCAADQQZAUBXzNYQIAAARJtg96fli0U86kZ3rcnuE5AAB8R5Bkc4//vEpe+nNduJsBAEDEIUiKAFPXHfB4P8NtAAD4jiAJAABAB0GSDZFjBABA8BEkAQAA6CBIsiiHjzlGJ8+el8Xbj4pDp5uJlCQAAHxHkGQhn83Z5vcyJP3enyfXfThfflqyO999jM4BAOA7giSLSM/I0pYiUcuQHEw969dab8r/lu8NQusAAIg+BEkW4XDp70nzUhwSAAAEH0GSRcS6JBpl+jh9zVvOETlJAAD4jiDJglTydd7AZsmOo/Le9E2SmZX/PgAAYL74IOwTAcrMyn/btR/M1y5LFi7g88y3LJcHZGRmSXwcsTEAAN7waWlBWR6G27Yeyk7Q9sWZ8xdznPb7kRQOAEA0IkiyIDWk5kqv9hEAAAgugiQLenrC6lzXNx7wvfcIAAAEhiDJIlw7i1bsOu6xZymQxG06pQAAMIYgyaIcJuYz6SVza8cgYgIAwC2CpAiw98TFZOy5m4/k/J569rycOHNe9zHnMjLl8jdnyeBxS/Pdt3rPCbnri0Wyfn9qkFoMAID1UQIgQqmp/k2e/cvt/fO2HNGWMlE/Y/Lcd+0H8+RcRpYs3XlMlo3sEfS2AgBgRfQk2cD2I6e9Dp3ldeR0ut/HUwGScixNvxcKAIBoQE+SRew6lub2vgd0hsS8+WvtAbf3/bJst4yds93nfQIAEE0IkizivembTd2fp6TsR35YYeqxAACIRAy3WUTeaf6BYn03AAACQ5BkEa5Lhyjr9zGzDACAcCJIsogpeXKIDp48F5TjUBoJAABjCJJsyMjInHOGWl79xy4wv0EAAEQggiSbUcHPK5PWe93urambdG/fccT9LDoAAHARQZLNLNh61NB2p85lBL0tAABEMoIkm9lw4GS4mwAAQFQgSAIAANBBkATJMrlGEwAAkYAgKQJtPXTKp+3T8tRoAgAABEkRafRE77PfAACAZwRJESg9U79Gkj/rvAEAEK0IkiLQzA2HfNp+3IKd5CUBAJAHQRK04bkJy/eEuxkAAFgKQRI0K3efCHcTAACwFIIkaL6Ytz3cTQAAwFIIkgAAAHQQJAEAAOggSEKOvcfPhLsJAABYBkEScrQfPV1+X7nX7f2nz2XIjiOnQ9omAADChSAJuYyZscXtfR1emS6dXpsp6/enhrRNAACEA0ESDFffPp52Xrucsd63YpUAANgRQRJyWb//ZLibAACAJRAkAQAA6CBIAgAA0EGQBI+mrTsgfd6ZLRsYhgMARJn4cDcA1nbXl4u1y/vHLQl3UwAACCl6kmDIybMZ4W4CAAAhRZAEQ1wrAzjEfZkAAAAiBUESAACADoIkAAAAHQRJMOjiEFuMxIS1JQAAhAJBEgAAgA6CJJhm0ur9suXQqXA3AwAAU1AnCYZ4WPdWM2vjIRn0TXYtpflPdpXyxQqGpmEAAAQJPUnwmV4JgJW7j+f8fv2H80PcIgAAzEeQBNPtPnYm3E0AACBgBEkw5Mjp9FzXz2dmydnzmfLZnG1aHpK34TgAAOyGIAk+e3XSBmk4crI899saef73tdLt9b8NPc5BJAUAsBGCJPglPTNLvlu4K+e6t/Dn2wU7pfnzU2TV7hNBbxsAAGYgSIIpth0+7fH+p35ZJcfTzsvDPywLWZsAAAgEQRJM8cuyPbmunz6XEba2AABgBoIkBMWrk9aHuwkAAASEIAlBsWj7sXA3AQCAgBAkAQAA6CBIAgAA0EGQhLCavGa/3PzxP7LvBFW6AQDWQpCEsLrv6yUyf+sRGfXrmnA3BQCAXAiSEBSn0zO0pUuUWRsPed3+aJ5lTwAACDeCJATFjiNp0vm1mdrvA8YuNH3/J86cl09nb5X9J86avm8AABSCJATNnuP584zMWr3tyfEr5YU/1smNH883aY8AAORGkARbmrH+UE6PFQAAwRAflL0CbsRcuEzPyJK0dJYuAQBYF0ESwqLHm3/LdpdeILOG4QAAMAvDbQipLYdOyzf/7MgVIAEAYEUESQi5Zyasznfbkh3HZPzS3WFpDwAAegiSYBlDf1whK3YdD3czAADQECTBUnYeNTYM5yCLCQAQZARJAAAAOqI6SOrXr5+UKFFCrrvuunA3BRfk7R+auGqfXPH2bNl66FSYWgQAiFZRHSQNGTJEvvrqq3A3I6J9MXdbQI+/f9xSWbsvVctXAgAglKI6SOrcubMkJyeHuxkR7dnf1pqyn9PncheejMkpSwkAgEWCpJMnT8rDDz8sVatWlYIFC0r79u1l0aJFpjZq1qxZ0rdvX6lQoYLExMTIhAkTdLcbM2aMVKtWTZKSkqRt27aycKH5C6kitBwOErIBADYNku6++26ZMmWKfP3117Jq1Srp0aOHdO/eXfbs2aO7/dy5c+X8+fP5bl+7dq0cOHBA9zGnT5+Wpk2bakGQOz/88IMMHTpURo0aJUuXLtW279mzpxw8eDBnm2bNmkmjRo3y/ezdu9fX0wYAAFHGpyDpzJkz8vPPP8urr74qHTt2lFq1asmzzz6rXX7wwQf5ts/KypLBgwfLLbfcIpmZmTm3b9iwQbp27Spffvml7nF69+4tL7zwgpZY7c4bb7wh99xzj9xxxx3SoEED+fDDD6VQoUIyduzYnG2WL18uq1evzvejeqh8pQI2dZzWrVv7/FiYjxIAAABLBUkZGRlasKOGt1ypYbc5c+bk33lsrPz555+ybNkyGTBggBY0bdmyRQuQrr76ahk+fLhfjU5PT5clS5ZoPViux1LX58+fL8Gggj3V+2X20CIAAIiAIEklObdr106ef/55bchKBUzffPONFpjs27dP9zGq12b69OlaEKV6lFSApIIZvZ4now4fPqwdu2zZsrluV9f3799veD+qHddff70WyFWqVCloARaMO5eRJVlZ9BIBAMIv3tcHqFykO++8UypWrChxcXHSokULufnmm7WeHXeqVKmiPa5Tp05So0YN+eyzz7SE7HCbOnVquJuAPIb/d6X8d/Fuee6qhvLj4l1utzt7Piuk7QIARB+fE7dr1qwpf//9t5w6dUp27dqlzShTidkq+HFHJWjfe++92oy1tLQ0eeSRRwJqdEpKihag5U38VtfLlSsX0L4Rfgu3H5Xeb8+Wz+duN22fx06nM3MOABCaOkmFCxeW8uXLy7Fjx2Ty5Mly1VVXuR0a69atm9SvX1/Gjx8v06ZN02amPfbYY/4eWhITE6Vly5bavpxUvpO6roYDEXkOnjzn92OnrTsgzZ+fIk/9strUNgEAIpvPw20qIFLfyOvWrSubN2+WYcOGSb169bRZZnmpwEXNVFM1lVRgFB8fr80QUyUEVG6SGrLT61VSvVRq307btm3TZqqVLFlSG7pT1PT/gQMHSqtWraRNmzby1ltvaaUD9NoB+ztxJn8ZCaNem7xBu/xu4U55+ZrGJrYKABDJfA6STpw4IU8++aTs3r1bC1quvfZaefHFFyUhISHftmrG2UsvvSSXXXaZ1vvjpGoaqXyg0qVL6x5j8eLF0qVLl5zrKiBSVFD0xRdfaL/feOONcujQIRk5cqSWrK1qIk2aNClfMjeiw7p9qfLOtE3yaI86UqtM7irqe4+fCVu7AAD2FeMgUcMnqampUqxYMS1YLFq0qGn7rfbEH6btK1JterG3JMTF5nu+to/uI/VHTJIz5zOlbNECsuCpi6Uh9LYFAESfVD8+v6N67TbYy6yNh9zepwIk5UCq/7lLAAC4IkiCbWQarJ80+Nul8vtKey89s3zXcZm4Sr/2GAAgNAiSYBtGa2v9sXKfPPjtMrGzq8fMlfvHLdVyrQAA4UGQBNtTPUdGLd15TOxkx5G0cDcBAKIWQRJs4/O529z2HBl1zfvzTGyR+FWg8ujpdFPbAAAIDoIk2Ma8LUdM2c+RU+fkXEZ2onegOVJqWOzuL40vevzutE3S4vkp8vU/OwI+PgAguAiSYCvP/bZGq6AdiJYvTJUur83UvW/N3hOGC1eu358qK3afkKnrDho+9utTNmqXIyZQ/RsAIq6YJBBOaj03M9Z023vibL7bFmw9Ijd+/I8UK5ggK0b18LoPKowBQGSjJwm4YNr6gwEvgQIAiBwESYhYKqnaXbK3smLX8ZC2BwBgLwRJiFhzNx+R535b6/b+3cfOBDRTLdDhtrPnM70e02BpKABAEBAkIWLtOmbtGkP1RkySAWMXasESAMB6CJIAHftOnJEpaw/41Lv046Jd8sTPKw0vn6LM3nRYLn1lhp+tBAAEE7PbELF8HQ5z3b796Ona9bdvaiZXNauov71cfMBD3y2T/63IXi8uLjZGUooUkCHdaktsrPfxssOnWJTXLCrP7I0pG+WpK+pL3XLJ4W4OAJsjSELEOuNlGMtTvo8zYJq7+bDbIMmVM0BSxi3YqV1WTyksVzf3/lgrU+dfNClBGlcqJnZw1Zi52qVa827h093D3RwANsdwm03cdWn1cDfBds5nZvm0vV7H0+l0//OFdvuYE3Xo5DlZZqG15fYePyO3frpA+r43x7R9pmdkadXGdxw5LcF08CS9cwACR5BkE8N61g13E2znzQvVrQPhy7pwgWr94lTp9/48yyzCq4Iks308a4tWbbyTm4rnAGAlBEmIWOcyfOtJigliztOx0+kyYdkeQ9vOd1mjbuG2oxJJFkTY+QCIbARJiFp5gyKzVxlJPZuR83ubl6bKwz8s93kfn81xXwwz2HyZpQcAkYggCQiSj2dtzfn9fKa9Ag5V+kCtYwcA0YwgyQaubVEp3E2ADnuFPebODASAaECQZANJCfw3hYK7mWUv/L5WHvlhuc/LlljVgdSzVPkGAAOokwRcsHSn/oK3n17IC3qgc02pXfZigcIsGwZNauq9mllWJrlAyOoIqYBswGcL5ZKapUJyPAAwC10UgEEqr0jV+XG6fexCr4+xWo/NjPUHQ15H6LcVe2Xh9qPyzrRNITsmAJiBIMkmWA3efEO+X65NzTdqzMzNUueZifLP1iP5Zq95WsTWjHpNdpbuY1FPALAKgiRE9Yf36InrfS4s+eiPK+SZCasMP+5tAz0o5zKs1eNkw5FEADAdQRKi2tR1B+TSV6bL7ysvrr3mzZ7jZ+Sbf7LXZzPLmOmbTd0fACBwBEmIakdOp8vuY2fkwW+XhbUdMzceMmVo9Xhautzz1WKZtHq/WEWMS9lOeqgA2AlBEmAxgQQSb0zZKFPWHpBB3yyRMz4szquWTPm/75ZZLtE8nE6cOS+/Lt8jaenec88ARCaCJJtw/TaOyLNy9wlT9nP41MVZa/VHTsp13RO1ZIqahfb1/B2mtCMSqB45ldz/9C+rw90UAGFCkARYzGuTN+jenpXl0PKhXJO9Nx886XFfqlfJF8fSjM/2i/SZmc7FhX8xuDCxN+r/y5fZlHamJjmokg+RUoAV0YsgyYLiYm36qYKgUr09HUZPl/+tyE4yr/vMJOn+xiyZtDp71l00BzRWlHeB4A6jZ0jz56fIwdSzEukGf7tUG/pdtF2/ij1gFwRJEaxi8YIyYXCHcDcDJnEGR3lnwj37v7VuH7N4+7GcApJKjJcoaOOBU24/8OkVMO7bBTul3oiJ2hBm1//MlPdnbs4Z+px/oc5WNDhicLgXsCqCpAjXrHLxcDcBQXbeQ7HGn5fulju+WCT7TxjrvVAz/fS0fWmqPDBuqV/ti8bOqad+WaVVaFfJ8FsPn5ZXJ+kPoQKwNoIkC4rGDxUEN8H/kMFlSNz1Fh0+lS4TLVRWQJm7+bDsOpoW7mbAA/oeYXcESRakNyJCrgicHDofPSqp21OPUnpmpm4QlDdvxhdNn/tLqj3xR0D78NeSHUfl1k8XyGWvzgj5sQFED4IkwOZUAH39R/OlxfNTZPcx/Z6Vl/7Mv/zKn6v2aXkzE1ddTPx2xlAOg3WElPdnhL5a+NIdx0N+TADRhyDJIrrXL+txyIScWTgdSM0/dLZkxzE5eTZDVript6Tuz0vlGKm8mft1co18Sbh1l8cUiK2HTslUH8sX2IW35PlIwvsW7I4gySJev6Gp6fvsVr+M6ftE+Dl7cHz9IFJBx7O/uZ8Jl7fcQKiGCvV0ff1vufurxTJvy+GgtAMAjCBIsohiBRMuXvHzi2ZSQu7/zjs6VA+wVYgkKugwGsQs23nctMAnkI6TVW56xozsU5U+uOLt2bJ+f6r/DQAQ1QiSLMjfz5TZw7vmuh5PUcqIleGSpG3m6E2XumVk8fbsStNG/bh4t1/HCnZRRVX6YO2+VLn3qyVBPQ4C7zkErIogyYKKF3LpVfJB6eQCsvnF3gHvB9Y3bsHOnN9Tz5i3AGvFEgXlug/n+/y4BX4USPx5qf/LfUxYbvyxqWdzD0+GG19dAPsgSLKgQZ1qStd6Zfz6RhYfFyuTH+4ov//fpZKcRJAUqZbuvJiIne5h6n+oPP7zyqAszuzuVb96D0NoAIKPIMmCCifGy9jbW/v9+LrlkqVRxWJmNgnwyfcLd8q9Xy2Ws+ez6zMZTTJXdZyWuQSAMJcqvjljw8VlaoKN2W2wu/hwNwDBp3JWeLOCESN/XWNoO+c6ZK6Onk6XQolxkpQQJ0+MX5UzLJhcwP3bzE+Ld2mBVP921bTrMzcc0nKJIlm4/hR/XLRLhl/o8Rt3d1vpUCslTC0B7IOeJCvSGZ1IjIvVco780YReJZhIzThr9cLUXLcdOZ2uFbNsnef2kyofKMZ98vmw/66UEb+ukYMns5O4J6/ZH7JzcK0FFQ2L9zoDJHd1swLx3vRNMnpi/oKlUVQSChGKIMlGBejmPdFVXurXOOe2ge2qyg2tKhl5cHAbh6ByV0U7XL5fdDFp3EkVstQuz2UYfim6rmZyJj33sJyZ8r76V+w6Ln3fmyMtLwR0T/+ySjr/Z6ac9tL2YLXH7lSw+5+/NsqHf2/J91qNgtgTEY4gyYLcvYkmxMXKLW2ryO3tq8lDXWvJc1c1kiIFDCRn805la78u32uZ/1JVyNJ1Zp03gbZTPV4N7Xlal87rPvJcn7M5d4FKdT47jqTJ/1bkf57D7UTaeen/2QL5ZZl/ZRbcMfP147qrcxnhn0QAmImcJBt69sqG4W4CwixcYe9z/zOWs+TO3M0XSwXsPJpmaHkSNbRXu0wRmTK0k0Sbt6dtktmbDms//Zob6DUOM76PQc+pcxlSxENuopXRkwTYQDCHo3wxfplvtY08fWb+vNR778jE1dk5SpsOnpJodPxMulidp+FDYiZMX39AGo2arJuzZgcESRYUTQtgwphvFxof4rKjKWsPaDlBVn3pHzppfMFfb8w4x1+X75HxBoLMQK3ec0Ib7lOXKgn/tk8XaOUdAKP+fWG9SJWzZkcESYANnEkPTVJxMBiJCV74Y510em2mnD2fO6cl76wzlZv0l48z4MyIu1q/mHvWXqjoFeJUvYpDvl8uQ39coeUsOZ+nr+Zvz1Vk1BOjxWmv/WCeNtR3w0fz5ZNZW7V8Lmd5B3d7drVyl/E1AAErIkiyIF/e1A292Vn16zkMcc4cs6N3pm2S85nGPpBVgvYvHobzHvtphdR+eqLc+7Vva7GdOZ8ZsuFKNRX+0R9XBPUY6S7J0erclL/WHtBqXF3z/jxTj+VMxE5Lz5RUN69DTz3fn87ZZmp7YD8OsTeCpCh4Uem9hf13UDuTj4JgUd3UrtPlld8sOBPLnUkB1D467RLc/HeJf8NLqneqwahJujPk3NVH0lvvTQ1xeaOmwhvJtTLK2/cb55ekzRbO2YqGGlSIXARJNmdkXSy9tyjynuzljSkbxS6qPfFHruvnLTAtXH1OH0jNLljpbg24/Sey71dDWE2e/SvftmqIa+Xu0A0fncvI1GoQ5WPCn64/cYuRtwy9/aoio4u3H5VwULlUoSpQishEkGRBoQhfGlYoqi0hUatMkRAcDQg/vRo+rosDq+n2Sp93Z7vdh6qnZPYXmx1HTsvmgydz3aZ6vZo9N0Um6NTIsptjaefl7q8Wh+XY/3p3jtz39RJZs/dEWI4P+yNIitKgS62vtWzk5TL54Y5haBGiSVaQh1vUcI6RYpPn8iSFu7P72Bn3x3I55qYDJ/0ucqnWuVO1Y9R+VMJ69zdm5RriU21w5ht5YrWRLHfNyco7XhxiWw+dDuvxYV8ESRYUqpGwAvFxEhfLsBuCa8G24A61qJlXbV6cqi2U6/skB98+vBddOJefl+6Ry9+cJXd96XsPiSp1oIagVO0Y19jBzDID4Rra33v8TL7hVl+eZRU0qnIQaj+AFRAkAbC1RduPaUM63hZtHfbTSi2QUnkq/vr6nx3aB7maaafM2njI531sOXzK9F6hcKUYqt6vDfsvDhXe/vki/Q0NnttvK/fJPV8tlvajpxtugxpKu+b9ufLP1ovV3GEdDov1dvrKnnXCAcBHa/elSr0Rk3LddvR0/llsRquAm9Ejkzu2cfgdEIXrg+iyV2Zo6/l5Y7R58/Ksq2fEwLGLtPIRN338j2wf3cfnxwOe0JNkQWZ/K6xRunDO75VLFpRXr2ti7gEAm1K9Fq6Op3lfBuSBcUvF7hwmvS8ZCZC04wUxilMBEhAsBElRoFjBhJzfZw/vKje0qhzW9gDBYMbn8Odzt4vV2x+TZ7bdz0t2hz0x2ht/Wjfy19WGgqtgDzWqsg8dRk+3VW0ymIfhNpsmSALIbeiPywPeh0qqtlMgcfMn/2iXFYsXlGCLCUUA6HKQr+bv0L7QNapYTIJJBWKe6sYN+nqJ7D1xVv7vu2XSt2mFoLYF1kNPEoCIcNDis8MCdfDkWW14UK2lltceX2eD+dHtFkhnldG14vLyNmPRaPDmLgZSBURVL9GYGZvdPta1lhZC939vFQRJFtKmeklRM/K71C0TtjZ83L8lZQEQtVT9omBztxq6t7jl2f+t0abHhysnauzcbbad4aQW59ULuN6cslHrJXpt8gYPj+b9MJoRJFnI9/dcImv/3UuKFbqYQ+RNzTIXk7LNGL7r0bAcOUuIWuM9LLAb7jXK9l1YNsUKVIK7vwHlkVPntHwj/VIM+d+rTp49L89MWCWLAljaZMXuE/K+Tm9RpgVynuzo2Ol0Q718kYCcJAuJjY2RpNg4nx5zY6vK2gu2Xc1SbrdJSU40oXUAPBk9cb3Pj+n3wTyxo2b/niJNKhnPFXINRYb/d6VMW39QyzkyMmX/P5M3yDf/7NR+tr50hZxOz5DkJONfJJ2W7fJv3T1ipPxBbssXpkpyUryseraneAum087ZO5giSLK5+LhYebBrbY/b3Nmhumzcf1Iub1AuZO0Cos1Hs7b6/JgVLh/cztwNVW36+g+tHzyt3G28KGf6hXXz1KUKkHyx9fDFJUVu/XSBzN96RGYN6yJVShXSbtOSroPYi0dPUm7Ooq0nz3qe5KCW3VHBtN0x3BYF1Dptb93UXPo0KR/upgDwYtT/1sjhU+mWy+sxw86jpwMKSFSApIxftvviY3xsw6uT1kuvt2YZnsnIbGP/bDmoX1nebgiSIOWKJvn92LsvrW5qW4Bo5QyCjBZoNErVUbr54+xSAXrUcL1a0iPYOVWeZGRmya/L9wR1zTbnNP/3Z26R9ftPyg+LdgU18FTPq93tOpqmrY2oJgxEK4IkBKRr/TIhqdECWJnPU/B1OD+wY0we9nn0pxU5PTA5x3L5XS3Uq5b0+H3lPgmXb/7ZIUO+Xy4zN+ReC++MkRIAbp4Xb0Hfv39fK8t3HfN7/+6ohGY1m67581Pk09m+D8FayRPjV8rCbUfzVaaPJuQkAUAA7v9mScDrublasE1/FteZdPMSYN+dvlkbhh/cpVbOsh6qWGLXemWkcIHQfyzM2ay/OO3AsQt9qs+kAiM1665UkQKGtt9yyPPwnxLjpudL1U8qlBifL0Bq8txfOTlYL/yxTu6+rIbY1VE/1jaMNPQkITAOEhsR3cwKkLwV3VNDRGbSqw2k8qGCZeuhU3L9h/Pz3b5m7wnZcuiUKQUsX/xjnTbzakIApRzy0qvG3ePNWdJg5GRJPZs7iNh04FROgGRlKpDs+vpMj0U0kY0gyQZKFmYKPxDpzMqP8aV+zWuTc5ctmLoueLknqqfqWFruoEIFGX3emSPbXGawBeLTOdkFL1/8c12++/z5LncuI1N3KNU5484508uq9p04I98t3JnvNfHBzM2y9dBpL0U0g1/7yw4IkizsnZubS48GZeW+TjVDetxb2lQxvjG9SIAp/vXuHLlBp6fFV+MW7DS87ZgZ+tW/g0Gv+OQGE3rH3M0+e2WS5wDAm/OZWfLv39aKnakA9Mnxq+SNKRtz3X4+0//gx2FCu+wUmBEkWdiVTSvIxwNaSZEQ5wg09qFIHADzLAygqrQZi/SmmjyzLm/Qkdczv6z2a19GPkfdLf9itIet9tMTfQo4rcgZmP6dJyHeyj1frV+cJm/8FViAayaCJATGWkE/gAAEsoitN3q1n9zlIvkkxlgQ9ffGQ2HvYbNaL0mwjPp1tYz41fcA+J1pm7SJBO9Mt06uFEESvCZeLx1xudzsyxAcAEtQ655ZWayfsz6OnM6ekafxIe4wPana4Vu9KlWBesmOwHsLPVm7N1XW7UuVUNl88KRWIkAl4Dtfc1/O32G4IvvOI2na8iWKFWNIgiQYShyvXaZIuJsBwEdP/LxKrMzbjD531u07adqQX6jOR9WrUoVC7/t6ie79szcdkru/XKQV1VSBjj+9TqpMxBXvzJbeb8/WlgUxSrVLBXG+PMZ1qRhVbPLqMXO161k+PMWqeGjH12ZoweMqH5a5CSXqJMGQq5pV0IqvAbA2176ZP1aFr0CkEYEkECvzNh/W6hUZ1eU/M8WKVHDT/7OF2u9T12WvbVesYIL8cN8lUq9cUcP7cS1J0GjU5JzfvXXYqVpf87Yc0YKdD/u39LhtTJ7rB1LP+f1/6bp2Yd/35shNrSuL1dCTBEPcFWezYO8ogAjn/KC+5dMFuvc7C2TmdfCk/u3h1vjZiwGNa+9Or7dmewysVAmCLJdEMn+Hq1SApExak7vmlyPP/mZtPCRfzd8hwcJwGyJSr4blwt0EAC6e+mWVXPvBPIlUaljICknQRoYL9dqphv3UennO+kUZfmTMD/x8ofZ//OX87T61JxADxi6UOZsPSzQhSELAHutZV966sZksfqZ7uJsCQES+XbDT8oUOA6Gqj6sPbDuYsSF7+MzVy3+u19bLU3lK/lJrqimqWGSwOBgrIEhC4NQaUFc3rygpBtdLAhA8Zi0TFOwk50DN3mSPHg21VEleY+dmVwb/w+RFhc3uXHP4sb+PZoWuQGkokLiNgJQtmhTuJgAw+YNSrWKvt7RHJDt2Ol1KhGEJqLR04zPKMrMccsNH86VSiYL5/r9V0va5MK8bl5XlkPdnGg+S8r5U957IvwRMuBEkQZpUKib7Tpw1vH2hxDj5ZEArOXTynNSiNAAQcaItQFKaPz9FvrijtXSuW8bvgFQvQF2w7agMbF/N7T62H04zfLyVu49rw6iuQ6mOC7WJmjz7l9h9sefZFuwdJEiCvHxNE6laaotc37KSx+0qlywou46ekemPdpZyxehBAqwo78r0vtp80IQq2Db19rRN0qFWiqn7nL7+oNQbMcmUvB+9/G6VGL5mb+iKR3pKSj+QavzLtl2W/iRIglYs8qkr6nvdTgVHZ85nStGkhJC0C4DvPpmdne/ir1s++Uei1bKdx33qDcmb/zV/a/ZU+uAJsK7UlsMy6tc1khjvPh1ZzbhTeaZGjnZNnhmUkZjmTeI2DEuIiyVAAiKcVWsJhcpD3y3z+7GjJ673eQ07M3LIjO7jlk8WyKaDpzz2PKleL7XUiHLQpWdomM5MPBVU+uKF39dq9Z86vzZDPr+QvG51BEkAAPghFKWavC0CbKQJp31IDlc+nrVVu0w9e/FxPy3Z7fVx3mpXfTpnm3R/42/ZfiRNnvvNHis4ECTBVPXKJYe7CQAQFDM3HJSfFu8K6TEHXFiuRHll4ga/oiSVS3oiLfiLHW884H1NPTXhx07Dc+QkwVQ/3NtOFmw7Ive6WcQRAOzq9s8X5RteU9W/r2pW0e99PualoOSe4xenxS/cnl1A0pXDYPL3Ip3Hmm2fD7Ok7YKeJJiqWKEE6eFmmZLyzIgDEEFUfs+Q75cHXD08ENsOnza9SyYjwIWHIwlBEkLmOi8lBgAAvpucZ2HaQCuxn/Oz2rrDx9jKDiUACJIQMnb4gwAAu/ly/g6v27wzbVNI2hJpCJIAAIhwK3afCHcTbIkgCWExvFfdcDcBAOBhnTgQJCGUXAbF/9W4QlibAgDIrrCd15Q1B6S+h6VUzGKHMIwSAAAAWFywpvC3eXFqvtvS/UzcjkT0JCEovrqzjVzeoKwUTsxeA0ghcRsAfDdr4yGZuMr4mnK+cK2qjfwIkhAUHeuUlk8GtJIyRZMCno4KANFswNiFHheltYq1+9yvCafHDh8D1n/WEZFCseYRAEQKOwRJR0/nXrw3Elj/WYetuS54SO8RgEik1iNL83ERWV9lZpEnFA4kbiNkYmzRuQoAvmmtk/xstjEztgT9GKHmEOujJwlBFeOm+8isXqVv7mprzo4AACGVnmH93jGCJIRMUkJsUMbp37m5uen7BQAE18M/BLY4cCgQJCFkbrukqjStXFyG9azr94w5vZynK5tWkMFdaprQQgAALorqIKlfv35SokQJue6668LdFMv58b52UrN0Yfn2HvOGswoXiJdfB3eQwV1qSenkAj4//uP+Ld3eV7Kw7/sDAMCTqA6ShgwZIl999VW4m2FJbaqXlGmPdpb2NVOCsv+khLiQPAYAAH9FdZDUuXNnSU5ODnczIlpMmBLDAQAIeZCUmZkpI0aMkOrVq0vBggWlZs2a8vzzz+eqhxOoWbNmSd++faVChQrah+CECRN0txszZoxUq1ZNkpKSpG3btrJw4ULT2gB7cL7uzHz9AQDgV5D0yiuvyAcffCDvvfeerFu3Trv+6quvyrvvvqu7/dy5c+X8+fP5bl+7dq0cOHBA9zGnT5+Wpk2bakGQOz/88IMMHTpURo0aJUuXLtW279mzpxw8eDBnm2bNmkmjRo3y/ezdu5f/fQAAYG4xyXnz5slVV10lffr00a6rnpzvvvtOtxcnKytLBg8eLLVr15bvv/9e4uKyc0o2bNggXbt21YKc4cOH53tc7969tR9P3njjDbnnnnvkjjvu0K5/+OGH8scff8jYsWPliSee0G5bvtz60wuR7ZHudeTNqRvD3QwAAPzvSWrfvr1MmzZNNm7M/kBbsWKFzJkzRzeoiY2NlT///FOWLVsmAwYM0IKmLVu2aAHS1VdfrRsgGZGeni5LliyR7t275zqWuj5//nwJBtWr1aBBA2ndunVQ9h/thnSvnfN77TJFwtoWAAD86klSvTSpqalSr149rWdI5Si9+OKLcuutt+pur/KKpk+fLpdddpnccsstWhCjghk1ZOevw4cPa8ctW7ZsrtvV9fXr1xvej2qHCvLU8F6lSpXkp59+knbt2uluq3rE1I8692LFivnd9mgzqHNNGf7fldK7UTnDj3n3luZyPO28/LXmgIydu83jtmQiAQAsEyT9+OOPMm7cOPn222+lYcOG2pDWww8/rAVDAwcO1H1MlSpV5Ouvv5ZOnTpJjRo15LPPPrPErKSpU4O/3k60u6FVZWlTraRULlnI67afDWwle0+clXrlimrX42JjvAZJAABYZrht2LBhWm/STTfdJI0bN5b+/fvLI488Ii+//LLbx6gE7XvvvVebsZaWlqZtH4iUlBStFytv4re6Xq6c8R4LhEa1lMJawONNt/plpf8lVXOuu05YS07yHM9f3byidgwDhwEAIDhBkgpyVP6PKxWwqHwjd0Nj3bp1k/r168v48eO1fCY1M+2xxx4TfyUmJkrLli21fTmp46vr7obLYG8tqpTweH9KkQKy9t895eP+rULWJgBAZPN5uE31BqkcJDWEpobbVFK2mml255135ttWBS4qobtq1apaYBQfH68lP0+ZMkVL3q5YsaJur9KpU6dk8+bNOde3bdumDeuVLFlSO66iZsap4b1WrVpJmzZt5K233tJyi5yz3RD56pVLlpZVLwZPBeLjxAKjuACAaA2SVD0kVUzygQce0GoSqVyk++67T0aOHJlvW9Xj9NJLL2lJ26r3x0nVNFL5QKVL51+wVFm8eLF06dIl57oKiBQVFH3xxRfa7zfeeKMcOnRIO+7+/fu1mkiTJk3Kl8yNyPTclQ1lQLuqAeW23duxhnw8a6v2e4H4WDmXod8bCgCITj4HSWoZD9Vro36MuPzyy3Vvb968ucflQoxUUH7wwQe1H0Q+vVeDXoDkS8z01BX1c4Kk4oUS5EDquUCaCACIMFG9dhvgVLxgonStVybczQAAWAhBEmwhbwdRMNZq61RHf/gXABCdCJIQUSoWL+Tf40oUNL0tAAB7I0hCRKlbLtmvxxVNig9K7xQAwL4IkhA2qsJ2ycKJ8uWdbYJ+rKGX1wn6MQAAUT67DTCLqrC95Jnufk3jD0afjxWWygEAWAc9SQgrT4GJmcNfN7Wp7HUbhtsAAK4IkmBLnvp8fhrUTp7oXS/XbWWSk+TpK+oHvV0AgMhBkARb8tTn07paSRnUqWa+2+PjPA+n0Y8EAHBFkISoERdLzhEAwDiCJNjCVc0qBLyPxDhe7gAA4/jUgGW5Dn/1a17R1H2XSS6QL4GcvG0AgCuCJNhyFpw/AY3rLlRytyernu0hs4d38f0gAICIQZCEqFS1VOF8txUtmJDze3JSglQu6X2Jk4466709mWdmHQDAngiSAJe8p75NK8hL/RobfsxXeaqFL36mu9ynM7POiHgSywFAcz4zS6yAituAiBRMjJOEuFh59+bmhh9TODEu1/UiBeIlpUjuXCdfNKlUTJbuPO734wEgUmRmOSQh91tsWNCTBFvyJ8e6Z8Ny2gy3y2qn5Nz28jWNpWnl4n6t7fb2TcYDKn/OSW8oDwAQOvQkwbIaVSymXapFcM1QvFCirH6upyS4FJW8uU0V7cedfzUpL7+v3Kd7X0qeGXLJSYH9OeVNRmfwDQDCiyAJlqWGr1RQY2Z9o8R43/b13i0tpEOtnfLk+FX57mtWuXiu65VLeE/09kUg6+22qV5SFm47amZzACDqMNwGywdKvgY2eT1/dSMJF73lUULhvo41PN6vhhgBAJ4RJMGWHD4USipfNElCISE+f9dP36blDT32/7rWyndbMIfbxt3dNoh7F3m4e+2g7h8AQoEgCfCiZdUSHu8ffU1jqVaqkLxwtfHSAXnVLpvstYCmUSqPyptgVxuoWLxgcA8AACFAkISIF+hqI3XKJsvv/3epLHyqm+79N7WpIjOHdZHqKfkLVOY15pYWUrdssjyaZzadilnMzL3yJb7qVKe0acnxTrGBJFQBgEUQJMGWiiZdrI7tjioMWaN0YelY5+KU/0Bm2pUpmiTd6pXRrve/pKqhx8XkGTTr06S8TH6ko9ym8/gX+uXOnQpmmOHarkKJcVLUZWaeet4C0bVeGalbLn/PGADYDUESbEVVw1aBRr8W3he8VYUhpw3tJAXizatIpma7fXFHa3nmX/UD2k+Jwona+nCuvV2qx+q5Kxvm3BZIZ0zLKiU93p93365De77ke+kZe3vrgB4PAFZBkARbuaVtFW3ISlXHNsLfvB5Plbk71y1jSuCl1ofLa2D7anJ/55paLaehl9f1e9/FCnnvaQsmI+veAYDVUScJsJjHe9XTcpYCzaXylxnHLVYwQZISYuXseWusvwQA/qAnCbCg+LhYv3OSfA1yVN6W67HiTOp9q0JvEgCbI0gCgsgRtv4gz1Qc9PP97bWikw92yV3TaHgv/4f5AMAMVpkgy3AbEASd65aWQyfPSb1yRQ1tHxOm+k/OGlDJBS/mMFUyeXkVf912SRX55p+d4W4GgChGkAQEwecXZngFkjju72Nj/ChN8OYNTWXI98tlcJf8lb/DpUZKkXA3AUCUI0gCgsDsWXW+MDLAl7d5NUoXkd/+71Jz2xHASOMTvetJu5qlzGwOAPiMnCQAlmPWwsDFw1wKAYC9ESQBFhXcituB2fLSFflua1W1hPz50GUXjxHgQQJ9/L0da8jEIRfbAwC+IkgCIkzTSsWCPhwYp7NC7h0dqkuDChcT1QMs3B2wwonxUr4YC+0C8B9BEmABRiuIe9K2ekkZ8a8Gcnv76l63tcjsWo/yBlmX1U6R8Q+0l1JuFuMtXyxJogU9ZIh0MRZ5lyJIAsLowS615NJaKdK9fvbCua7ydvao3pu7L80dABWIv/gnfGWzCnLXpdUl0eU2d0KRV16huLm9OOr8W1QpoS00rOfmNlUkWqh1/gAEH7PbgDB6rKexwo1qvboeDctqPU6fztmmu03DCt6H2UI5++6Va5vIyF9Xy19rD5i638KJcbYu5GmWGimFZevh0+FuBhDR6EkCbEAtHeIckuvbtEKuS6dmlYtLOOWNu8oVS5KPB7Tye3/ucpoKFdD/blfIYPAUCawxEAFEPoIkwKLc9fa8dl0T+WxgK3n12iaG9tOmekntMtGEvCelnJvhLn8lxOU+zx4Nyvo9HHd7+2o+P65qKWtUGPcZkRIQdARJgA24xktJCXHSrX5ZKWiw5+SBzjXlp0Ht5KP+LU1pi9lFJ38a1F5aVCku397TVj4Z0EreuqmZ3/tSz42v/h7WRcw0oF1ViZZ1rYBIR5AE2JiRD0vVw9K6WslcSd6BKJ1cQMykhgnHP9BB2tdMkcsblJVCifE5w3Xhsvq5ngHVZwIQGV8ECJKACBd74d3GjDRm1dMTKiULJ8ovD7TPue58z/T03ulaHiCQGWBF3OQ9WX1JGgDmIkgCItQtbatIm2oltfpJvnq8Vz3tUpUUcBr5rwZaT487wSge2bxKCcPbquP3b1dVqpUqJC2rlpDejcpJOIQiRCIQA0KDEgBAhHqpX2O/g5hBnWrIFY3LSZWSheQzNyUHrEjlJM00OccIQPSiJwmwgUolCgVcldaXukGqp6JqqcK5eiyaVwlviQGnoZfXEav46s42+W6jkweIHPQkARa2fOTlkp6ZFVCOTKDDYXMe7yI7j6b5NPQVTE0rF5d1/+4l9UdOCndTpGOd0mFbToFYDAg+giTAwooX0l+nzLXG0Jnzwe/FcteTFS565Q+s0oOTnMTbqhFqtuW5jKxwNwPwiOE2wMY+v6ONtrDr+7e28LqtFRbp8KcMwUNda2nB4OO9s5PJw+GVaxvL38M6S4dapTxu1695RSlsQq9fNDCrbhciU4xYA3/NgI2pWVzzn+xmaFtHMKafhcDQHnXloW61Jd5LxXB3p7fq2R4yYdkeub5VZak3wvchOpWLdWPr7MVzx919ibw5ZaO8PW2T7rajr82dLB9M9vzfvIgZerADepIAhIy/n4veAiRPkpMSpH+7atrMt7zruxlZ785obHlr2ypSIN59xe+eDf1bbgVA+BAkAVHC7J6Hz29vLde0qCh28s9TuXvdXItVBtM7NzeXj/q3ktplioidqRpUZqEfCXZAkAREC5OjpC71ysgbN/i/zlo4FE1KkOKFEnwa8sm7Sd6n0bkg74B27hfXrVm6sHZ5dXN7BZV5MUSGaENOEhAixQpe/HAOB1/qJIXCM33qS6tqvlcDt2IC8qlzGdqwnp7rW1aSBuWLSiQgREK0oScJCLJxd7fVPiS/vit/4cFQCnbedqUSBX3a/u7LahjKCTKqQYWiYetdcRcgDepUU167vmlQemAiKWDppFNvCtEtxiK9lvQkAUHWoVaK/DnksnA3I2hB0g/3XiI7jqZpRR69CUahxeophbW15i6p4Xl6Pqzrstop8vfGQ+FuBizEob1hhT9QoicJQEDa1iglN7SqHLbj1ypTRHoFcTHbvL1dhXUKWRoVH+v5Tb9icd9640JNzRA0YvvoPtriyp7EuTwXBQzuFwg1giQgSrSrWUqSC8RL62olwj6sYmYw4Ot3zQrFjB17yiMd5bEedWRYz7q5bu/frqq0r1lKRvVt4L1teRp3S9sqkuihnEGjitbOXRrxL+/nbDQHzjVIKlc0KaB2IfI4xBoYbgOihKoEvWTE5Vr16nB55bom0qJqcenbtIJp+1QVx32hqpM/99saub9zLd37n7oiu7J37bLJ2k9ehRLj5dt7LvGrrSp36a9HOkrn/8z06/GLn+kuiQarlqvq4HM3HxEzVShu/LnO8uFTLvyDKoA+epKAKKI+YEOdEOmcIu+c4Xdvx5pS3mBvjidf3NFarmpWQavI7YtqKYW15VzaVM8/HNS0UjGtfYFyBqIda5ubkJxSpIBWxsAIVR1cj8od8zdh3pecMk8V3j+/o7VfxwdCjSAJQFDVLhucAoqd65aRt29qHvbSCnr+ebKbVqhSDXGGIrm+S13jwVit0kVkwuAOfh3Hl/jaU49Xl7plxOqusXlNK7uLs8jsNoIkADBZqSIFpHmV0OV+fXBbSxne62KP2k+D2mmXXeu5D0bmPdHV1Dbc0SG7mOZ1LStply/2ayxVShaS0dfor2dn9aUEX7+habibENVivUxyCBWCJABwCvG3V1VHyV+uw6Zq1lnzyheDstYXZpap5VDcqVC8oG4S/4zHOmvDjv4k5S8dcbm8dl0T7XrN0kVk1vAuclObKmHNg8tbwNQXnoJMRAeCJAAIk4e715ZfXYa+VL6UGTk/TkUKxPtVd8oZZPmqZOFE3Zy3uy6tEbTY9F9Nysvdl1aXtjo5Znl5y8VyDSrV0+uuFwzRg9ltABBGrgHCQ91qy5n0TLmicXm5+ZN/gr7sjBoaW7T9mM6+7NFB98GtLaR34/La77cYeL6Keslf69ukvPyxcq+W52aV4Z5oVN7HGavBRE8SgKC6vX117UOn/yVVw90US3B4mDGmen6ev7qRbsJ3wyAsu6KKgKoE8z4XAg1fgp1eDXMX8AzmrEnVo3N/5/xDk84Ayag6OiUd8p7DR/1byavXBZaPNNJAPamJFqjCb1WPXF5HrIKeJABBVTq5gJar4lo8ENmMxBX3dayhTf2/qnkF04+pggKVYF4wcWfubdw+9uI9RQvm/vgwu0hp8UIJcjztvPa7yms6kXZePpi5RSJF/QhZ9DjS0ZMEIOisHiD1vrCsyf2dPOfOhEPxQolyT8caUibZ8xBEy6oltGGKdmFYw06VFFBFNkOpe33rJlVbfOKe5cWIdRAkAYh6Y25poU2J79XIt+EbKykQHydzHu8q397TNmgf8urDSyVJJyXEyoNdaufc7m32WqnCiR7v92ekrmml3EnYz13ZUBvWfbJ3dsV0wAwESQCinkrSVVPi7UQvB0j12HnLDQr0W/oz/2ogq5/tKVVKFXLZp+e93haEfLS8p6mWkFk24nK5r1NN+fn+9lKjtPGZgrCWGIsUklQIkgAgjLx9Hvgza81XA9tlF4LsbLByd7yHRXr1qDpON7Wu7Pa8yhU1EKDmeZ7K6CyK65yRpoYeh/cMvEepSJJ5Q4i3XVLFtH1FuhixDoIkAIhyjSsVk+UjL5exAz2vqRasL/gNKhTVZrC5pq7lPZQaSruicTmtDbe3r2Zg2ZDAg0uVZ6V6pdQMQPfbxBna1/NXNTK03daXrpDVz/WUG1plVy73JBz5Z6HQp4l1hr0JkgAgirgLdFSCuLfaQO6G1YwET95qX6oZbB3rXOzJ0tv8/VtbyraX+8izVzb02ptl1rInqlfK0xIzqtK4q5f6NTZU6NMTVQrimhbeg6ShPawzVd5MqufRKgiSAACWWFdN1RdKKZIoT1/h2/Ih3tzZoboM63lxbbtAuRuWHNylptzS1pxhtUsM9BLFWih3J1IRJAFAFAQsifHZb/e3tvU/iTqQz+S8j9UrGVCjdBFZ9HR3reRBoFyftpF9G8jgLrXETDe3yQ6GVKK4mdz9d6sio2YsO+OOp0T3GiYnwbeo4nl5GCuhmCQARIEv7mgt9coV1dZXswK1iK5KZq5c4uIsOTNnNgW7N+zlaxrLs1c2kHMZWR63e/36ptLEjwWD87q8flkZMWF1zpp/avmauuU8VxD3xbShnaT6k3/muq1GSmF5/Yamkno2QwaOXaj7OFWt/Y9V+3wOMJfuPC52QJAEACGuQO4UHxur1RwKhaJJCQEHSO4rcfuxr5gYeeHq4C0gG4xlXJxiXGpTpbsESXqB2bUts3OLXPOUnL16Rn3Uv2Wu5/iODtlL/QTb9Mc6a5d/bzzkdpu3bmomtcoUkbenbTK8X1VB3i4IkgAghNQQyfRHO0lCXKxW16hm6SLaN+vSRYLTw/PC1Y1k7/Ez0qhi4L0ZdlItpbD8/n+Xeg0MyxYtIAdSz/m0b2/Vz/WCJdceMmeAU6JQghy7sPSKng0v9JJDJ89JpRKF5EDqWQmXGA/3qdexWmvNlyApb7K7lREkAUCIqdwb1w9PNXSTV8XiBWXP8TPSo0HZgI7lTyFHX3NQChtYkiQcOcbBCgyfcKnqHczCh6qnSgVIkSbW4ssUuSJxGwAsaNqjnbSlUlQl6VBThR+HXl5H/juoXe478ny2vdivkQzvVVcqlzT/g9zZ26CCRStRM+9K+DlsqZZOUTHV2zc2c7tNoOUDnKY80tHwjL565ZK1YG/GY52lVVVzFyq2e1FNepIAwKK1YsK1VIqqQfRQt4trs7mrkxTITDkjw4RNKhXXCkjasRjii3+uyzeLa2D7anJr2yo+VyxXyiQX0Panpv0XNVAJXAXXf6094HU7lQT+f12z/6+rpxSW925pIX3fmyM356mQbpbrW2bvVwWL4So14QuCJACAWx1qlZK5m4+E/LjJSQly16XVg34cXz+ojYyuqeB21bM9dIch/QmQso8bo1X/dv5uFlUxXOXGOZUrliQLn+qW6xgxbg6nKp97CupUPtj6/Sdz3e4M/O0QICkMtwEA3HrnpuY5v0di7cJgnZMK8rxWMPfx4Gr7QAKkp64wtp6d0WM8e2VD3dvXP99LFjzVLddMTuX7ey/Jd5vVESQBAAyJwBjJ1Ocky4TekWBW0f5XkwoSbFc1q6ANFesFWk0r2aeIpBPDbQAAt9SabglxMdrwSChq84Sar8M+Dcq7r7+Ukem5sKQnfz50mVY/KVgzv769p23Yctx88XH/lmIlBEkAYEOh6tVR+Sqrnu0ZUD6NlfqhfhrUTgaPWyoHT2bXRjLacTPp4ctk88FT0r5WitttzvsYJDWvXFymrT8oBeJjpUEQil+6zpRrXzPFp0WLXdX1obJ3FZeZjr4GoGo2p9UCOYbbAAAeqeETK63MHojW1UrKfwdlJ0AbDRIUtaSL3nCVa5B13YWZW0a9el0TGdSppvw55DKxsjLJSTLzsc6y+JnuHnuqBrSrKvd3vriWncPtSnSSk5Tfs2FZrWJ35ZIFpWxRz0U6w4GeJABAVHENbDx9kPuaQ6Sqd/uiVJECuQpTmmXIhfINenlBfz3SUf67ZLd8PGurzxXMPVE9Ve56q/Sea3Xe3euXleZVimtVuxXXWXZWQU8SACDorm9VKWdh23BzHQYKdCq66mFTNaXu61RDyligJ0QteaNqH7lTp2yyPHVF/ZC0JcZDL50KjNrVLKU9fyo4smKApNCTBAAIuhZVSsj8J7vaanFTo1R1cisueYPA0ZMEAAiJ8sUK5gytwBz9mlcM+hIn0YxXKwDYUMMgzIaKFoHmIVnJmx7WgbOaBBsGyAy3AYANPXdlI616cb/m2bk+gJV9NrAVQRIAIDSKFUqQp/s0CHczbK9gYmSUNvBX6+olg7bvGJdc7PoeinBaGUESACBqvX1Tc3n4+2XyiIWSr83iKSVJra22+1iaNKscvKVCEl16jpKT7Blu2LPVAACYoF65ZJk5rItEG1W40Z/ijdVKFZLtR9KkXY1SXrd1HV5TC/7aEUESAAAw5Nt7LtGKUd7atopEA4IkAABgSIXiBbXimdHCfqnmAAAEIFrKB0XJaQYVQRIAADY2uEv2orLP9AnNciPRhOE2AABs7LEedaX/JdWkXLHwrx0XaehJAgDAxmJiYgiQgoQgCQAAQAdBEgAAEbz4rZGaRtBHThIAABGocslCsvq5nlIoITxLrwzpXlsmrdkvA9tVFbsiSAIARBWHm/XFIlGRAuH7mK9fvqhseKGXFIi37/p4DLcBAICgKGDjAEkhSAIAANBBkAQAiCrFC15cbDUu0sfbEBBykgAAUaVE4UT5bGArbSgo3mWleiAvgiQAQNTpVr9suJsAGyCEBgAA0EGQBAAAoIMgCQAAQAdBEgAAgA6CJAAAAB0ESQAAADoIkgAAAHQQJAEAAOggSAIAANBBkAQAAKCDIAkAAEAHQRIAAIAOgiQAAAAd8Xo3wj2Hw6FdpqamhrspAADAIOfntvNz3AiCJB+dPHlSu6xcuXK4mwIAAPz4HC9WrJihbWMcvoRUkKysLNm7d68kJydLTEyMqRGuCrx27dolRYsWlUjEOdpfpJ+fwjlGBs7R/lJNPj8V7qgAqUKFChIbayzbiJ4kH6kntlKlSkHbv3ohROKL3RXnaH+Rfn4K5xgZOEf7K2ri+RntQXIicRsAAEAHQRIAAIAOgiSLKFCggIwaNUq7jFSco/1F+vkpnGNk4Bztr4AFzo/EbQAAAB30JAEAAOggSAIAANBBkAQAAKCDIAkAAEAHQRIAAIAOgiSLGDNmjFSrVk2SkpKkbdu2snDhQrGal19+WVq3bq0tyVKmTBm5+uqrZcOGDbm2OXv2rAwePFhKlSolRYoUkWuvvVYOHDiQa5udO3dKnz59pFChQtp+hg0bJhkZGbm2mTlzprRo0UKb+lmrVi354osvJBxGjx6tLT/z8MMPR9Q57tmzR2677TbtHAoWLCiNGzeWxYsX59yvJr2OHDlSypcvr93fvXt32bRpU659HD16VG699VatEm7x4sXlrrvuklOnTuXaZuXKlXLZZZdpr2u1vMCrr74akvPLzMyUESNGSPXq1bX216xZU55//vlcC1va7RxnzZolffv21ZZUUK/JCRMm5Lo/lOfz008/Sb169bRt1Gvnzz//DOr5nT9/Xh5//HHtWIULF9a2GTBggLZElF3Oz9s55jVo0CBtm7feeiviznHdunVy5ZVXatWv1f+n+lxR75mWfI9VJQAQXt9//70jMTHRMXbsWMeaNWsc99xzj6N48eKOAwcOOKykZ8+ejs8//9yxevVqx/Llyx1XXHGFo0qVKo5Tp07lbDNo0CBH5cqVHdOmTXMsXrzYcckllzjat2+fc39GRoajUaNGju7duzuWLVvm+PPPPx0pKSmOJ598MmebrVu3OgoVKuQYOnSoY+3atY53333XERcX55g0aVJIz3fhwoWOatWqOZo0aeIYMmRIxJzj0aNHHVWrVnXcfvvtjgULFmhtmTx5smPz5s0524wePdpRrFgxx4QJExwrVqxwXHnllY7q1as7zpw5k7NNr169HE2bNnX8888/jtmzZztq1arluPnmm3PuP3HihKNs2bKOW2+9VXvNfPfdd46CBQs6Pvroo6Cf44svvugoVaqU4/fff3ds27bN8dNPPzmKFCniePvtt217jup19PTTTzvGjx+vIj3HL7/8kuv+UJ3P3Llztdfqq6++qr12n3nmGUdCQoJj1apVQTu/48ePa39PP/zwg2P9+vWO+fPnO9q0aeNo2bJlrn1Y+fy8naMrdb86jwoVKjjefPPNiDrHzZs3O0qWLOkYNmyYY+nSpdr1X3/9NdfnnZXeYwmSLED9sQ8ePDjnemZmpvbH8fLLLzus7ODBg9ofwd9//53zRqb+0NQHktO6deu0bdSbmqJezLGxsY79+/fnbPPBBx84ihYt6jh37px2ffjw4Y6GDRvmOtaNN96oBWmhcvLkSUft2rUdU6ZMcXTq1CknSIqEc3z88ccdl156qdv7s7KyHOXKlXO89tprObep8y5QoID2hquoNx11zosWLcrZZuLEiY6YmBjHnj17tOvvv/++o0SJEjnn7Dx23bp1HcHWp08fx5133pnrtmuuuUb74IiEc8z74RPK87nhhhu059dV27ZtHffdd1/Qzs/dlxi13Y4dO2x3fp7Ocffu3Y6KFStqAY76MuMaJEXCOd54442O2267ze1jrPYey3BbmKWnp8uSJUu0rnHXRXTV9fnz54uVnThxQrssWbKkdqnOQ3WLu56L6s6tUqVKzrmoS9W1W7Zs2Zxtevbsqa32vGbNmpxtXPfh3CaUz4fq6lVduXnbEQnn+L///U9atWol119/vdZN3bx5c/nkk09y7t+2bZvs378/V/tUt7gaBnY9R9XVr/bjpLZXr90FCxbkbNOxY0dJTEzMdY5qiPbYsWNBPcf27dvLtGnTZOPGjdr1FStWyJw5c6R3794Rc46uQnk+Vvj7dL7/qOEcdU6Rcn5ZWVnSv39/beioYcOG+e63+zlmZWXJH3/8IXXq1NGOp95/1GvUdUjOau+xBElhdvjwYS1/wvU/W1HX1ZueVakXu8rT6dChgzRq1Ei7TbVX/WE637T0zkVd6p2r8z5P26g/gDNnzkiwff/997J06VItByuvSDjHrVu3ygcffCC1a9eWyZMny/333y8PPfSQfPnll7na6Ok1qS7VG5yr+Ph4LWD25XkIlieeeEJuuukm7c01ISFBCwTV61XlckTKOboK5fm42yaU56tyVlSO0s0335yzOnwknN8rr7yitVn9Peqx+zkePHhQy59SuZ69evWSv/76S/r16yfXXHON/P3335Z8j43381wR5VRPy+rVq7Vv55Fk165dMmTIEJkyZYqW0BiJVICrvom+9NJL2nUVQKj/yw8//FAGDhwokeDHH3+UcePGybfffqt9I1++fLkWJKlk0kg5x2ilehluuOEGLVFdBfuRQvWgvP3229oXNNVDFqnvPcpVV10ljzzyiPZ7s2bNZN68edr7T6dOncRq6EkKs5SUFImLi8uXua+ulytXTqzowQcflN9//11mzJghlSpVyrldtVcNHx4/ftztuahLvXN13udpG/WNUc3aCfYblfq2o2ZEqG9o6kd9w3nnnXe039U3Ebufo5r91KBBg1y31a9fP2d2ibONnl6T6lI9T67UzBI188aX5yFY1HCFszdJdcurIQz1puzsHYyEc3QVyvNxt00oztcZIO3YsUP7IuPsRYqE85s9e7bWfjWs5HzvUef56KOPajOfI+EcU1JStPPy9v5jpfdYgqQwU92KLVu21PInXKNtdb1du3ZiJeqbmwqQfvnlF5k+fbo2vdqVOg81tOF6LmocXL34neeiLletWpXrD935Zuf8w1HbuO7DuU0ono9u3bpp7VM9D84f1euihmmcv9v9HNUQad7SDSp3p2rVqtrv6v9VvcG4tk91UaucB9dzVG9iKqh0Uq8J9dpVOQbObdR0YPXB5nqOdevWlRIlSgT1HNPS0rQ8DVfqy4jzm2wknKOrUJ5PuF67zgBJlTWYOnWqNj3cld3PTwXyauq+63uP6vlUAb8aFo+Ec0xMTNSm+3t6/7Hc54hPad4IWgkANQvliy++0GYv3HvvvVoJANfMfSu4//77tSnGM2fOdOzbty/nJy0tLdfUTVUWYPr06drUzXbt2mk/eadu9ujRQysjoKZjli5dWnfqppoiqmY1jBkzJiwlAJxcZ7dFwjmqWUHx8fHaNPlNmzY5xo0bp7Xlm2++yTWdXL0G1dTclStXOq666ird6eTNmzfXygjMmTNHmw3oOhVZzVJRU5H79++vzdRRr3N1nFCUABg4cKA2Q8hZAkBNR1ZThNWMF7ueo5pxqaY7qx/11v3GG29ovztnd4XqfNT0cfX6+c9//qO9dkeNGmXK9HFP55eenq6VNKhUqZL2N+X6/uM6i8vK5+ftHPXknd0WCec4fvx47Vgff/yx9v7jnJqvyhlY8T2WIMki1AtFvShUvSRVEkDVwLAa9YLX+1G1k5zUG/IDDzygTUFVL9B+/fppb2Sutm/f7ujdu7dWu0N9cD366KOO8+fP59pmxowZjmbNmmnPR40aNXIdI9xBUiSc42+//aa9yajgvF69etoblis1pXzEiBHam63aplu3bo4NGzbk2ubIkSPam7OqP6Sm3t5xxx3aG6QrVa9HlRtQ+1BBi/ogD4XU1FTt/0z9TSUlJWnPr6rd4vqBardzVK8Xvb8/FRCG+nx+/PFHR506dbTXrppm/ccffwT1/FSg6+79Rz3ODufn7RyNBkmRcI6fffaZVt9J/W2qmk+qtpcrK73Hxqh/fO80AwAAiGzkJAEAAOggSAIAANBBkAQAAKCDIAkAAEAHQRIAAIAOgiQAAAAdBEkAAAA6CJIAAAB0ECQBAADoIEgCAADQQZAEAAAg+f0/tZ+rmxRThUoAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lossi)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8aaa78-e607-4bc4-8cde-49a529984c1b",
   "metadata": {},
   "source": [
    "## Trained Decoder Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "974730ff-1cda-43e0-a72f-df0986bb9daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from scipy.stats import ConstantInputWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94fd5355-31c8-47fc-a2fd-9b1fa9f91576",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.eval()\n",
    "correlations = []\n",
    "mses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "417a9438-c9f1-4cb0-8da2-e9fe7ec831a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_steps_per_epoch = val_total_examples // BATCH_SIZE\n",
    "test_steps_per_epoch = test_total_examples // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27c89be7-508c-4e06-9dfc-9f4809807077",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  41%|                                               | 140/345 [00:15<00:22,  9.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading /Users/djemec/data/jepa/v0_2/training/val/shard_k562e_val_0000.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|| 345/345 [00:37<00:00,  9.16it/s]\n"
     ]
    }
   ],
   "source": [
    "for step in tqdm(range(val_steps_per_epoch), desc=\"Evaluating\"):\n",
    "    \n",
    "    # Custom Loader Call\n",
    "    cont_x, cont_tot, case_x, case_tot, act_id = val_loader.next_batch()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        z_context = model.student(cont_x, cont_tot)\n",
    "        z_pred = model.predictor(z_context, act_id)\n",
    "        \n",
    "        pred_delta = decoder(z_pred) - decoder(z_context)\n",
    "        real_delta = case_x - cont_x\n",
    "        \n",
    "    # Per-Sample Metrics\n",
    "    for i in range(len(pred_delta)):\n",
    "        p = pred_delta[i].cpu().numpy()\n",
    "        t = real_delta[i].cpu().numpy()\n",
    "        \n",
    "        # Metric: Pearson Correlation on Top 20 DEGs\n",
    "        top_20_idx = np.argsort(np.abs(t))[-20:]\n",
    "        \n",
    "        if np.std(p[top_20_idx]) > 1e-9 and np.std(t[top_20_idx]) > 1e-9:\n",
    "            \n",
    "            try:\n",
    "                corr, _ = pearsonr(p[top_20_idx], t[top_20_idx])\n",
    "                correlations.append(corr)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        mses.append(np.mean((p - t)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "750afdb9-ba92-4a7a-874d-0cc221525de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global MSE: 1.0973\n",
      "Top-20 Pearson R: 0.3589\n"
     ]
    }
   ],
   "source": [
    "mean_mse = np.mean(mses)\n",
    "mean_corr = np.mean(correlations)\n",
    "print(f'Global MSE: {mean_mse:.4f}')\n",
    "print(f'Top-20 Pearson R: {mean_corr:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "44680d47-980a-4b0e-b910-471b65efd220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " NEEDS IMPROVEMENT\n"
     ]
    }
   ],
   "source": [
    "if mean_corr > 0.75:\n",
    "    print(' SOTA COMPETITIVE (Matches GEARS)')\n",
    "elif mean_corr > 0.40:\n",
    "    print(' FUNCTIONAL (Better than random)')\n",
    "else:\n",
    "    print(' NEEDS IMPROVEMENT')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6b2741-9067-4cee-b37f-2ee471d3c7e6",
   "metadata": {},
   "source": [
    "## Trained Decoder Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd5cb77-d9a4-43d8-ad77-b6d102d00caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_correlations = []\n",
    "eval_mses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c58931-04dc-4c75-a84d-23fbf3f7abc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoaderLite(batch=batch_size, split='test', device=DEVICE, tok_dir=eval_dir)\n",
    "test_steps_per_epoch = test_total_examples // batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9022a3d0-68f1-4de8-bb53-33e5bd05e212",
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in tqdm(range(test_steps_per_epoch), desc=\"Evaluating\"):\n",
    "    \n",
    "    # Custom Loader Call\n",
    "    cont_x, cont_tot, case_x, case_tot, act_id = test_loader.next_batch()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        z_context = model.student(cont_x, cont_tot)\n",
    "        z_pred = model.predictor(z_context, act_id)\n",
    "        \n",
    "        pred_delta = decoder(z_pred) - decoder(z_context)\n",
    "        real_delta = case_x - cont_x\n",
    "        \n",
    "    # Per-Sample Metrics\n",
    "    for i in range(len(pred_delta)):\n",
    "        p = pred_delta[i].cpu().numpy()\n",
    "        t = real_delta[i].cpu().numpy()\n",
    "        \n",
    "        # Metric: Pearson Correlation on Top 20 DEGs\n",
    "        top_20_idx = np.argsort(np.abs(t))[-20:]\n",
    "        \n",
    "        if np.std(p[top_20_idx]) > 1e-9 and np.std(t[top_20_idx]) > 1e-9:\n",
    "            corr, _ = pearsonr(p[top_20_idx], t[top_20_idx])\n",
    "            eval_correlations.append(corr)\n",
    "            \n",
    "        eval_mses.append(np.mean((p - t)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ee16bc-f9f8-4ab6-bc12-9e72fafe1b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_mean_mse = np.mean(eval_mses)\n",
    "eval_mean_corr = np.mean(eval_correlations)\n",
    "print(f'Global MSE: {eval_mean_mse:.4f}')\n",
    "print(f'Top-20 Pearson R: {eval_mean_corr:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
