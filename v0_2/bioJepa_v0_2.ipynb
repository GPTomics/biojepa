{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfe968dc-a937-469a-89d8-7ba179a2e01a",
   "metadata": {},
   "source": [
    "# Bio-JEPA AC \n",
    "\n",
    "Based on [V-JEPA 2 AC](https://arxiv.org/abs/2506.09985)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b67dcd-a768-447b-ad7b-77e8cddd59c0",
   "metadata": {},
   "source": [
    "**Updates**\n",
    "The main update here is to do pre-train an action-free model. This means we'll run masked training on the student/teacher model then freeze those and train the action predictor after. \n",
    "\n",
    "In pretraining we'll used the MaskedPredictor instead of the ActionPredictor to help drive training of the latent space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33a357f2-d497-4dfc-9920-568422b5403a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import copy\n",
    "import math\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9242cf00-c812-41d0-ae92-9c7dd0af6f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d513e0d-33a2-400c-a40a-194b365bdd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: nvidia-smi: command not found\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a66afb3-044f-44b3-bf5a-3b34e37e2f95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3396f99a-eb28-4ea3-a10d-4e64f777fa08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cpu\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    device = 'cpu'\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(1337)\n",
    "        device = 'cuda'\n",
    "    print(f'using {device}')\n",
    "    return device\n",
    "\n",
    "DEVICE = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d6fc33-8c5c-43bf-960a-2f83fea1601e",
   "metadata": {},
   "source": [
    "## Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358918d9-36ba-48e5-aade-398109b4d660",
   "metadata": {},
   "source": [
    "### Multi-Headed Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d1ff083-ffb8-4102-a8ea-e1f6f17bee12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioMultiHeadAttention(nn.Module):\n",
    "    # mirrors nn.MultiheadAttention(dim, heads, batch_first=True) \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        assert config.embed_dim % config.heads == 0\n",
    "        \n",
    "        self.head_dim = config.embed_dim // config.heads\n",
    "        self.heads = config.heads\n",
    "        \n",
    "        # Projections\n",
    "        self.q_proj = nn.Linear(config.embed_dim, config.embed_dim)\n",
    "        self.k_proj = nn.Linear(config.embed_dim, config.embed_dim)\n",
    "        self.v_proj = nn.Linear(config.embed_dim, config.embed_dim)\n",
    "        \n",
    "        self.c_proj = nn.Linear(config.embed_dim, config.embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # Batch, Seq, Embed Dim\n",
    "        \n",
    "        # 1. Project\n",
    "        q = self.q_proj(x).view(B, T, self.heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(x).view(B, T, self.heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(x).view(B, T, self.heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # 2. Standard Scaled Dot Product Attention (Permutation Invariant)\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=False)\n",
    "        \n",
    "        # 5. Reassemble\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.c_proj(y)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715a67a5-efb2-4b22-8ec0-54a0294e01e2",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bb01e77-936e-4235-a962-9cb7656f889b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.c_fc = nn.Linear(config.embed_dim, int(config.mlp_ratio * config.embed_dim))\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "        self.c_proj = nn.Linear(int(config.mlp_ratio * config.embed_dim), config.embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5be00e5-ce0a-4335-95c5-2519e8157824",
   "metadata": {},
   "source": [
    "### Hidden Transfomer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02208352-b690-47af-8c90-06210718b5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CellStateBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.ln_1 = nn.LayerNorm(config.embed_dim)\n",
    "        self.attn = BioMultiHeadAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.embed_dim)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. Attention \n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "\n",
    "        # 2. MLP\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69942da-c18f-4585-9041-911fed8107f7",
   "metadata": {},
   "source": [
    "### Cell State Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e45f71db-0836-4c3b-bc8b-3957f4c02630",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CellStateEncoderConfig:\n",
    "    num_genes: int = 8192\n",
    "    num_pathways: int = 1024 \n",
    "    n_layer: int = 24 \n",
    "    heads: int = 12\n",
    "    embed_dim: int = 768\n",
    "    mlp_ratio: float = 4.0 # Changed to float for precision\n",
    "    mask_matrix: np.ndarray = None \n",
    "\n",
    "class CellStateEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Learnable Network, initialized based on known pathwasy 1 == connection\n",
    "        # wrapping as a \"parameter\" allows it to be learned\n",
    "        assert config.mask_matrix is not None, 'Must provide binary_pathway_mask!'\n",
    "        init_weights = torch.tensor(config.mask_matrix).float().T \n",
    "        self.pathway_weights = nn.Parameter(init_weights)\n",
    "        \n",
    "        # Learnable Gene Embeddings [num_genes, Dim]\n",
    "        self.gene_embeddings = nn.Parameter(torch.randn(config.num_genes, config.embed_dim) * 0.02)\n",
    "        \n",
    "        # Context Injector\n",
    "        self.total_count_proj = nn.Linear(1, config.embed_dim)\n",
    "\n",
    "        # Transfomer\n",
    "        self.blocks = nn.ModuleList([CellStateBlock(config) for _ in range(config.n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(config.embed_dim)\n",
    "\n",
    "        # Initiation \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None: \n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        \n",
    "    def forward(self, x_genes, x_total_ct):\n",
    "        # 1. Project Genes\n",
    "        x_genes = x_genes.unsqueeze(-1) \n",
    "        gene_repr = x_genes * self.gene_embeddings.unsqueeze(0)\n",
    "\n",
    "        # 2. Gene Embeddings @ pathway weights\n",
    "        x_pathway = self.pathway_weights @ gene_repr\n",
    "\n",
    "        # 3. Context Injection\n",
    "        x_total_ct = x_total_ct.unsqueeze(-1)\n",
    "        x_total_ct = self.total_count_proj(x_total_ct)\n",
    "        x_total_ct = x_total_ct.unsqueeze(1)\n",
    "        x = x_pathway + x_total_ct\n",
    "\n",
    "        # 4. Set Transformer\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # 5. Layer Norm\n",
    "        x = self.ln_f(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e165225-f747-4be2-a0a8-cc7b8b1b6d1f",
   "metadata": {},
   "source": [
    "### Adaptive Layer Normalization AdaLN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab2556a4-03b2-47e6-9392-eb2dbb648284",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaLN(nn.Module):\n",
    "    '''\n",
    "    Adaptive Layer Norm for conditioning the predictor on action embeddings.\n",
    "    The action vector regresses the Scale (gamma) and Shift (beta) of the normalization.\n",
    "    '''\n",
    "    def __init__(self, embed_dim, action_embed_dim):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(embed_dim, elementwise_affine=False)\n",
    "        self.action_mlp = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(action_embed_dim, 2 * embed_dim)\n",
    "        )\n",
    "        # Initialize to identity (gamma=0, beta=0 originally, effectively gamma=1 after logic)\n",
    "        # Zero-init the last layer so the action starts as a \"no-op\" (identity)\n",
    "        nn.init.zeros_(self.action_mlp[1].weight)\n",
    "        nn.init.zeros_(self.action_mlp[1].bias)\n",
    "\n",
    "    def forward(self, x, action_emb):\n",
    "        # x: [Batch, Seq, Dim]\n",
    "        # action_emb: [Batch, action_embed_dim]\n",
    "        \n",
    "        # Project action to style: [B, 2*D] -> [B, 1, 2*D]\n",
    "        style = self.action_mlp(action_emb).unsqueeze(1) \n",
    "        gamma, beta = style.chunk(2, dim=-1)\n",
    "        \n",
    "        # Apply affine transformation based on action\n",
    "        return self.norm(x) * (1 + gamma) + beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d966f7-8f5d-4c06-b6db-d02aaba87de8",
   "metadata": {},
   "source": [
    "### Action Predictor Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc8a2a58-7549-4bd1-908f-d82608063959",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictorBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # 1. Conditioning (AdaLN) replaces standard LayerNorm\n",
    "        self.ada_ln1 = AdaLN(config.embed_dim, config.action_embed_dim)\n",
    "        \n",
    "        # 2. Attention (Using the shared BioMultiHeadAttention)\n",
    "        self.attn = BioMultiHeadAttention(config)\n",
    "        \n",
    "        # 3. Conditioning (AdaLN) for the MLP block\n",
    "        self.ada_ln2 = AdaLN(config.embed_dim, config.action_embed_dim)\n",
    "        \n",
    "        # 4. MLP (Using the shared MLP)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x, action_emb):\n",
    "        # 1. AdaLN -> Attention  -> Residual\n",
    "        x_norm = self.ada_ln1(x, action_emb)\n",
    "        x = x + self.attn(x_norm)\n",
    "        \n",
    "        # 2. AdaLN -> MLP -> Residual\n",
    "        x_norm = self.ada_ln2(x, action_emb)\n",
    "        x = x + self.mlp(x_norm)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b7340a-5225-4131-83d2-614631a55a10",
   "metadata": {},
   "source": [
    "### Main Predictor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b641afb7-7d8d-4b4b-833c-72400e48610d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ACPredictorConfig:\n",
    "    num_pathways: int = 1024\n",
    "    n_layer: int = 6 \n",
    "    heads: int = 4\n",
    "    embed_dim: int = 384\n",
    "    action_embed_dim: int = 256 \n",
    "    mlp_ratio: float = 4.0\n",
    "    max_perturb: int = 2058 ## eventually try to get to a 2**N power\n",
    "    pert_embd_dim: int = 320 # based on action embeddings\n",
    "\n",
    "class ACPredictor(nn.Module):\n",
    "    def __init__(self, config, pert_embd):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Action Embedding (Discrete ID -> Vector)\n",
    "        self.register_buffer('pert_bank', torch.tensor(pert_embd, dtype=torch.float32))\n",
    "\n",
    "        # Perturbation Embedding\n",
    "        self.adapter = nn.Sequential(\n",
    "            nn.Linear(config.pert_embd_dim, config.action_embed_dim),\n",
    "            nn.LayerNorm(config.action_embed_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(config.action_embed_dim, config.action_embed_dim)\n",
    "        )\n",
    "        \n",
    "        # Learnable Queries (\"Mask Tokens\") for the future state\n",
    "        # One query vector per pathway position\n",
    "        self.mask_queries = nn.Parameter(torch.randn(1, config.num_pathways, config.embed_dim) * 0.02)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            PredictorBlock(config) for _ in range(config.n_layer)\n",
    "        ])\n",
    "        \n",
    "        self.final_norm = AdaLN(config.embed_dim, config.action_embed_dim)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, context_latents, action_ids):\n",
    "        \"\"\"\n",
    "        context_latents: [Batch, N, Dim] (From Student Encoder)\n",
    "        action_ids: [Batch] (Ints)\n",
    "        \"\"\"\n",
    "        B, N, D = context_latents.shape\n",
    "\n",
    "        # Get Perturbations Embeddings\n",
    "        raw_pert = self.pert_bank[action_ids]\n",
    "\n",
    "        # Project to Embedding Space\n",
    "        action_emb = self.adapter(raw_pert)\n",
    "        \n",
    "        # 2. Construct Input: [Context, Mask_Queries]\n",
    "        # We concatenate the learned queries to the context. \n",
    "        # The predictor will attend to the context to update the queries.\n",
    "        queries = self.mask_queries.repeat(B, 1, 1) # [B, N, D]\n",
    "        sequence = torch.cat([context_latents, queries], dim=1) # [B, 2N, D]     \n",
    "        \n",
    "        # 3. Pass through AdaLN Blocks\n",
    "        for block in self.blocks:\n",
    "            sequence = block(sequence, action_emb)\n",
    "            \n",
    "        sequence = self.final_norm(sequence, action_emb)\n",
    "        \n",
    "        # 4. Return only the predicted part (The Queries corresponding to N..2N)\n",
    "        predictions = sequence[:, N:, :] \n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f114d0-d7aa-4262-8921-978bf17db385",
   "metadata": {},
   "source": [
    "### Masked Predictor (pretraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b03afb88-ead2-4c07-bcca-e2f08b997ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedPredictor(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Shallow transformer for reconstruction (typically fewer layers than encoder)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            CellStateBlock(config) for _ in range(config.n_pre_layer) \n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(config.embed_dim)\n",
    "        self.pred_head = nn.Linear(config.embed_dim, config.embed_dim)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "    def _init_weights(self, module):\n",
    "        ## fan_in, _ = nn.init._calculate_fan_in_and_fan_out(w)\n",
    "        ##std = math.sqrt(2.0 / fan_in)\n",
    "        if isinstance(module, nn.Linear) or isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None: \n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.pred_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb48919-caa3-4d81-93d0-dca2d4d30f96",
   "metadata": {},
   "source": [
    "## Bio-JEPA AC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16297aa1-3670-4b89-9e3a-19cfab546ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BioJepaConfig:\n",
    "    mask_matrix: np.ndarray\n",
    "    num_genes: int = 8192\n",
    "    num_pathways: int = 1024\n",
    "    n_layer: int = 6\n",
    "    heads: int = 4\n",
    "    embed_dim: int = 256\n",
    "    action_embed_dim: int=256 \n",
    "    mlp_ratio: float = 4.0\n",
    "    max_perturb: int= 2058 ## eventually try to get to a 2**N power\n",
    "\n",
    "    # pretraining\n",
    "    mask_ratio: float = 0.6\n",
    "    n_pre_layer: int = 3\n",
    "    \n",
    "class BioJepa(nn.Module):\n",
    "    def __init__(self, config, pert_embd):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        enc_conf = CellStateEncoderConfig(\n",
    "            num_genes=config.num_genes,\n",
    "            num_pathways=config.num_pathways,\n",
    "            n_layer=config.n_layer,\n",
    "            heads=config.heads,\n",
    "            embed_dim=config.embed_dim,\n",
    "            mlp_ratio=config.mlp_ratio,\n",
    "            mask_matrix=config.mask_matrix\n",
    "        )\n",
    "        \n",
    "        self.student = CellStateEncoder(enc_conf)   \n",
    "        self.teacher = copy.deepcopy(self.student)\n",
    "        \n",
    "        # Freeze teacher\n",
    "        for p in self.teacher.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        ## Action Predictor\n",
    "        pred_conf = ACPredictorConfig(\n",
    "            num_pathways=config.num_pathways,\n",
    "            n_layer=config.n_layer,\n",
    "            heads=config.heads,\n",
    "            embed_dim=config.embed_dim,\n",
    "            action_embed_dim=config.action_embed_dim,\n",
    "            mlp_ratio=config.mlp_ratio,\n",
    "            max_perturb=config.max_perturb,\n",
    "            pert_embd_dim=pert_embd.shape[1] # Auto-detect (320)\n",
    "        )\n",
    "        self.predictor = ACPredictor(pred_conf, pert_embd)\n",
    "\n",
    "        ## Pretraining \n",
    "        self.mask_token = nn.Parameter(torch.randn(1, 1, config.embed_dim) * 0.02)\n",
    "\n",
    "        mask_pred_conf = copy.deepcopy(enc_conf)\n",
    "        mask_pred_conf.n_pre_layer = config.n_pre_layer\n",
    "        self.masked_predictor = MaskedPredictor(mask_pred_conf)\n",
    "\n",
    "    def forward(self, x_control, total_control, x_case, total_case, action_id):\n",
    "        # 1. Teacher\n",
    "        with torch.no_grad():\n",
    "            target_latents = self.teacher(x_case, total_case)\n",
    "            \n",
    "        # 2. Student \n",
    "        context_latents = self.student(x_control, total_control)\n",
    "        \n",
    "        # 3. Predictor \n",
    "        predicted_latents = self.predictor(context_latents, action_id)\n",
    "        \n",
    "        # 4. Latent Loss (L1)\n",
    "        loss = F.l1_loss(predicted_latents, target_latents)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def forward_pretrain(self, x, x_total_ct):\n",
    "        batch = x.shape[0]\n",
    "        num_pathways = self.config.num_pathways\n",
    "        \n",
    "        # 1. Teacher Target (Full view)\n",
    "        with torch.no_grad():\n",
    "            target_latents = self.teacher(x, x_total_ct) # [B, N, D]\n",
    "\n",
    "        # 2. Manual Student Overwrite for masking\n",
    "        x_genes = x.unsqueeze(-1) \n",
    "        gene_repr = x_genes * self.student.gene_embeddings.unsqueeze(0)\n",
    "        x_pathway = self.student.pathway_weights @ gene_repr # [B, N, D]\n",
    "        \n",
    "        x_total_ct = x_total_ct.unsqueeze(-1)\n",
    "        x_total_ct = self.student.total_count_proj(x_total_ct)\n",
    "        x_total_ct = x_total_ct.unsqueeze(1)\n",
    "        x = x_pathway + x_total_ct\n",
    "        \n",
    "        # 3. Create Mask for learning 1 = Masked, 0 = Visible\n",
    "        mask_noise = torch.rand(batch, num_pathways, device=x.device)\n",
    "        mask_indices = mask_noise < self.config.mask_ratio\n",
    "        \n",
    "        # Replace masked tokens with self.mask_token\n",
    "        # Expand mask token to [B, N, D] then select positions\n",
    "        mask_token_expand = self.mask_token.expand(batch, num_pathways, -1)\n",
    "        x_masked = x.clone()\n",
    "        x_masked[mask_indices] = mask_token_expand[mask_indices]\n",
    "\n",
    "        # 4. Pass through Student Transformer Blocks\n",
    "        for block in self.student.blocks:\n",
    "            x_masked = block(x_masked)\n",
    "        z_student = self.student.ln_f(x_masked)\n",
    "        \n",
    "        # 5. Masked Predictor tries to reconstruct the teacher latent at the masked positions\n",
    "        z_pred = self.masked_predictor(z_student)\n",
    "        \n",
    "        # 6. Loss calculation (Compute loss ONLY on masked patches)\n",
    "        pred_masked = z_pred[mask_indices]\n",
    "        target_masked = target_latents[mask_indices]\n",
    "        \n",
    "        loss = F.l1_loss(pred_masked, target_masked)\n",
    "        \n",
    "        return loss \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_teacher(self, m=0.996):\n",
    "        for param_s, param_t in zip(self.student.parameters(), self.teacher.parameters()):\n",
    "            param_t.data.mul_(m).add_((1 - m) * param_s.data)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdbd6fb-ec72-4af9-ba48-13d7731c3343",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96184936-3410-4d44-8be1-e01ed3c7cc9e",
   "metadata": {},
   "source": [
    "#### Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6dfbb5f-2a56-4709-a9a2-9ab3fe0973a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('/Users/djemec/data/jepa/v0_2')\n",
    "train_dir = data_dir / 'training'\n",
    "pretrain_dir = data_dir / 'pretraining'\n",
    "mask_path = data_dir / 'binary_pathway_mask.npy'\n",
    "checkpoint_dir = data_dir / 'checkpoint'\n",
    "pert_dir = data_dir / 'pert_embd'\n",
    "pert_embd_path = pert_dir / 'action_embeddings_esm2.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a287ede-b205-4377-9ffe-646694b977dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Pathway Mask...\n",
      "Mask Loaded: 5000 Genes -> 1024 Pathways\n"
     ]
    }
   ],
   "source": [
    "print('Loading Pathway Mask...')\n",
    "binary_mask = np.load(mask_path)\n",
    "N_GENES, N_PATHWAYS = binary_mask.shape\n",
    "print(f'Mask Loaded: {N_GENES} Genes -> {N_PATHWAYS} Pathways')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "214e91a4-c45c-4ab5-9d03-6b7e1dd641ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Action Embedding ...\n",
      "Bank Loaded. Shape: (1087, 320)\n"
     ]
    }
   ],
   "source": [
    "print('Loading Action Embedding ...')\n",
    "pert_embd = np.load(pert_embd_path)\n",
    "print(f'Bank Loaded. Shape: {pert_embd.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7885c628-45b4-4c23-97e5-e3ee6f0400e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "n_embd = 8\n",
    "n_pathways = 1024\n",
    "PT_EPOCHS = 10\n",
    "training_file_chunk = 25000\n",
    "pretraining_file_chunk = 50000\n",
    "n_heads = 2\n",
    "n_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "055c50c0-4aff-4ba6-bdf9-0be0df8216e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrain_shard(filename):\n",
    "    print(f'loading {filename}') # Optional: reduce noise\n",
    "    with np.load(filename) as data:\n",
    "        x = data['x'].astype(np.float32)\n",
    "        total = data['total'].astype(np.float32)\n",
    "    return x, total\n",
    "\n",
    "\n",
    "class PTDataLoaderLite:\n",
    "    def __init__(self, B, split, device):\n",
    "        self.B = B\n",
    "        self.split = split\n",
    "        self.device = device\n",
    "        \n",
    "        data_root = pretrain_dir / f'{split}'\n",
    "        shards = sorted(list(data_root.glob('*.npz')))\n",
    "\n",
    "        self.total_files = len(shards)\n",
    "        self.shards = sorted(shards)\n",
    "        \n",
    "        print(f'Found {len(self.shards)} pretrain shards')\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.remaining_shards = list(self.shards)\n",
    "        random.shuffle(self.remaining_shards)\n",
    "        \n",
    "        self.current_shard_idx = -1\n",
    "        self.load_next_shard()\n",
    "\n",
    "    def load_next_shard(self):\n",
    "        self.current_shard_idx += 1\n",
    "        \n",
    "        if self.current_shard_idx >= len(self.remaining_shards):\n",
    "            self.reset()\n",
    "            return\n",
    "\n",
    "        filename = self.remaining_shards[self.current_shard_idx]\n",
    "        self.data_tuple = load_pretrain_shard(filename)\n",
    "\n",
    "\n",
    "        n_samples = len(self.data_tuple[0])\n",
    "        self.perm = np.random.permutation(n_samples)\n",
    "        self.current_position = 0\n",
    "        self.total_samples_in_shard = n_samples\n",
    "\n",
    "    def next_batch(self):\n",
    "        if self.current_position + self.B > self.total_samples_in_shard:\n",
    "            self.load_next_shard()\n",
    "            return self.next_batch()\n",
    "            \n",
    "        indices = self.perm[self.current_position : self.current_position + self.B]\n",
    "        self.current_position += self.B\n",
    "        \n",
    "        batch_x = torch.from_numpy(self.data_tuple[0][indices]).to(self.device)\n",
    "        batch_tot = torch.from_numpy(self.data_tuple[1][indices]).to(self.device)\n",
    "        return batch_x, batch_tot\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39a1967a-562d-4772-904a-d1092ba2cf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_shard(filename):\n",
    "    print(f'loading {filename}') # Optional: reduce noise\n",
    "    with np.load(filename) as data:\n",
    "        # Load all arrays into memory\n",
    "        # We convert to correct types immediately to save hassle later\n",
    "        control_x = data['control'].astype(np.float32)\n",
    "        control_tot = data['control_total'].astype(np.float32)\n",
    "        case_x = data['case'].astype(np.float32)\n",
    "        case_tot = data['case_total'].astype(np.float32)\n",
    "        action_ids = data['action_ids'].astype(np.int64)\n",
    "        \n",
    "    return control_x, control_tot, case_x, case_tot, action_ids\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, split, device):\n",
    "        self.B = B\n",
    "        self.split = split\n",
    "        self.device = device\n",
    "        \n",
    "        # 1. Find Shards\n",
    "        data_root = train_dir / f'{split}'\n",
    "        shards = list(data_root.glob('*.npz'))\n",
    "\n",
    "        self.total_files = len(shards)\n",
    "        self.shards = sorted(shards)\n",
    "\n",
    "        assert len(shards) > 0, f'no shards found for split {split}'\n",
    "        print(f'found {len(shards)} shards for split {split}')\n",
    "        \n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # Create a randomized queue of shards\n",
    "        self.remaining_shards = list(self.shards)\n",
    "        random.shuffle(self.remaining_shards)\n",
    "        \n",
    "        self.current_shard_idx = -1\n",
    "        self.load_next_shard()\n",
    "\n",
    "    def load_next_shard(self):\n",
    "        self.current_shard_idx += 1\n",
    "        \n",
    "        # If we ran out of shards, reset (Epoch done)\n",
    "        if self.current_shard_idx >= len(self.remaining_shards):\n",
    "            self.reset() # This resets shard_idx to -1 and reshuffles\n",
    "            return \n",
    "\n",
    "        # Load the file\n",
    "        filename = self.remaining_shards[self.current_shard_idx]\n",
    "        self.data_tuple = load_shard(filename)\n",
    "        \n",
    "        # Shuffle the items INSIDE the shard\n",
    "        # This is critical so we don't just memorize the sorted order of the shard\n",
    "        n_samples = len(self.data_tuple[0])\n",
    "        self.perm = np.random.permutation(n_samples)\n",
    "        self.current_position = 0\n",
    "        self.total_samples_in_shard = n_samples\n",
    "\n",
    "    def next_batch(self):\n",
    "        B = self.B\n",
    "        \n",
    "        # Check if we have enough data left in current shard\n",
    "        if self.current_position + B > self.total_samples_in_shard:\n",
    "            self.load_next_shard()\n",
    "            # Recursively call to get batch from the new shard\n",
    "            return self.next_batch()\n",
    "            \n",
    "        # Get indices for this batch\n",
    "        indices = self.perm[self.current_position : self.current_position + B]\n",
    "        self.current_position += B\n",
    "        \n",
    "        # Slice data using the shuffled indices\n",
    "        # data_tuple structure: (xc, xct, xt, xtt, aid)\n",
    "        batch_xc  = torch.from_numpy(self.data_tuple[0][indices]).to(self.device)\n",
    "        batch_xct = torch.from_numpy(self.data_tuple[1][indices]).to(self.device)\n",
    "        batch_xt  = torch.from_numpy(self.data_tuple[2][indices]).to(self.device)\n",
    "        batch_xtt = torch.from_numpy(self.data_tuple[3][indices]).to(self.device)\n",
    "        batch_aid = torch.from_numpy(self.data_tuple[4][indices]).to(self.device)\n",
    "        \n",
    "        return batch_xc, batch_xct, batch_xt, batch_xtt, batch_aid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b8077a-3c09-4c72-bf05-7f8a7f2c4ce8",
   "metadata": {},
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "763e79ef-052b-460e-863f-b045b3e72184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 pretrain shards\n",
      "loading /Users/djemec/data/jepa/v0_2/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Found 1 pretrain shards\n",
      "loading /Users/djemec/data/jepa/v0_2/pretraining/val/pt_shard_k562e_val_0000.npz\n"
     ]
    }
   ],
   "source": [
    "pt_train_loader = PTDataLoaderLite(B=BATCH_SIZE, split='train', device=DEVICE)\n",
    "pt_val_loader = PTDataLoaderLite(B=BATCH_SIZE, split='val', device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d83667-24a3-4ad7-b927-aed9b4c2f5a1",
   "metadata": {},
   "source": [
    "**Model Config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd24671e-b91e-4d66-829c-f3f7cb668fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_LR = 4e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc191f49-c815-467a-a182-62c5c6622f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BioJepaConfig(\n",
    "    mask_matrix=binary_mask, \n",
    "    num_genes=N_GENES,\n",
    "    num_pathways=N_PATHWAYS,\n",
    "    embed_dim=n_embd,\n",
    "    n_layer=n_layers,\n",
    "    heads=n_heads,\n",
    "    n_pre_layer = n_layers\n",
    ")\n",
    "model = BioJepa(config, pert_embd=pert_embd).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f18d16-5f9d-4fb0-8957-79d32a1ceae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Student/Teacher: {sum(p.numel() for p in model.student.parameters() if p.requires_grad)}')\n",
    "print(f'ACpredictor: {sum(p.numel() for p in model.predictor.parameters() if p.requires_grad)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "42f98f75-c7b6-4187-a6be-acf465b4a5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=pt_LR, weight_decay=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dac83f33-406d-4869-be79-248c31c12ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3511, 35110)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_train_total = 112373\n",
    "pt_val_total = 11044\n",
    "steps_per_epoch = pt_train_total // BATCH_SIZE\n",
    "pt_max_steps = PT_EPOCHS * steps_per_epoch\n",
    "steps_per_epoch, pt_max_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "179eb304-3dab-458e-98b0-6832be99dcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=pt_LR, total_steps=pt_max_steps, pct_start=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26236f71-9559-4286-8ada-64be2e49411e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_lossi = []\n",
    "step = 0\n",
    "total_epoch_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a8c7f824-b9e3-4be9-8327-89c121c79469",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss: 0.8213\n",
      "Step 0 | Loss: 0.82010 | LR: 1.60e-04\n",
      "Step 25 | Loss: 0.80844 | LR: 1.62e-04\n",
      "Step 50 | Loss: 0.79911 | LR: 1.68e-04\n",
      "Step 75 | Loss: 0.78723 | LR: 1.78e-04\n",
      "val loss: 0.7655\n",
      "Step 100 | Loss: 0.76631 | LR: 1.91e-04\n",
      "Step 125 | Loss: 0.73966 | LR: 2.09e-04\n",
      "Step 150 | Loss: 0.70754 | LR: 2.30e-04\n",
      "Step 175 | Loss: 0.67632 | LR: 2.55e-04\n",
      "val loss: 0.6347\n",
      "Step 200 | Loss: 0.63778 | LR: 2.83e-04\n",
      "Step 225 | Loss: 0.59527 | LR: 3.15e-04\n",
      "Step 250 | Loss: 0.56209 | LR: 3.51e-04\n",
      "Step 275 | Loss: 0.53127 | LR: 3.90e-04\n",
      "val loss: 0.5245\n",
      "Step 300 | Loss: 0.52743 | LR: 4.32e-04\n",
      "Step 325 | Loss: 0.52664 | LR: 4.78e-04\n",
      "Step 350 | Loss: 0.55862 | LR: 5.27e-04\n",
      "Step 375 | Loss: 0.56244 | LR: 5.79e-04\n",
      "val loss: 0.5739\n",
      "Step 400 | Loss: 0.55722 | LR: 6.34e-04\n",
      "Step 425 | Loss: 0.58810 | LR: 6.92e-04\n",
      "Step 450 | Loss: 0.60071 | LR: 7.53e-04\n",
      "Step 475 | Loss: 0.63010 | LR: 8.16e-04\n",
      "val loss: 0.6368\n",
      "Step 500 | Loss: 0.62825 | LR: 8.82e-04\n",
      "Step 525 | Loss: 0.65368 | LR: 9.50e-04\n",
      "Step 550 | Loss: 0.67843 | LR: 1.02e-03\n",
      "Step 575 | Loss: 0.70197 | LR: 1.09e-03\n",
      "val loss: 0.7220\n",
      "Step 600 | Loss: 0.71657 | LR: 1.17e-03\n",
      "Step 625 | Loss: 0.75620 | LR: 1.25e-03\n",
      "Step 650 | Loss: 0.73526 | LR: 1.32e-03\n",
      "Step 675 | Loss: 0.73929 | LR: 1.40e-03\n",
      "val loss: 0.7456\n",
      "Step 700 | Loss: 0.74970 | LR: 1.48e-03\n",
      "Step 725 | Loss: 0.74802 | LR: 1.57e-03\n",
      "Step 750 | Loss: 0.74703 | LR: 1.65e-03\n",
      "Step 775 | Loss: 0.75446 | LR: 1.73e-03\n",
      "val loss: 0.7371\n",
      "Step 800 | Loss: 0.74726 | LR: 1.82e-03\n",
      "Step 825 | Loss: 0.71984 | LR: 1.90e-03\n",
      "Step 850 | Loss: 0.73791 | LR: 1.99e-03\n",
      "Step 875 | Loss: 0.71798 | LR: 2.08e-03\n",
      "val loss: 0.7268\n",
      "Step 900 | Loss: 0.74365 | LR: 2.16e-03\n",
      "Step 925 | Loss: 0.73438 | LR: 2.25e-03\n",
      "Step 950 | Loss: 0.69756 | LR: 2.33e-03\n",
      "Step 975 | Loss: 0.70070 | LR: 2.42e-03\n",
      "val loss: 0.7004\n",
      "Step 1000 | Loss: 0.69555 | LR: 2.50e-03\n",
      "Step 1025 | Loss: 0.71585 | LR: 2.59e-03\n",
      "Step 1050 | Loss: 0.68662 | LR: 2.67e-03\n",
      "Step 1075 | Loss: 0.68068 | LR: 2.75e-03\n",
      "val loss: 0.6865\n",
      "Step 1100 | Loss: 0.68385 | LR: 2.83e-03\n",
      "Step 1125 | Loss: 0.66495 | LR: 2.91e-03\n",
      "Step 1150 | Loss: 0.67413 | LR: 2.98e-03\n",
      "Step 1175 | Loss: 0.67080 | LR: 3.06e-03\n",
      "val loss: 0.6587\n",
      "Step 1200 | Loss: 0.64886 | LR: 3.13e-03\n",
      "Step 1225 | Loss: 0.66147 | LR: 3.20e-03\n",
      "Step 1250 | Loss: 0.64456 | LR: 3.27e-03\n",
      "Step 1275 | Loss: 0.67415 | LR: 3.34e-03\n",
      "val loss: 0.6355\n",
      "Step 1300 | Loss: 0.64582 | LR: 3.40e-03\n",
      "Step 1325 | Loss: 0.64892 | LR: 3.46e-03\n",
      "Step 1350 | Loss: 0.63590 | LR: 3.52e-03\n",
      "Step 1375 | Loss: 0.60071 | LR: 3.58e-03\n",
      "val loss: 0.6268\n",
      "Step 1400 | Loss: 0.63293 | LR: 3.63e-03\n",
      "Step 1425 | Loss: 0.59937 | LR: 3.68e-03\n",
      "Step 1450 | Loss: 0.60141 | LR: 3.72e-03\n",
      "Step 1475 | Loss: 0.59844 | LR: 3.77e-03\n",
      "val loss: 0.5941\n",
      "Step 1500 | Loss: 0.61988 | LR: 3.81e-03\n",
      "Step 1525 | Loss: 0.60506 | LR: 3.84e-03\n",
      "Step 1550 | Loss: 0.57544 | LR: 3.87e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 1575 | Loss: 0.58392 | LR: 3.90e-03\n",
      "val loss: 0.5781\n",
      "Step 1600 | Loss: 0.55846 | LR: 3.93e-03\n",
      "Step 1625 | Loss: 0.56531 | LR: 3.95e-03\n",
      "Step 1650 | Loss: 0.55877 | LR: 3.97e-03\n",
      "Step 1675 | Loss: 0.56377 | LR: 3.98e-03\n",
      "val loss: 0.5574\n",
      "Step 1700 | Loss: 0.56540 | LR: 3.99e-03\n",
      "Step 1725 | Loss: 0.56786 | LR: 4.00e-03\n",
      "Step 1750 | Loss: 0.54697 | LR: 4.00e-03\n",
      "Step 1775 | Loss: 0.55021 | LR: 4.00e-03\n",
      "val loss: 0.5230\n",
      "Step 1800 | Loss: 0.54177 | LR: 4.00e-03\n",
      "Step 1825 | Loss: 0.51284 | LR: 4.00e-03\n",
      "Step 1850 | Loss: 0.49349 | LR: 4.00e-03\n",
      "Step 1875 | Loss: 0.52455 | LR: 4.00e-03\n",
      "val loss: 0.5111\n",
      "Step 1900 | Loss: 0.48959 | LR: 4.00e-03\n",
      "Step 1925 | Loss: 0.51622 | LR: 4.00e-03\n",
      "Step 1950 | Loss: 0.50790 | LR: 4.00e-03\n",
      "Step 1975 | Loss: 0.47122 | LR: 4.00e-03\n",
      "val loss: 0.4886\n",
      "Step 2000 | Loss: 0.50137 | LR: 4.00e-03\n",
      "Step 2025 | Loss: 0.47877 | LR: 4.00e-03\n",
      "Step 2050 | Loss: 0.49344 | LR: 4.00e-03\n",
      "Step 2075 | Loss: 0.46822 | LR: 4.00e-03\n",
      "val loss: 0.4723\n",
      "Step 2100 | Loss: 0.47761 | LR: 4.00e-03\n",
      "Step 2125 | Loss: 0.44268 | LR: 4.00e-03\n",
      "Step 2150 | Loss: 0.46170 | LR: 4.00e-03\n",
      "Step 2175 | Loss: 0.47843 | LR: 4.00e-03\n",
      "val loss: 0.4543\n",
      "Step 2200 | Loss: 0.43061 | LR: 4.00e-03\n",
      "Step 2225 | Loss: 0.45278 | LR: 4.00e-03\n",
      "Step 2250 | Loss: 0.44594 | LR: 4.00e-03\n",
      "Step 2275 | Loss: 0.44602 | LR: 4.00e-03\n",
      "val loss: 0.4417\n",
      "Step 2300 | Loss: 0.45851 | LR: 4.00e-03\n",
      "Step 2325 | Loss: 0.44451 | LR: 4.00e-03\n",
      "Step 2350 | Loss: 0.42832 | LR: 4.00e-03\n",
      "Step 2375 | Loss: 0.42627 | LR: 4.00e-03\n",
      "val loss: 0.4365\n",
      "Step 2400 | Loss: 0.45247 | LR: 4.00e-03\n",
      "Step 2425 | Loss: 0.43398 | LR: 4.00e-03\n",
      "Step 2450 | Loss: 0.43465 | LR: 4.00e-03\n",
      "Step 2475 | Loss: 0.44526 | LR: 4.00e-03\n",
      "val loss: 0.4231\n",
      "Step 2500 | Loss: 0.42347 | LR: 4.00e-03\n",
      "Step 2525 | Loss: 0.44383 | LR: 3.99e-03\n",
      "Step 2550 | Loss: 0.43114 | LR: 3.99e-03\n",
      "Step 2575 | Loss: 0.43426 | LR: 3.99e-03\n",
      "val loss: 0.4231\n",
      "Step 2600 | Loss: 0.41923 | LR: 3.99e-03\n",
      "Step 2625 | Loss: 0.40982 | LR: 3.99e-03\n",
      "Step 2650 | Loss: 0.40988 | LR: 3.99e-03\n",
      "Step 2675 | Loss: 0.42872 | LR: 3.99e-03\n",
      "val loss: 0.4144\n",
      "Step 2700 | Loss: 0.43903 | LR: 3.99e-03\n",
      "Step 2725 | Loss: 0.40695 | LR: 3.99e-03\n",
      "Step 2750 | Loss: 0.40984 | LR: 3.99e-03\n",
      "Step 2775 | Loss: 0.40414 | LR: 3.99e-03\n",
      "val loss: 0.4062\n",
      "Step 2800 | Loss: 0.41439 | LR: 3.99e-03\n",
      "Step 2825 | Loss: 0.40970 | LR: 3.99e-03\n",
      "Step 2850 | Loss: 0.37726 | LR: 3.99e-03\n",
      "Step 2875 | Loss: 0.41167 | LR: 3.99e-03\n",
      "val loss: 0.4036\n",
      "Step 2900 | Loss: 0.40516 | LR: 3.99e-03\n",
      "Step 2925 | Loss: 0.38514 | LR: 3.99e-03\n",
      "Step 2950 | Loss: 0.39705 | LR: 3.99e-03\n",
      "Step 2975 | Loss: 0.41723 | LR: 3.99e-03\n",
      "val loss: 0.4047\n",
      "Step 3000 | Loss: 0.39676 | LR: 3.99e-03\n",
      "Step 3025 | Loss: 0.38331 | LR: 3.99e-03\n",
      "Step 3050 | Loss: 0.38831 | LR: 3.99e-03\n",
      "Step 3075 | Loss: 0.37529 | LR: 3.98e-03\n",
      "val loss: 0.3891\n",
      "Step 3100 | Loss: 0.38972 | LR: 3.98e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/pretraining/train/pt_shard_k562e_train_0002.npz\n",
      "Step 3125 | Loss: 0.39916 | LR: 3.98e-03\n",
      "Step 3150 | Loss: 0.39626 | LR: 3.98e-03\n",
      "Step 3175 | Loss: 0.39507 | LR: 3.98e-03\n",
      "val loss: 0.3842\n",
      "Step 3200 | Loss: 0.38636 | LR: 3.98e-03\n",
      "Step 3225 | Loss: 0.38553 | LR: 3.98e-03\n",
      "Step 3250 | Loss: 0.38180 | LR: 3.98e-03\n",
      "Step 3275 | Loss: 0.37805 | LR: 3.98e-03\n",
      "val loss: 0.3848\n",
      "Step 3300 | Loss: 0.37821 | LR: 3.98e-03\n",
      "Step 3325 | Loss: 0.37508 | LR: 3.98e-03\n",
      "Step 3350 | Loss: 0.37347 | LR: 3.98e-03\n",
      "Step 3375 | Loss: 0.39538 | LR: 3.98e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/pretraining/val/pt_shard_k562e_val_0000.npz\n",
      "val loss: 0.3730\n",
      "Step 3400 | Loss: 0.37892 | LR: 3.98e-03\n",
      "Step 3425 | Loss: 0.38561 | LR: 3.98e-03\n",
      "Step 3450 | Loss: 0.37546 | LR: 3.97e-03\n",
      "Step 3475 | Loss: 0.39487 | LR: 3.97e-03\n",
      "val loss: 0.3693\n",
      "Step 3500 | Loss: 0.36989 | LR: 3.97e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "=== Step 3510 Done. Avg Loss: 0.54328 ===\n",
      "Step 3525 | Loss: 0.35659 | LR: 3.97e-03\n",
      "Step 3550 | Loss: 0.36479 | LR: 3.97e-03\n",
      "Step 3575 | Loss: 0.37960 | LR: 3.97e-03\n",
      "val loss: 0.3649\n",
      "Step 3600 | Loss: 0.36008 | LR: 3.97e-03\n",
      "Step 3625 | Loss: 0.37854 | LR: 3.97e-03\n",
      "Step 3650 | Loss: 0.35967 | LR: 3.97e-03\n",
      "Step 3675 | Loss: 0.36018 | LR: 3.97e-03\n",
      "val loss: 0.3645\n",
      "Step 3700 | Loss: 0.36776 | LR: 3.97e-03\n",
      "Step 3725 | Loss: 0.35147 | LR: 3.97e-03\n",
      "Step 3750 | Loss: 0.33780 | LR: 3.96e-03\n",
      "Step 3775 | Loss: 0.34018 | LR: 3.96e-03\n",
      "val loss: 0.3512\n",
      "Step 3800 | Loss: 0.35908 | LR: 3.96e-03\n",
      "Step 3825 | Loss: 0.35392 | LR: 3.96e-03\n",
      "Step 3850 | Loss: 0.33986 | LR: 3.96e-03\n",
      "Step 3875 | Loss: 0.34933 | LR: 3.96e-03\n",
      "val loss: 0.3510\n",
      "Step 3900 | Loss: 0.33380 | LR: 3.96e-03\n",
      "Step 3925 | Loss: 0.34187 | LR: 3.96e-03\n",
      "Step 3950 | Loss: 0.32434 | LR: 3.96e-03\n",
      "Step 3975 | Loss: 0.34447 | LR: 3.96e-03\n",
      "val loss: 0.3311\n",
      "Step 4000 | Loss: 0.33453 | LR: 3.96e-03\n",
      "Step 4025 | Loss: 0.33111 | LR: 3.95e-03\n",
      "Step 4050 | Loss: 0.31601 | LR: 3.95e-03\n",
      "Step 4075 | Loss: 0.30789 | LR: 3.95e-03\n",
      "val loss: 0.3182\n",
      "Step 4100 | Loss: 0.32687 | LR: 3.95e-03\n",
      "Step 4125 | Loss: 0.30286 | LR: 3.95e-03\n",
      "Step 4150 | Loss: 0.31751 | LR: 3.95e-03\n",
      "Step 4175 | Loss: 0.30709 | LR: 3.95e-03\n",
      "val loss: 0.3086\n",
      "Step 4200 | Loss: 0.31619 | LR: 3.95e-03\n",
      "Step 4225 | Loss: 0.29854 | LR: 3.95e-03\n",
      "Step 4250 | Loss: 0.31201 | LR: 3.94e-03\n",
      "Step 4275 | Loss: 0.30296 | LR: 3.94e-03\n",
      "val loss: 0.3009\n",
      "Step 4300 | Loss: 0.29544 | LR: 3.94e-03\n",
      "Step 4325 | Loss: 0.29962 | LR: 3.94e-03\n",
      "Step 4350 | Loss: 0.27386 | LR: 3.94e-03\n",
      "Step 4375 | Loss: 0.28664 | LR: 3.94e-03\n",
      "val loss: 0.2745\n",
      "Step 4400 | Loss: 0.28617 | LR: 3.94e-03\n",
      "Step 4425 | Loss: 0.27517 | LR: 3.94e-03\n",
      "Step 4450 | Loss: 0.29116 | LR: 3.94e-03\n",
      "Step 4475 | Loss: 0.28387 | LR: 3.93e-03\n",
      "val loss: 0.2728\n",
      "Step 4500 | Loss: 0.26651 | LR: 3.93e-03\n",
      "Step 4525 | Loss: 0.27816 | LR: 3.93e-03\n",
      "Step 4550 | Loss: 0.24894 | LR: 3.93e-03\n",
      "Step 4575 | Loss: 0.26978 | LR: 3.93e-03\n",
      "val loss: 0.2622\n",
      "Step 4600 | Loss: 0.26182 | LR: 3.93e-03\n",
      "Step 4625 | Loss: 0.24320 | LR: 3.93e-03\n",
      "Step 4650 | Loss: 0.26739 | LR: 3.93e-03\n",
      "Step 4675 | Loss: 0.25230 | LR: 3.92e-03\n",
      "val loss: 0.2464\n",
      "Step 4700 | Loss: 0.24534 | LR: 3.92e-03\n",
      "Step 4725 | Loss: 0.24018 | LR: 3.92e-03\n",
      "Step 4750 | Loss: 0.25339 | LR: 3.92e-03\n",
      "Step 4775 | Loss: 0.22947 | LR: 3.92e-03\n",
      "val loss: 0.2423\n",
      "Step 4800 | Loss: 0.23749 | LR: 3.92e-03\n",
      "Step 4825 | Loss: 0.25156 | LR: 3.92e-03\n",
      "Step 4850 | Loss: 0.23272 | LR: 3.92e-03\n",
      "Step 4875 | Loss: 0.22050 | LR: 3.91e-03\n",
      "val loss: 0.2219\n",
      "Step 4900 | Loss: 0.24407 | LR: 3.91e-03\n",
      "Step 4925 | Loss: 0.23834 | LR: 3.91e-03\n",
      "Step 4950 | Loss: 0.22272 | LR: 3.91e-03\n",
      "Step 4975 | Loss: 0.22498 | LR: 3.91e-03\n",
      "val loss: 0.2164\n",
      "Step 5000 | Loss: 0.20963 | LR: 3.91e-03\n",
      "Step 5025 | Loss: 0.21416 | LR: 3.91e-03\n",
      "Step 5050 | Loss: 0.19433 | LR: 3.90e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 5075 | Loss: 0.19443 | LR: 3.90e-03\n",
      "val loss: 0.2049\n",
      "Step 5100 | Loss: 0.21078 | LR: 3.90e-03\n",
      "Step 5125 | Loss: 0.20448 | LR: 3.90e-03\n",
      "Step 5150 | Loss: 0.19880 | LR: 3.90e-03\n",
      "Step 5175 | Loss: 0.19624 | LR: 3.90e-03\n",
      "val loss: 0.1964\n",
      "Step 5200 | Loss: 0.19819 | LR: 3.90e-03\n",
      "Step 5225 | Loss: 0.19282 | LR: 3.89e-03\n",
      "Step 5250 | Loss: 0.18943 | LR: 3.89e-03\n",
      "Step 5275 | Loss: 0.17718 | LR: 3.89e-03\n",
      "val loss: 0.1897\n",
      "Step 5300 | Loss: 0.19748 | LR: 3.89e-03\n",
      "Step 5325 | Loss: 0.19807 | LR: 3.89e-03\n",
      "Step 5350 | Loss: 0.18651 | LR: 3.89e-03\n",
      "Step 5375 | Loss: 0.18473 | LR: 3.88e-03\n",
      "val loss: 0.1824\n",
      "Step 5400 | Loss: 0.17353 | LR: 3.88e-03\n",
      "Step 5425 | Loss: 0.18285 | LR: 3.88e-03\n",
      "Step 5450 | Loss: 0.19753 | LR: 3.88e-03\n",
      "Step 5475 | Loss: 0.18625 | LR: 3.88e-03\n",
      "val loss: 0.1787\n",
      "Step 5500 | Loss: 0.17597 | LR: 3.88e-03\n",
      "Step 5525 | Loss: 0.16729 | LR: 3.88e-03\n",
      "Step 5550 | Loss: 0.16390 | LR: 3.87e-03\n",
      "Step 5575 | Loss: 0.16492 | LR: 3.87e-03\n",
      "val loss: 0.1847\n",
      "Step 5600 | Loss: 0.19116 | LR: 3.87e-03\n",
      "Step 5625 | Loss: 0.16686 | LR: 3.87e-03\n",
      "Step 5650 | Loss: 0.17056 | LR: 3.87e-03\n",
      "Step 5675 | Loss: 0.15320 | LR: 3.87e-03\n",
      "val loss: 0.1658\n",
      "Step 5700 | Loss: 0.16258 | LR: 3.86e-03\n",
      "Step 5725 | Loss: 0.16902 | LR: 3.86e-03\n",
      "Step 5750 | Loss: 0.17794 | LR: 3.86e-03\n",
      "Step 5775 | Loss: 0.16358 | LR: 3.86e-03\n",
      "val loss: 0.1557\n",
      "Step 5800 | Loss: 0.16744 | LR: 3.86e-03\n",
      "Step 5825 | Loss: 0.15321 | LR: 3.85e-03\n",
      "Step 5850 | Loss: 0.14243 | LR: 3.85e-03\n",
      "Step 5875 | Loss: 0.15177 | LR: 3.85e-03\n",
      "val loss: 0.1561\n",
      "Step 5900 | Loss: 0.14311 | LR: 3.85e-03\n",
      "Step 5925 | Loss: 0.14918 | LR: 3.85e-03\n",
      "Step 5950 | Loss: 0.14703 | LR: 3.85e-03\n",
      "Step 5975 | Loss: 0.14490 | LR: 3.84e-03\n",
      "val loss: 0.1564\n",
      "Step 6000 | Loss: 0.14597 | LR: 3.84e-03\n",
      "Step 6025 | Loss: 0.13342 | LR: 3.84e-03\n",
      "Step 6050 | Loss: 0.14415 | LR: 3.84e-03\n",
      "Step 6075 | Loss: 0.15711 | LR: 3.84e-03\n",
      "val loss: 0.1449\n",
      "Step 6100 | Loss: 0.13715 | LR: 3.83e-03\n",
      "Step 6125 | Loss: 0.14846 | LR: 3.83e-03\n",
      "Step 6150 | Loss: 0.13536 | LR: 3.83e-03\n",
      "Step 6175 | Loss: 0.14508 | LR: 3.83e-03\n",
      "val loss: 0.1458\n",
      "Step 6200 | Loss: 0.14302 | LR: 3.83e-03\n",
      "Step 6225 | Loss: 0.13141 | LR: 3.83e-03\n",
      "Step 6250 | Loss: 0.13360 | LR: 3.82e-03\n",
      "Step 6275 | Loss: 0.14461 | LR: 3.82e-03\n",
      "val loss: 0.1419\n",
      "Step 6300 | Loss: 0.13150 | LR: 3.82e-03\n",
      "Step 6325 | Loss: 0.14687 | LR: 3.82e-03\n",
      "Step 6350 | Loss: 0.13907 | LR: 3.82e-03\n",
      "Step 6375 | Loss: 0.13650 | LR: 3.81e-03\n",
      "val loss: 0.1410\n",
      "Step 6400 | Loss: 0.15166 | LR: 3.81e-03\n",
      "Step 6425 | Loss: 0.12787 | LR: 3.81e-03\n",
      "Step 6450 | Loss: 0.13693 | LR: 3.81e-03\n",
      "Step 6475 | Loss: 0.13564 | LR: 3.81e-03\n",
      "val loss: 0.1331\n",
      "Step 6500 | Loss: 0.12998 | LR: 3.80e-03\n",
      "Step 6525 | Loss: 0.13590 | LR: 3.80e-03\n",
      "Step 6550 | Loss: 0.14411 | LR: 3.80e-03\n",
      "Step 6575 | Loss: 0.13451 | LR: 3.80e-03\n",
      "val loss: 0.1322\n",
      "Step 6600 | Loss: 0.15147 | LR: 3.80e-03\n",
      "Step 6625 | Loss: 0.13925 | LR: 3.79e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/pretraining/train/pt_shard_k562e_train_0002.npz\n",
      "Step 6650 | Loss: 0.12537 | LR: 3.79e-03\n",
      "Step 6675 | Loss: 0.13878 | LR: 3.79e-03\n",
      "val loss: 0.1290\n",
      "Step 6700 | Loss: 0.12076 | LR: 3.79e-03\n",
      "Step 6725 | Loss: 0.11838 | LR: 3.78e-03\n",
      "Step 6750 | Loss: 0.13061 | LR: 3.78e-03\n",
      "Step 6775 | Loss: 0.11007 | LR: 3.78e-03\n",
      "val loss: 0.1252\n",
      "Step 6800 | Loss: 0.12026 | LR: 3.78e-03\n",
      "Step 6825 | Loss: 0.12612 | LR: 3.78e-03\n",
      "Step 6850 | Loss: 0.10837 | LR: 3.77e-03\n",
      "Step 6875 | Loss: 0.12761 | LR: 3.77e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/pretraining/val/pt_shard_k562e_val_0000.npz\n",
      "val loss: 0.1213\n",
      "Step 6900 | Loss: 0.13054 | LR: 3.77e-03\n",
      "Step 6925 | Loss: 0.12201 | LR: 3.77e-03\n",
      "Step 6950 | Loss: 0.11045 | LR: 3.77e-03\n",
      "Step 6975 | Loss: 0.12020 | LR: 3.76e-03\n",
      "val loss: 0.1191\n",
      "Step 7000 | Loss: 0.12536 | LR: 3.76e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "=== Step 7021 Done. Avg Loss: 0.21709 ===\n",
      "Step 7025 | Loss: 0.12422 | LR: 3.76e-03\n",
      "Step 7050 | Loss: 0.10825 | LR: 3.76e-03\n",
      "Step 7075 | Loss: 0.11845 | LR: 3.75e-03\n",
      "val loss: 0.1181\n",
      "Step 7100 | Loss: 0.11442 | LR: 3.75e-03\n",
      "Step 7125 | Loss: 0.11958 | LR: 3.75e-03\n",
      "Step 7150 | Loss: 0.11882 | LR: 3.75e-03\n",
      "Step 7175 | Loss: 0.12389 | LR: 3.74e-03\n",
      "val loss: 0.1144\n",
      "Step 7200 | Loss: 0.10848 | LR: 3.74e-03\n",
      "Step 7225 | Loss: 0.11604 | LR: 3.74e-03\n",
      "Step 7250 | Loss: 0.10967 | LR: 3.74e-03\n",
      "Step 7275 | Loss: 0.11586 | LR: 3.74e-03\n",
      "val loss: 0.1136\n",
      "Step 7300 | Loss: 0.11906 | LR: 3.73e-03\n",
      "Step 7325 | Loss: 0.11295 | LR: 3.73e-03\n",
      "Step 7350 | Loss: 0.11933 | LR: 3.73e-03\n",
      "Step 7375 | Loss: 0.10611 | LR: 3.73e-03\n",
      "val loss: 0.1123\n",
      "Step 7400 | Loss: 0.11815 | LR: 3.72e-03\n",
      "Step 7425 | Loss: 0.11309 | LR: 3.72e-03\n",
      "Step 7450 | Loss: 0.10860 | LR: 3.72e-03\n",
      "Step 7475 | Loss: 0.10994 | LR: 3.72e-03\n",
      "val loss: 0.1061\n",
      "Step 7500 | Loss: 0.11053 | LR: 3.71e-03\n",
      "Step 7525 | Loss: 0.09553 | LR: 3.71e-03\n",
      "Step 7550 | Loss: 0.12196 | LR: 3.71e-03\n",
      "Step 7575 | Loss: 0.10103 | LR: 3.71e-03\n",
      "val loss: 0.1064\n",
      "Step 7600 | Loss: 0.10103 | LR: 3.70e-03\n",
      "Step 7625 | Loss: 0.09810 | LR: 3.70e-03\n",
      "Step 7650 | Loss: 0.11270 | LR: 3.70e-03\n",
      "Step 7675 | Loss: 0.09971 | LR: 3.70e-03\n",
      "val loss: 0.1047\n",
      "Step 7700 | Loss: 0.10858 | LR: 3.69e-03\n",
      "Step 7725 | Loss: 0.09883 | LR: 3.69e-03\n",
      "Step 7750 | Loss: 0.10399 | LR: 3.69e-03\n",
      "Step 7775 | Loss: 0.10143 | LR: 3.69e-03\n",
      "val loss: 0.1038\n",
      "Step 7800 | Loss: 0.09433 | LR: 3.68e-03\n",
      "Step 7825 | Loss: 0.09167 | LR: 3.68e-03\n",
      "Step 7850 | Loss: 0.10061 | LR: 3.68e-03\n",
      "Step 7875 | Loss: 0.09400 | LR: 3.68e-03\n",
      "val loss: 0.1037\n",
      "Step 7900 | Loss: 0.08982 | LR: 3.67e-03\n",
      "Step 7925 | Loss: 0.09998 | LR: 3.67e-03\n",
      "Step 7950 | Loss: 0.09407 | LR: 3.67e-03\n",
      "Step 7975 | Loss: 0.10304 | LR: 3.67e-03\n",
      "val loss: 0.0995\n",
      "Step 8000 | Loss: 0.09778 | LR: 3.66e-03\n",
      "Step 8025 | Loss: 0.08862 | LR: 3.66e-03\n",
      "Step 8050 | Loss: 0.09012 | LR: 3.66e-03\n",
      "Step 8075 | Loss: 0.09713 | LR: 3.66e-03\n",
      "val loss: 0.0921\n",
      "Step 8100 | Loss: 0.09905 | LR: 3.65e-03\n",
      "Step 8125 | Loss: 0.09791 | LR: 3.65e-03\n",
      "Step 8150 | Loss: 0.08932 | LR: 3.65e-03\n",
      "Step 8175 | Loss: 0.09364 | LR: 3.65e-03\n",
      "val loss: 0.0929\n",
      "Step 8200 | Loss: 0.10056 | LR: 3.64e-03\n",
      "Step 8225 | Loss: 0.09455 | LR: 3.64e-03\n",
      "Step 8250 | Loss: 0.08585 | LR: 3.64e-03\n",
      "Step 8275 | Loss: 0.08067 | LR: 3.63e-03\n",
      "val loss: 0.0851\n",
      "Step 8300 | Loss: 0.08602 | LR: 3.63e-03\n",
      "Step 8325 | Loss: 0.08709 | LR: 3.63e-03\n",
      "Step 8350 | Loss: 0.09355 | LR: 3.63e-03\n",
      "Step 8375 | Loss: 0.09198 | LR: 3.62e-03\n",
      "val loss: 0.0846\n",
      "Step 8400 | Loss: 0.08995 | LR: 3.62e-03\n",
      "Step 8425 | Loss: 0.08101 | LR: 3.62e-03\n",
      "Step 8450 | Loss: 0.08547 | LR: 3.62e-03\n",
      "Step 8475 | Loss: 0.08557 | LR: 3.61e-03\n",
      "val loss: 0.0852\n",
      "Step 8500 | Loss: 0.08459 | LR: 3.61e-03\n",
      "Step 8525 | Loss: 0.07951 | LR: 3.61e-03\n",
      "Step 8550 | Loss: 0.08289 | LR: 3.60e-03\n",
      "Step 8575 | Loss: 0.08812 | LR: 3.60e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/pretraining/train/pt_shard_k562e_train_0002.npz\n",
      "val loss: 0.0890\n",
      "Step 8600 | Loss: 0.08020 | LR: 3.60e-03\n",
      "Step 8625 | Loss: 0.08197 | LR: 3.60e-03\n",
      "Step 8650 | Loss: 0.09118 | LR: 3.59e-03\n",
      "Step 8675 | Loss: 0.07398 | LR: 3.59e-03\n",
      "val loss: 0.0797\n",
      "Step 8700 | Loss: 0.07798 | LR: 3.59e-03\n",
      "Step 8725 | Loss: 0.07828 | LR: 3.58e-03\n",
      "Step 8750 | Loss: 0.07685 | LR: 3.58e-03\n",
      "Step 8775 | Loss: 0.07910 | LR: 3.58e-03\n",
      "val loss: 0.0816\n",
      "Step 8800 | Loss: 0.08498 | LR: 3.58e-03\n",
      "Step 8825 | Loss: 0.07212 | LR: 3.57e-03\n",
      "Step 8850 | Loss: 0.07965 | LR: 3.57e-03\n",
      "Step 8875 | Loss: 0.07114 | LR: 3.57e-03\n",
      "val loss: 0.0746\n",
      "Step 8900 | Loss: 0.08298 | LR: 3.56e-03\n",
      "Step 8925 | Loss: 0.07372 | LR: 3.56e-03\n",
      "Step 8950 | Loss: 0.09291 | LR: 3.56e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 8975 | Loss: 0.07335 | LR: 3.55e-03\n",
      "val loss: 0.0765\n",
      "Step 9000 | Loss: 0.07287 | LR: 3.55e-03\n",
      "Step 9025 | Loss: 0.07768 | LR: 3.55e-03\n",
      "Step 9050 | Loss: 0.07710 | LR: 3.55e-03\n",
      "Step 9075 | Loss: 0.07999 | LR: 3.54e-03\n",
      "val loss: 0.0742\n",
      "Step 9100 | Loss: 0.07136 | LR: 3.54e-03\n",
      "Step 9125 | Loss: 0.07365 | LR: 3.54e-03\n",
      "Step 9150 | Loss: 0.07087 | LR: 3.53e-03\n",
      "Step 9175 | Loss: 0.06910 | LR: 3.53e-03\n",
      "val loss: 0.0735\n",
      "Step 9200 | Loss: 0.07592 | LR: 3.53e-03\n",
      "Step 9225 | Loss: 0.07043 | LR: 3.52e-03\n",
      "Step 9250 | Loss: 0.08446 | LR: 3.52e-03\n",
      "Step 9275 | Loss: 0.06411 | LR: 3.52e-03\n",
      "val loss: 0.0720\n",
      "Step 9300 | Loss: 0.06539 | LR: 3.52e-03\n",
      "Step 9325 | Loss: 0.07927 | LR: 3.51e-03\n",
      "Step 9350 | Loss: 0.07121 | LR: 3.51e-03\n",
      "Step 9375 | Loss: 0.07466 | LR: 3.51e-03\n",
      "val loss: 0.0735\n",
      "Step 9400 | Loss: 0.07105 | LR: 3.50e-03\n",
      "Step 9425 | Loss: 0.07519 | LR: 3.50e-03\n",
      "Step 9450 | Loss: 0.07104 | LR: 3.50e-03\n",
      "Step 9475 | Loss: 0.06876 | LR: 3.49e-03\n",
      "val loss: 0.0704\n",
      "Step 9500 | Loss: 0.06413 | LR: 3.49e-03\n",
      "Step 9525 | Loss: 0.07151 | LR: 3.49e-03\n",
      "Step 9550 | Loss: 0.07794 | LR: 3.48e-03\n",
      "Step 9575 | Loss: 0.07157 | LR: 3.48e-03\n",
      "val loss: 0.0680\n",
      "Step 9600 | Loss: 0.06445 | LR: 3.48e-03\n",
      "Step 9625 | Loss: 0.07083 | LR: 3.48e-03\n",
      "Step 9650 | Loss: 0.06664 | LR: 3.47e-03\n",
      "Step 9675 | Loss: 0.06509 | LR: 3.47e-03\n",
      "val loss: 0.0679\n",
      "Step 9700 | Loss: 0.06391 | LR: 3.47e-03\n",
      "Step 9725 | Loss: 0.06621 | LR: 3.46e-03\n",
      "Step 9750 | Loss: 0.06711 | LR: 3.46e-03\n",
      "Step 9775 | Loss: 0.06119 | LR: 3.46e-03\n",
      "val loss: 0.0635\n",
      "Step 9800 | Loss: 0.05912 | LR: 3.45e-03\n",
      "Step 9825 | Loss: 0.07028 | LR: 3.45e-03\n",
      "Step 9850 | Loss: 0.06595 | LR: 3.45e-03\n",
      "Step 9875 | Loss: 0.06579 | LR: 3.44e-03\n",
      "val loss: 0.0641\n",
      "Step 9900 | Loss: 0.06723 | LR: 3.44e-03\n",
      "Step 9925 | Loss: 0.06855 | LR: 3.44e-03\n",
      "Step 9950 | Loss: 0.06568 | LR: 3.43e-03\n",
      "Step 9975 | Loss: 0.06460 | LR: 3.43e-03\n",
      "val loss: 0.0644\n",
      "Step 10000 | Loss: 0.07189 | LR: 3.43e-03\n",
      "Step 10025 | Loss: 0.06143 | LR: 3.42e-03\n",
      "Step 10050 | Loss: 0.06746 | LR: 3.42e-03\n",
      "Step 10075 | Loss: 0.07857 | LR: 3.42e-03\n",
      "val loss: 0.0652\n",
      "Step 10100 | Loss: 0.06584 | LR: 3.41e-03\n",
      "Step 10125 | Loss: 0.06591 | LR: 3.41e-03\n",
      "Step 10150 | Loss: 0.06667 | LR: 3.41e-03\n",
      "Step 10175 | Loss: 0.06248 | LR: 3.40e-03\n",
      "val loss: 0.0660\n",
      "Step 10200 | Loss: 0.06420 | LR: 3.40e-03\n",
      "Step 10225 | Loss: 0.05824 | LR: 3.40e-03\n",
      "Step 10250 | Loss: 0.05479 | LR: 3.39e-03\n",
      "Step 10275 | Loss: 0.05298 | LR: 3.39e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/pretraining/val/pt_shard_k562e_val_0000.npz\n",
      "val loss: 0.0606\n",
      "Step 10300 | Loss: 0.05953 | LR: 3.39e-03\n",
      "Step 10325 | Loss: 0.06412 | LR: 3.38e-03\n",
      "Step 10350 | Loss: 0.06034 | LR: 3.38e-03\n",
      "Step 10375 | Loss: 0.05817 | LR: 3.38e-03\n",
      "val loss: 0.0607\n",
      "Step 10400 | Loss: 0.05256 | LR: 3.37e-03\n",
      "Step 10425 | Loss: 0.06661 | LR: 3.37e-03\n",
      "Step 10450 | Loss: 0.05756 | LR: 3.37e-03\n",
      "Step 10475 | Loss: 0.06798 | LR: 3.36e-03\n",
      "val loss: 0.0597\n",
      "Step 10500 | Loss: 0.06090 | LR: 3.36e-03\n",
      "Step 10525 | Loss: 0.05642 | LR: 3.36e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "=== Step 10532 Done. Avg Loss: 0.08350 ===\n",
      "Step 10550 | Loss: 0.06080 | LR: 3.35e-03\n",
      "Step 10575 | Loss: 0.05759 | LR: 3.35e-03\n",
      "val loss: 0.0572\n",
      "Step 10600 | Loss: 0.06103 | LR: 3.34e-03\n",
      "Step 10625 | Loss: 0.05820 | LR: 3.34e-03\n",
      "Step 10650 | Loss: 0.04872 | LR: 3.34e-03\n",
      "Step 10675 | Loss: 0.06265 | LR: 3.33e-03\n",
      "val loss: 0.0592\n",
      "Step 10700 | Loss: 0.05792 | LR: 3.33e-03\n",
      "Step 10725 | Loss: 0.05540 | LR: 3.33e-03\n",
      "Step 10750 | Loss: 0.05277 | LR: 3.32e-03\n",
      "Step 10775 | Loss: 0.05650 | LR: 3.32e-03\n",
      "val loss: 0.0560\n",
      "Step 10800 | Loss: 0.05502 | LR: 3.32e-03\n",
      "Step 10825 | Loss: 0.05035 | LR: 3.31e-03\n",
      "Step 10850 | Loss: 0.04765 | LR: 3.31e-03\n",
      "Step 10875 | Loss: 0.05501 | LR: 3.31e-03\n",
      "val loss: 0.0546\n",
      "Step 10900 | Loss: 0.05217 | LR: 3.30e-03\n",
      "Step 10925 | Loss: 0.05852 | LR: 3.30e-03\n",
      "Step 10950 | Loss: 0.05520 | LR: 3.30e-03\n",
      "Step 10975 | Loss: 0.05251 | LR: 3.29e-03\n",
      "val loss: 0.0527\n",
      "Step 11000 | Loss: 0.05342 | LR: 3.29e-03\n",
      "Step 11025 | Loss: 0.05258 | LR: 3.28e-03\n",
      "Step 11050 | Loss: 0.05511 | LR: 3.28e-03\n",
      "Step 11075 | Loss: 0.05265 | LR: 3.28e-03\n",
      "val loss: 0.0517\n",
      "Step 11100 | Loss: 0.04662 | LR: 3.27e-03\n",
      "Step 11125 | Loss: 0.05480 | LR: 3.27e-03\n",
      "Step 11150 | Loss: 0.05033 | LR: 3.27e-03\n",
      "Step 11175 | Loss: 0.04624 | LR: 3.26e-03\n",
      "val loss: 0.0525\n",
      "Step 11200 | Loss: 0.05510 | LR: 3.26e-03\n",
      "Step 11225 | Loss: 0.04895 | LR: 3.26e-03\n",
      "Step 11250 | Loss: 0.05112 | LR: 3.25e-03\n",
      "Step 11275 | Loss: 0.04835 | LR: 3.25e-03\n",
      "val loss: 0.0488\n",
      "Step 11300 | Loss: 0.05046 | LR: 3.24e-03\n",
      "Step 11325 | Loss: 0.04615 | LR: 3.24e-03\n",
      "Step 11350 | Loss: 0.05174 | LR: 3.24e-03\n",
      "Step 11375 | Loss: 0.04341 | LR: 3.23e-03\n",
      "val loss: 0.0502\n",
      "Step 11400 | Loss: 0.05268 | LR: 3.23e-03\n",
      "Step 11425 | Loss: 0.05123 | LR: 3.23e-03\n",
      "Step 11450 | Loss: 0.04781 | LR: 3.22e-03\n",
      "Step 11475 | Loss: 0.04397 | LR: 3.22e-03\n",
      "val loss: 0.0491\n",
      "Step 11500 | Loss: 0.05030 | LR: 3.21e-03\n",
      "Step 11525 | Loss: 0.04983 | LR: 3.21e-03\n",
      "Step 11550 | Loss: 0.05055 | LR: 3.21e-03\n",
      "Step 11575 | Loss: 0.04652 | LR: 3.20e-03\n",
      "val loss: 0.0470\n",
      "Step 11600 | Loss: 0.04413 | LR: 3.20e-03\n",
      "Step 11625 | Loss: 0.04859 | LR: 3.20e-03\n",
      "Step 11650 | Loss: 0.04768 | LR: 3.19e-03\n",
      "Step 11675 | Loss: 0.04796 | LR: 3.19e-03\n",
      "val loss: 0.0449\n",
      "Step 11700 | Loss: 0.04190 | LR: 3.18e-03\n",
      "Step 11725 | Loss: 0.04688 | LR: 3.18e-03\n",
      "Step 11750 | Loss: 0.04512 | LR: 3.18e-03\n",
      "Step 11775 | Loss: 0.05041 | LR: 3.17e-03\n",
      "val loss: 0.0463\n",
      "Step 11800 | Loss: 0.04802 | LR: 3.17e-03\n",
      "Step 11825 | Loss: 0.05011 | LR: 3.17e-03\n",
      "Step 11850 | Loss: 0.04411 | LR: 3.16e-03\n",
      "Step 11875 | Loss: 0.04371 | LR: 3.16e-03\n",
      "val loss: 0.0459\n",
      "Step 11900 | Loss: 0.04495 | LR: 3.15e-03\n",
      "Step 11925 | Loss: 0.04920 | LR: 3.15e-03\n",
      "Step 11950 | Loss: 0.04510 | LR: 3.15e-03\n",
      "Step 11975 | Loss: 0.04767 | LR: 3.14e-03\n",
      "val loss: 0.0465\n",
      "Step 12000 | Loss: 0.04292 | LR: 3.14e-03\n",
      "Step 12025 | Loss: 0.04672 | LR: 3.13e-03\n",
      "Step 12050 | Loss: 0.04337 | LR: 3.13e-03\n",
      "Step 12075 | Loss: 0.04333 | LR: 3.13e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "val loss: 0.0453\n",
      "Step 12100 | Loss: 0.04587 | LR: 3.12e-03\n",
      "Step 12125 | Loss: 0.04626 | LR: 3.12e-03\n",
      "Step 12150 | Loss: 0.03995 | LR: 3.12e-03\n",
      "Step 12175 | Loss: 0.04183 | LR: 3.11e-03\n",
      "val loss: 0.0444\n",
      "Step 12200 | Loss: 0.04292 | LR: 3.11e-03\n",
      "Step 12225 | Loss: 0.04389 | LR: 3.10e-03\n",
      "Step 12250 | Loss: 0.04496 | LR: 3.10e-03\n",
      "Step 12275 | Loss: 0.04057 | LR: 3.10e-03\n",
      "val loss: 0.0431\n",
      "Step 12300 | Loss: 0.04374 | LR: 3.09e-03\n",
      "Step 12325 | Loss: 0.04384 | LR: 3.09e-03\n",
      "Step 12350 | Loss: 0.04118 | LR: 3.08e-03\n",
      "Step 12375 | Loss: 0.04499 | LR: 3.08e-03\n",
      "val loss: 0.0437\n",
      "Step 12400 | Loss: 0.04126 | LR: 3.08e-03\n",
      "Step 12425 | Loss: 0.04459 | LR: 3.07e-03\n",
      "Step 12450 | Loss: 0.04304 | LR: 3.07e-03\n",
      "Step 12475 | Loss: 0.04064 | LR: 3.06e-03\n",
      "val loss: 0.0431\n",
      "Step 12500 | Loss: 0.04491 | LR: 3.06e-03\n",
      "Step 12525 | Loss: 0.04593 | LR: 3.06e-03\n",
      "Step 12550 | Loss: 0.04378 | LR: 3.05e-03\n",
      "Step 12575 | Loss: 0.04547 | LR: 3.05e-03\n",
      "val loss: 0.0436\n",
      "Step 12600 | Loss: 0.04180 | LR: 3.04e-03\n",
      "Step 12625 | Loss: 0.04076 | LR: 3.04e-03\n",
      "Step 12650 | Loss: 0.04310 | LR: 3.04e-03\n",
      "Step 12675 | Loss: 0.04141 | LR: 3.03e-03\n",
      "val loss: 0.0445\n",
      "Step 12700 | Loss: 0.04330 | LR: 3.03e-03\n",
      "Step 12725 | Loss: 0.04222 | LR: 3.02e-03\n",
      "Step 12750 | Loss: 0.04337 | LR: 3.02e-03\n",
      "Step 12775 | Loss: 0.04725 | LR: 3.02e-03\n",
      "val loss: 0.0426\n",
      "Step 12800 | Loss: 0.04132 | LR: 3.01e-03\n",
      "Step 12825 | Loss: 0.04146 | LR: 3.01e-03\n",
      "Step 12850 | Loss: 0.04338 | LR: 3.00e-03\n",
      "Step 12875 | Loss: 0.04155 | LR: 3.00e-03\n",
      "val loss: 0.0419\n",
      "Step 12900 | Loss: 0.04372 | LR: 3.00e-03\n",
      "Step 12925 | Loss: 0.04167 | LR: 2.99e-03\n",
      "Step 12950 | Loss: 0.03931 | LR: 2.99e-03\n",
      "Step 12975 | Loss: 0.04109 | LR: 2.98e-03\n",
      "val loss: 0.0420\n",
      "Step 13000 | Loss: 0.04217 | LR: 2.98e-03\n",
      "Step 13025 | Loss: 0.03962 | LR: 2.97e-03\n",
      "Step 13050 | Loss: 0.04219 | LR: 2.97e-03\n",
      "Step 13075 | Loss: 0.04179 | LR: 2.97e-03\n",
      "val loss: 0.0408\n",
      "Step 13100 | Loss: 0.04386 | LR: 2.96e-03\n",
      "Step 13125 | Loss: 0.04236 | LR: 2.96e-03\n",
      "Step 13150 | Loss: 0.04264 | LR: 2.95e-03\n",
      "Step 13175 | Loss: 0.04350 | LR: 2.95e-03\n",
      "val loss: 0.0409\n",
      "Step 13200 | Loss: 0.04174 | LR: 2.95e-03\n",
      "Step 13225 | Loss: 0.04042 | LR: 2.94e-03\n",
      "Step 13250 | Loss: 0.04263 | LR: 2.94e-03\n",
      "Step 13275 | Loss: 0.04376 | LR: 2.93e-03\n",
      "val loss: 0.0416\n",
      "Step 13300 | Loss: 0.03974 | LR: 2.93e-03\n",
      "Step 13325 | Loss: 0.04404 | LR: 2.93e-03\n",
      "Step 13350 | Loss: 0.03892 | LR: 2.92e-03\n",
      "Step 13375 | Loss: 0.03935 | LR: 2.92e-03\n",
      "val loss: 0.0422\n",
      "Step 13400 | Loss: 0.04286 | LR: 2.91e-03\n",
      "Step 13425 | Loss: 0.04607 | LR: 2.91e-03\n",
      "Step 13450 | Loss: 0.04120 | LR: 2.90e-03\n",
      "Step 13475 | Loss: 0.04169 | LR: 2.90e-03\n",
      "val loss: 0.0398\n",
      "Step 13500 | Loss: 0.04377 | LR: 2.90e-03\n",
      "Step 13525 | Loss: 0.04247 | LR: 2.89e-03\n",
      "Step 13550 | Loss: 0.04244 | LR: 2.89e-03\n",
      "Step 13575 | Loss: 0.04288 | LR: 2.88e-03\n",
      "val loss: 0.0408\n",
      "Step 13600 | Loss: 0.04396 | LR: 2.88e-03\n",
      "Step 13625 | Loss: 0.03853 | LR: 2.87e-03\n",
      "Step 13650 | Loss: 0.03995 | LR: 2.87e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/pretraining/train/pt_shard_k562e_train_0002.npz\n",
      "Step 13675 | Loss: 0.04329 | LR: 2.87e-03\n",
      "val loss: 0.0403\n",
      "Step 13700 | Loss: 0.03804 | LR: 2.86e-03\n",
      "Step 13725 | Loss: 0.04122 | LR: 2.86e-03\n",
      "Step 13750 | Loss: 0.04323 | LR: 2.85e-03\n",
      "Step 13775 | Loss: 0.03981 | LR: 2.85e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/pretraining/val/pt_shard_k562e_val_0000.npz\n",
      "val loss: 0.0414\n",
      "Step 13800 | Loss: 0.04264 | LR: 2.84e-03\n",
      "Step 13825 | Loss: 0.04251 | LR: 2.84e-03\n",
      "Step 13850 | Loss: 0.04396 | LR: 2.84e-03\n",
      "Step 13875 | Loss: 0.03996 | LR: 2.83e-03\n",
      "val loss: 0.0397\n",
      "Step 13900 | Loss: 0.04252 | LR: 2.83e-03\n",
      "Step 13925 | Loss: 0.03981 | LR: 2.82e-03\n",
      "Step 13950 | Loss: 0.03864 | LR: 2.82e-03\n",
      "Step 13975 | Loss: 0.04155 | LR: 2.81e-03\n",
      "val loss: 0.0389\n",
      "Step 14000 | Loss: 0.03960 | LR: 2.81e-03\n",
      "Step 14025 | Loss: 0.03935 | LR: 2.81e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "=== Step 14043 Done. Avg Loss: 0.04580 ===\n",
      "Step 14050 | Loss: 0.03930 | LR: 2.80e-03\n",
      "Step 14075 | Loss: 0.04307 | LR: 2.80e-03\n",
      "val loss: 0.0382\n",
      "Step 14100 | Loss: 0.04116 | LR: 2.79e-03\n",
      "Step 14125 | Loss: 0.04135 | LR: 2.79e-03\n",
      "Step 14150 | Loss: 0.03957 | LR: 2.78e-03\n",
      "Step 14175 | Loss: 0.03493 | LR: 2.78e-03\n",
      "val loss: 0.0374\n",
      "Step 14200 | Loss: 0.03595 | LR: 2.78e-03\n",
      "Step 14225 | Loss: 0.03684 | LR: 2.77e-03\n",
      "Step 14250 | Loss: 0.03688 | LR: 2.77e-03\n",
      "Step 14275 | Loss: 0.04041 | LR: 2.76e-03\n",
      "val loss: 0.0377\n",
      "Step 14300 | Loss: 0.03578 | LR: 2.76e-03\n",
      "Step 14325 | Loss: 0.03874 | LR: 2.75e-03\n",
      "Step 14350 | Loss: 0.03795 | LR: 2.75e-03\n",
      "Step 14375 | Loss: 0.03788 | LR: 2.75e-03\n",
      "val loss: 0.0371\n",
      "Step 14400 | Loss: 0.03683 | LR: 2.74e-03\n",
      "Step 14425 | Loss: 0.03413 | LR: 2.74e-03\n",
      "Step 14450 | Loss: 0.03632 | LR: 2.73e-03\n",
      "Step 14475 | Loss: 0.03488 | LR: 2.73e-03\n",
      "val loss: 0.0376\n",
      "Step 14500 | Loss: 0.03818 | LR: 2.72e-03\n",
      "Step 14525 | Loss: 0.03669 | LR: 2.72e-03\n",
      "Step 14550 | Loss: 0.03666 | LR: 2.71e-03\n",
      "Step 14575 | Loss: 0.03930 | LR: 2.71e-03\n",
      "val loss: 0.0361\n",
      "Step 14600 | Loss: 0.03414 | LR: 2.71e-03\n",
      "Step 14625 | Loss: 0.03217 | LR: 2.70e-03\n",
      "Step 14650 | Loss: 0.03527 | LR: 2.70e-03\n",
      "Step 14675 | Loss: 0.03626 | LR: 2.69e-03\n",
      "val loss: 0.0353\n",
      "Step 14700 | Loss: 0.03864 | LR: 2.69e-03\n",
      "Step 14725 | Loss: 0.03368 | LR: 2.68e-03\n",
      "Step 14750 | Loss: 0.03857 | LR: 2.68e-03\n",
      "Step 14775 | Loss: 0.03440 | LR: 2.68e-03\n",
      "val loss: 0.0359\n",
      "Step 14800 | Loss: 0.03718 | LR: 2.67e-03\n",
      "Step 14825 | Loss: 0.03391 | LR: 2.67e-03\n",
      "Step 14850 | Loss: 0.03676 | LR: 2.66e-03\n",
      "Step 14875 | Loss: 0.03707 | LR: 2.66e-03\n",
      "val loss: 0.0370\n",
      "Step 14900 | Loss: 0.04027 | LR: 2.65e-03\n",
      "Step 14925 | Loss: 0.03138 | LR: 2.65e-03\n",
      "Step 14950 | Loss: 0.03447 | LR: 2.64e-03\n",
      "Step 14975 | Loss: 0.03901 | LR: 2.64e-03\n",
      "val loss: 0.0350\n",
      "Step 15000 | Loss: 0.03337 | LR: 2.64e-03\n",
      "Step 15025 | Loss: 0.03396 | LR: 2.63e-03\n",
      "Step 15050 | Loss: 0.03776 | LR: 2.63e-03\n",
      "Step 15075 | Loss: 0.03573 | LR: 2.62e-03\n",
      "val loss: 0.0366\n",
      "Step 15100 | Loss: 0.03841 | LR: 2.62e-03\n",
      "Step 15125 | Loss: 0.03472 | LR: 2.61e-03\n",
      "Step 15150 | Loss: 0.03650 | LR: 2.61e-03\n",
      "Step 15175 | Loss: 0.03612 | LR: 2.60e-03\n",
      "val loss: 0.0352\n",
      "Step 15200 | Loss: 0.03214 | LR: 2.60e-03\n",
      "Step 15225 | Loss: 0.03243 | LR: 2.59e-03\n",
      "Step 15250 | Loss: 0.03494 | LR: 2.59e-03\n",
      "Step 15275 | Loss: 0.03550 | LR: 2.59e-03\n",
      "val loss: 0.0338\n",
      "Step 15300 | Loss: 0.03063 | LR: 2.58e-03\n",
      "Step 15325 | Loss: 0.02844 | LR: 2.58e-03\n",
      "Step 15350 | Loss: 0.02803 | LR: 2.57e-03\n",
      "Step 15375 | Loss: 0.03578 | LR: 2.57e-03\n",
      "val loss: 0.0337\n",
      "Step 15400 | Loss: 0.03321 | LR: 2.56e-03\n",
      "Step 15425 | Loss: 0.03629 | LR: 2.56e-03\n",
      "Step 15450 | Loss: 0.03213 | LR: 2.55e-03\n",
      "Step 15475 | Loss: 0.03001 | LR: 2.55e-03\n",
      "val loss: 0.0315\n",
      "Step 15500 | Loss: 0.02889 | LR: 2.55e-03\n",
      "Step 15525 | Loss: 0.03167 | LR: 2.54e-03\n",
      "Step 15550 | Loss: 0.02890 | LR: 2.54e-03\n",
      "Step 15575 | Loss: 0.03757 | LR: 2.53e-03\n",
      "val loss: 0.0310\n",
      "Step 15600 | Loss: 0.03313 | LR: 2.53e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/pretraining/train/pt_shard_k562e_train_0002.npz\n",
      "Step 15625 | Loss: 0.02850 | LR: 2.52e-03\n",
      "Step 15650 | Loss: 0.03915 | LR: 2.52e-03\n",
      "Step 15675 | Loss: 0.02773 | LR: 2.51e-03\n",
      "val loss: 0.0304\n",
      "Step 15700 | Loss: 0.03447 | LR: 2.51e-03\n",
      "Step 15725 | Loss: 0.02898 | LR: 2.50e-03\n",
      "Step 15750 | Loss: 0.03278 | LR: 2.50e-03\n",
      "Step 15775 | Loss: 0.03408 | LR: 2.50e-03\n",
      "val loss: 0.0367\n",
      "Step 15800 | Loss: 0.03606 | LR: 2.49e-03\n",
      "Step 15825 | Loss: 0.03025 | LR: 2.49e-03\n",
      "Step 15850 | Loss: 0.03088 | LR: 2.48e-03\n",
      "Step 15875 | Loss: 0.03174 | LR: 2.48e-03\n",
      "val loss: 0.0296\n",
      "Step 15900 | Loss: 0.03013 | LR: 2.47e-03\n",
      "Step 15925 | Loss: 0.02834 | LR: 2.47e-03\n",
      "Step 15950 | Loss: 0.03415 | LR: 2.46e-03\n",
      "Step 15975 | Loss: 0.02827 | LR: 2.46e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "val loss: 0.0292\n",
      "Step 16000 | Loss: 0.03191 | LR: 2.45e-03\n",
      "Step 16025 | Loss: 0.02869 | LR: 2.45e-03\n",
      "Step 16050 | Loss: 0.03205 | LR: 2.44e-03\n",
      "Step 16075 | Loss: 0.02880 | LR: 2.44e-03\n",
      "val loss: 0.0303\n",
      "Step 16100 | Loss: 0.03200 | LR: 2.44e-03\n",
      "Step 16125 | Loss: 0.03024 | LR: 2.43e-03\n",
      "Step 16150 | Loss: 0.03030 | LR: 2.43e-03\n",
      "Step 16175 | Loss: 0.02538 | LR: 2.42e-03\n",
      "val loss: 0.0282\n",
      "Step 16200 | Loss: 0.03008 | LR: 2.42e-03\n",
      "Step 16225 | Loss: 0.02735 | LR: 2.41e-03\n",
      "Step 16250 | Loss: 0.02740 | LR: 2.41e-03\n",
      "Step 16275 | Loss: 0.02887 | LR: 2.40e-03\n",
      "val loss: 0.0281\n",
      "Step 16300 | Loss: 0.02822 | LR: 2.40e-03\n",
      "Step 16325 | Loss: 0.02841 | LR: 2.39e-03\n",
      "Step 16350 | Loss: 0.02783 | LR: 2.39e-03\n",
      "Step 16375 | Loss: 0.02891 | LR: 2.38e-03\n",
      "val loss: 0.0308\n",
      "Step 16400 | Loss: 0.02864 | LR: 2.38e-03\n",
      "Step 16425 | Loss: 0.02523 | LR: 2.38e-03\n",
      "Step 16450 | Loss: 0.02821 | LR: 2.37e-03\n",
      "Step 16475 | Loss: 0.02781 | LR: 2.37e-03\n",
      "val loss: 0.0262\n",
      "Step 16500 | Loss: 0.02907 | LR: 2.36e-03\n",
      "Step 16525 | Loss: 0.02454 | LR: 2.36e-03\n",
      "Step 16550 | Loss: 0.02615 | LR: 2.35e-03\n",
      "Step 16575 | Loss: 0.03096 | LR: 2.35e-03\n",
      "val loss: 0.0260\n",
      "Step 16600 | Loss: 0.02654 | LR: 2.34e-03\n",
      "Step 16625 | Loss: 0.02937 | LR: 2.34e-03\n",
      "Step 16650 | Loss: 0.02606 | LR: 2.33e-03\n",
      "Step 16675 | Loss: 0.02581 | LR: 2.33e-03\n",
      "val loss: 0.0267\n",
      "Step 16700 | Loss: 0.02977 | LR: 2.32e-03\n",
      "Step 16725 | Loss: 0.02855 | LR: 2.32e-03\n",
      "Step 16750 | Loss: 0.02800 | LR: 2.32e-03\n",
      "Step 16775 | Loss: 0.02722 | LR: 2.31e-03\n",
      "val loss: 0.0266\n",
      "Step 16800 | Loss: 0.02148 | LR: 2.31e-03\n",
      "Step 16825 | Loss: 0.02477 | LR: 2.30e-03\n",
      "Step 16850 | Loss: 0.02595 | LR: 2.30e-03\n",
      "Step 16875 | Loss: 0.02374 | LR: 2.29e-03\n",
      "val loss: 0.0267\n",
      "Step 16900 | Loss: 0.02845 | LR: 2.29e-03\n",
      "Step 16925 | Loss: 0.02509 | LR: 2.28e-03\n",
      "Step 16950 | Loss: 0.02318 | LR: 2.28e-03\n",
      "Step 16975 | Loss: 0.02515 | LR: 2.27e-03\n",
      "val loss: 0.0264\n",
      "Step 17000 | Loss: 0.02741 | LR: 2.27e-03\n",
      "Step 17025 | Loss: 0.02509 | LR: 2.26e-03\n",
      "Step 17050 | Loss: 0.02924 | LR: 2.26e-03\n",
      "Step 17075 | Loss: 0.03452 | LR: 2.25e-03\n",
      "val loss: 0.0257\n",
      "Step 17100 | Loss: 0.02545 | LR: 2.25e-03\n",
      "Step 17125 | Loss: 0.02605 | LR: 2.25e-03\n",
      "Step 17150 | Loss: 0.02683 | LR: 2.24e-03\n",
      "Step 17175 | Loss: 0.02981 | LR: 2.24e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/pretraining/val/pt_shard_k562e_val_0000.npz\n",
      "val loss: 0.0301\n",
      "Step 17200 | Loss: 0.03064 | LR: 2.23e-03\n",
      "Step 17225 | Loss: 0.02358 | LR: 2.23e-03\n",
      "Step 17250 | Loss: 0.02369 | LR: 2.22e-03\n",
      "Step 17275 | Loss: 0.02170 | LR: 2.22e-03\n",
      "val loss: 0.0256\n",
      "Step 17300 | Loss: 0.02407 | LR: 2.21e-03\n",
      "Step 17325 | Loss: 0.02407 | LR: 2.21e-03\n",
      "Step 17350 | Loss: 0.02398 | LR: 2.20e-03\n",
      "Step 17375 | Loss: 0.02239 | LR: 2.20e-03\n",
      "val loss: 0.0247\n",
      "Step 17400 | Loss: 0.02422 | LR: 2.19e-03\n",
      "Step 17425 | Loss: 0.02391 | LR: 2.19e-03\n",
      "Step 17450 | Loss: 0.02282 | LR: 2.18e-03\n",
      "Step 17475 | Loss: 0.02384 | LR: 2.18e-03\n",
      "val loss: 0.0247\n",
      "Step 17500 | Loss: 0.02717 | LR: 2.18e-03\n",
      "Step 17525 | Loss: 0.02217 | LR: 2.17e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 17550 | Loss: 0.02332 | LR: 2.17e-03\n",
      "=== Step 17554 Done. Avg Loss: 0.03133 ===\n",
      "Step 17575 | Loss: 0.02775 | LR: 2.16e-03\n",
      "val loss: 0.0240\n",
      "Step 17600 | Loss: 0.02210 | LR: 2.16e-03\n",
      "Step 17625 | Loss: 0.02491 | LR: 2.15e-03\n",
      "Step 17650 | Loss: 0.02543 | LR: 2.15e-03\n",
      "Step 17675 | Loss: 0.02168 | LR: 2.14e-03\n",
      "val loss: 0.0232\n",
      "Step 17700 | Loss: 0.02491 | LR: 2.14e-03\n",
      "Step 17725 | Loss: 0.02149 | LR: 2.13e-03\n",
      "Step 17750 | Loss: 0.02242 | LR: 2.13e-03\n",
      "Step 17775 | Loss: 0.02467 | LR: 2.12e-03\n",
      "val loss: 0.0236\n",
      "Step 17800 | Loss: 0.02275 | LR: 2.12e-03\n",
      "Step 17825 | Loss: 0.02139 | LR: 2.11e-03\n",
      "Step 17850 | Loss: 0.02445 | LR: 2.11e-03\n",
      "Step 17875 | Loss: 0.02426 | LR: 2.10e-03\n",
      "val loss: 0.0234\n",
      "Step 17900 | Loss: 0.02202 | LR: 2.10e-03\n",
      "Step 17925 | Loss: 0.02466 | LR: 2.10e-03\n",
      "Step 17950 | Loss: 0.02130 | LR: 2.09e-03\n",
      "Step 17975 | Loss: 0.02032 | LR: 2.09e-03\n",
      "val loss: 0.0229\n",
      "Step 18000 | Loss: 0.02504 | LR: 2.08e-03\n",
      "Step 18025 | Loss: 0.02183 | LR: 2.08e-03\n",
      "Step 18050 | Loss: 0.02631 | LR: 2.07e-03\n",
      "Step 18075 | Loss: 0.02365 | LR: 2.07e-03\n",
      "val loss: 0.0224\n",
      "Step 18100 | Loss: 0.02150 | LR: 2.06e-03\n",
      "Step 18125 | Loss: 0.02292 | LR: 2.06e-03\n",
      "Step 18150 | Loss: 0.02009 | LR: 2.05e-03\n",
      "Step 18175 | Loss: 0.01883 | LR: 2.05e-03\n",
      "val loss: 0.0246\n",
      "Step 18200 | Loss: 0.02392 | LR: 2.04e-03\n",
      "Step 18225 | Loss: 0.02197 | LR: 2.04e-03\n",
      "Step 18250 | Loss: 0.02437 | LR: 2.03e-03\n",
      "Step 18275 | Loss: 0.02030 | LR: 2.03e-03\n",
      "val loss: 0.0216\n",
      "Step 18300 | Loss: 0.02090 | LR: 2.02e-03\n",
      "Step 18325 | Loss: 0.01916 | LR: 2.02e-03\n",
      "Step 18350 | Loss: 0.02689 | LR: 2.02e-03\n",
      "Step 18375 | Loss: 0.02348 | LR: 2.01e-03\n",
      "val loss: 0.0209\n",
      "Step 18400 | Loss: 0.02125 | LR: 2.01e-03\n",
      "Step 18425 | Loss: 0.02200 | LR: 2.00e-03\n",
      "Step 18450 | Loss: 0.01972 | LR: 2.00e-03\n",
      "Step 18475 | Loss: 0.02105 | LR: 1.99e-03\n",
      "val loss: 0.0210\n",
      "Step 18500 | Loss: 0.02223 | LR: 1.99e-03\n",
      "Step 18525 | Loss: 0.02139 | LR: 1.98e-03\n",
      "Step 18550 | Loss: 0.02276 | LR: 1.98e-03\n",
      "Step 18575 | Loss: 0.01996 | LR: 1.97e-03\n",
      "val loss: 0.0223\n",
      "Step 18600 | Loss: 0.02364 | LR: 1.97e-03\n",
      "Step 18625 | Loss: 0.02063 | LR: 1.96e-03\n",
      "Step 18650 | Loss: 0.02063 | LR: 1.96e-03\n",
      "Step 18675 | Loss: 0.02195 | LR: 1.95e-03\n",
      "val loss: 0.0212\n",
      "Step 18700 | Loss: 0.02295 | LR: 1.95e-03\n",
      "Step 18725 | Loss: 0.02020 | LR: 1.94e-03\n",
      "Step 18750 | Loss: 0.01917 | LR: 1.94e-03\n",
      "Step 18775 | Loss: 0.01873 | LR: 1.94e-03\n",
      "val loss: 0.0204\n",
      "Step 18800 | Loss: 0.01762 | LR: 1.93e-03\n",
      "Step 18825 | Loss: 0.01878 | LR: 1.93e-03\n",
      "Step 18850 | Loss: 0.01957 | LR: 1.92e-03\n",
      "Step 18875 | Loss: 0.02033 | LR: 1.92e-03\n",
      "val loss: 0.0211\n",
      "Step 18900 | Loss: 0.01898 | LR: 1.91e-03\n",
      "Step 18925 | Loss: 0.01919 | LR: 1.91e-03\n",
      "Step 18950 | Loss: 0.01903 | LR: 1.90e-03\n",
      "Step 18975 | Loss: 0.02045 | LR: 1.90e-03\n",
      "val loss: 0.0202\n",
      "Step 19000 | Loss: 0.02138 | LR: 1.89e-03\n",
      "Step 19025 | Loss: 0.01828 | LR: 1.89e-03\n",
      "Step 19050 | Loss: 0.02236 | LR: 1.88e-03\n",
      "Step 19075 | Loss: 0.01945 | LR: 1.88e-03\n",
      "val loss: 0.0210\n",
      "Step 19100 | Loss: 0.02118 | LR: 1.87e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 19125 | Loss: 0.02096 | LR: 1.87e-03\n",
      "Step 19150 | Loss: 0.02261 | LR: 1.86e-03\n",
      "Step 19175 | Loss: 0.02161 | LR: 1.86e-03\n",
      "val loss: 0.0196\n",
      "Step 19200 | Loss: 0.01938 | LR: 1.86e-03\n",
      "Step 19225 | Loss: 0.02039 | LR: 1.85e-03\n",
      "Step 19250 | Loss: 0.01999 | LR: 1.85e-03\n",
      "Step 19275 | Loss: 0.01791 | LR: 1.84e-03\n",
      "val loss: 0.0190\n",
      "Step 19300 | Loss: 0.01996 | LR: 1.84e-03\n",
      "Step 19325 | Loss: 0.01907 | LR: 1.83e-03\n",
      "Step 19350 | Loss: 0.02175 | LR: 1.83e-03\n",
      "Step 19375 | Loss: 0.01796 | LR: 1.82e-03\n",
      "val loss: 0.0192\n",
      "Step 19400 | Loss: 0.01991 | LR: 1.82e-03\n",
      "Step 19425 | Loss: 0.01982 | LR: 1.81e-03\n",
      "Step 19450 | Loss: 0.01802 | LR: 1.81e-03\n",
      "Step 19475 | Loss: 0.01764 | LR: 1.80e-03\n",
      "val loss: 0.0195\n",
      "Step 19500 | Loss: 0.01992 | LR: 1.80e-03\n",
      "Step 19525 | Loss: 0.01936 | LR: 1.79e-03\n",
      "Step 19550 | Loss: 0.01940 | LR: 1.79e-03\n",
      "Step 19575 | Loss: 0.01746 | LR: 1.78e-03\n",
      "val loss: 0.0185\n",
      "Step 19600 | Loss: 0.01749 | LR: 1.78e-03\n",
      "Step 19625 | Loss: 0.01955 | LR: 1.78e-03\n",
      "Step 19650 | Loss: 0.02058 | LR: 1.77e-03\n",
      "Step 19675 | Loss: 0.01966 | LR: 1.77e-03\n",
      "val loss: 0.0182\n",
      "Step 19700 | Loss: 0.01653 | LR: 1.76e-03\n",
      "Step 19725 | Loss: 0.01685 | LR: 1.76e-03\n",
      "Step 19750 | Loss: 0.01829 | LR: 1.75e-03\n",
      "Step 19775 | Loss: 0.01754 | LR: 1.75e-03\n",
      "val loss: 0.0170\n",
      "Step 19800 | Loss: 0.01902 | LR: 1.74e-03\n",
      "Step 19825 | Loss: 0.01714 | LR: 1.74e-03\n",
      "Step 19850 | Loss: 0.01748 | LR: 1.73e-03\n",
      "Step 19875 | Loss: 0.01783 | LR: 1.73e-03\n",
      "val loss: 0.0183\n",
      "Step 19900 | Loss: 0.01783 | LR: 1.72e-03\n",
      "Step 19925 | Loss: 0.01623 | LR: 1.72e-03\n",
      "Step 19950 | Loss: 0.01718 | LR: 1.71e-03\n",
      "Step 19975 | Loss: 0.01830 | LR: 1.71e-03\n",
      "val loss: 0.0182\n",
      "Step 20000 | Loss: 0.01806 | LR: 1.71e-03\n",
      "Step 20025 | Loss: 0.01815 | LR: 1.70e-03\n",
      "Step 20050 | Loss: 0.01883 | LR: 1.70e-03\n",
      "Step 20075 | Loss: 0.01881 | LR: 1.69e-03\n",
      "val loss: 0.0175\n",
      "Step 20100 | Loss: 0.01599 | LR: 1.69e-03\n",
      "Step 20125 | Loss: 0.01829 | LR: 1.68e-03\n",
      "Step 20150 | Loss: 0.01729 | LR: 1.68e-03\n",
      "Step 20175 | Loss: 0.01713 | LR: 1.67e-03\n",
      "val loss: 0.0173\n",
      "Step 20200 | Loss: 0.01631 | LR: 1.67e-03\n",
      "Step 20225 | Loss: 0.01779 | LR: 1.66e-03\n",
      "Step 20250 | Loss: 0.01884 | LR: 1.66e-03\n",
      "Step 20275 | Loss: 0.01725 | LR: 1.65e-03\n",
      "val loss: 0.0179\n",
      "Step 20300 | Loss: 0.02007 | LR: 1.65e-03\n",
      "Step 20325 | Loss: 0.01708 | LR: 1.65e-03\n",
      "Step 20350 | Loss: 0.01616 | LR: 1.64e-03\n",
      "Step 20375 | Loss: 0.01627 | LR: 1.64e-03\n",
      "val loss: 0.0175\n",
      "Step 20400 | Loss: 0.01829 | LR: 1.63e-03\n",
      "Step 20425 | Loss: 0.01500 | LR: 1.63e-03\n",
      "Step 20450 | Loss: 0.01557 | LR: 1.62e-03\n",
      "Step 20475 | Loss: 0.01563 | LR: 1.62e-03\n",
      "val loss: 0.0163\n",
      "Step 20500 | Loss: 0.01803 | LR: 1.61e-03\n",
      "Step 20525 | Loss: 0.01719 | LR: 1.61e-03\n",
      "Step 20550 | Loss: 0.01930 | LR: 1.60e-03\n",
      "Step 20575 | Loss: 0.01801 | LR: 1.60e-03\n",
      "val loss: 0.0167\n",
      "Step 20600 | Loss: 0.01570 | LR: 1.59e-03\n",
      "Step 20625 | Loss: 0.01735 | LR: 1.59e-03\n",
      "Step 20650 | Loss: 0.01682 | LR: 1.58e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/pretraining/train/pt_shard_k562e_train_0002.npz\n",
      "Step 20675 | Loss: 0.01572 | LR: 1.58e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/pretraining/val/pt_shard_k562e_val_0000.npz\n",
      "val loss: 0.0165\n",
      "Step 20700 | Loss: 0.01441 | LR: 1.58e-03\n",
      "Step 20725 | Loss: 0.01777 | LR: 1.57e-03\n",
      "Step 20750 | Loss: 0.01709 | LR: 1.57e-03\n",
      "Step 20775 | Loss: 0.01497 | LR: 1.56e-03\n",
      "val loss: 0.0168\n",
      "Step 20800 | Loss: 0.01459 | LR: 1.56e-03\n",
      "Step 20825 | Loss: 0.01708 | LR: 1.55e-03\n",
      "Step 20850 | Loss: 0.01514 | LR: 1.55e-03\n",
      "Step 20875 | Loss: 0.01717 | LR: 1.54e-03\n",
      "val loss: 0.0156\n",
      "Step 20900 | Loss: 0.01553 | LR: 1.54e-03\n",
      "Step 20925 | Loss: 0.01604 | LR: 1.53e-03\n",
      "Step 20950 | Loss: 0.01779 | LR: 1.53e-03\n",
      "Step 20975 | Loss: 0.01519 | LR: 1.53e-03\n",
      "val loss: 0.0162\n",
      "Step 21000 | Loss: 0.01747 | LR: 1.52e-03\n",
      "Step 21025 | Loss: 0.01390 | LR: 1.52e-03\n",
      "Step 21050 | Loss: 0.01548 | LR: 1.51e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "=== Step 21065 Done. Avg Loss: 0.01951 ===\n",
      "Step 21075 | Loss: 0.01590 | LR: 1.51e-03\n",
      "val loss: 0.0160\n",
      "Step 21100 | Loss: 0.01556 | LR: 1.50e-03\n",
      "Step 21125 | Loss: 0.01652 | LR: 1.50e-03\n",
      "Step 21150 | Loss: 0.01626 | LR: 1.49e-03\n",
      "Step 21175 | Loss: 0.01469 | LR: 1.49e-03\n",
      "val loss: 0.0165\n",
      "Step 21200 | Loss: 0.01458 | LR: 1.48e-03\n",
      "Step 21225 | Loss: 0.01454 | LR: 1.48e-03\n",
      "Step 21250 | Loss: 0.01594 | LR: 1.48e-03\n",
      "Step 21275 | Loss: 0.01495 | LR: 1.47e-03\n",
      "val loss: 0.0159\n",
      "Step 21300 | Loss: 0.01629 | LR: 1.47e-03\n",
      "Step 21325 | Loss: 0.01533 | LR: 1.46e-03\n",
      "Step 21350 | Loss: 0.01607 | LR: 1.46e-03\n",
      "Step 21375 | Loss: 0.01408 | LR: 1.45e-03\n",
      "val loss: 0.0147\n",
      "Step 21400 | Loss: 0.01324 | LR: 1.45e-03\n",
      "Step 21425 | Loss: 0.01505 | LR: 1.44e-03\n",
      "Step 21450 | Loss: 0.01450 | LR: 1.44e-03\n",
      "Step 21475 | Loss: 0.01532 | LR: 1.43e-03\n",
      "val loss: 0.0161\n",
      "Step 21500 | Loss: 0.01405 | LR: 1.43e-03\n",
      "Step 21525 | Loss: 0.01334 | LR: 1.43e-03\n",
      "Step 21550 | Loss: 0.01505 | LR: 1.42e-03\n",
      "Step 21575 | Loss: 0.01275 | LR: 1.42e-03\n",
      "val loss: 0.0154\n",
      "Step 21600 | Loss: 0.01706 | LR: 1.41e-03\n",
      "Step 21625 | Loss: 0.01632 | LR: 1.41e-03\n",
      "Step 21650 | Loss: 0.01618 | LR: 1.40e-03\n",
      "Step 21675 | Loss: 0.01495 | LR: 1.40e-03\n",
      "val loss: 0.0150\n",
      "Step 21700 | Loss: 0.01408 | LR: 1.39e-03\n",
      "Step 21725 | Loss: 0.01593 | LR: 1.39e-03\n",
      "Step 21750 | Loss: 0.01412 | LR: 1.38e-03\n",
      "Step 21775 | Loss: 0.01434 | LR: 1.38e-03\n",
      "val loss: 0.0145\n",
      "Step 21800 | Loss: 0.01638 | LR: 1.38e-03\n",
      "Step 21825 | Loss: 0.01540 | LR: 1.37e-03\n",
      "Step 21850 | Loss: 0.01553 | LR: 1.37e-03\n",
      "Step 21875 | Loss: 0.01513 | LR: 1.36e-03\n",
      "val loss: 0.0145\n",
      "Step 21900 | Loss: 0.01533 | LR: 1.36e-03\n",
      "Step 21925 | Loss: 0.01349 | LR: 1.35e-03\n",
      "Step 21950 | Loss: 0.01492 | LR: 1.35e-03\n",
      "Step 21975 | Loss: 0.01516 | LR: 1.34e-03\n",
      "val loss: 0.0150\n",
      "Step 22000 | Loss: 0.01385 | LR: 1.34e-03\n",
      "Step 22025 | Loss: 0.01614 | LR: 1.34e-03\n",
      "Step 22050 | Loss: 0.01320 | LR: 1.33e-03\n",
      "Step 22075 | Loss: 0.01575 | LR: 1.33e-03\n",
      "val loss: 0.0149\n",
      "Step 22100 | Loss: 0.01329 | LR: 1.32e-03\n",
      "Step 22125 | Loss: 0.01340 | LR: 1.32e-03\n",
      "Step 22150 | Loss: 0.01566 | LR: 1.31e-03\n",
      "Step 22175 | Loss: 0.01370 | LR: 1.31e-03\n",
      "val loss: 0.0139\n",
      "Step 22200 | Loss: 0.01367 | LR: 1.30e-03\n",
      "Step 22225 | Loss: 0.01487 | LR: 1.30e-03\n",
      "Step 22250 | Loss: 0.01363 | LR: 1.30e-03\n",
      "Step 22275 | Loss: 0.01419 | LR: 1.29e-03\n",
      "val loss: 0.0136\n",
      "Step 22300 | Loss: 0.01262 | LR: 1.29e-03\n",
      "Step 22325 | Loss: 0.01333 | LR: 1.28e-03\n",
      "Step 22350 | Loss: 0.01430 | LR: 1.28e-03\n",
      "Step 22375 | Loss: 0.01365 | LR: 1.27e-03\n",
      "val loss: 0.0141\n",
      "Step 22400 | Loss: 0.01368 | LR: 1.27e-03\n",
      "Step 22425 | Loss: 0.01389 | LR: 1.27e-03\n",
      "Step 22450 | Loss: 0.01391 | LR: 1.26e-03\n",
      "Step 22475 | Loss: 0.01493 | LR: 1.26e-03\n",
      "val loss: 0.0137\n",
      "Step 22500 | Loss: 0.01406 | LR: 1.25e-03\n",
      "Step 22525 | Loss: 0.01358 | LR: 1.25e-03\n",
      "Step 22550 | Loss: 0.01267 | LR: 1.24e-03\n",
      "Step 22575 | Loss: 0.01263 | LR: 1.24e-03\n",
      "val loss: 0.0136\n",
      "Step 22600 | Loss: 0.01344 | LR: 1.23e-03\n",
      "loading /Users/djemec/data/jepa/v0_2/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 22625 | Loss: 0.01274 | LR: 1.23e-03\n",
      "Step 22650 | Loss: 0.01358 | LR: 1.23e-03\n",
      "Step 22675 | Loss: 0.01487 | LR: 1.22e-03\n",
      "val loss: 0.0138\n",
      "Step 22700 | Loss: 0.01536 | LR: 1.22e-03\n",
      "Step 22725 | Loss: 0.01317 | LR: 1.21e-03\n",
      "Step 22750 | Loss: 0.01561 | LR: 1.21e-03\n",
      "Step 22775 | Loss: 0.01235 | LR: 1.20e-03\n",
      "val loss: 0.0135\n",
      "Step 22800 | Loss: 0.01298 | LR: 1.20e-03\n",
      "Step 22825 | Loss: 0.01336 | LR: 1.20e-03\n",
      "Step 22850 | Loss: 0.01168 | LR: 1.19e-03\n",
      "Step 22875 | Loss: 0.01317 | LR: 1.19e-03\n",
      "val loss: 0.0132\n",
      "Step 22900 | Loss: 0.01307 | LR: 1.18e-03\n",
      "Step 22925 | Loss: 0.01380 | LR: 1.18e-03\n",
      "Step 22950 | Loss: 0.01572 | LR: 1.17e-03\n",
      "Step 22975 | Loss: 0.01386 | LR: 1.17e-03\n",
      "val loss: 0.0136\n",
      "Step 23000 | Loss: 0.01340 | LR: 1.17e-03\n",
      "Step 23025 | Loss: 0.01463 | LR: 1.16e-03\n",
      "Step 23050 | Loss: 0.01346 | LR: 1.16e-03\n",
      "Step 23075 | Loss: 0.01332 | LR: 1.15e-03\n",
      "val loss: 0.0130\n",
      "Step 23100 | Loss: 0.01263 | LR: 1.15e-03\n",
      "Step 23125 | Loss: 0.01158 | LR: 1.14e-03\n",
      "Step 23150 | Loss: 0.01359 | LR: 1.14e-03\n",
      "Step 23175 | Loss: 0.01345 | LR: 1.14e-03\n",
      "val loss: 0.0132\n",
      "Step 23200 | Loss: 0.01234 | LR: 1.13e-03\n",
      "Step 23225 | Loss: 0.01379 | LR: 1.13e-03\n",
      "Step 23250 | Loss: 0.01319 | LR: 1.12e-03\n",
      "Step 23275 | Loss: 0.01292 | LR: 1.12e-03\n",
      "val loss: 0.0127\n",
      "Step 23300 | Loss: 0.01273 | LR: 1.11e-03\n",
      "Step 23325 | Loss: 0.01218 | LR: 1.11e-03\n",
      "Step 23350 | Loss: 0.01301 | LR: 1.11e-03\n",
      "Step 23375 | Loss: 0.01437 | LR: 1.10e-03\n",
      "val loss: 0.0131\n",
      "Step 23400 | Loss: 0.01177 | LR: 1.10e-03\n",
      "Step 23425 | Loss: 0.01270 | LR: 1.09e-03\n",
      "Step 23450 | Loss: 0.01312 | LR: 1.09e-03\n",
      "Step 23475 | Loss: 0.01379 | LR: 1.09e-03\n",
      "val loss: 0.0126\n",
      "Step 23500 | Loss: 0.01319 | LR: 1.08e-03\n",
      "Step 23525 | Loss: 0.01250 | LR: 1.08e-03\n",
      "Step 23550 | Loss: 0.01222 | LR: 1.07e-03\n",
      "Step 23575 | Loss: 0.01365 | LR: 1.07e-03\n",
      "val loss: 0.0128\n",
      "Step 23600 | Loss: 0.01258 | LR: 1.06e-03\n",
      "Step 23625 | Loss: 0.01260 | LR: 1.06e-03\n",
      "Step 23650 | Loss: 0.01234 | LR: 1.06e-03\n",
      "Step 23675 | Loss: 0.01307 | LR: 1.05e-03\n",
      "val loss: 0.0127\n",
      "Step 23700 | Loss: 0.01208 | LR: 1.05e-03\n",
      "Step 23725 | Loss: 0.01269 | LR: 1.04e-03\n",
      "Step 23750 | Loss: 0.01235 | LR: 1.04e-03\n",
      "Step 23775 | Loss: 0.01161 | LR: 1.04e-03\n",
      "val loss: 0.0128\n",
      "Step 23800 | Loss: 0.01230 | LR: 1.03e-03\n",
      "Step 23825 | Loss: 0.01287 | LR: 1.03e-03\n",
      "Step 23850 | Loss: 0.01207 | LR: 1.02e-03\n",
      "Step 23875 | Loss: 0.01199 | LR: 1.02e-03\n",
      "val loss: 0.0124\n",
      "Step 23900 | Loss: 0.01260 | LR: 1.01e-03\n",
      "Step 23925 | Loss: 0.01232 | LR: 1.01e-03\n",
      "Step 23950 | Loss: 0.01360 | LR: 1.01e-03\n",
      "Step 23975 | Loss: 0.01296 | LR: 1.00e-03\n",
      "val loss: 0.0124\n",
      "Step 24000 | Loss: 0.01231 | LR: 9.98e-04\n",
      "Step 24025 | Loss: 0.01242 | LR: 9.94e-04\n",
      "Step 24050 | Loss: 0.01195 | LR: 9.90e-04\n",
      "Step 24075 | Loss: 0.01146 | LR: 9.86e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/pretraining/val/pt_shard_k562e_val_0000.npz\n",
      "val loss: 0.0123\n",
      "Step 24100 | Loss: 0.01242 | LR: 9.82e-04\n",
      "Step 24125 | Loss: 0.01245 | LR: 9.78e-04\n",
      "Step 24150 | Loss: 0.01199 | LR: 9.74e-04\n",
      "Step 24175 | Loss: 0.01265 | LR: 9.70e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/pretraining/train/pt_shard_k562e_train_0002.npz\n",
      "val loss: 0.0122\n",
      "Step 24200 | Loss: 0.01251 | LR: 9.66e-04\n",
      "Step 24225 | Loss: 0.01310 | LR: 9.62e-04\n",
      "Step 24250 | Loss: 0.01313 | LR: 9.58e-04\n",
      "Step 24275 | Loss: 0.01112 | LR: 9.54e-04\n",
      "val loss: 0.0125\n",
      "Step 24300 | Loss: 0.01160 | LR: 9.50e-04\n",
      "Step 24325 | Loss: 0.01281 | LR: 9.46e-04\n",
      "Step 24350 | Loss: 0.01212 | LR: 9.42e-04\n",
      "Step 24375 | Loss: 0.01172 | LR: 9.38e-04\n",
      "val loss: 0.0122\n",
      "Step 24400 | Loss: 0.01153 | LR: 9.34e-04\n",
      "Step 24425 | Loss: 0.01245 | LR: 9.30e-04\n",
      "Step 24450 | Loss: 0.01172 | LR: 9.26e-04\n",
      "Step 24475 | Loss: 0.01118 | LR: 9.22e-04\n",
      "val loss: 0.0119\n",
      "Step 24500 | Loss: 0.01184 | LR: 9.18e-04\n",
      "Step 24525 | Loss: 0.01219 | LR: 9.14e-04\n",
      "Step 24550 | Loss: 0.01214 | LR: 9.10e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/pretraining/train/pt_shard_k562e_train_0002.npz\n",
      "Step 24575 | Loss: 0.01243 | LR: 9.06e-04\n",
      "=== Step 24576 Done. Avg Loss: 0.01369 ===\n",
      "val loss: 0.0120\n",
      "Step 24600 | Loss: 0.01109 | LR: 9.02e-04\n",
      "Step 24625 | Loss: 0.01196 | LR: 8.98e-04\n",
      "Step 24650 | Loss: 0.01132 | LR: 8.94e-04\n",
      "Step 24675 | Loss: 0.01119 | LR: 8.90e-04\n",
      "val loss: 0.0117\n",
      "Step 24700 | Loss: 0.01051 | LR: 8.86e-04\n",
      "Step 24725 | Loss: 0.01204 | LR: 8.83e-04\n",
      "Step 24750 | Loss: 0.01116 | LR: 8.79e-04\n",
      "Step 24775 | Loss: 0.01155 | LR: 8.75e-04\n",
      "val loss: 0.0117\n",
      "Step 24800 | Loss: 0.01177 | LR: 8.71e-04\n",
      "Step 24825 | Loss: 0.01232 | LR: 8.67e-04\n",
      "Step 24850 | Loss: 0.01252 | LR: 8.63e-04\n",
      "Step 24875 | Loss: 0.01210 | LR: 8.59e-04\n",
      "val loss: 0.0117\n",
      "Step 24900 | Loss: 0.01188 | LR: 8.55e-04\n",
      "Step 24925 | Loss: 0.01161 | LR: 8.52e-04\n",
      "Step 24950 | Loss: 0.01182 | LR: 8.48e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 24975 | Loss: 0.01256 | LR: 8.44e-04\n",
      "val loss: 0.0119\n",
      "Step 25000 | Loss: 0.01134 | LR: 8.40e-04\n",
      "Step 25025 | Loss: 0.01150 | LR: 8.36e-04\n",
      "Step 25050 | Loss: 0.01134 | LR: 8.32e-04\n",
      "Step 25075 | Loss: 0.01224 | LR: 8.29e-04\n",
      "val loss: 0.0115\n",
      "Step 25100 | Loss: 0.01150 | LR: 8.25e-04\n",
      "Step 25125 | Loss: 0.01145 | LR: 8.21e-04\n",
      "Step 25150 | Loss: 0.01018 | LR: 8.17e-04\n",
      "Step 25175 | Loss: 0.01093 | LR: 8.13e-04\n",
      "val loss: 0.0118\n",
      "Step 25200 | Loss: 0.01137 | LR: 8.10e-04\n",
      "Step 25225 | Loss: 0.01284 | LR: 8.06e-04\n",
      "Step 25250 | Loss: 0.01070 | LR: 8.02e-04\n",
      "Step 25275 | Loss: 0.01156 | LR: 7.98e-04\n",
      "val loss: 0.0117\n",
      "Step 25300 | Loss: 0.01303 | LR: 7.94e-04\n",
      "Step 25325 | Loss: 0.01163 | LR: 7.91e-04\n",
      "Step 25350 | Loss: 0.01192 | LR: 7.87e-04\n",
      "Step 25375 | Loss: 0.01215 | LR: 7.83e-04\n",
      "val loss: 0.0115\n",
      "Step 25400 | Loss: 0.01149 | LR: 7.79e-04\n",
      "Step 25425 | Loss: 0.01116 | LR: 7.76e-04\n",
      "Step 25450 | Loss: 0.01103 | LR: 7.72e-04\n",
      "Step 25475 | Loss: 0.01289 | LR: 7.68e-04\n",
      "val loss: 0.0116\n",
      "Step 25500 | Loss: 0.01206 | LR: 7.65e-04\n",
      "Step 25525 | Loss: 0.01149 | LR: 7.61e-04\n",
      "Step 25550 | Loss: 0.01188 | LR: 7.57e-04\n",
      "Step 25575 | Loss: 0.01048 | LR: 7.53e-04\n",
      "val loss: 0.0111\n",
      "Step 25600 | Loss: 0.01130 | LR: 7.50e-04\n",
      "Step 25625 | Loss: 0.01067 | LR: 7.46e-04\n",
      "Step 25650 | Loss: 0.01102 | LR: 7.42e-04\n",
      "Step 25675 | Loss: 0.01212 | LR: 7.39e-04\n",
      "val loss: 0.0111\n",
      "Step 25700 | Loss: 0.01124 | LR: 7.35e-04\n",
      "Step 25725 | Loss: 0.01058 | LR: 7.32e-04\n",
      "Step 25750 | Loss: 0.01209 | LR: 7.28e-04\n",
      "Step 25775 | Loss: 0.01110 | LR: 7.24e-04\n",
      "val loss: 0.0114\n",
      "Step 25800 | Loss: 0.01062 | LR: 7.21e-04\n",
      "Step 25825 | Loss: 0.01075 | LR: 7.17e-04\n",
      "Step 25850 | Loss: 0.01069 | LR: 7.13e-04\n",
      "Step 25875 | Loss: 0.01128 | LR: 7.10e-04\n",
      "val loss: 0.0113\n",
      "Step 25900 | Loss: 0.01070 | LR: 7.06e-04\n",
      "Step 25925 | Loss: 0.01144 | LR: 7.03e-04\n",
      "Step 25950 | Loss: 0.01068 | LR: 6.99e-04\n",
      "Step 25975 | Loss: 0.01150 | LR: 6.95e-04\n",
      "val loss: 0.0110\n",
      "Step 26000 | Loss: 0.01118 | LR: 6.92e-04\n",
      "Step 26025 | Loss: 0.01094 | LR: 6.88e-04\n",
      "Step 26050 | Loss: 0.01084 | LR: 6.85e-04\n",
      "Step 26075 | Loss: 0.01127 | LR: 6.81e-04\n",
      "val loss: 0.0111\n",
      "Step 26100 | Loss: 0.01027 | LR: 6.78e-04\n",
      "Step 26125 | Loss: 0.01089 | LR: 6.74e-04\n",
      "Step 26150 | Loss: 0.01137 | LR: 6.71e-04\n",
      "Step 26175 | Loss: 0.01102 | LR: 6.67e-04\n",
      "val loss: 0.0109\n",
      "Step 26200 | Loss: 0.01097 | LR: 6.64e-04\n",
      "Step 26225 | Loss: 0.01045 | LR: 6.60e-04\n",
      "Step 26250 | Loss: 0.01042 | LR: 6.57e-04\n",
      "Step 26275 | Loss: 0.01117 | LR: 6.53e-04\n",
      "val loss: 0.0107\n",
      "Step 26300 | Loss: 0.01089 | LR: 6.50e-04\n",
      "Step 26325 | Loss: 0.01007 | LR: 6.46e-04\n",
      "Step 26350 | Loss: 0.01069 | LR: 6.43e-04\n",
      "Step 26375 | Loss: 0.01289 | LR: 6.39e-04\n",
      "val loss: 0.0111\n",
      "Step 26400 | Loss: 0.01129 | LR: 6.36e-04\n",
      "Step 26425 | Loss: 0.01064 | LR: 6.32e-04\n",
      "Step 26450 | Loss: 0.01008 | LR: 6.29e-04\n",
      "Step 26475 | Loss: 0.01037 | LR: 6.26e-04\n",
      "val loss: 0.0108\n",
      "Step 26500 | Loss: 0.01017 | LR: 6.22e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 26525 | Loss: 0.01064 | LR: 6.19e-04\n",
      "Step 26550 | Loss: 0.01074 | LR: 6.15e-04\n",
      "Step 26575 | Loss: 0.00993 | LR: 6.12e-04\n",
      "val loss: 0.0110\n",
      "Step 26600 | Loss: 0.01132 | LR: 6.09e-04\n",
      "Step 26625 | Loss: 0.01151 | LR: 6.05e-04\n",
      "Step 26650 | Loss: 0.01042 | LR: 6.02e-04\n",
      "Step 26675 | Loss: 0.01057 | LR: 5.98e-04\n",
      "val loss: 0.0106\n",
      "Step 26700 | Loss: 0.01175 | LR: 5.95e-04\n",
      "Step 26725 | Loss: 0.01155 | LR: 5.92e-04\n",
      "Step 26750 | Loss: 0.01030 | LR: 5.88e-04\n",
      "Step 26775 | Loss: 0.01008 | LR: 5.85e-04\n",
      "val loss: 0.0106\n",
      "Step 26800 | Loss: 0.01054 | LR: 5.82e-04\n",
      "Step 26825 | Loss: 0.00966 | LR: 5.78e-04\n",
      "Step 26850 | Loss: 0.01054 | LR: 5.75e-04\n",
      "Step 26875 | Loss: 0.01013 | LR: 5.72e-04\n",
      "val loss: 0.0104\n",
      "Step 26900 | Loss: 0.00957 | LR: 5.69e-04\n",
      "Step 26925 | Loss: 0.00986 | LR: 5.65e-04\n",
      "Step 26950 | Loss: 0.01142 | LR: 5.62e-04\n",
      "Step 26975 | Loss: 0.00983 | LR: 5.59e-04\n",
      "val loss: 0.0102\n",
      "Step 27000 | Loss: 0.01003 | LR: 5.55e-04\n",
      "Step 27025 | Loss: 0.01042 | LR: 5.52e-04\n",
      "Step 27050 | Loss: 0.01038 | LR: 5.49e-04\n",
      "Step 27075 | Loss: 0.01006 | LR: 5.46e-04\n",
      "val loss: 0.0101\n",
      "Step 27100 | Loss: 0.01033 | LR: 5.42e-04\n",
      "Step 27125 | Loss: 0.01060 | LR: 5.39e-04\n",
      "Step 27150 | Loss: 0.00935 | LR: 5.36e-04\n",
      "Step 27175 | Loss: 0.01028 | LR: 5.33e-04\n",
      "val loss: 0.0098\n",
      "Step 27200 | Loss: 0.01075 | LR: 5.30e-04\n",
      "Step 27225 | Loss: 0.01024 | LR: 5.26e-04\n",
      "Step 27250 | Loss: 0.01102 | LR: 5.23e-04\n",
      "Step 27275 | Loss: 0.00997 | LR: 5.20e-04\n",
      "val loss: 0.0102\n",
      "Step 27300 | Loss: 0.01027 | LR: 5.17e-04\n",
      "Step 27325 | Loss: 0.00963 | LR: 5.14e-04\n",
      "Step 27350 | Loss: 0.00993 | LR: 5.11e-04\n",
      "Step 27375 | Loss: 0.01042 | LR: 5.07e-04\n",
      "val loss: 0.0101\n",
      "Step 27400 | Loss: 0.00907 | LR: 5.04e-04\n",
      "Step 27425 | Loss: 0.00967 | LR: 5.01e-04\n",
      "Step 27450 | Loss: 0.00970 | LR: 4.98e-04\n",
      "Step 27475 | Loss: 0.01027 | LR: 4.95e-04\n",
      "val loss: 0.0102\n",
      "Step 27500 | Loss: 0.00987 | LR: 4.92e-04\n",
      "Step 27525 | Loss: 0.00873 | LR: 4.89e-04\n",
      "Step 27550 | Loss: 0.00984 | LR: 4.86e-04\n",
      "Step 27575 | Loss: 0.01031 | LR: 4.83e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/pretraining/val/pt_shard_k562e_val_0000.npz\n",
      "val loss: 0.0096\n",
      "Step 27600 | Loss: 0.01122 | LR: 4.80e-04\n",
      "Step 27625 | Loss: 0.00957 | LR: 4.77e-04\n",
      "Step 27650 | Loss: 0.00966 | LR: 4.73e-04\n",
      "Step 27675 | Loss: 0.01063 | LR: 4.70e-04\n",
      "val loss: 0.0097\n",
      "Step 27700 | Loss: 0.00973 | LR: 4.67e-04\n",
      "Step 27725 | Loss: 0.00882 | LR: 4.64e-04\n",
      "Step 27750 | Loss: 0.00971 | LR: 4.61e-04\n",
      "Step 27775 | Loss: 0.01003 | LR: 4.58e-04\n",
      "val loss: 0.0098\n",
      "Step 27800 | Loss: 0.00960 | LR: 4.55e-04\n",
      "Step 27825 | Loss: 0.00926 | LR: 4.52e-04\n",
      "Step 27850 | Loss: 0.01040 | LR: 4.49e-04\n",
      "Step 27875 | Loss: 0.01042 | LR: 4.46e-04\n",
      "val loss: 0.0097\n",
      "Step 27900 | Loss: 0.00969 | LR: 4.43e-04\n",
      "Step 27925 | Loss: 0.00966 | LR: 4.41e-04\n",
      "Step 27950 | Loss: 0.01049 | LR: 4.38e-04\n",
      "Step 27975 | Loss: 0.00921 | LR: 4.35e-04\n",
      "val loss: 0.0098\n",
      "Step 28000 | Loss: 0.00997 | LR: 4.32e-04\n",
      "Step 28025 | Loss: 0.00923 | LR: 4.29e-04\n",
      "Step 28050 | Loss: 0.00913 | LR: 4.26e-04\n",
      "Step 28075 | Loss: 0.00964 | LR: 4.23e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "=== Step 28087 Done. Avg Loss: 0.01081 ===\n",
      "val loss: 0.0096\n",
      "Step 28100 | Loss: 0.00904 | LR: 4.20e-04\n",
      "Step 28125 | Loss: 0.00857 | LR: 4.17e-04\n",
      "Step 28150 | Loss: 0.00972 | LR: 4.14e-04\n",
      "Step 28175 | Loss: 0.00938 | LR: 4.11e-04\n",
      "val loss: 0.0095\n",
      "Step 28200 | Loss: 0.00936 | LR: 4.09e-04\n",
      "Step 28225 | Loss: 0.00977 | LR: 4.06e-04\n",
      "Step 28250 | Loss: 0.00939 | LR: 4.03e-04\n",
      "Step 28275 | Loss: 0.00928 | LR: 4.00e-04\n",
      "val loss: 0.0095\n",
      "Step 28300 | Loss: 0.00849 | LR: 3.97e-04\n",
      "Step 28325 | Loss: 0.01022 | LR: 3.94e-04\n",
      "Step 28350 | Loss: 0.00924 | LR: 3.92e-04\n",
      "Step 28375 | Loss: 0.00915 | LR: 3.89e-04\n",
      "val loss: 0.0093\n",
      "Step 28400 | Loss: 0.01014 | LR: 3.86e-04\n",
      "Step 28425 | Loss: 0.01032 | LR: 3.83e-04\n",
      "Step 28450 | Loss: 0.00946 | LR: 3.81e-04\n",
      "Step 28475 | Loss: 0.00867 | LR: 3.78e-04\n",
      "val loss: 0.0094\n",
      "Step 28500 | Loss: 0.00855 | LR: 3.75e-04\n",
      "Step 28525 | Loss: 0.00878 | LR: 3.72e-04\n",
      "Step 28550 | Loss: 0.00897 | LR: 3.70e-04\n",
      "Step 28575 | Loss: 0.00967 | LR: 3.67e-04\n",
      "val loss: 0.0094\n",
      "Step 28600 | Loss: 0.00982 | LR: 3.64e-04\n",
      "Step 28625 | Loss: 0.00824 | LR: 3.61e-04\n",
      "Step 28650 | Loss: 0.00916 | LR: 3.59e-04\n",
      "Step 28675 | Loss: 0.00966 | LR: 3.56e-04\n",
      "val loss: 0.0093\n",
      "Step 28700 | Loss: 0.00822 | LR: 3.53e-04\n",
      "Step 28725 | Loss: 0.00855 | LR: 3.51e-04\n",
      "Step 28750 | Loss: 0.00887 | LR: 3.48e-04\n",
      "Step 28775 | Loss: 0.00953 | LR: 3.45e-04\n",
      "val loss: 0.0089\n",
      "Step 28800 | Loss: 0.00892 | LR: 3.43e-04\n",
      "Step 28825 | Loss: 0.00855 | LR: 3.40e-04\n",
      "Step 28850 | Loss: 0.00873 | LR: 3.37e-04\n",
      "Step 28875 | Loss: 0.00921 | LR: 3.35e-04\n",
      "val loss: 0.0091\n",
      "Step 28900 | Loss: 0.00800 | LR: 3.32e-04\n",
      "Step 28925 | Loss: 0.00875 | LR: 3.30e-04\n",
      "Step 28950 | Loss: 0.00877 | LR: 3.27e-04\n",
      "Step 28975 | Loss: 0.00920 | LR: 3.25e-04\n",
      "val loss: 0.0088\n",
      "Step 29000 | Loss: 0.00831 | LR: 3.22e-04\n",
      "Step 29025 | Loss: 0.00861 | LR: 3.19e-04\n",
      "Step 29050 | Loss: 0.00909 | LR: 3.17e-04\n",
      "Step 29075 | Loss: 0.00935 | LR: 3.14e-04\n",
      "val loss: 0.0088\n",
      "Step 29100 | Loss: 0.01022 | LR: 3.12e-04\n",
      "Step 29125 | Loss: 0.00914 | LR: 3.09e-04\n",
      "Step 29150 | Loss: 0.00858 | LR: 3.07e-04\n",
      "Step 29175 | Loss: 0.00901 | LR: 3.04e-04\n",
      "val loss: 0.0087\n",
      "Step 29200 | Loss: 0.00807 | LR: 3.02e-04\n",
      "Step 29225 | Loss: 0.00876 | LR: 2.99e-04\n",
      "Step 29250 | Loss: 0.00886 | LR: 2.97e-04\n",
      "Step 29275 | Loss: 0.00785 | LR: 2.94e-04\n",
      "val loss: 0.0085\n",
      "Step 29300 | Loss: 0.00947 | LR: 2.92e-04\n",
      "Step 29325 | Loss: 0.00785 | LR: 2.89e-04\n",
      "Step 29350 | Loss: 0.00941 | LR: 2.87e-04\n",
      "Step 29375 | Loss: 0.00832 | LR: 2.85e-04\n",
      "val loss: 0.0090\n",
      "Step 29400 | Loss: 0.00882 | LR: 2.82e-04\n",
      "Step 29425 | Loss: 0.00813 | LR: 2.80e-04\n",
      "Step 29450 | Loss: 0.00831 | LR: 2.77e-04\n",
      "Step 29475 | Loss: 0.00819 | LR: 2.75e-04\n",
      "val loss: 0.0088\n",
      "Step 29500 | Loss: 0.00786 | LR: 2.73e-04\n",
      "Step 29525 | Loss: 0.00862 | LR: 2.70e-04\n",
      "Step 29550 | Loss: 0.00879 | LR: 2.68e-04\n",
      "Step 29575 | Loss: 0.00848 | LR: 2.66e-04\n",
      "val loss: 0.0086\n",
      "Step 29600 | Loss: 0.00818 | LR: 2.63e-04\n",
      "Step 29625 | Loss: 0.00901 | LR: 2.61e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "Step 29650 | Loss: 0.00850 | LR: 2.59e-04\n",
      "Step 29675 | Loss: 0.00791 | LR: 2.56e-04\n",
      "val loss: 0.0084\n",
      "Step 29700 | Loss: 0.00853 | LR: 2.54e-04\n",
      "Step 29725 | Loss: 0.00868 | LR: 2.52e-04\n",
      "Step 29750 | Loss: 0.00848 | LR: 2.49e-04\n",
      "Step 29775 | Loss: 0.00880 | LR: 2.47e-04\n",
      "val loss: 0.0084\n",
      "Step 29800 | Loss: 0.00809 | LR: 2.45e-04\n",
      "Step 29825 | Loss: 0.00825 | LR: 2.43e-04\n",
      "Step 29850 | Loss: 0.00788 | LR: 2.40e-04\n",
      "Step 29875 | Loss: 0.00853 | LR: 2.38e-04\n",
      "val loss: 0.0085\n",
      "Step 29900 | Loss: 0.00912 | LR: 2.36e-04\n",
      "Step 29925 | Loss: 0.00977 | LR: 2.34e-04\n",
      "Step 29950 | Loss: 0.00814 | LR: 2.31e-04\n",
      "Step 29975 | Loss: 0.00788 | LR: 2.29e-04\n",
      "val loss: 0.0085\n",
      "Step 30000 | Loss: 0.00890 | LR: 2.27e-04\n",
      "Step 30025 | Loss: 0.00921 | LR: 2.25e-04\n",
      "Step 30050 | Loss: 0.00926 | LR: 2.23e-04\n",
      "Step 30075 | Loss: 0.00844 | LR: 2.21e-04\n",
      "val loss: 0.0084\n",
      "Step 30100 | Loss: 0.00830 | LR: 2.18e-04\n",
      "Step 30125 | Loss: 0.00826 | LR: 2.16e-04\n",
      "Step 30150 | Loss: 0.00751 | LR: 2.14e-04\n",
      "Step 30175 | Loss: 0.00834 | LR: 2.12e-04\n",
      "val loss: 0.0084\n",
      "Step 30200 | Loss: 0.00871 | LR: 2.10e-04\n",
      "Step 30225 | Loss: 0.00894 | LR: 2.08e-04\n",
      "Step 30250 | Loss: 0.00809 | LR: 2.06e-04\n",
      "Step 30275 | Loss: 0.00827 | LR: 2.04e-04\n",
      "val loss: 0.0081\n",
      "Step 30300 | Loss: 0.00862 | LR: 2.02e-04\n",
      "Step 30325 | Loss: 0.00800 | LR: 2.00e-04\n",
      "Step 30350 | Loss: 0.00792 | LR: 1.98e-04\n",
      "Step 30375 | Loss: 0.00825 | LR: 1.95e-04\n",
      "val loss: 0.0085\n",
      "Step 30400 | Loss: 0.00800 | LR: 1.93e-04\n",
      "Step 30425 | Loss: 0.00758 | LR: 1.91e-04\n",
      "Step 30450 | Loss: 0.00792 | LR: 1.89e-04\n",
      "Step 30475 | Loss: 0.00995 | LR: 1.87e-04\n",
      "val loss: 0.0083\n",
      "Step 30500 | Loss: 0.00776 | LR: 1.85e-04\n",
      "Step 30525 | Loss: 0.00833 | LR: 1.83e-04\n",
      "Step 30550 | Loss: 0.00859 | LR: 1.82e-04\n",
      "Step 30575 | Loss: 0.00832 | LR: 1.80e-04\n",
      "val loss: 0.0082\n",
      "Step 30600 | Loss: 0.00779 | LR: 1.78e-04\n",
      "Step 30625 | Loss: 0.00817 | LR: 1.76e-04\n",
      "Step 30650 | Loss: 0.00774 | LR: 1.74e-04\n",
      "Step 30675 | Loss: 0.00759 | LR: 1.72e-04\n",
      "val loss: 0.0081\n",
      "Step 30700 | Loss: 0.00808 | LR: 1.70e-04\n",
      "Step 30725 | Loss: 0.00750 | LR: 1.68e-04\n",
      "Step 30750 | Loss: 0.00913 | LR: 1.66e-04\n",
      "Step 30775 | Loss: 0.00772 | LR: 1.64e-04\n",
      "val loss: 0.0082\n",
      "Step 30800 | Loss: 0.00894 | LR: 1.62e-04\n",
      "Step 30825 | Loss: 0.00779 | LR: 1.61e-04\n",
      "Step 30850 | Loss: 0.00867 | LR: 1.59e-04\n",
      "Step 30875 | Loss: 0.00820 | LR: 1.57e-04\n",
      "val loss: 0.0080\n",
      "Step 30900 | Loss: 0.00784 | LR: 1.55e-04\n",
      "Step 30925 | Loss: 0.00817 | LR: 1.53e-04\n",
      "Step 30950 | Loss: 0.00803 | LR: 1.51e-04\n",
      "Step 30975 | Loss: 0.00800 | LR: 1.50e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/pretraining/val/pt_shard_k562e_val_0000.npz\n",
      "val loss: 0.0079\n",
      "Step 31000 | Loss: 0.00919 | LR: 1.48e-04\n",
      "Step 31025 | Loss: 0.00796 | LR: 1.46e-04\n",
      "Step 31050 | Loss: 0.00747 | LR: 1.44e-04\n",
      "Step 31075 | Loss: 0.00733 | LR: 1.43e-04\n",
      "val loss: 0.0081\n",
      "Step 31100 | Loss: 0.00767 | LR: 1.41e-04\n",
      "Step 31125 | Loss: 0.00771 | LR: 1.39e-04\n",
      "Step 31150 | Loss: 0.00823 | LR: 1.37e-04\n",
      "Step 31175 | Loss: 0.00870 | LR: 1.36e-04\n",
      "val loss: 0.0077\n",
      "Step 31200 | Loss: 0.00788 | LR: 1.34e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/pretraining/train/pt_shard_k562e_train_0002.npz\n",
      "Step 31225 | Loss: 0.00800 | LR: 1.32e-04\n",
      "Step 31250 | Loss: 0.00888 | LR: 1.31e-04\n",
      "Step 31275 | Loss: 0.00832 | LR: 1.29e-04\n",
      "val loss: 0.0077\n",
      "Step 31300 | Loss: 0.00784 | LR: 1.27e-04\n",
      "Step 31325 | Loss: 0.00820 | LR: 1.26e-04\n",
      "Step 31350 | Loss: 0.00734 | LR: 1.24e-04\n",
      "Step 31375 | Loss: 0.00777 | LR: 1.22e-04\n",
      "val loss: 0.0080\n",
      "Step 31400 | Loss: 0.00713 | LR: 1.21e-04\n",
      "Step 31425 | Loss: 0.00756 | LR: 1.19e-04\n",
      "Step 31450 | Loss: 0.00820 | LR: 1.18e-04\n",
      "Step 31475 | Loss: 0.00793 | LR: 1.16e-04\n",
      "val loss: 0.0077\n",
      "Step 31500 | Loss: 0.00812 | LR: 1.14e-04\n",
      "Step 31525 | Loss: 0.00808 | LR: 1.13e-04\n",
      "Step 31550 | Loss: 0.00777 | LR: 1.11e-04\n",
      "Step 31575 | Loss: 0.00716 | LR: 1.10e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/pretraining/train/pt_shard_k562e_train_0002.npz\n",
      "=== Step 31598 Done. Avg Loss: 0.00856 ===\n",
      "val loss: 0.0076\n",
      "Step 31600 | Loss: 0.00778 | LR: 1.08e-04\n",
      "Step 31625 | Loss: 0.00805 | LR: 1.07e-04\n",
      "Step 31650 | Loss: 0.00788 | LR: 1.05e-04\n",
      "Step 31675 | Loss: 0.00762 | LR: 1.04e-04\n",
      "val loss: 0.0081\n",
      "Step 31700 | Loss: 0.00825 | LR: 1.02e-04\n",
      "Step 31725 | Loss: 0.00796 | LR: 1.01e-04\n",
      "Step 31750 | Loss: 0.00724 | LR: 9.92e-05\n",
      "Step 31775 | Loss: 0.00788 | LR: 9.78e-05\n",
      "val loss: 0.0075\n",
      "Step 31800 | Loss: 0.00819 | LR: 9.63e-05\n",
      "Step 31825 | Loss: 0.00758 | LR: 9.49e-05\n",
      "Step 31850 | Loss: 0.00692 | LR: 9.34e-05\n",
      "Step 31875 | Loss: 0.00751 | LR: 9.20e-05\n",
      "val loss: 0.0076\n",
      "Step 31900 | Loss: 0.00696 | LR: 9.06e-05\n",
      "Step 31925 | Loss: 0.00888 | LR: 8.92e-05\n",
      "Step 31950 | Loss: 0.00675 | LR: 8.78e-05\n",
      "Step 31975 | Loss: 0.00821 | LR: 8.65e-05\n",
      "loading /Users/djemec/data/jepa/v0_2/pretraining/train/pt_shard_k562e_train_0001.npz\n",
      "val loss: 0.0073\n",
      "Step 32000 | Loss: 0.00723 | LR: 8.51e-05\n",
      "Step 32025 | Loss: 0.00858 | LR: 8.37e-05\n",
      "Step 32050 | Loss: 0.00692 | LR: 8.24e-05\n",
      "Step 32075 | Loss: 0.00748 | LR: 8.11e-05\n",
      "val loss: 0.0075\n",
      "Step 32100 | Loss: 0.00689 | LR: 7.97e-05\n",
      "Step 32125 | Loss: 0.00738 | LR: 7.84e-05\n",
      "Step 32150 | Loss: 0.00787 | LR: 7.71e-05\n",
      "Step 32175 | Loss: 0.00724 | LR: 7.58e-05\n",
      "val loss: 0.0075\n",
      "Step 32200 | Loss: 0.00824 | LR: 7.46e-05\n",
      "Step 32225 | Loss: 0.00739 | LR: 7.33e-05\n",
      "Step 32250 | Loss: 0.00794 | LR: 7.20e-05\n",
      "Step 32275 | Loss: 0.00789 | LR: 7.08e-05\n",
      "val loss: 0.0072\n",
      "Step 32300 | Loss: 0.00721 | LR: 6.96e-05\n",
      "Step 32325 | Loss: 0.00722 | LR: 6.83e-05\n",
      "Step 32350 | Loss: 0.00688 | LR: 6.71e-05\n",
      "Step 32375 | Loss: 0.00725 | LR: 6.59e-05\n",
      "val loss: 0.0075\n",
      "Step 32400 | Loss: 0.00809 | LR: 6.47e-05\n",
      "Step 32425 | Loss: 0.00726 | LR: 6.35e-05\n",
      "Step 32450 | Loss: 0.00734 | LR: 6.24e-05\n",
      "Step 32475 | Loss: 0.00763 | LR: 6.12e-05\n",
      "val loss: 0.0074\n",
      "Step 32500 | Loss: 0.00724 | LR: 6.01e-05\n",
      "Step 32525 | Loss: 0.00775 | LR: 5.89e-05\n",
      "Step 32550 | Loss: 0.00771 | LR: 5.78e-05\n",
      "Step 32575 | Loss: 0.00742 | LR: 5.67e-05\n",
      "val loss: 0.0076\n",
      "Step 32600 | Loss: 0.00764 | LR: 5.56e-05\n",
      "Step 32625 | Loss: 0.00815 | LR: 5.45e-05\n",
      "Step 32650 | Loss: 0.00817 | LR: 5.34e-05\n",
      "Step 32675 | Loss: 0.00775 | LR: 5.23e-05\n",
      "val loss: 0.0074\n",
      "Step 32700 | Loss: 0.00727 | LR: 5.12e-05\n",
      "Step 32725 | Loss: 0.00775 | LR: 5.02e-05\n",
      "Step 32750 | Loss: 0.00736 | LR: 4.91e-05\n",
      "Step 32775 | Loss: 0.00737 | LR: 4.81e-05\n",
      "val loss: 0.0072\n",
      "Step 32800 | Loss: 0.00727 | LR: 4.71e-05\n",
      "Step 32825 | Loss: 0.00722 | LR: 4.61e-05\n",
      "Step 32850 | Loss: 0.00725 | LR: 4.51e-05\n",
      "Step 32875 | Loss: 0.00653 | LR: 4.41e-05\n",
      "val loss: 0.0072\n",
      "Step 32900 | Loss: 0.00658 | LR: 4.31e-05\n",
      "Step 32925 | Loss: 0.00685 | LR: 4.21e-05\n",
      "Step 32950 | Loss: 0.00704 | LR: 4.12e-05\n",
      "Step 32975 | Loss: 0.00778 | LR: 4.02e-05\n",
      "val loss: 0.0073\n",
      "Step 33000 | Loss: 0.00737 | LR: 3.93e-05\n",
      "Step 33025 | Loss: 0.00707 | LR: 3.84e-05\n",
      "Step 33050 | Loss: 0.00691 | LR: 3.75e-05\n",
      "Step 33075 | Loss: 0.00660 | LR: 3.66e-05\n",
      "val loss: 0.0073\n",
      "Step 33100 | Loss: 0.00734 | LR: 3.57e-05\n",
      "Step 33125 | Loss: 0.00684 | LR: 3.48e-05\n",
      "Step 33150 | Loss: 0.00675 | LR: 3.39e-05\n",
      "Step 33175 | Loss: 0.00676 | LR: 3.31e-05\n",
      "val loss: 0.0073\n",
      "Step 33200 | Loss: 0.00735 | LR: 3.22e-05\n",
      "Step 33225 | Loss: 0.00823 | LR: 3.14e-05\n",
      "Step 33250 | Loss: 0.00842 | LR: 3.06e-05\n",
      "Step 33275 | Loss: 0.00668 | LR: 2.97e-05\n",
      "val loss: 0.0074\n",
      "Step 33300 | Loss: 0.00685 | LR: 2.89e-05\n",
      "Step 33325 | Loss: 0.00758 | LR: 2.82e-05\n",
      "Step 33350 | Loss: 0.00685 | LR: 2.74e-05\n",
      "Step 33375 | Loss: 0.00733 | LR: 2.66e-05\n",
      "val loss: 0.0070\n",
      "Step 33400 | Loss: 0.00709 | LR: 2.58e-05\n",
      "Step 33425 | Loss: 0.00683 | LR: 2.51e-05\n",
      "Step 33450 | Loss: 0.00707 | LR: 2.44e-05\n",
      "Step 33475 | Loss: 0.00763 | LR: 2.36e-05\n",
      "val loss: 0.0071\n",
      "Step 33500 | Loss: 0.00644 | LR: 2.29e-05\n",
      "Step 33525 | Loss: 0.00651 | LR: 2.22e-05\n",
      "loading /Users/djemec/data/jepa/v0_2/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 33550 | Loss: 0.00710 | LR: 2.15e-05\n",
      "Step 33575 | Loss: 0.00745 | LR: 2.08e-05\n",
      "val loss: 0.0073\n",
      "Step 33600 | Loss: 0.00784 | LR: 2.02e-05\n",
      "Step 33625 | Loss: 0.00755 | LR: 1.95e-05\n",
      "Step 33650 | Loss: 0.00803 | LR: 1.88e-05\n",
      "Step 33675 | Loss: 0.00685 | LR: 1.82e-05\n",
      "val loss: 0.0071\n",
      "Step 33700 | Loss: 0.00674 | LR: 1.76e-05\n",
      "Step 33725 | Loss: 0.00693 | LR: 1.70e-05\n",
      "Step 33750 | Loss: 0.00741 | LR: 1.64e-05\n",
      "Step 33775 | Loss: 0.00765 | LR: 1.58e-05\n",
      "val loss: 0.0074\n",
      "Step 33800 | Loss: 0.00676 | LR: 1.52e-05\n",
      "Step 33825 | Loss: 0.00703 | LR: 1.46e-05\n",
      "Step 33850 | Loss: 0.00778 | LR: 1.40e-05\n",
      "Step 33875 | Loss: 0.00792 | LR: 1.35e-05\n",
      "val loss: 0.0070\n",
      "Step 33900 | Loss: 0.00703 | LR: 1.29e-05\n",
      "Step 33925 | Loss: 0.00675 | LR: 1.24e-05\n",
      "Step 33950 | Loss: 0.00724 | LR: 1.19e-05\n",
      "Step 33975 | Loss: 0.00704 | LR: 1.14e-05\n",
      "val loss: 0.0073\n",
      "Step 34000 | Loss: 0.00699 | LR: 1.09e-05\n",
      "Step 34025 | Loss: 0.00666 | LR: 1.04e-05\n",
      "Step 34050 | Loss: 0.00779 | LR: 9.94e-06\n",
      "Step 34075 | Loss: 0.00711 | LR: 9.48e-06\n",
      "val loss: 0.0073\n",
      "Step 34100 | Loss: 0.00739 | LR: 9.02e-06\n",
      "Step 34125 | Loss: 0.00645 | LR: 8.58e-06\n",
      "Step 34150 | Loss: 0.00757 | LR: 8.15e-06\n",
      "Step 34175 | Loss: 0.00808 | LR: 7.73e-06\n",
      "val loss: 0.0071\n",
      "Step 34200 | Loss: 0.00734 | LR: 7.33e-06\n",
      "Step 34225 | Loss: 0.00730 | LR: 6.93e-06\n",
      "Step 34250 | Loss: 0.00715 | LR: 6.54e-06\n",
      "Step 34275 | Loss: 0.00686 | LR: 6.17e-06\n",
      "val loss: 0.0074\n",
      "Step 34300 | Loss: 0.00654 | LR: 5.80e-06\n",
      "Step 34325 | Loss: 0.00650 | LR: 5.45e-06\n",
      "Step 34350 | Loss: 0.00792 | LR: 5.11e-06\n",
      "Step 34375 | Loss: 0.00698 | LR: 4.78e-06\n",
      "val loss: 0.0073\n",
      "Step 34400 | Loss: 0.00741 | LR: 4.46e-06\n",
      "Step 34425 | Loss: 0.00782 | LR: 4.15e-06\n",
      "Step 34450 | Loss: 0.00706 | LR: 3.86e-06\n",
      "Step 34475 | Loss: 0.00615 | LR: 3.57e-06\n",
      "loading /Users/djemec/data/jepa/v0_2/pretraining/val/pt_shard_k562e_val_0000.npz\n",
      "val loss: 0.0073\n",
      "Step 34500 | Loss: 0.00653 | LR: 3.29e-06\n",
      "Step 34525 | Loss: 0.00763 | LR: 3.03e-06\n",
      "Step 34550 | Loss: 0.00662 | LR: 2.78e-06\n",
      "Step 34575 | Loss: 0.00785 | LR: 2.54e-06\n",
      "val loss: 0.0072\n",
      "Step 34600 | Loss: 0.00708 | LR: 2.30e-06\n",
      "Step 34625 | Loss: 0.00762 | LR: 2.09e-06\n",
      "Step 34650 | Loss: 0.00692 | LR: 1.88e-06\n",
      "Step 34675 | Loss: 0.00632 | LR: 1.68e-06\n",
      "val loss: 0.0071\n",
      "Step 34700 | Loss: 0.00737 | LR: 1.49e-06\n",
      "Step 34725 | Loss: 0.00754 | LR: 1.32e-06\n",
      "Step 34750 | Loss: 0.00785 | LR: 1.15e-06\n",
      "Step 34775 | Loss: 0.00707 | LR: 1.00e-06\n",
      "val loss: 0.0071\n",
      "Step 34800 | Loss: 0.00691 | LR: 8.58e-07\n",
      "Step 34825 | Loss: 0.00762 | LR: 7.26e-07\n",
      "Step 34850 | Loss: 0.00728 | LR: 6.06e-07\n",
      "Step 34875 | Loss: 0.00680 | LR: 4.98e-07\n",
      "val loss: 0.0074\n",
      "Step 34900 | Loss: 0.00912 | LR: 4.00e-07\n",
      "Step 34925 | Loss: 0.00694 | LR: 3.13e-07\n",
      "Step 34950 | Loss: 0.00669 | LR: 2.37e-07\n",
      "Step 34975 | Loss: 0.00706 | LR: 1.73e-07\n",
      "val loss: 0.0074\n",
      "Step 35000 | Loss: 0.00776 | LR: 1.19e-07\n",
      "Step 35025 | Loss: 0.00650 | LR: 7.71e-08\n",
      "Step 35050 | Loss: 0.00658 | LR: 4.58e-08\n",
      "Step 35075 | Loss: 0.00744 | LR: 2.57e-08\n",
      "val loss: 0.0070\n",
      "loading /Users/djemec/data/jepa/v0_2/pretraining/train/pt_shard_k562e_train_0000.npz\n",
      "Step 35100 | Loss: 0.00731 | LR: 1.66e-08\n",
      "val loss: 0.0071\n",
      "=== Step 35109 Done. Avg Loss: 0.00730 ===\n"
     ]
    }
   ],
   "source": [
    "for step in range(pt_max_steps):\n",
    "    last_step = (step == pt_max_steps - 1)\n",
    "\n",
    "    # once in a while evaluate our validation set loss\n",
    "    if step % 100 == 0 or last_step:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss_accum = 0.0\n",
    "            val_loss_steps = 10\n",
    "            for i in range(val_loss_steps):\n",
    "                x, x_tot = pt_val_loader.next_batch()\n",
    "                loss = model.forward_pretrain(x, x_tot)\n",
    "                loss = loss / val_loss_steps\n",
    "                val_loss_accum += loss.detach()\n",
    "\n",
    "        print(f'val loss: {val_loss_accum.item():.4f}')\n",
    "        # with open(log_file, \"a\") as f:\n",
    "        #    f.write(f'{step} test {test_loss_accum.item():.4f}\\n')\n",
    "\n",
    "\n",
    "    if step > 0 and (step+1) % steps_per_epoch ==0 and not last_step:\n",
    "        # Save Checkpoint\n",
    "        torch.save({\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'step': step\n",
    "        }, checkpoint_dir / f'bio_jepa_pt_ckpt_{step}.pt')\n",
    "\n",
    "\n",
    "    model.train()\n",
    "    x, x_tot = pt_train_loader.next_batch()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss = model.forward_pretrain(x, x_tot)\n",
    "    loss.backward()\n",
    "\n",
    "    # gradient clipping\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Update Teacher (V-JEPA Momentum)\n",
    "    model.update_teacher()\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    pt_lossi.append(loss.item())\n",
    "    total_epoch_loss += loss.item()\n",
    "    \n",
    "    if step % 25 == 0:\n",
    "        print(f\"Step {step} | Loss: {loss.item():.5f} | LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "    \n",
    "    \n",
    "    if step > 0 and (step+1) % steps_per_epoch == 0:   \n",
    "        avg_loss = total_epoch_loss / steps_per_epoch\n",
    "        print(f\"=== Step {step} Done. Avg Loss: {avg_loss:.5f} ===\")\n",
    "        total_epoch_loss = 0\n",
    "    \n",
    "    \n",
    "    if last_step:\n",
    "        # Save Checkpoint\n",
    "        torch.save({\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'step': step\n",
    "        }, checkpoint_dir / f'bio_jepa_pt_ckpt_{step}_final.pt')\n",
    "\n",
    "    step += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a2cd37f-48ce-400d-ab25-f04ebfe8546b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pt_lossi[:])\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42d5473-c3a2-411e-bcb8-dcf59b65ef8a",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cc07f0-eb9e-4e6a-8784-0e02e5eb0faf",
   "metadata": {},
   "source": [
    "#### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c02da536-0088-4376-8d89-37839eebb8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 5 shards for split train\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0002.npz\n",
      "found 1 shards for split val\n",
      "loading /Users/djemec/data/jepa/v0_2/training/val/shard_k562e_val_0000.npz\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoaderLite(B=BATCH_SIZE, split='train', device=DEVICE)\n",
    "val_loader = DataLoaderLite(B=BATCH_SIZE, split='val', device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0202749-f533-40fd-a891-08f9c9ff8edb",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ff27eb-93fa-439c-a31d-366dff143d7d",
   "metadata": {},
   "source": [
    "#### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "737e4f9e-1eb0-49de-aea9-40dbd2b38025",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8ff8ba83-7ff9-4a99-92c7-1fec94759781",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57196ead-30ec-4098-a645-9ec7bdf3a300",
   "metadata": {},
   "source": [
    "#### Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b66ffc0d-d4f0-4f3d-af4c-390acfd7d5ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3177, 31770)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch = 101682 // BATCH_SIZE\n",
    "max_steps = EPOCHS * steps_per_epoch\n",
    "steps_per_epoch, max_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e95819c7-d216-4a1b-93cc-467a20311ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=LR, total_steps=max_steps, pct_start=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89952bd9-d632-44db-ae1e-0063389ff31f",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "62bc4b56-b682-4b56-922e-ef040ab8d29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossi = []\n",
    "step = 0\n",
    "total_epoch_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "96cee680-aa5a-464f-99ef-27b5c758d21c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.7025\n",
      "Step 0 | Loss: 0.70124 | LR: 4.00e-05\n",
      "Step 25 | Loss: 0.52954 | LR: 4.06e-05\n",
      "Step 50 | Loss: 0.41019 | LR: 4.24e-05\n",
      "Step 75 | Loss: 0.26506 | LR: 4.54e-05\n",
      "test loss: 0.0527\n",
      "Step 100 | Loss: 0.05434 | LR: 4.96e-05\n",
      "Step 125 | Loss: 0.03840 | LR: 5.48e-05\n",
      "Step 150 | Loss: 0.03480 | LR: 6.13e-05\n",
      "Step 175 | Loss: 0.02560 | LR: 6.88e-05\n",
      "test loss: 0.0252\n",
      "Step 200 | Loss: 0.02700 | LR: 7.75e-05\n",
      "Step 225 | Loss: 0.02412 | LR: 8.72e-05\n",
      "Step 250 | Loss: 0.02460 | LR: 9.80e-05\n",
      "Step 275 | Loss: 0.02229 | LR: 1.10e-04\n",
      "test loss: 0.0215\n",
      "Step 300 | Loss: 0.02579 | LR: 1.23e-04\n",
      "Step 325 | Loss: 0.02280 | LR: 1.36e-04\n",
      "Step 350 | Loss: 0.02128 | LR: 1.51e-04\n",
      "Step 375 | Loss: 0.02266 | LR: 1.67e-04\n",
      "test loss: 0.0191\n",
      "Step 400 | Loss: 0.01852 | LR: 1.83e-04\n",
      "Step 425 | Loss: 0.02106 | LR: 2.01e-04\n",
      "Step 450 | Loss: 0.01881 | LR: 2.19e-04\n",
      "Step 475 | Loss: 0.01787 | LR: 2.38e-04\n",
      "test loss: 0.0191\n",
      "Step 500 | Loss: 0.01853 | LR: 2.57e-04\n",
      "Step 525 | Loss: 0.01869 | LR: 2.77e-04\n",
      "Step 550 | Loss: 0.02104 | LR: 2.98e-04\n",
      "Step 575 | Loss: 0.02273 | LR: 3.20e-04\n",
      "test loss: 0.0231\n",
      "Step 600 | Loss: 0.02163 | LR: 3.41e-04\n",
      "Step 625 | Loss: 0.01767 | LR: 3.64e-04\n",
      "Step 650 | Loss: 0.01793 | LR: 3.86e-04\n",
      "Step 675 | Loss: 0.01729 | LR: 4.09e-04\n",
      "test loss: 0.0184\n",
      "Step 700 | Loss: 0.01612 | LR: 4.32e-04\n",
      "Step 725 | Loss: 0.01678 | LR: 4.56e-04\n",
      "Step 750 | Loss: 0.01875 | LR: 4.79e-04\n",
      "Step 775 | Loss: 0.02406 | LR: 5.03e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0003.npz\n",
      "test loss: 0.0201\n",
      "Step 800 | Loss: 0.02120 | LR: 5.27e-04\n",
      "Step 825 | Loss: 0.01896 | LR: 5.51e-04\n",
      "Step 850 | Loss: 0.01908 | LR: 5.74e-04\n",
      "Step 875 | Loss: 0.01850 | LR: 5.98e-04\n",
      "test loss: 0.0202\n",
      "Step 900 | Loss: 0.02204 | LR: 6.21e-04\n",
      "Step 925 | Loss: 0.03867 | LR: 6.44e-04\n",
      "Step 950 | Loss: 0.02165 | LR: 6.67e-04\n",
      "Step 975 | Loss: 0.02067 | LR: 6.89e-04\n",
      "test loss: 0.0209\n",
      "Step 1000 | Loss: 0.01804 | LR: 7.11e-04\n",
      "Step 1025 | Loss: 0.01684 | LR: 7.33e-04\n",
      "Step 1050 | Loss: 0.01931 | LR: 7.54e-04\n",
      "Step 1075 | Loss: 0.02532 | LR: 7.74e-04\n",
      "test loss: 0.0265\n",
      "Step 1100 | Loss: 0.02933 | LR: 7.94e-04\n",
      "Step 1125 | Loss: 0.01705 | LR: 8.13e-04\n",
      "Step 1150 | Loss: 0.01952 | LR: 8.32e-04\n",
      "Step 1175 | Loss: 0.02235 | LR: 8.49e-04\n",
      "test loss: 0.0184\n",
      "Step 1200 | Loss: 0.02180 | LR: 8.66e-04\n",
      "Step 1225 | Loss: 0.02140 | LR: 8.82e-04\n",
      "Step 1250 | Loss: 0.01632 | LR: 8.97e-04\n",
      "Step 1275 | Loss: 0.01967 | LR: 9.12e-04\n",
      "test loss: 0.0195\n",
      "Step 1300 | Loss: 0.01894 | LR: 9.25e-04\n",
      "Step 1325 | Loss: 0.02308 | LR: 9.37e-04\n",
      "Step 1350 | Loss: 0.01980 | LR: 9.48e-04\n",
      "Step 1375 | Loss: 0.01981 | LR: 9.59e-04\n",
      "test loss: 0.0190\n",
      "Step 1400 | Loss: 0.02087 | LR: 9.68e-04\n",
      "Step 1425 | Loss: 0.02041 | LR: 9.76e-04\n",
      "Step 1450 | Loss: 0.02125 | LR: 9.83e-04\n",
      "Step 1475 | Loss: 0.01529 | LR: 9.88e-04\n",
      "test loss: 0.0156\n",
      "Step 1500 | Loss: 0.01666 | LR: 9.93e-04\n",
      "Step 1525 | Loss: 0.01591 | LR: 9.96e-04\n",
      "Step 1550 | Loss: 0.01525 | LR: 9.99e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0001.npz\n",
      "Step 1575 | Loss: 0.01267 | LR: 1.00e-03\n",
      "test loss: 0.0159\n",
      "Step 1600 | Loss: 0.01777 | LR: 1.00e-03\n",
      "Step 1625 | Loss: 0.01292 | LR: 1.00e-03\n",
      "Step 1650 | Loss: 0.01600 | LR: 1.00e-03\n",
      "Step 1675 | Loss: 0.01346 | LR: 1.00e-03\n",
      "test loss: 0.0150\n",
      "Step 1700 | Loss: 0.01517 | LR: 1.00e-03\n",
      "Step 1725 | Loss: 0.01399 | LR: 1.00e-03\n",
      "Step 1750 | Loss: 0.01363 | LR: 1.00e-03\n",
      "Step 1775 | Loss: 0.01440 | LR: 1.00e-03\n",
      "test loss: 0.0119\n",
      "Step 1800 | Loss: 0.01159 | LR: 1.00e-03\n",
      "Step 1825 | Loss: 0.01088 | LR: 1.00e-03\n",
      "Step 1850 | Loss: 0.01205 | LR: 1.00e-03\n",
      "Step 1875 | Loss: 0.01227 | LR: 1.00e-03\n",
      "test loss: 0.0127\n",
      "Step 1900 | Loss: 0.01284 | LR: 1.00e-03\n",
      "Step 1925 | Loss: 0.01053 | LR: 1.00e-03\n",
      "Step 1950 | Loss: 0.01156 | LR: 1.00e-03\n",
      "Step 1975 | Loss: 0.01134 | LR: 1.00e-03\n",
      "test loss: 0.0102\n",
      "Step 2000 | Loss: 0.01073 | LR: 1.00e-03\n",
      "Step 2025 | Loss: 0.01064 | LR: 9.99e-04\n",
      "Step 2050 | Loss: 0.01045 | LR: 9.99e-04\n",
      "Step 2075 | Loss: 0.00968 | LR: 9.99e-04\n",
      "test loss: 0.0096\n",
      "Step 2100 | Loss: 0.00939 | LR: 9.99e-04\n",
      "Step 2125 | Loss: 0.00919 | LR: 9.99e-04\n",
      "Step 2150 | Loss: 0.00831 | LR: 9.99e-04\n",
      "Step 2175 | Loss: 0.00913 | LR: 9.99e-04\n",
      "test loss: 0.0106\n",
      "Step 2200 | Loss: 0.01106 | LR: 9.99e-04\n",
      "Step 2225 | Loss: 0.01132 | LR: 9.99e-04\n",
      "Step 2250 | Loss: 0.01023 | LR: 9.99e-04\n",
      "Step 2275 | Loss: 0.00937 | LR: 9.99e-04\n",
      "test loss: 0.0091\n",
      "Step 2300 | Loss: 0.00951 | LR: 9.99e-04\n",
      "Step 2325 | Loss: 0.00929 | LR: 9.99e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0000.npz\n",
      "Step 2350 | Loss: 0.00786 | LR: 9.98e-04\n",
      "Step 2375 | Loss: 0.00783 | LR: 9.98e-04\n",
      "test loss: 0.0079\n",
      "Step 2400 | Loss: 0.00800 | LR: 9.98e-04\n",
      "Step 2425 | Loss: 0.00848 | LR: 9.98e-04\n",
      "Step 2450 | Loss: 0.01018 | LR: 9.98e-04\n",
      "Step 2475 | Loss: 0.00776 | LR: 9.98e-04\n",
      "test loss: 0.0091\n",
      "Step 2500 | Loss: 0.00888 | LR: 9.98e-04\n",
      "Step 2525 | Loss: 0.00893 | LR: 9.98e-04\n",
      "Step 2550 | Loss: 0.00923 | LR: 9.97e-04\n",
      "Step 2575 | Loss: 0.00822 | LR: 9.97e-04\n",
      "test loss: 0.0089\n",
      "Step 2600 | Loss: 0.00925 | LR: 9.97e-04\n",
      "Step 2625 | Loss: 0.00935 | LR: 9.97e-04\n",
      "Step 2650 | Loss: 0.00932 | LR: 9.97e-04\n",
      "Step 2675 | Loss: 0.00806 | LR: 9.97e-04\n",
      "test loss: 0.0077\n",
      "Step 2700 | Loss: 0.00860 | LR: 9.97e-04\n",
      "Step 2725 | Loss: 0.00764 | LR: 9.96e-04\n",
      "Step 2750 | Loss: 0.00896 | LR: 9.96e-04\n",
      "Step 2775 | Loss: 0.00804 | LR: 9.96e-04\n",
      "test loss: 0.0074\n",
      "Step 2800 | Loss: 0.00731 | LR: 9.96e-04\n",
      "Step 2825 | Loss: 0.00992 | LR: 9.96e-04\n",
      "Step 2850 | Loss: 0.00726 | LR: 9.96e-04\n",
      "Step 2875 | Loss: 0.00600 | LR: 9.96e-04\n",
      "test loss: 0.0069\n",
      "Step 2900 | Loss: 0.00641 | LR: 9.95e-04\n",
      "Step 2925 | Loss: 0.00715 | LR: 9.95e-04\n",
      "Step 2950 | Loss: 0.00790 | LR: 9.95e-04\n",
      "Step 2975 | Loss: 0.00609 | LR: 9.95e-04\n",
      "test loss: 0.0089\n",
      "Step 3000 | Loss: 0.00904 | LR: 9.95e-04\n",
      "Step 3025 | Loss: 0.00666 | LR: 9.94e-04\n",
      "Step 3050 | Loss: 0.00670 | LR: 9.94e-04\n",
      "Step 3075 | Loss: 0.00751 | LR: 9.94e-04\n",
      "test loss: 0.0085\n",
      "Step 3100 | Loss: 0.00789 | LR: 9.94e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0004.npz\n",
      "Step 3125 | Loss: 0.00737 | LR: 9.94e-04\n",
      "Step 3150 | Loss: 0.00826 | LR: 9.93e-04\n",
      "Step 3175 | Loss: 0.00700 | LR: 9.93e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0000.npz\n",
      "=== Step 3176 Done. Avg Loss: 0.02710 ===\n",
      "test loss: 0.0057\n",
      "Step 3200 | Loss: 0.00601 | LR: 9.93e-04\n",
      "Step 3225 | Loss: 0.00638 | LR: 9.93e-04\n",
      "Step 3250 | Loss: 0.00554 | LR: 9.93e-04\n",
      "Step 3275 | Loss: 0.00646 | LR: 9.92e-04\n",
      "test loss: 0.0073\n",
      "Step 3300 | Loss: 0.00716 | LR: 9.92e-04\n",
      "Step 3325 | Loss: 0.00674 | LR: 9.92e-04\n",
      "Step 3350 | Loss: 0.00567 | LR: 9.92e-04\n",
      "Step 3375 | Loss: 0.00632 | LR: 9.91e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/training/val/shard_k562e_val_0000.npz\n",
      "test loss: 0.0078\n",
      "Step 3400 | Loss: 0.00769 | LR: 9.91e-04\n",
      "Step 3425 | Loss: 0.00616 | LR: 9.91e-04\n",
      "Step 3450 | Loss: 0.00713 | LR: 9.91e-04\n",
      "Step 3475 | Loss: 0.00644 | LR: 9.90e-04\n",
      "test loss: 0.0053\n",
      "Step 3500 | Loss: 0.00548 | LR: 9.90e-04\n",
      "Step 3525 | Loss: 0.00805 | LR: 9.90e-04\n",
      "Step 3550 | Loss: 0.00568 | LR: 9.90e-04\n",
      "Step 3575 | Loss: 0.00703 | LR: 9.89e-04\n",
      "test loss: 0.0071\n",
      "Step 3600 | Loss: 0.00663 | LR: 9.89e-04\n",
      "Step 3625 | Loss: 0.00610 | LR: 9.89e-04\n",
      "Step 3650 | Loss: 0.00840 | LR: 9.89e-04\n",
      "Step 3675 | Loss: 0.00663 | LR: 9.88e-04\n",
      "test loss: 0.0054\n",
      "Step 3700 | Loss: 0.00598 | LR: 9.88e-04\n",
      "Step 3725 | Loss: 0.00705 | LR: 9.88e-04\n",
      "Step 3750 | Loss: 0.00667 | LR: 9.87e-04\n",
      "Step 3775 | Loss: 0.00786 | LR: 9.87e-04\n",
      "test loss: 0.0072\n",
      "Step 3800 | Loss: 0.00675 | LR: 9.87e-04\n",
      "Step 3825 | Loss: 0.00660 | LR: 9.86e-04\n",
      "Step 3850 | Loss: 0.00613 | LR: 9.86e-04\n",
      "Step 3875 | Loss: 0.00523 | LR: 9.86e-04\n",
      "test loss: 0.0051\n",
      "Step 3900 | Loss: 0.00544 | LR: 9.86e-04\n",
      "Step 3925 | Loss: 0.00576 | LR: 9.85e-04\n",
      "Step 3950 | Loss: 0.00476 | LR: 9.85e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0004.npz\n",
      "Step 3975 | Loss: 0.00645 | LR: 9.85e-04\n",
      "test loss: 0.0051\n",
      "Step 4000 | Loss: 0.00512 | LR: 9.84e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0003.npz\n",
      "Step 4025 | Loss: 0.00440 | LR: 9.84e-04\n",
      "Step 4050 | Loss: 0.00590 | LR: 9.84e-04\n",
      "Step 4075 | Loss: 0.00643 | LR: 9.83e-04\n",
      "test loss: 0.0059\n",
      "Step 4100 | Loss: 0.00628 | LR: 9.83e-04\n",
      "Step 4125 | Loss: 0.00599 | LR: 9.83e-04\n",
      "Step 4150 | Loss: 0.00667 | LR: 9.82e-04\n",
      "Step 4175 | Loss: 0.00600 | LR: 9.82e-04\n",
      "test loss: 0.0072\n",
      "Step 4200 | Loss: 0.00761 | LR: 9.82e-04\n",
      "Step 4225 | Loss: 0.00545 | LR: 9.81e-04\n",
      "Step 4250 | Loss: 0.00535 | LR: 9.81e-04\n",
      "Step 4275 | Loss: 0.00627 | LR: 9.81e-04\n",
      "test loss: 0.0056\n",
      "Step 4300 | Loss: 0.00557 | LR: 9.80e-04\n",
      "Step 4325 | Loss: 0.00994 | LR: 9.80e-04\n",
      "Step 4350 | Loss: 0.00749 | LR: 9.79e-04\n",
      "Step 4375 | Loss: 0.00569 | LR: 9.79e-04\n",
      "test loss: 0.0047\n",
      "Step 4400 | Loss: 0.00506 | LR: 9.79e-04\n",
      "Step 4425 | Loss: 0.00658 | LR: 9.78e-04\n",
      "Step 4450 | Loss: 0.00569 | LR: 9.78e-04\n",
      "Step 4475 | Loss: 0.00663 | LR: 9.78e-04\n",
      "test loss: 0.0054\n",
      "Step 4500 | Loss: 0.00556 | LR: 9.77e-04\n",
      "Step 4525 | Loss: 0.00511 | LR: 9.77e-04\n",
      "Step 4550 | Loss: 0.00437 | LR: 9.76e-04\n",
      "Step 4575 | Loss: 0.00502 | LR: 9.76e-04\n",
      "test loss: 0.0052\n",
      "Step 4600 | Loss: 0.00548 | LR: 9.76e-04\n",
      "Step 4625 | Loss: 0.00603 | LR: 9.75e-04\n",
      "Step 4650 | Loss: 0.00737 | LR: 9.75e-04\n",
      "Step 4675 | Loss: 0.00742 | LR: 9.74e-04\n",
      "test loss: 0.0044\n",
      "Step 4700 | Loss: 0.00432 | LR: 9.74e-04\n",
      "Step 4725 | Loss: 0.00835 | LR: 9.74e-04\n",
      "Step 4750 | Loss: 0.00597 | LR: 9.73e-04\n",
      "Step 4775 | Loss: 0.00704 | LR: 9.73e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0002.npz\n",
      "test loss: 0.0062\n",
      "Step 4800 | Loss: 0.00613 | LR: 9.72e-04\n",
      "Step 4825 | Loss: 0.00493 | LR: 9.72e-04\n",
      "Step 4850 | Loss: 0.00555 | LR: 9.71e-04\n",
      "Step 4875 | Loss: 0.00610 | LR: 9.71e-04\n",
      "test loss: 0.0055\n",
      "Step 4900 | Loss: 0.00589 | LR: 9.71e-04\n",
      "Step 4925 | Loss: 0.00688 | LR: 9.70e-04\n",
      "Step 4950 | Loss: 0.00543 | LR: 9.70e-04\n",
      "Step 4975 | Loss: 0.00625 | LR: 9.69e-04\n",
      "test loss: 0.0079\n",
      "Step 5000 | Loss: 0.00775 | LR: 9.69e-04\n",
      "Step 5025 | Loss: 0.00513 | LR: 9.68e-04\n",
      "Step 5050 | Loss: 0.00430 | LR: 9.68e-04\n",
      "Step 5075 | Loss: 0.00425 | LR: 9.67e-04\n",
      "test loss: 0.0068\n",
      "Step 5100 | Loss: 0.00690 | LR: 9.67e-04\n",
      "Step 5125 | Loss: 0.00591 | LR: 9.66e-04\n",
      "Step 5150 | Loss: 0.00586 | LR: 9.66e-04\n",
      "Step 5175 | Loss: 0.00491 | LR: 9.66e-04\n",
      "test loss: 0.0055\n",
      "Step 5200 | Loss: 0.00562 | LR: 9.65e-04\n",
      "Step 5225 | Loss: 0.00564 | LR: 9.65e-04\n",
      "Step 5250 | Loss: 0.00566 | LR: 9.64e-04\n",
      "Step 5275 | Loss: 0.00459 | LR: 9.64e-04\n",
      "test loss: 0.0053\n",
      "Step 5300 | Loss: 0.00530 | LR: 9.63e-04\n",
      "Step 5325 | Loss: 0.00428 | LR: 9.63e-04\n",
      "Step 5350 | Loss: 0.00643 | LR: 9.62e-04\n",
      "Step 5375 | Loss: 0.00843 | LR: 9.62e-04\n",
      "test loss: 0.0068\n",
      "Step 5400 | Loss: 0.00676 | LR: 9.61e-04\n",
      "Step 5425 | Loss: 0.00689 | LR: 9.61e-04\n",
      "Step 5450 | Loss: 0.00441 | LR: 9.60e-04\n",
      "Step 5475 | Loss: 0.00521 | LR: 9.60e-04\n",
      "test loss: 0.0058\n",
      "Step 5500 | Loss: 0.00562 | LR: 9.59e-04\n",
      "Step 5525 | Loss: 0.00564 | LR: 9.59e-04\n",
      "Step 5550 | Loss: 0.00828 | LR: 9.58e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0001.npz\n",
      "Step 5575 | Loss: 0.00563 | LR: 9.58e-04\n",
      "test loss: 0.0039\n",
      "Step 5600 | Loss: 0.00392 | LR: 9.57e-04\n",
      "Step 5625 | Loss: 0.00724 | LR: 9.56e-04\n",
      "Step 5650 | Loss: 0.00627 | LR: 9.56e-04\n",
      "Step 5675 | Loss: 0.00448 | LR: 9.55e-04\n",
      "test loss: 0.0045\n",
      "Step 5700 | Loss: 0.00438 | LR: 9.55e-04\n",
      "Step 5725 | Loss: 0.00603 | LR: 9.54e-04\n",
      "Step 5750 | Loss: 0.00386 | LR: 9.54e-04\n",
      "Step 5775 | Loss: 0.00499 | LR: 9.53e-04\n",
      "test loss: 0.0043\n",
      "Step 5800 | Loss: 0.00422 | LR: 9.53e-04\n",
      "Step 5825 | Loss: 0.00592 | LR: 9.52e-04\n",
      "Step 5850 | Loss: 0.00522 | LR: 9.52e-04\n",
      "Step 5875 | Loss: 0.00593 | LR: 9.51e-04\n",
      "test loss: 0.0073\n",
      "Step 5900 | Loss: 0.00736 | LR: 9.50e-04\n",
      "Step 5925 | Loss: 0.00462 | LR: 9.50e-04\n",
      "Step 5950 | Loss: 0.00555 | LR: 9.49e-04\n",
      "Step 5975 | Loss: 0.00521 | LR: 9.49e-04\n",
      "test loss: 0.0050\n",
      "Step 6000 | Loss: 0.00495 | LR: 9.48e-04\n",
      "Step 6025 | Loss: 0.00412 | LR: 9.48e-04\n",
      "Step 6050 | Loss: 0.00480 | LR: 9.47e-04\n",
      "Step 6075 | Loss: 0.00720 | LR: 9.46e-04\n",
      "test loss: 0.0049\n",
      "Step 6100 | Loss: 0.00498 | LR: 9.46e-04\n",
      "Step 6125 | Loss: 0.00485 | LR: 9.45e-04\n",
      "Step 6150 | Loss: 0.00365 | LR: 9.45e-04\n",
      "Step 6175 | Loss: 0.00524 | LR: 9.44e-04\n",
      "test loss: 0.0048\n",
      "Step 6200 | Loss: 0.00499 | LR: 9.43e-04\n",
      "Step 6225 | Loss: 0.00448 | LR: 9.43e-04\n",
      "Step 6250 | Loss: 0.00566 | LR: 9.42e-04\n",
      "Step 6275 | Loss: 0.00407 | LR: 9.42e-04\n",
      "test loss: 0.0056\n",
      "Step 6300 | Loss: 0.00569 | LR: 9.41e-04\n",
      "Step 6325 | Loss: 0.00493 | LR: 9.40e-04\n",
      "Step 6350 | Loss: 0.00472 | LR: 9.40e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0001.npz\n",
      "=== Step 6353 Done. Avg Loss: 0.00580 ===\n",
      "Step 6375 | Loss: 0.00500 | LR: 9.39e-04\n",
      "test loss: 0.0040\n",
      "Step 6400 | Loss: 0.00401 | LR: 9.39e-04\n",
      "Step 6425 | Loss: 0.00426 | LR: 9.38e-04\n",
      "Step 6450 | Loss: 0.00461 | LR: 9.37e-04\n",
      "Step 6475 | Loss: 0.00453 | LR: 9.37e-04\n",
      "test loss: 0.0065\n",
      "Step 6500 | Loss: 0.00637 | LR: 9.36e-04\n",
      "Step 6525 | Loss: 0.00402 | LR: 9.35e-04\n",
      "Step 6550 | Loss: 0.00381 | LR: 9.35e-04\n",
      "Step 6575 | Loss: 0.00429 | LR: 9.34e-04\n",
      "test loss: 0.0041\n",
      "Step 6600 | Loss: 0.00409 | LR: 9.33e-04\n",
      "Step 6625 | Loss: 0.00743 | LR: 9.33e-04\n",
      "Step 6650 | Loss: 0.00428 | LR: 9.32e-04\n",
      "Step 6675 | Loss: 0.00542 | LR: 9.31e-04\n",
      "test loss: 0.0060\n",
      "Step 6700 | Loss: 0.00601 | LR: 9.31e-04\n",
      "Step 6725 | Loss: 0.00443 | LR: 9.30e-04\n",
      "Step 6750 | Loss: 0.00546 | LR: 9.30e-04\n",
      "Step 6775 | Loss: 0.00593 | LR: 9.29e-04\n",
      "test loss: 0.0045\n",
      "Step 6800 | Loss: 0.00476 | LR: 9.28e-04\n",
      "Step 6825 | Loss: 0.00418 | LR: 9.27e-04\n",
      "Step 6850 | Loss: 0.00403 | LR: 9.27e-04\n",
      "Step 6875 | Loss: 0.00580 | LR: 9.26e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/training/val/shard_k562e_val_0000.npz\n",
      "test loss: 0.0069\n",
      "Step 6900 | Loss: 0.00702 | LR: 9.25e-04\n",
      "Step 6925 | Loss: 0.00427 | LR: 9.25e-04\n",
      "Step 6950 | Loss: 0.00478 | LR: 9.24e-04\n",
      "Step 6975 | Loss: 0.00623 | LR: 9.23e-04\n",
      "test loss: 0.0049\n",
      "Step 7000 | Loss: 0.00468 | LR: 9.23e-04\n",
      "Step 7025 | Loss: 0.00444 | LR: 9.22e-04\n",
      "Step 7050 | Loss: 0.00540 | LR: 9.21e-04\n",
      "Step 7075 | Loss: 0.00595 | LR: 9.21e-04\n",
      "test loss: 0.0050\n",
      "Step 7100 | Loss: 0.00508 | LR: 9.20e-04\n",
      "Step 7125 | Loss: 0.00493 | LR: 9.19e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0003.npz\n",
      "Step 7150 | Loss: 0.00505 | LR: 9.18e-04\n",
      "Step 7175 | Loss: 0.00582 | LR: 9.18e-04\n",
      "test loss: 0.0037\n",
      "Step 7200 | Loss: 0.00347 | LR: 9.17e-04\n",
      "Step 7225 | Loss: 0.00455 | LR: 9.16e-04\n",
      "Step 7250 | Loss: 0.00558 | LR: 9.16e-04\n",
      "Step 7275 | Loss: 0.00420 | LR: 9.15e-04\n",
      "test loss: 0.0041\n",
      "Step 7300 | Loss: 0.00388 | LR: 9.14e-04\n",
      "Step 7325 | Loss: 0.00331 | LR: 9.13e-04\n",
      "Step 7350 | Loss: 0.00426 | LR: 9.13e-04\n",
      "Step 7375 | Loss: 0.00464 | LR: 9.12e-04\n",
      "test loss: 0.0043\n",
      "Step 7400 | Loss: 0.00429 | LR: 9.11e-04\n",
      "Step 7425 | Loss: 0.00423 | LR: 9.10e-04\n",
      "Step 7450 | Loss: 0.00409 | LR: 9.10e-04\n",
      "Step 7475 | Loss: 0.00490 | LR: 9.09e-04\n",
      "test loss: 0.0042\n",
      "Step 7500 | Loss: 0.00439 | LR: 9.08e-04\n",
      "Step 7525 | Loss: 0.00338 | LR: 9.07e-04\n",
      "Step 7550 | Loss: 0.00331 | LR: 9.07e-04\n",
      "Step 7575 | Loss: 0.00424 | LR: 9.06e-04\n",
      "test loss: 0.0044\n",
      "Step 7600 | Loss: 0.00452 | LR: 9.05e-04\n",
      "Step 7625 | Loss: 0.00519 | LR: 9.04e-04\n",
      "Step 7650 | Loss: 0.00337 | LR: 9.04e-04\n",
      "Step 7675 | Loss: 0.00337 | LR: 9.03e-04\n",
      "test loss: 0.0038\n",
      "Step 7700 | Loss: 0.00353 | LR: 9.02e-04\n",
      "Step 7725 | Loss: 0.00511 | LR: 9.01e-04\n",
      "Step 7750 | Loss: 0.00597 | LR: 9.01e-04\n",
      "Step 7775 | Loss: 0.00375 | LR: 9.00e-04\n",
      "test loss: 0.0039\n",
      "Step 7800 | Loss: 0.00390 | LR: 8.99e-04\n",
      "Step 7825 | Loss: 0.00509 | LR: 8.98e-04\n",
      "Step 7850 | Loss: 0.00421 | LR: 8.97e-04\n",
      "Step 7875 | Loss: 0.00389 | LR: 8.97e-04\n",
      "test loss: 0.0053\n",
      "Step 7900 | Loss: 0.00525 | LR: 8.96e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0000.npz\n",
      "Step 7925 | Loss: 0.00501 | LR: 8.95e-04\n",
      "Step 7950 | Loss: 0.00660 | LR: 8.94e-04\n",
      "Step 7975 | Loss: 0.00483 | LR: 8.93e-04\n",
      "test loss: 0.0029\n",
      "Step 8000 | Loss: 0.00330 | LR: 8.93e-04\n",
      "Step 8025 | Loss: 0.00405 | LR: 8.92e-04\n",
      "Step 8050 | Loss: 0.00511 | LR: 8.91e-04\n",
      "Step 8075 | Loss: 0.00727 | LR: 8.90e-04\n",
      "test loss: 0.0037\n",
      "Step 8100 | Loss: 0.00366 | LR: 8.89e-04\n",
      "Step 8125 | Loss: 0.00436 | LR: 8.89e-04\n",
      "Step 8150 | Loss: 0.00381 | LR: 8.88e-04\n",
      "Step 8175 | Loss: 0.00532 | LR: 8.87e-04\n",
      "test loss: 0.0043\n",
      "Step 8200 | Loss: 0.00423 | LR: 8.86e-04\n",
      "Step 8225 | Loss: 0.00342 | LR: 8.85e-04\n",
      "Step 8250 | Loss: 0.00502 | LR: 8.84e-04\n",
      "Step 8275 | Loss: 0.00424 | LR: 8.84e-04\n",
      "test loss: 0.0031\n",
      "Step 8300 | Loss: 0.00310 | LR: 8.83e-04\n",
      "Step 8325 | Loss: 0.00493 | LR: 8.82e-04\n",
      "Step 8350 | Loss: 0.00450 | LR: 8.81e-04\n",
      "Step 8375 | Loss: 0.00362 | LR: 8.80e-04\n",
      "test loss: 0.0029\n",
      "Step 8400 | Loss: 0.00316 | LR: 8.79e-04\n",
      "Step 8425 | Loss: 0.00466 | LR: 8.79e-04\n",
      "Step 8450 | Loss: 0.00343 | LR: 8.78e-04\n",
      "Step 8475 | Loss: 0.00357 | LR: 8.77e-04\n",
      "test loss: 0.0031\n",
      "Step 8500 | Loss: 0.00315 | LR: 8.76e-04\n",
      "Step 8525 | Loss: 0.00366 | LR: 8.75e-04\n",
      "Step 8550 | Loss: 0.00329 | LR: 8.74e-04\n",
      "Step 8575 | Loss: 0.00536 | LR: 8.73e-04\n",
      "test loss: 0.0053\n",
      "Step 8600 | Loss: 0.00523 | LR: 8.73e-04\n",
      "Step 8625 | Loss: 0.00646 | LR: 8.72e-04\n",
      "Step 8650 | Loss: 0.00400 | LR: 8.71e-04\n",
      "Step 8675 | Loss: 0.00669 | LR: 8.70e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0002.npz\n",
      "test loss: 0.0048\n",
      "Step 8700 | Loss: 0.00501 | LR: 8.69e-04\n",
      "Step 8725 | Loss: 0.00437 | LR: 8.68e-04\n",
      "Step 8750 | Loss: 0.00714 | LR: 8.67e-04\n",
      "Step 8775 | Loss: 0.00366 | LR: 8.66e-04\n",
      "test loss: 0.0030\n",
      "Step 8800 | Loss: 0.00306 | LR: 8.66e-04\n",
      "Step 8825 | Loss: 0.00317 | LR: 8.65e-04\n",
      "Step 8850 | Loss: 0.00438 | LR: 8.64e-04\n",
      "Step 8875 | Loss: 0.00537 | LR: 8.63e-04\n",
      "test loss: 0.0039\n",
      "Step 8900 | Loss: 0.00401 | LR: 8.62e-04\n",
      "Step 8925 | Loss: 0.00380 | LR: 8.61e-04\n",
      "Step 8950 | Loss: 0.00383 | LR: 8.60e-04\n",
      "Step 8975 | Loss: 0.00531 | LR: 8.59e-04\n",
      "test loss: 0.0045\n",
      "Step 9000 | Loss: 0.00453 | LR: 8.58e-04\n",
      "Step 9025 | Loss: 0.00570 | LR: 8.57e-04\n",
      "Step 9050 | Loss: 0.00452 | LR: 8.57e-04\n",
      "Step 9075 | Loss: 0.00405 | LR: 8.56e-04\n",
      "test loss: 0.0054\n",
      "Step 9100 | Loss: 0.00515 | LR: 8.55e-04\n",
      "Step 9125 | Loss: 0.00443 | LR: 8.54e-04\n",
      "Step 9150 | Loss: 0.00458 | LR: 8.53e-04\n",
      "Step 9175 | Loss: 0.00444 | LR: 8.52e-04\n",
      "test loss: 0.0047\n",
      "Step 9200 | Loss: 0.00453 | LR: 8.51e-04\n",
      "Step 9225 | Loss: 0.00707 | LR: 8.50e-04\n",
      "Step 9250 | Loss: 0.00326 | LR: 8.49e-04\n",
      "Step 9275 | Loss: 0.00466 | LR: 8.48e-04\n",
      "test loss: 0.0047\n",
      "Step 9300 | Loss: 0.00477 | LR: 8.47e-04\n",
      "Step 9325 | Loss: 0.00384 | LR: 8.46e-04\n",
      "Step 9350 | Loss: 0.00289 | LR: 8.45e-04\n",
      "Step 9375 | Loss: 0.00472 | LR: 8.44e-04\n",
      "test loss: 0.0036\n",
      "Step 9400 | Loss: 0.00368 | LR: 8.44e-04\n",
      "Step 9425 | Loss: 0.00399 | LR: 8.43e-04\n",
      "Step 9450 | Loss: 0.00509 | LR: 8.42e-04\n",
      "Step 9475 | Loss: 0.00277 | LR: 8.41e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0004.npz\n",
      "test loss: 0.0068\n",
      "Step 9500 | Loss: 0.00706 | LR: 8.40e-04\n",
      "Step 9525 | Loss: 0.00309 | LR: 8.39e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0002.npz\n",
      "=== Step 9530 Done. Avg Loss: 0.00455 ===\n",
      "Step 9550 | Loss: 0.00287 | LR: 8.38e-04\n",
      "Step 9575 | Loss: 0.00356 | LR: 8.37e-04\n",
      "test loss: 0.0050\n",
      "Step 9600 | Loss: 0.00487 | LR: 8.36e-04\n",
      "Step 9625 | Loss: 0.00323 | LR: 8.35e-04\n",
      "Step 9650 | Loss: 0.00464 | LR: 8.34e-04\n",
      "Step 9675 | Loss: 0.00607 | LR: 8.33e-04\n",
      "test loss: 0.0032\n",
      "Step 9700 | Loss: 0.00318 | LR: 8.32e-04\n",
      "Step 9725 | Loss: 0.00443 | LR: 8.31e-04\n",
      "Step 9750 | Loss: 0.00506 | LR: 8.30e-04\n",
      "Step 9775 | Loss: 0.00318 | LR: 8.29e-04\n",
      "test loss: 0.0034\n",
      "Step 9800 | Loss: 0.00327 | LR: 8.28e-04\n",
      "Step 9825 | Loss: 0.00429 | LR: 8.27e-04\n",
      "Step 9850 | Loss: 0.00286 | LR: 8.26e-04\n",
      "Step 9875 | Loss: 0.00441 | LR: 8.25e-04\n",
      "test loss: 0.0036\n",
      "Step 9900 | Loss: 0.00356 | LR: 8.24e-04\n",
      "Step 9925 | Loss: 0.00479 | LR: 8.23e-04\n",
      "Step 9950 | Loss: 0.00387 | LR: 8.22e-04\n",
      "Step 9975 | Loss: 0.00391 | LR: 8.21e-04\n",
      "test loss: 0.0030\n",
      "Step 10000 | Loss: 0.00302 | LR: 8.20e-04\n",
      "Step 10025 | Loss: 0.00441 | LR: 8.19e-04\n",
      "Step 10050 | Loss: 0.00352 | LR: 8.18e-04\n",
      "Step 10075 | Loss: 0.00365 | LR: 8.17e-04\n",
      "test loss: 0.0044\n",
      "Step 10100 | Loss: 0.00434 | LR: 8.16e-04\n",
      "Step 10125 | Loss: 0.00576 | LR: 8.15e-04\n",
      "Step 10150 | Loss: 0.00419 | LR: 8.14e-04\n",
      "Step 10175 | Loss: 0.00358 | LR: 8.13e-04\n",
      "test loss: 0.0032\n",
      "Step 10200 | Loss: 0.00326 | LR: 8.12e-04\n",
      "Step 10225 | Loss: 0.00746 | LR: 8.11e-04\n",
      "Step 10250 | Loss: 0.00374 | LR: 8.10e-04\n",
      "Step 10275 | Loss: 0.00489 | LR: 8.09e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/training/val/shard_k562e_val_0000.npz\n",
      "test loss: 0.0030\n",
      "Step 10300 | Loss: 0.00318 | LR: 8.08e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0001.npz\n",
      "Step 10325 | Loss: 0.00344 | LR: 8.07e-04\n",
      "Step 10350 | Loss: 0.00368 | LR: 8.06e-04\n",
      "Step 10375 | Loss: 0.00406 | LR: 8.05e-04\n",
      "test loss: 0.0029\n",
      "Step 10400 | Loss: 0.00285 | LR: 8.04e-04\n",
      "Step 10425 | Loss: 0.00466 | LR: 8.03e-04\n",
      "Step 10450 | Loss: 0.00271 | LR: 8.02e-04\n",
      "Step 10475 | Loss: 0.00309 | LR: 8.01e-04\n",
      "test loss: 0.0028\n",
      "Step 10500 | Loss: 0.00242 | LR: 8.00e-04\n",
      "Step 10525 | Loss: 0.00444 | LR: 7.99e-04\n",
      "Step 10550 | Loss: 0.00351 | LR: 7.98e-04\n",
      "Step 10575 | Loss: 0.00404 | LR: 7.97e-04\n",
      "test loss: 0.0030\n",
      "Step 10600 | Loss: 0.00317 | LR: 7.96e-04\n",
      "Step 10625 | Loss: 0.00721 | LR: 7.95e-04\n",
      "Step 10650 | Loss: 0.00341 | LR: 7.94e-04\n",
      "Step 10675 | Loss: 0.00524 | LR: 7.92e-04\n",
      "test loss: 0.0047\n",
      "Step 10700 | Loss: 0.00478 | LR: 7.91e-04\n",
      "Step 10725 | Loss: 0.00334 | LR: 7.90e-04\n",
      "Step 10750 | Loss: 0.00501 | LR: 7.89e-04\n",
      "Step 10775 | Loss: 0.00318 | LR: 7.88e-04\n",
      "test loss: 0.0039\n",
      "Step 10800 | Loss: 0.00393 | LR: 7.87e-04\n",
      "Step 10825 | Loss: 0.00287 | LR: 7.86e-04\n",
      "Step 10850 | Loss: 0.00314 | LR: 7.85e-04\n",
      "Step 10875 | Loss: 0.00370 | LR: 7.84e-04\n",
      "test loss: 0.0038\n",
      "Step 10900 | Loss: 0.00383 | LR: 7.83e-04\n",
      "Step 10925 | Loss: 0.00381 | LR: 7.82e-04\n",
      "Step 10950 | Loss: 0.00346 | LR: 7.81e-04\n",
      "Step 10975 | Loss: 0.00468 | LR: 7.80e-04\n",
      "test loss: 0.0042\n",
      "Step 11000 | Loss: 0.00424 | LR: 7.79e-04\n",
      "Step 11025 | Loss: 0.00389 | LR: 7.77e-04\n",
      "Step 11050 | Loss: 0.00357 | LR: 7.76e-04\n",
      "Step 11075 | Loss: 0.00206 | LR: 7.75e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0003.npz\n",
      "test loss: 0.0039\n",
      "Step 11100 | Loss: 0.00371 | LR: 7.74e-04\n",
      "Step 11125 | Loss: 0.00489 | LR: 7.73e-04\n",
      "Step 11150 | Loss: 0.00368 | LR: 7.72e-04\n",
      "Step 11175 | Loss: 0.00335 | LR: 7.71e-04\n",
      "test loss: 0.0036\n",
      "Step 11200 | Loss: 0.00370 | LR: 7.70e-04\n",
      "Step 11225 | Loss: 0.00388 | LR: 7.69e-04\n",
      "Step 11250 | Loss: 0.00355 | LR: 7.68e-04\n",
      "Step 11275 | Loss: 0.00439 | LR: 7.67e-04\n",
      "test loss: 0.0035\n",
      "Step 11300 | Loss: 0.00326 | LR: 7.65e-04\n",
      "Step 11325 | Loss: 0.00500 | LR: 7.64e-04\n",
      "Step 11350 | Loss: 0.00398 | LR: 7.63e-04\n",
      "Step 11375 | Loss: 0.00381 | LR: 7.62e-04\n",
      "test loss: 0.0042\n",
      "Step 11400 | Loss: 0.00436 | LR: 7.61e-04\n",
      "Step 11425 | Loss: 0.00581 | LR: 7.60e-04\n",
      "Step 11450 | Loss: 0.00227 | LR: 7.59e-04\n",
      "Step 11475 | Loss: 0.00331 | LR: 7.58e-04\n",
      "test loss: 0.0029\n",
      "Step 11500 | Loss: 0.00283 | LR: 7.57e-04\n",
      "Step 11525 | Loss: 0.00234 | LR: 7.55e-04\n",
      "Step 11550 | Loss: 0.00529 | LR: 7.54e-04\n",
      "Step 11575 | Loss: 0.00359 | LR: 7.53e-04\n",
      "test loss: 0.0051\n",
      "Step 11600 | Loss: 0.00497 | LR: 7.52e-04\n",
      "Step 11625 | Loss: 0.00299 | LR: 7.51e-04\n",
      "Step 11650 | Loss: 0.00380 | LR: 7.50e-04\n",
      "Step 11675 | Loss: 0.00296 | LR: 7.49e-04\n",
      "test loss: 0.0030\n",
      "Step 11700 | Loss: 0.00306 | LR: 7.48e-04\n",
      "Step 11725 | Loss: 0.00347 | LR: 7.46e-04\n",
      "Step 11750 | Loss: 0.00485 | LR: 7.45e-04\n",
      "Step 11775 | Loss: 0.00417 | LR: 7.44e-04\n",
      "test loss: 0.0039\n",
      "Step 11800 | Loss: 0.00393 | LR: 7.43e-04\n",
      "Step 11825 | Loss: 0.00423 | LR: 7.42e-04\n",
      "Step 11850 | Loss: 0.00437 | LR: 7.41e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0000.npz\n",
      "Step 11875 | Loss: 0.00605 | LR: 7.40e-04\n",
      "test loss: 0.0049\n",
      "Step 11900 | Loss: 0.00501 | LR: 7.39e-04\n",
      "Step 11925 | Loss: 0.00336 | LR: 7.37e-04\n",
      "Step 11950 | Loss: 0.00320 | LR: 7.36e-04\n",
      "Step 11975 | Loss: 0.00322 | LR: 7.35e-04\n",
      "test loss: 0.0039\n",
      "Step 12000 | Loss: 0.00383 | LR: 7.34e-04\n",
      "Step 12025 | Loss: 0.00436 | LR: 7.33e-04\n",
      "Step 12050 | Loss: 0.00322 | LR: 7.32e-04\n",
      "Step 12075 | Loss: 0.00247 | LR: 7.30e-04\n",
      "test loss: 0.0040\n",
      "Step 12100 | Loss: 0.00411 | LR: 7.29e-04\n",
      "Step 12125 | Loss: 0.00269 | LR: 7.28e-04\n",
      "Step 12150 | Loss: 0.00297 | LR: 7.27e-04\n",
      "Step 12175 | Loss: 0.00551 | LR: 7.26e-04\n",
      "test loss: 0.0044\n",
      "Step 12200 | Loss: 0.00434 | LR: 7.25e-04\n",
      "Step 12225 | Loss: 0.00227 | LR: 7.24e-04\n",
      "Step 12250 | Loss: 0.00324 | LR: 7.22e-04\n",
      "Step 12275 | Loss: 0.00438 | LR: 7.21e-04\n",
      "test loss: 0.0047\n",
      "Step 12300 | Loss: 0.00485 | LR: 7.20e-04\n",
      "Step 12325 | Loss: 0.00288 | LR: 7.19e-04\n",
      "Step 12350 | Loss: 0.00302 | LR: 7.18e-04\n",
      "Step 12375 | Loss: 0.00304 | LR: 7.17e-04\n",
      "test loss: 0.0029\n",
      "Step 12400 | Loss: 0.00308 | LR: 7.15e-04\n",
      "Step 12425 | Loss: 0.00315 | LR: 7.14e-04\n",
      "Step 12450 | Loss: 0.00271 | LR: 7.13e-04\n",
      "Step 12475 | Loss: 0.00256 | LR: 7.12e-04\n",
      "test loss: 0.0031\n",
      "Step 12500 | Loss: 0.00309 | LR: 7.11e-04\n",
      "Step 12525 | Loss: 0.00372 | LR: 7.09e-04\n",
      "Step 12550 | Loss: 0.00264 | LR: 7.08e-04\n",
      "Step 12575 | Loss: 0.00349 | LR: 7.07e-04\n",
      "test loss: 0.0028\n",
      "Step 12600 | Loss: 0.00284 | LR: 7.06e-04\n",
      "Step 12625 | Loss: 0.00236 | LR: 7.05e-04\n",
      "Step 12650 | Loss: 0.00358 | LR: 7.04e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0004.npz\n",
      "Step 12675 | Loss: 0.00329 | LR: 7.02e-04\n",
      "test loss: 0.0034\n",
      "Step 12700 | Loss: 0.00332 | LR: 7.01e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0001.npz\n",
      "=== Step 12707 Done. Avg Loss: 0.00382 ===\n",
      "Step 12725 | Loss: 0.00285 | LR: 7.00e-04\n",
      "Step 12750 | Loss: 0.00365 | LR: 6.99e-04\n",
      "Step 12775 | Loss: 0.00308 | LR: 6.98e-04\n",
      "test loss: 0.0033\n",
      "Step 12800 | Loss: 0.00335 | LR: 6.96e-04\n",
      "Step 12825 | Loss: 0.00321 | LR: 6.95e-04\n",
      "Step 12850 | Loss: 0.00371 | LR: 6.94e-04\n",
      "Step 12875 | Loss: 0.00352 | LR: 6.93e-04\n",
      "test loss: 0.0022\n",
      "Step 12900 | Loss: 0.00217 | LR: 6.92e-04\n",
      "Step 12925 | Loss: 0.00385 | LR: 6.90e-04\n",
      "Step 12950 | Loss: 0.00277 | LR: 6.89e-04\n",
      "Step 12975 | Loss: 0.00275 | LR: 6.88e-04\n",
      "test loss: 0.0033\n",
      "Step 13000 | Loss: 0.00322 | LR: 6.87e-04\n",
      "Step 13025 | Loss: 0.00223 | LR: 6.86e-04\n",
      "Step 13050 | Loss: 0.00313 | LR: 6.84e-04\n",
      "Step 13075 | Loss: 0.00320 | LR: 6.83e-04\n",
      "test loss: 0.0026\n",
      "Step 13100 | Loss: 0.00246 | LR: 6.82e-04\n",
      "Step 13125 | Loss: 0.00604 | LR: 6.81e-04\n",
      "Step 13150 | Loss: 0.00202 | LR: 6.79e-04\n",
      "Step 13175 | Loss: 0.00263 | LR: 6.78e-04\n",
      "test loss: 0.0032\n",
      "Step 13200 | Loss: 0.00340 | LR: 6.77e-04\n",
      "Step 13225 | Loss: 0.00215 | LR: 6.76e-04\n",
      "Step 13250 | Loss: 0.00220 | LR: 6.75e-04\n",
      "Step 13275 | Loss: 0.00259 | LR: 6.73e-04\n",
      "test loss: 0.0028\n",
      "Step 13300 | Loss: 0.00288 | LR: 6.72e-04\n",
      "Step 13325 | Loss: 0.00341 | LR: 6.71e-04\n",
      "Step 13350 | Loss: 0.00254 | LR: 6.70e-04\n",
      "Step 13375 | Loss: 0.00364 | LR: 6.69e-04\n",
      "test loss: 0.0031\n",
      "Step 13400 | Loss: 0.00306 | LR: 6.67e-04\n",
      "Step 13425 | Loss: 0.00250 | LR: 6.66e-04\n",
      "Step 13450 | Loss: 0.00272 | LR: 6.65e-04\n",
      "Step 13475 | Loss: 0.00286 | LR: 6.64e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0003.npz\n",
      "test loss: 0.0023\n",
      "Step 13500 | Loss: 0.00232 | LR: 6.62e-04\n",
      "Step 13525 | Loss: 0.00331 | LR: 6.61e-04\n",
      "Step 13550 | Loss: 0.00247 | LR: 6.60e-04\n",
      "Step 13575 | Loss: 0.00226 | LR: 6.59e-04\n",
      "test loss: 0.0030\n",
      "Step 13600 | Loss: 0.00314 | LR: 6.57e-04\n",
      "Step 13625 | Loss: 0.00430 | LR: 6.56e-04\n",
      "Step 13650 | Loss: 0.00315 | LR: 6.55e-04\n",
      "Step 13675 | Loss: 0.00392 | LR: 6.54e-04\n",
      "test loss: 0.0033\n",
      "Step 13700 | Loss: 0.00326 | LR: 6.52e-04\n",
      "Step 13725 | Loss: 0.00513 | LR: 6.51e-04\n",
      "Step 13750 | Loss: 0.00232 | LR: 6.50e-04\n",
      "Step 13775 | Loss: 0.00345 | LR: 6.49e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/training/val/shard_k562e_val_0000.npz\n",
      "test loss: 0.0034\n",
      "Step 13800 | Loss: 0.00348 | LR: 6.48e-04\n",
      "Step 13825 | Loss: 0.00434 | LR: 6.46e-04\n",
      "Step 13850 | Loss: 0.00272 | LR: 6.45e-04\n",
      "Step 13875 | Loss: 0.00238 | LR: 6.44e-04\n",
      "test loss: 0.0024\n",
      "Step 13900 | Loss: 0.00242 | LR: 6.43e-04\n",
      "Step 13925 | Loss: 0.00288 | LR: 6.41e-04\n",
      "Step 13950 | Loss: 0.00237 | LR: 6.40e-04\n",
      "Step 13975 | Loss: 0.00272 | LR: 6.39e-04\n",
      "test loss: 0.0030\n",
      "Step 14000 | Loss: 0.00307 | LR: 6.38e-04\n",
      "Step 14025 | Loss: 0.00228 | LR: 6.36e-04\n",
      "Step 14050 | Loss: 0.00190 | LR: 6.35e-04\n",
      "Step 14075 | Loss: 0.00229 | LR: 6.34e-04\n",
      "test loss: 0.0034\n",
      "Step 14100 | Loss: 0.00343 | LR: 6.33e-04\n",
      "Step 14125 | Loss: 0.00256 | LR: 6.31e-04\n",
      "Step 14150 | Loss: 0.00296 | LR: 6.30e-04\n",
      "Step 14175 | Loss: 0.00356 | LR: 6.29e-04\n",
      "test loss: 0.0021\n",
      "Step 14200 | Loss: 0.00205 | LR: 6.28e-04\n",
      "Step 14225 | Loss: 0.00240 | LR: 6.26e-04\n",
      "Step 14250 | Loss: 0.00265 | LR: 6.25e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0002.npz\n",
      "Step 14275 | Loss: 0.00318 | LR: 6.24e-04\n",
      "test loss: 0.0020\n",
      "Step 14300 | Loss: 0.00210 | LR: 6.22e-04\n",
      "Step 14325 | Loss: 0.00243 | LR: 6.21e-04\n",
      "Step 14350 | Loss: 0.00329 | LR: 6.20e-04\n",
      "Step 14375 | Loss: 0.00346 | LR: 6.19e-04\n",
      "test loss: 0.0029\n",
      "Step 14400 | Loss: 0.00276 | LR: 6.17e-04\n",
      "Step 14425 | Loss: 0.00445 | LR: 6.16e-04\n",
      "Step 14450 | Loss: 0.00443 | LR: 6.15e-04\n",
      "Step 14475 | Loss: 0.00370 | LR: 6.14e-04\n",
      "test loss: 0.0034\n",
      "Step 14500 | Loss: 0.00338 | LR: 6.12e-04\n",
      "Step 14525 | Loss: 0.00252 | LR: 6.11e-04\n",
      "Step 14550 | Loss: 0.00306 | LR: 6.10e-04\n",
      "Step 14575 | Loss: 0.00433 | LR: 6.09e-04\n",
      "test loss: 0.0033\n",
      "Step 14600 | Loss: 0.00348 | LR: 6.07e-04\n",
      "Step 14625 | Loss: 0.00237 | LR: 6.06e-04\n",
      "Step 14650 | Loss: 0.00310 | LR: 6.05e-04\n",
      "Step 14675 | Loss: 0.00241 | LR: 6.03e-04\n",
      "test loss: 0.0026\n",
      "Step 14700 | Loss: 0.00262 | LR: 6.02e-04\n",
      "Step 14725 | Loss: 0.00253 | LR: 6.01e-04\n",
      "Step 14750 | Loss: 0.00215 | LR: 6.00e-04\n",
      "Step 14775 | Loss: 0.00272 | LR: 5.98e-04\n",
      "test loss: 0.0025\n",
      "Step 14800 | Loss: 0.00248 | LR: 5.97e-04\n",
      "Step 14825 | Loss: 0.00278 | LR: 5.96e-04\n",
      "Step 14850 | Loss: 0.00251 | LR: 5.95e-04\n",
      "Step 14875 | Loss: 0.00281 | LR: 5.93e-04\n",
      "test loss: 0.0030\n",
      "Step 14900 | Loss: 0.00298 | LR: 5.92e-04\n",
      "Step 14925 | Loss: 0.00258 | LR: 5.91e-04\n",
      "Step 14950 | Loss: 0.00259 | LR: 5.89e-04\n",
      "Step 14975 | Loss: 0.00254 | LR: 5.88e-04\n",
      "test loss: 0.0034\n",
      "Step 15000 | Loss: 0.00342 | LR: 5.87e-04\n",
      "Step 15025 | Loss: 0.00391 | LR: 5.86e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0000.npz\n",
      "Step 15050 | Loss: 0.00254 | LR: 5.84e-04\n",
      "Step 15075 | Loss: 0.00280 | LR: 5.83e-04\n",
      "test loss: 0.0034\n",
      "Step 15100 | Loss: 0.00351 | LR: 5.82e-04\n",
      "Step 15125 | Loss: 0.00316 | LR: 5.80e-04\n",
      "Step 15150 | Loss: 0.00413 | LR: 5.79e-04\n",
      "Step 15175 | Loss: 0.00198 | LR: 5.78e-04\n",
      "test loss: 0.0034\n",
      "Step 15200 | Loss: 0.00342 | LR: 5.77e-04\n",
      "Step 15225 | Loss: 0.00327 | LR: 5.75e-04\n",
      "Step 15250 | Loss: 0.00309 | LR: 5.74e-04\n",
      "Step 15275 | Loss: 0.00213 | LR: 5.73e-04\n",
      "test loss: 0.0023\n",
      "Step 15300 | Loss: 0.00235 | LR: 5.71e-04\n",
      "Step 15325 | Loss: 0.00243 | LR: 5.70e-04\n",
      "Step 15350 | Loss: 0.00221 | LR: 5.69e-04\n",
      "Step 15375 | Loss: 0.00264 | LR: 5.68e-04\n",
      "test loss: 0.0026\n",
      "Step 15400 | Loss: 0.00255 | LR: 5.66e-04\n",
      "Step 15425 | Loss: 0.00236 | LR: 5.65e-04\n",
      "Step 15450 | Loss: 0.00292 | LR: 5.64e-04\n",
      "Step 15475 | Loss: 0.00283 | LR: 5.62e-04\n",
      "test loss: 0.0029\n",
      "Step 15500 | Loss: 0.00296 | LR: 5.61e-04\n",
      "Step 15525 | Loss: 0.00230 | LR: 5.60e-04\n",
      "Step 15550 | Loss: 0.00282 | LR: 5.59e-04\n",
      "Step 15575 | Loss: 0.00285 | LR: 5.57e-04\n",
      "test loss: 0.0024\n",
      "Step 15600 | Loss: 0.00252 | LR: 5.56e-04\n",
      "Step 15625 | Loss: 0.00391 | LR: 5.55e-04\n",
      "Step 15650 | Loss: 0.00412 | LR: 5.53e-04\n",
      "Step 15675 | Loss: 0.00318 | LR: 5.52e-04\n",
      "test loss: 0.0023\n",
      "Step 15700 | Loss: 0.00249 | LR: 5.51e-04\n",
      "Step 15725 | Loss: 0.00269 | LR: 5.49e-04\n",
      "Step 15750 | Loss: 0.00234 | LR: 5.48e-04\n",
      "Step 15775 | Loss: 0.00297 | LR: 5.47e-04\n",
      "test loss: 0.0031\n",
      "Step 15800 | Loss: 0.00306 | LR: 5.46e-04\n",
      "Step 15825 | Loss: 0.00287 | LR: 5.44e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0004.npz\n",
      "Step 15850 | Loss: 0.00193 | LR: 5.43e-04\n",
      "Step 15875 | Loss: 0.00262 | LR: 5.42e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0002.npz\n",
      "=== Step 15884 Done. Avg Loss: 0.00297 ===\n",
      "test loss: 0.0043\n",
      "Step 15900 | Loss: 0.00422 | LR: 5.40e-04\n",
      "Step 15925 | Loss: 0.00268 | LR: 5.39e-04\n",
      "Step 15950 | Loss: 0.00287 | LR: 5.38e-04\n",
      "Step 15975 | Loss: 0.00241 | LR: 5.37e-04\n",
      "test loss: 0.0027\n",
      "Step 16000 | Loss: 0.00277 | LR: 5.35e-04\n",
      "Step 16025 | Loss: 0.00208 | LR: 5.34e-04\n",
      "Step 16050 | Loss: 0.00191 | LR: 5.33e-04\n",
      "Step 16075 | Loss: 0.00250 | LR: 5.31e-04\n",
      "test loss: 0.0027\n",
      "Step 16100 | Loss: 0.00279 | LR: 5.30e-04\n",
      "Step 16125 | Loss: 0.00197 | LR: 5.29e-04\n",
      "Step 16150 | Loss: 0.00338 | LR: 5.27e-04\n",
      "Step 16175 | Loss: 0.00337 | LR: 5.26e-04\n",
      "test loss: 0.0023\n",
      "Step 16200 | Loss: 0.00233 | LR: 5.25e-04\n",
      "Step 16225 | Loss: 0.00241 | LR: 5.24e-04\n",
      "Step 16250 | Loss: 0.00243 | LR: 5.22e-04\n",
      "Step 16275 | Loss: 0.00185 | LR: 5.21e-04\n",
      "test loss: 0.0019\n",
      "Step 16300 | Loss: 0.00202 | LR: 5.20e-04\n",
      "Step 16325 | Loss: 0.00234 | LR: 5.18e-04\n",
      "Step 16350 | Loss: 0.00263 | LR: 5.17e-04\n",
      "Step 16375 | Loss: 0.00259 | LR: 5.16e-04\n",
      "test loss: 0.0034\n",
      "Step 16400 | Loss: 0.00334 | LR: 5.14e-04\n",
      "Step 16425 | Loss: 0.00231 | LR: 5.13e-04\n",
      "Step 16450 | Loss: 0.00195 | LR: 5.12e-04\n",
      "Step 16475 | Loss: 0.00310 | LR: 5.11e-04\n",
      "test loss: 0.0021\n",
      "Step 16500 | Loss: 0.00213 | LR: 5.09e-04\n",
      "Step 16525 | Loss: 0.00272 | LR: 5.08e-04\n",
      "Step 16550 | Loss: 0.00291 | LR: 5.07e-04\n",
      "Step 16575 | Loss: 0.00255 | LR: 5.05e-04\n",
      "test loss: 0.0027\n",
      "Step 16600 | Loss: 0.00278 | LR: 5.04e-04\n",
      "Step 16625 | Loss: 0.00338 | LR: 5.03e-04\n",
      "Step 16650 | Loss: 0.00242 | LR: 5.01e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0003.npz\n",
      "Step 16675 | Loss: 0.00243 | LR: 5.00e-04\n",
      "test loss: 0.0019\n",
      "Step 16700 | Loss: 0.00198 | LR: 4.99e-04\n",
      "Step 16725 | Loss: 0.00168 | LR: 4.98e-04\n",
      "Step 16750 | Loss: 0.00263 | LR: 4.96e-04\n",
      "Step 16775 | Loss: 0.00278 | LR: 4.95e-04\n",
      "test loss: 0.0016\n",
      "Step 16800 | Loss: 0.00162 | LR: 4.94e-04\n",
      "Step 16825 | Loss: 0.00259 | LR: 4.92e-04\n",
      "Step 16850 | Loss: 0.00187 | LR: 4.91e-04\n",
      "Step 16875 | Loss: 0.00244 | LR: 4.90e-04\n",
      "test loss: 0.0021\n",
      "Step 16900 | Loss: 0.00212 | LR: 4.88e-04\n",
      "Step 16925 | Loss: 0.00266 | LR: 4.87e-04\n",
      "Step 16950 | Loss: 0.00309 | LR: 4.86e-04\n",
      "Step 16975 | Loss: 0.00187 | LR: 4.85e-04\n",
      "test loss: 0.0019\n",
      "Step 17000 | Loss: 0.00182 | LR: 4.83e-04\n",
      "Step 17025 | Loss: 0.00219 | LR: 4.82e-04\n",
      "Step 17050 | Loss: 0.00216 | LR: 4.81e-04\n",
      "Step 17075 | Loss: 0.00271 | LR: 4.79e-04\n",
      "test loss: 0.0018\n",
      "Step 17100 | Loss: 0.00179 | LR: 4.78e-04\n",
      "Step 17125 | Loss: 0.00211 | LR: 4.77e-04\n",
      "Step 17150 | Loss: 0.00225 | LR: 4.75e-04\n",
      "Step 17175 | Loss: 0.00209 | LR: 4.74e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/training/val/shard_k562e_val_0000.npz\n",
      "test loss: 0.0020\n",
      "Step 17200 | Loss: 0.00190 | LR: 4.73e-04\n",
      "Step 17225 | Loss: 0.00226 | LR: 4.72e-04\n",
      "Step 17250 | Loss: 0.00254 | LR: 4.70e-04\n",
      "Step 17275 | Loss: 0.00217 | LR: 4.69e-04\n",
      "test loss: 0.0024\n",
      "Step 17300 | Loss: 0.00245 | LR: 4.68e-04\n",
      "Step 17325 | Loss: 0.00190 | LR: 4.66e-04\n",
      "Step 17350 | Loss: 0.00220 | LR: 4.65e-04\n",
      "Step 17375 | Loss: 0.00283 | LR: 4.64e-04\n",
      "test loss: 0.0023\n",
      "Step 17400 | Loss: 0.00233 | LR: 4.62e-04\n",
      "Step 17425 | Loss: 0.00242 | LR: 4.61e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0000.npz\n",
      "Step 17450 | Loss: 0.00201 | LR: 4.60e-04\n",
      "Step 17475 | Loss: 0.00199 | LR: 4.59e-04\n",
      "test loss: 0.0023\n",
      "Step 17500 | Loss: 0.00220 | LR: 4.57e-04\n",
      "Step 17525 | Loss: 0.00223 | LR: 4.56e-04\n",
      "Step 17550 | Loss: 0.00191 | LR: 4.55e-04\n",
      "Step 17575 | Loss: 0.00178 | LR: 4.53e-04\n",
      "test loss: 0.0015\n",
      "Step 17600 | Loss: 0.00162 | LR: 4.52e-04\n",
      "Step 17625 | Loss: 0.00149 | LR: 4.51e-04\n",
      "Step 17650 | Loss: 0.00224 | LR: 4.49e-04\n",
      "Step 17675 | Loss: 0.00225 | LR: 4.48e-04\n",
      "test loss: 0.0024\n",
      "Step 17700 | Loss: 0.00229 | LR: 4.47e-04\n",
      "Step 17725 | Loss: 0.00212 | LR: 4.46e-04\n",
      "Step 17750 | Loss: 0.00252 | LR: 4.44e-04\n",
      "Step 17775 | Loss: 0.00212 | LR: 4.43e-04\n",
      "test loss: 0.0022\n",
      "Step 17800 | Loss: 0.00210 | LR: 4.42e-04\n",
      "Step 17825 | Loss: 0.00209 | LR: 4.40e-04\n",
      "Step 17850 | Loss: 0.00150 | LR: 4.39e-04\n",
      "Step 17875 | Loss: 0.00205 | LR: 4.38e-04\n",
      "test loss: 0.0021\n",
      "Step 17900 | Loss: 0.00220 | LR: 4.37e-04\n",
      "Step 17925 | Loss: 0.00181 | LR: 4.35e-04\n",
      "Step 17950 | Loss: 0.00207 | LR: 4.34e-04\n",
      "Step 17975 | Loss: 0.00249 | LR: 4.33e-04\n",
      "test loss: 0.0023\n",
      "Step 18000 | Loss: 0.00238 | LR: 4.31e-04\n",
      "Step 18025 | Loss: 0.00179 | LR: 4.30e-04\n",
      "Step 18050 | Loss: 0.00180 | LR: 4.29e-04\n",
      "Step 18075 | Loss: 0.00177 | LR: 4.28e-04\n",
      "test loss: 0.0023\n",
      "Step 18100 | Loss: 0.00225 | LR: 4.26e-04\n",
      "Step 18125 | Loss: 0.00189 | LR: 4.25e-04\n",
      "Step 18150 | Loss: 0.00152 | LR: 4.24e-04\n",
      "Step 18175 | Loss: 0.00142 | LR: 4.22e-04\n",
      "test loss: 0.0028\n",
      "Step 18200 | Loss: 0.00281 | LR: 4.21e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0004.npz\n",
      "Step 18225 | Loss: 0.00342 | LR: 4.20e-04\n",
      "Step 18250 | Loss: 0.00175 | LR: 4.19e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0001.npz\n",
      "Step 18275 | Loss: 0.00227 | LR: 4.17e-04\n",
      "test loss: 0.0022\n",
      "Step 18300 | Loss: 0.00222 | LR: 4.16e-04\n",
      "Step 18325 | Loss: 0.00300 | LR: 4.15e-04\n",
      "Step 18350 | Loss: 0.00212 | LR: 4.13e-04\n",
      "Step 18375 | Loss: 0.00151 | LR: 4.12e-04\n",
      "test loss: 0.0017\n",
      "Step 18400 | Loss: 0.00175 | LR: 4.11e-04\n",
      "Step 18425 | Loss: 0.00151 | LR: 4.10e-04\n",
      "Step 18450 | Loss: 0.00256 | LR: 4.08e-04\n",
      "Step 18475 | Loss: 0.00211 | LR: 4.07e-04\n",
      "test loss: 0.0031\n",
      "Step 18500 | Loss: 0.00327 | LR: 4.06e-04\n",
      "Step 18525 | Loss: 0.00176 | LR: 4.04e-04\n",
      "Step 18550 | Loss: 0.00182 | LR: 4.03e-04\n",
      "Step 18575 | Loss: 0.00178 | LR: 4.02e-04\n",
      "test loss: 0.0019\n",
      "Step 18600 | Loss: 0.00190 | LR: 4.01e-04\n",
      "Step 18625 | Loss: 0.00250 | LR: 3.99e-04\n",
      "Step 18650 | Loss: 0.00271 | LR: 3.98e-04\n",
      "Step 18675 | Loss: 0.00233 | LR: 3.97e-04\n",
      "test loss: 0.0015\n",
      "Step 18700 | Loss: 0.00153 | LR: 3.96e-04\n",
      "Step 18725 | Loss: 0.00192 | LR: 3.94e-04\n",
      "Step 18750 | Loss: 0.00175 | LR: 3.93e-04\n",
      "Step 18775 | Loss: 0.00232 | LR: 3.92e-04\n",
      "test loss: 0.0019\n",
      "Step 18800 | Loss: 0.00188 | LR: 3.90e-04\n",
      "Step 18825 | Loss: 0.00241 | LR: 3.89e-04\n",
      "Step 18850 | Loss: 0.00197 | LR: 3.88e-04\n",
      "Step 18875 | Loss: 0.00332 | LR: 3.87e-04\n",
      "test loss: 0.0025\n",
      "Step 18900 | Loss: 0.00257 | LR: 3.85e-04\n",
      "Step 18925 | Loss: 0.00171 | LR: 3.84e-04\n",
      "Step 18950 | Loss: 0.00237 | LR: 3.83e-04\n",
      "Step 18975 | Loss: 0.00250 | LR: 3.82e-04\n",
      "test loss: 0.0015\n",
      "Step 19000 | Loss: 0.00152 | LR: 3.80e-04\n",
      "Step 19025 | Loss: 0.00164 | LR: 3.79e-04\n",
      "Step 19050 | Loss: 0.00239 | LR: 3.78e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0000.npz\n",
      "=== Step 19061 Done. Avg Loss: 0.00222 ===\n",
      "Step 19075 | Loss: 0.00178 | LR: 3.77e-04\n",
      "test loss: 0.0017\n",
      "Step 19100 | Loss: 0.00170 | LR: 3.75e-04\n",
      "Step 19125 | Loss: 0.00168 | LR: 3.74e-04\n",
      "Step 19150 | Loss: 0.00162 | LR: 3.73e-04\n",
      "Step 19175 | Loss: 0.00214 | LR: 3.71e-04\n",
      "test loss: 0.0016\n",
      "Step 19200 | Loss: 0.00151 | LR: 3.70e-04\n",
      "Step 19225 | Loss: 0.00199 | LR: 3.69e-04\n",
      "Step 19250 | Loss: 0.00158 | LR: 3.68e-04\n",
      "Step 19275 | Loss: 0.00199 | LR: 3.66e-04\n",
      "test loss: 0.0022\n",
      "Step 19300 | Loss: 0.00226 | LR: 3.65e-04\n",
      "Step 19325 | Loss: 0.00200 | LR: 3.64e-04\n",
      "Step 19350 | Loss: 0.00169 | LR: 3.63e-04\n",
      "Step 19375 | Loss: 0.00177 | LR: 3.61e-04\n",
      "test loss: 0.0020\n",
      "Step 19400 | Loss: 0.00204 | LR: 3.60e-04\n",
      "Step 19425 | Loss: 0.00200 | LR: 3.59e-04\n",
      "Step 19450 | Loss: 0.00167 | LR: 3.58e-04\n",
      "Step 19475 | Loss: 0.00170 | LR: 3.56e-04\n",
      "test loss: 0.0016\n",
      "Step 19500 | Loss: 0.00162 | LR: 3.55e-04\n",
      "Step 19525 | Loss: 0.00165 | LR: 3.54e-04\n",
      "Step 19550 | Loss: 0.00162 | LR: 3.53e-04\n",
      "Step 19575 | Loss: 0.00157 | LR: 3.51e-04\n",
      "test loss: 0.0022\n",
      "Step 19600 | Loss: 0.00220 | LR: 3.50e-04\n",
      "Step 19625 | Loss: 0.00202 | LR: 3.49e-04\n",
      "Step 19650 | Loss: 0.00166 | LR: 3.48e-04\n",
      "Step 19675 | Loss: 0.00137 | LR: 3.47e-04\n",
      "test loss: 0.0019\n",
      "Step 19700 | Loss: 0.00192 | LR: 3.45e-04\n",
      "Step 19725 | Loss: 0.00182 | LR: 3.44e-04\n",
      "Step 19750 | Loss: 0.00224 | LR: 3.43e-04\n",
      "Step 19775 | Loss: 0.00190 | LR: 3.42e-04\n",
      "test loss: 0.0020\n",
      "Step 19800 | Loss: 0.00196 | LR: 3.40e-04\n",
      "Step 19825 | Loss: 0.00163 | LR: 3.39e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0004.npz\n",
      "Step 19850 | Loss: 0.00187 | LR: 3.38e-04\n",
      "Step 19875 | Loss: 0.00206 | LR: 3.37e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0002.npz\n",
      "test loss: 0.0015\n",
      "Step 19900 | Loss: 0.00160 | LR: 3.35e-04\n",
      "Step 19925 | Loss: 0.00162 | LR: 3.34e-04\n",
      "Step 19950 | Loss: 0.00156 | LR: 3.33e-04\n",
      "Step 19975 | Loss: 0.00163 | LR: 3.32e-04\n",
      "test loss: 0.0018\n",
      "Step 20000 | Loss: 0.00181 | LR: 3.30e-04\n",
      "Step 20025 | Loss: 0.00160 | LR: 3.29e-04\n",
      "Step 20050 | Loss: 0.00159 | LR: 3.28e-04\n",
      "Step 20075 | Loss: 0.00150 | LR: 3.27e-04\n",
      "test loss: 0.0016\n",
      "Step 20100 | Loss: 0.00162 | LR: 3.26e-04\n",
      "Step 20125 | Loss: 0.00154 | LR: 3.24e-04\n",
      "Step 20150 | Loss: 0.00141 | LR: 3.23e-04\n",
      "Step 20175 | Loss: 0.00162 | LR: 3.22e-04\n",
      "test loss: 0.0015\n",
      "Step 20200 | Loss: 0.00148 | LR: 3.21e-04\n",
      "Step 20225 | Loss: 0.00171 | LR: 3.20e-04\n",
      "Step 20250 | Loss: 0.00144 | LR: 3.18e-04\n",
      "Step 20275 | Loss: 0.00165 | LR: 3.17e-04\n",
      "test loss: 0.0016\n",
      "Step 20300 | Loss: 0.00165 | LR: 3.16e-04\n",
      "Step 20325 | Loss: 0.00154 | LR: 3.15e-04\n",
      "Step 20350 | Loss: 0.00169 | LR: 3.13e-04\n",
      "Step 20375 | Loss: 0.00151 | LR: 3.12e-04\n",
      "test loss: 0.0018\n",
      "Step 20400 | Loss: 0.00173 | LR: 3.11e-04\n",
      "Step 20425 | Loss: 0.00211 | LR: 3.10e-04\n",
      "Step 20450 | Loss: 0.00163 | LR: 3.09e-04\n",
      "Step 20475 | Loss: 0.00158 | LR: 3.07e-04\n",
      "test loss: 0.0019\n",
      "Step 20500 | Loss: 0.00199 | LR: 3.06e-04\n",
      "Step 20525 | Loss: 0.00163 | LR: 3.05e-04\n",
      "Step 20550 | Loss: 0.00164 | LR: 3.04e-04\n",
      "Step 20575 | Loss: 0.00170 | LR: 3.03e-04\n",
      "test loss: 0.0017\n",
      "Step 20600 | Loss: 0.00159 | LR: 3.01e-04\n",
      "Step 20625 | Loss: 0.00166 | LR: 3.00e-04\n",
      "Step 20650 | Loss: 0.00183 | LR: 2.99e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0003.npz\n",
      "Step 20675 | Loss: 0.00173 | LR: 2.98e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/training/val/shard_k562e_val_0000.npz\n",
      "test loss: 0.0018\n",
      "Step 20700 | Loss: 0.00174 | LR: 2.97e-04\n",
      "Step 20725 | Loss: 0.00154 | LR: 2.96e-04\n",
      "Step 20750 | Loss: 0.00153 | LR: 2.94e-04\n",
      "Step 20775 | Loss: 0.00160 | LR: 2.93e-04\n",
      "test loss: 0.0016\n",
      "Step 20800 | Loss: 0.00160 | LR: 2.92e-04\n",
      "Step 20825 | Loss: 0.00171 | LR: 2.91e-04\n",
      "Step 20850 | Loss: 0.00158 | LR: 2.90e-04\n",
      "Step 20875 | Loss: 0.00181 | LR: 2.88e-04\n",
      "test loss: 0.0016\n",
      "Step 20900 | Loss: 0.00161 | LR: 2.87e-04\n",
      "Step 20925 | Loss: 0.00187 | LR: 2.86e-04\n",
      "Step 20950 | Loss: 0.00174 | LR: 2.85e-04\n",
      "Step 20975 | Loss: 0.00163 | LR: 2.84e-04\n",
      "test loss: 0.0016\n",
      "Step 21000 | Loss: 0.00157 | LR: 2.83e-04\n",
      "Step 21025 | Loss: 0.00174 | LR: 2.81e-04\n",
      "Step 21050 | Loss: 0.00150 | LR: 2.80e-04\n",
      "Step 21075 | Loss: 0.00161 | LR: 2.79e-04\n",
      "test loss: 0.0017\n",
      "Step 21100 | Loss: 0.00157 | LR: 2.78e-04\n",
      "Step 21125 | Loss: 0.00166 | LR: 2.77e-04\n",
      "Step 21150 | Loss: 0.00164 | LR: 2.76e-04\n",
      "Step 21175 | Loss: 0.00154 | LR: 2.74e-04\n",
      "test loss: 0.0016\n",
      "Step 21200 | Loss: 0.00157 | LR: 2.73e-04\n",
      "Step 21225 | Loss: 0.00155 | LR: 2.72e-04\n",
      "Step 21250 | Loss: 0.00146 | LR: 2.71e-04\n",
      "Step 21275 | Loss: 0.00157 | LR: 2.70e-04\n",
      "test loss: 0.0016\n",
      "Step 21300 | Loss: 0.00158 | LR: 2.69e-04\n",
      "Step 21325 | Loss: 0.00171 | LR: 2.67e-04\n",
      "Step 21350 | Loss: 0.00160 | LR: 2.66e-04\n",
      "Step 21375 | Loss: 0.00161 | LR: 2.65e-04\n",
      "test loss: 0.0015\n",
      "Step 21400 | Loss: 0.00156 | LR: 2.64e-04\n",
      "Step 21425 | Loss: 0.00175 | LR: 2.63e-04\n",
      "Step 21450 | Loss: 0.00155 | LR: 2.62e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0001.npz\n",
      "Step 21475 | Loss: 0.00157 | LR: 2.61e-04\n",
      "test loss: 0.0016\n",
      "Step 21500 | Loss: 0.00159 | LR: 2.59e-04\n",
      "Step 21525 | Loss: 0.00163 | LR: 2.58e-04\n",
      "Step 21550 | Loss: 0.00149 | LR: 2.57e-04\n",
      "Step 21575 | Loss: 0.00165 | LR: 2.56e-04\n",
      "test loss: 0.0016\n",
      "Step 21600 | Loss: 0.00155 | LR: 2.55e-04\n",
      "Step 21625 | Loss: 0.00178 | LR: 2.54e-04\n",
      "Step 21650 | Loss: 0.00166 | LR: 2.53e-04\n",
      "Step 21675 | Loss: 0.00153 | LR: 2.51e-04\n",
      "test loss: 0.0015\n",
      "Step 21700 | Loss: 0.00148 | LR: 2.50e-04\n",
      "Step 21725 | Loss: 0.00158 | LR: 2.49e-04\n",
      "Step 21750 | Loss: 0.00160 | LR: 2.48e-04\n",
      "Step 21775 | Loss: 0.00164 | LR: 2.47e-04\n",
      "test loss: 0.0017\n",
      "Step 21800 | Loss: 0.00172 | LR: 2.46e-04\n",
      "Step 21825 | Loss: 0.00155 | LR: 2.45e-04\n",
      "Step 21850 | Loss: 0.00160 | LR: 2.44e-04\n",
      "Step 21875 | Loss: 0.00153 | LR: 2.42e-04\n",
      "test loss: 0.0016\n",
      "Step 21900 | Loss: 0.00166 | LR: 2.41e-04\n",
      "Step 21925 | Loss: 0.00158 | LR: 2.40e-04\n",
      "Step 21950 | Loss: 0.00162 | LR: 2.39e-04\n",
      "Step 21975 | Loss: 0.00163 | LR: 2.38e-04\n",
      "test loss: 0.0016\n",
      "Step 22000 | Loss: 0.00155 | LR: 2.37e-04\n",
      "Step 22025 | Loss: 0.00155 | LR: 2.36e-04\n",
      "Step 22050 | Loss: 0.00162 | LR: 2.35e-04\n",
      "Step 22075 | Loss: 0.00160 | LR: 2.34e-04\n",
      "test loss: 0.0016\n",
      "Step 22100 | Loss: 0.00158 | LR: 2.33e-04\n",
      "Step 22125 | Loss: 0.00160 | LR: 2.31e-04\n",
      "Step 22150 | Loss: 0.00172 | LR: 2.30e-04\n",
      "Step 22175 | Loss: 0.00156 | LR: 2.29e-04\n",
      "test loss: 0.0017\n",
      "Step 22200 | Loss: 0.00163 | LR: 2.28e-04\n",
      "Step 22225 | Loss: 0.00164 | LR: 2.27e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0003.npz\n",
      "=== Step 22238 Done. Avg Loss: 0.00169 ===\n",
      "Step 22250 | Loss: 0.00163 | LR: 2.26e-04\n",
      "Step 22275 | Loss: 0.00160 | LR: 2.25e-04\n",
      "test loss: 0.0016\n",
      "Step 22300 | Loss: 0.00165 | LR: 2.24e-04\n",
      "Step 22325 | Loss: 0.00164 | LR: 2.23e-04\n",
      "Step 22350 | Loss: 0.00164 | LR: 2.22e-04\n",
      "Step 22375 | Loss: 0.00164 | LR: 2.21e-04\n",
      "test loss: 0.0016\n",
      "Step 22400 | Loss: 0.00166 | LR: 2.19e-04\n",
      "Step 22425 | Loss: 0.00167 | LR: 2.18e-04\n",
      "Step 22450 | Loss: 0.00170 | LR: 2.17e-04\n",
      "Step 22475 | Loss: 0.00167 | LR: 2.16e-04\n",
      "test loss: 0.0016\n",
      "Step 22500 | Loss: 0.00165 | LR: 2.15e-04\n",
      "Step 22525 | Loss: 0.00158 | LR: 2.14e-04\n",
      "Step 22550 | Loss: 0.00154 | LR: 2.13e-04\n",
      "Step 22575 | Loss: 0.00161 | LR: 2.12e-04\n",
      "test loss: 0.0016\n",
      "Step 22600 | Loss: 0.00165 | LR: 2.11e-04\n",
      "Step 22625 | Loss: 0.00152 | LR: 2.10e-04\n",
      "Step 22650 | Loss: 0.00172 | LR: 2.09e-04\n",
      "Step 22675 | Loss: 0.00174 | LR: 2.08e-04\n",
      "test loss: 0.0016\n",
      "Step 22700 | Loss: 0.00159 | LR: 2.07e-04\n",
      "Step 22725 | Loss: 0.00159 | LR: 2.06e-04\n",
      "Step 22750 | Loss: 0.00165 | LR: 2.05e-04\n",
      "Step 22775 | Loss: 0.00158 | LR: 2.04e-04\n",
      "test loss: 0.0016\n",
      "Step 22800 | Loss: 0.00160 | LR: 2.02e-04\n",
      "Step 22825 | Loss: 0.00167 | LR: 2.01e-04\n",
      "Step 22850 | Loss: 0.00166 | LR: 2.00e-04\n",
      "Step 22875 | Loss: 0.00170 | LR: 1.99e-04\n",
      "test loss: 0.0017\n",
      "Step 22900 | Loss: 0.00166 | LR: 1.98e-04\n",
      "Step 22925 | Loss: 0.00175 | LR: 1.97e-04\n",
      "Step 22950 | Loss: 0.00166 | LR: 1.96e-04\n",
      "Step 22975 | Loss: 0.00160 | LR: 1.95e-04\n",
      "test loss: 0.0017\n",
      "Step 23000 | Loss: 0.00172 | LR: 1.94e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0004.npz\n",
      "Step 23025 | Loss: 0.00165 | LR: 1.93e-04\n",
      "Step 23050 | Loss: 0.00162 | LR: 1.92e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0001.npz\n",
      "Step 23075 | Loss: 0.00164 | LR: 1.91e-04\n",
      "test loss: 0.0017\n",
      "Step 23100 | Loss: 0.00163 | LR: 1.90e-04\n",
      "Step 23125 | Loss: 0.00177 | LR: 1.89e-04\n",
      "Step 23150 | Loss: 0.00169 | LR: 1.88e-04\n",
      "Step 23175 | Loss: 0.00170 | LR: 1.87e-04\n",
      "test loss: 0.0016\n",
      "Step 23200 | Loss: 0.00170 | LR: 1.86e-04\n",
      "Step 23225 | Loss: 0.00161 | LR: 1.85e-04\n",
      "Step 23250 | Loss: 0.00181 | LR: 1.84e-04\n",
      "Step 23275 | Loss: 0.00166 | LR: 1.83e-04\n",
      "test loss: 0.0017\n",
      "Step 23300 | Loss: 0.00164 | LR: 1.82e-04\n",
      "Step 23325 | Loss: 0.00173 | LR: 1.81e-04\n",
      "Step 23350 | Loss: 0.00175 | LR: 1.80e-04\n",
      "Step 23375 | Loss: 0.00177 | LR: 1.79e-04\n",
      "test loss: 0.0017\n",
      "Step 23400 | Loss: 0.00169 | LR: 1.78e-04\n",
      "Step 23425 | Loss: 0.00182 | LR: 1.77e-04\n",
      "Step 23450 | Loss: 0.00165 | LR: 1.76e-04\n",
      "Step 23475 | Loss: 0.00181 | LR: 1.75e-04\n",
      "test loss: 0.0017\n",
      "Step 23500 | Loss: 0.00167 | LR: 1.74e-04\n",
      "Step 23525 | Loss: 0.00177 | LR: 1.73e-04\n",
      "Step 23550 | Loss: 0.00174 | LR: 1.72e-04\n",
      "Step 23575 | Loss: 0.00169 | LR: 1.71e-04\n",
      "test loss: 0.0017\n",
      "Step 23600 | Loss: 0.00185 | LR: 1.70e-04\n",
      "Step 23625 | Loss: 0.00197 | LR: 1.69e-04\n",
      "Step 23650 | Loss: 0.00170 | LR: 1.68e-04\n",
      "Step 23675 | Loss: 0.00181 | LR: 1.67e-04\n",
      "test loss: 0.0017\n",
      "Step 23700 | Loss: 0.00171 | LR: 1.66e-04\n",
      "Step 23725 | Loss: 0.00179 | LR: 1.65e-04\n",
      "Step 23750 | Loss: 0.00170 | LR: 1.64e-04\n",
      "Step 23775 | Loss: 0.00171 | LR: 1.63e-04\n",
      "test loss: 0.0017\n",
      "Step 23800 | Loss: 0.00174 | LR: 1.62e-04\n",
      "Step 23825 | Loss: 0.00165 | LR: 1.61e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0002.npz\n",
      "Step 23850 | Loss: 0.00179 | LR: 1.60e-04\n",
      "Step 23875 | Loss: 0.00175 | LR: 1.59e-04\n",
      "test loss: 0.0018\n",
      "Step 23900 | Loss: 0.00178 | LR: 1.59e-04\n",
      "Step 23925 | Loss: 0.00179 | LR: 1.58e-04\n",
      "Step 23950 | Loss: 0.00173 | LR: 1.57e-04\n",
      "Step 23975 | Loss: 0.00178 | LR: 1.56e-04\n",
      "test loss: 0.0017\n",
      "Step 24000 | Loss: 0.00175 | LR: 1.55e-04\n",
      "Step 24025 | Loss: 0.00186 | LR: 1.54e-04\n",
      "Step 24050 | Loss: 0.00183 | LR: 1.53e-04\n",
      "Step 24075 | Loss: 0.00181 | LR: 1.52e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/training/val/shard_k562e_val_0000.npz\n",
      "test loss: 0.0018\n",
      "Step 24100 | Loss: 0.00187 | LR: 1.51e-04\n",
      "Step 24125 | Loss: 0.00174 | LR: 1.50e-04\n",
      "Step 24150 | Loss: 0.00181 | LR: 1.49e-04\n",
      "Step 24175 | Loss: 0.00179 | LR: 1.48e-04\n",
      "test loss: 0.0018\n",
      "Step 24200 | Loss: 0.00176 | LR: 1.47e-04\n",
      "Step 24225 | Loss: 0.00179 | LR: 1.46e-04\n",
      "Step 24250 | Loss: 0.00186 | LR: 1.45e-04\n",
      "Step 24275 | Loss: 0.00193 | LR: 1.45e-04\n",
      "test loss: 0.0018\n",
      "Step 24300 | Loss: 0.00175 | LR: 1.44e-04\n",
      "Step 24325 | Loss: 0.00187 | LR: 1.43e-04\n",
      "Step 24350 | Loss: 0.00176 | LR: 1.42e-04\n",
      "Step 24375 | Loss: 0.00187 | LR: 1.41e-04\n",
      "test loss: 0.0018\n",
      "Step 24400 | Loss: 0.00181 | LR: 1.40e-04\n",
      "Step 24425 | Loss: 0.00182 | LR: 1.39e-04\n",
      "Step 24450 | Loss: 0.00180 | LR: 1.38e-04\n",
      "Step 24475 | Loss: 0.00180 | LR: 1.37e-04\n",
      "test loss: 0.0018\n",
      "Step 24500 | Loss: 0.00179 | LR: 1.36e-04\n",
      "Step 24525 | Loss: 0.00190 | LR: 1.35e-04\n",
      "Step 24550 | Loss: 0.00187 | LR: 1.35e-04\n",
      "Step 24575 | Loss: 0.00193 | LR: 1.34e-04\n",
      "test loss: 0.0019\n",
      "Step 24600 | Loss: 0.00186 | LR: 1.33e-04\n",
      "Step 24625 | Loss: 0.00190 | LR: 1.32e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0000.npz\n",
      "Step 24650 | Loss: 0.00186 | LR: 1.31e-04\n",
      "Step 24675 | Loss: 0.00185 | LR: 1.30e-04\n",
      "test loss: 0.0018\n",
      "Step 24700 | Loss: 0.00175 | LR: 1.29e-04\n",
      "Step 24725 | Loss: 0.00187 | LR: 1.28e-04\n",
      "Step 24750 | Loss: 0.00181 | LR: 1.28e-04\n",
      "Step 24775 | Loss: 0.00189 | LR: 1.27e-04\n",
      "test loss: 0.0018\n",
      "Step 24800 | Loss: 0.00189 | LR: 1.26e-04\n",
      "Step 24825 | Loss: 0.00192 | LR: 1.25e-04\n",
      "Step 24850 | Loss: 0.00180 | LR: 1.24e-04\n",
      "Step 24875 | Loss: 0.00179 | LR: 1.23e-04\n",
      "test loss: 0.0019\n",
      "Step 24900 | Loss: 0.00193 | LR: 1.22e-04\n",
      "Step 24925 | Loss: 0.00191 | LR: 1.22e-04\n",
      "Step 24950 | Loss: 0.00183 | LR: 1.21e-04\n",
      "Step 24975 | Loss: 0.00186 | LR: 1.20e-04\n",
      "test loss: 0.0018\n",
      "Step 25000 | Loss: 0.00189 | LR: 1.19e-04\n",
      "Step 25025 | Loss: 0.00200 | LR: 1.18e-04\n",
      "Step 25050 | Loss: 0.00191 | LR: 1.17e-04\n",
      "Step 25075 | Loss: 0.00188 | LR: 1.17e-04\n",
      "test loss: 0.0019\n",
      "Step 25100 | Loss: 0.00185 | LR: 1.16e-04\n",
      "Step 25125 | Loss: 0.00184 | LR: 1.15e-04\n",
      "Step 25150 | Loss: 0.00174 | LR: 1.14e-04\n",
      "Step 25175 | Loss: 0.00184 | LR: 1.13e-04\n",
      "test loss: 0.0018\n",
      "Step 25200 | Loss: 0.00184 | LR: 1.12e-04\n",
      "Step 25225 | Loss: 0.00185 | LR: 1.12e-04\n",
      "Step 25250 | Loss: 0.00195 | LR: 1.11e-04\n",
      "Step 25275 | Loss: 0.00189 | LR: 1.10e-04\n",
      "test loss: 0.0018\n",
      "Step 25300 | Loss: 0.00198 | LR: 1.09e-04\n",
      "Step 25325 | Loss: 0.00185 | LR: 1.08e-04\n",
      "Step 25350 | Loss: 0.00182 | LR: 1.07e-04\n",
      "Step 25375 | Loss: 0.00198 | LR: 1.07e-04\n",
      "test loss: 0.0019\n",
      "Step 25400 | Loss: 0.00179 | LR: 1.06e-04\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0003.npz\n",
      "=== Step 25415 Done. Avg Loss: 0.00176 ===\n",
      "Step 25425 | Loss: 0.00175 | LR: 1.05e-04\n",
      "Step 25450 | Loss: 0.00183 | LR: 1.04e-04\n",
      "Step 25475 | Loss: 0.00191 | LR: 1.03e-04\n",
      "test loss: 0.0019\n",
      "Step 25500 | Loss: 0.00193 | LR: 1.03e-04\n",
      "Step 25525 | Loss: 0.00199 | LR: 1.02e-04\n",
      "Step 25550 | Loss: 0.00192 | LR: 1.01e-04\n",
      "Step 25575 | Loss: 0.00194 | LR: 1.00e-04\n",
      "test loss: 0.0019\n",
      "Step 25600 | Loss: 0.00188 | LR: 9.96e-05\n",
      "Step 25625 | Loss: 0.00194 | LR: 9.88e-05\n",
      "Step 25650 | Loss: 0.00190 | LR: 9.80e-05\n",
      "Step 25675 | Loss: 0.00197 | LR: 9.72e-05\n",
      "test loss: 0.0019\n",
      "Step 25700 | Loss: 0.00184 | LR: 9.65e-05\n",
      "Step 25725 | Loss: 0.00194 | LR: 9.57e-05\n",
      "Step 25750 | Loss: 0.00199 | LR: 9.49e-05\n",
      "Step 25775 | Loss: 0.00189 | LR: 9.42e-05\n",
      "test loss: 0.0019\n",
      "Step 25800 | Loss: 0.00199 | LR: 9.34e-05\n",
      "Step 25825 | Loss: 0.00184 | LR: 9.27e-05\n",
      "Step 25850 | Loss: 0.00188 | LR: 9.19e-05\n",
      "Step 25875 | Loss: 0.00199 | LR: 9.12e-05\n",
      "test loss: 0.0019\n",
      "Step 25900 | Loss: 0.00199 | LR: 9.04e-05\n",
      "Step 25925 | Loss: 0.00194 | LR: 8.97e-05\n",
      "Step 25950 | Loss: 0.00203 | LR: 8.89e-05\n",
      "Step 25975 | Loss: 0.00182 | LR: 8.82e-05\n",
      "test loss: 0.0019\n",
      "Step 26000 | Loss: 0.00191 | LR: 8.74e-05\n",
      "Step 26025 | Loss: 0.00194 | LR: 8.67e-05\n",
      "Step 26050 | Loss: 0.00195 | LR: 8.60e-05\n",
      "Step 26075 | Loss: 0.00181 | LR: 8.53e-05\n",
      "test loss: 0.0019\n",
      "Step 26100 | Loss: 0.00183 | LR: 8.45e-05\n",
      "Step 26125 | Loss: 0.00206 | LR: 8.38e-05\n",
      "Step 26150 | Loss: 0.00194 | LR: 8.31e-05\n",
      "Step 26175 | Loss: 0.00191 | LR: 8.24e-05\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0001.npz\n",
      "test loss: 0.0019\n",
      "Step 26200 | Loss: 0.00192 | LR: 8.17e-05\n",
      "Step 26225 | Loss: 0.00197 | LR: 8.09e-05\n",
      "Step 26250 | Loss: 0.00193 | LR: 8.02e-05\n",
      "Step 26275 | Loss: 0.00196 | LR: 7.95e-05\n",
      "test loss: 0.0019\n",
      "Step 26300 | Loss: 0.00200 | LR: 7.88e-05\n",
      "Step 26325 | Loss: 0.00192 | LR: 7.81e-05\n",
      "Step 26350 | Loss: 0.00192 | LR: 7.74e-05\n",
      "Step 26375 | Loss: 0.00201 | LR: 7.67e-05\n",
      "test loss: 0.0019\n",
      "Step 26400 | Loss: 0.00191 | LR: 7.60e-05\n",
      "Step 26425 | Loss: 0.00195 | LR: 7.54e-05\n",
      "Step 26450 | Loss: 0.00199 | LR: 7.47e-05\n",
      "Step 26475 | Loss: 0.00192 | LR: 7.40e-05\n",
      "test loss: 0.0019\n",
      "Step 26500 | Loss: 0.00200 | LR: 7.33e-05\n",
      "Step 26525 | Loss: 0.00193 | LR: 7.26e-05\n",
      "Step 26550 | Loss: 0.00202 | LR: 7.20e-05\n",
      "Step 26575 | Loss: 0.00203 | LR: 7.13e-05\n",
      "test loss: 0.0020\n",
      "Step 26600 | Loss: 0.00202 | LR: 7.06e-05\n",
      "Step 26625 | Loss: 0.00209 | LR: 7.00e-05\n",
      "Step 26650 | Loss: 0.00202 | LR: 6.93e-05\n",
      "Step 26675 | Loss: 0.00196 | LR: 6.86e-05\n",
      "test loss: 0.0020\n",
      "Step 26700 | Loss: 0.00197 | LR: 6.80e-05\n",
      "Step 26725 | Loss: 0.00202 | LR: 6.73e-05\n",
      "Step 26750 | Loss: 0.00194 | LR: 6.67e-05\n",
      "Step 26775 | Loss: 0.00205 | LR: 6.60e-05\n",
      "test loss: 0.0020\n",
      "Step 26800 | Loss: 0.00196 | LR: 6.54e-05\n",
      "Step 26825 | Loss: 0.00199 | LR: 6.47e-05\n",
      "Step 26850 | Loss: 0.00201 | LR: 6.41e-05\n",
      "Step 26875 | Loss: 0.00203 | LR: 6.35e-05\n",
      "test loss: 0.0020\n",
      "Step 26900 | Loss: 0.00200 | LR: 6.28e-05\n",
      "Step 26925 | Loss: 0.00207 | LR: 6.22e-05\n",
      "Step 26950 | Loss: 0.00198 | LR: 6.16e-05\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0002.npz\n",
      "Step 26975 | Loss: 0.00203 | LR: 6.09e-05\n",
      "test loss: 0.0020\n",
      "Step 27000 | Loss: 0.00195 | LR: 6.03e-05\n",
      "Step 27025 | Loss: 0.00194 | LR: 5.97e-05\n",
      "Step 27050 | Loss: 0.00199 | LR: 5.91e-05\n",
      "Step 27075 | Loss: 0.00197 | LR: 5.85e-05\n",
      "test loss: 0.0020\n",
      "Step 27100 | Loss: 0.00191 | LR: 5.79e-05\n",
      "Step 27125 | Loss: 0.00195 | LR: 5.73e-05\n",
      "Step 27150 | Loss: 0.00201 | LR: 5.67e-05\n",
      "Step 27175 | Loss: 0.00196 | LR: 5.61e-05\n",
      "test loss: 0.0020\n",
      "Step 27200 | Loss: 0.00200 | LR: 5.55e-05\n",
      "Step 27225 | Loss: 0.00198 | LR: 5.49e-05\n",
      "Step 27250 | Loss: 0.00191 | LR: 5.43e-05\n",
      "Step 27275 | Loss: 0.00193 | LR: 5.37e-05\n",
      "test loss: 0.0020\n",
      "Step 27300 | Loss: 0.00196 | LR: 5.31e-05\n",
      "Step 27325 | Loss: 0.00204 | LR: 5.25e-05\n",
      "Step 27350 | Loss: 0.00192 | LR: 5.19e-05\n",
      "Step 27375 | Loss: 0.00199 | LR: 5.14e-05\n",
      "test loss: 0.0020\n",
      "Step 27400 | Loss: 0.00200 | LR: 5.08e-05\n",
      "Step 27425 | Loss: 0.00202 | LR: 5.02e-05\n",
      "Step 27450 | Loss: 0.00201 | LR: 4.97e-05\n",
      "Step 27475 | Loss: 0.00198 | LR: 4.91e-05\n",
      "test loss: 0.0020\n",
      "Step 27500 | Loss: 0.00210 | LR: 4.85e-05\n",
      "Step 27525 | Loss: 0.00205 | LR: 4.80e-05\n",
      "Step 27550 | Loss: 0.00205 | LR: 4.74e-05\n",
      "Step 27575 | Loss: 0.00207 | LR: 4.69e-05\n",
      "loading /Users/djemec/data/jepa/v0_2/training/val/shard_k562e_val_0000.npz\n",
      "test loss: 0.0021\n",
      "Step 27600 | Loss: 0.00199 | LR: 4.63e-05\n",
      "Step 27625 | Loss: 0.00200 | LR: 4.58e-05\n",
      "Step 27650 | Loss: 0.00202 | LR: 4.52e-05\n",
      "Step 27675 | Loss: 0.00204 | LR: 4.47e-05\n",
      "test loss: 0.0020\n",
      "Step 27700 | Loss: 0.00196 | LR: 4.42e-05\n",
      "Step 27725 | Loss: 0.00197 | LR: 4.36e-05\n",
      "Step 27750 | Loss: 0.00209 | LR: 4.31e-05\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0000.npz\n",
      "Step 27775 | Loss: 0.00200 | LR: 4.26e-05\n",
      "test loss: 0.0020\n",
      "Step 27800 | Loss: 0.00192 | LR: 4.20e-05\n",
      "Step 27825 | Loss: 0.00198 | LR: 4.15e-05\n",
      "Step 27850 | Loss: 0.00191 | LR: 4.10e-05\n",
      "Step 27875 | Loss: 0.00195 | LR: 4.05e-05\n",
      "test loss: 0.0020\n",
      "Step 27900 | Loss: 0.00210 | LR: 4.00e-05\n",
      "Step 27925 | Loss: 0.00198 | LR: 3.95e-05\n",
      "Step 27950 | Loss: 0.00198 | LR: 3.90e-05\n",
      "Step 27975 | Loss: 0.00203 | LR: 3.85e-05\n",
      "test loss: 0.0020\n",
      "Step 28000 | Loss: 0.00202 | LR: 3.80e-05\n",
      "Step 28025 | Loss: 0.00207 | LR: 3.75e-05\n",
      "Step 28050 | Loss: 0.00206 | LR: 3.70e-05\n",
      "Step 28075 | Loss: 0.00206 | LR: 3.65e-05\n",
      "test loss: 0.0020\n",
      "Step 28100 | Loss: 0.00192 | LR: 3.60e-05\n",
      "Step 28125 | Loss: 0.00196 | LR: 3.55e-05\n",
      "Step 28150 | Loss: 0.00193 | LR: 3.50e-05\n",
      "Step 28175 | Loss: 0.00209 | LR: 3.46e-05\n",
      "test loss: 0.0020\n",
      "Step 28200 | Loss: 0.00201 | LR: 3.41e-05\n",
      "Step 28225 | Loss: 0.00201 | LR: 3.36e-05\n",
      "Step 28250 | Loss: 0.00199 | LR: 3.32e-05\n",
      "Step 28275 | Loss: 0.00209 | LR: 3.27e-05\n",
      "test loss: 0.0020\n",
      "Step 28300 | Loss: 0.00212 | LR: 3.22e-05\n",
      "Step 28325 | Loss: 0.00196 | LR: 3.18e-05\n",
      "Step 28350 | Loss: 0.00203 | LR: 3.13e-05\n",
      "Step 28375 | Loss: 0.00204 | LR: 3.09e-05\n",
      "test loss: 0.0020\n",
      "Step 28400 | Loss: 0.00213 | LR: 3.04e-05\n",
      "Step 28425 | Loss: 0.00203 | LR: 3.00e-05\n",
      "Step 28450 | Loss: 0.00212 | LR: 2.95e-05\n",
      "Step 28475 | Loss: 0.00195 | LR: 2.91e-05\n",
      "test loss: 0.0020\n",
      "Step 28500 | Loss: 0.00196 | LR: 2.87e-05\n",
      "Step 28525 | Loss: 0.00204 | LR: 2.82e-05\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0004.npz\n",
      "Step 28550 | Loss: 0.00208 | LR: 2.78e-05\n",
      "Step 28575 | Loss: 0.00211 | LR: 2.74e-05\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0003.npz\n",
      "=== Step 28592 Done. Avg Loss: 0.00198 ===\n",
      "test loss: 0.0020\n",
      "Step 28600 | Loss: 0.00199 | LR: 2.69e-05\n",
      "Step 28625 | Loss: 0.00212 | LR: 2.65e-05\n",
      "Step 28650 | Loss: 0.00203 | LR: 2.61e-05\n",
      "Step 28675 | Loss: 0.00202 | LR: 2.57e-05\n",
      "test loss: 0.0021\n",
      "Step 28700 | Loss: 0.00203 | LR: 2.53e-05\n",
      "Step 28725 | Loss: 0.00202 | LR: 2.49e-05\n",
      "Step 28750 | Loss: 0.00202 | LR: 2.45e-05\n",
      "Step 28775 | Loss: 0.00198 | LR: 2.41e-05\n",
      "test loss: 0.0020\n",
      "Step 28800 | Loss: 0.00214 | LR: 2.37e-05\n",
      "Step 28825 | Loss: 0.00204 | LR: 2.33e-05\n",
      "Step 28850 | Loss: 0.00205 | LR: 2.29e-05\n",
      "Step 28875 | Loss: 0.00205 | LR: 2.25e-05\n",
      "test loss: 0.0020\n",
      "Step 28900 | Loss: 0.00211 | LR: 2.21e-05\n",
      "Step 28925 | Loss: 0.00202 | LR: 2.17e-05\n",
      "Step 28950 | Loss: 0.00210 | LR: 2.14e-05\n",
      "Step 28975 | Loss: 0.00198 | LR: 2.10e-05\n",
      "test loss: 0.0021\n",
      "Step 29000 | Loss: 0.00204 | LR: 2.06e-05\n",
      "Step 29025 | Loss: 0.00202 | LR: 2.02e-05\n",
      "Step 29050 | Loss: 0.00207 | LR: 1.99e-05\n",
      "Step 29075 | Loss: 0.00204 | LR: 1.95e-05\n",
      "test loss: 0.0020\n",
      "Step 29100 | Loss: 0.00202 | LR: 1.92e-05\n",
      "Step 29125 | Loss: 0.00201 | LR: 1.88e-05\n",
      "Step 29150 | Loss: 0.00196 | LR: 1.85e-05\n",
      "Step 29175 | Loss: 0.00201 | LR: 1.81e-05\n",
      "test loss: 0.0020\n",
      "Step 29200 | Loss: 0.00196 | LR: 1.78e-05\n",
      "Step 29225 | Loss: 0.00209 | LR: 1.74e-05\n",
      "Step 29250 | Loss: 0.00200 | LR: 1.71e-05\n",
      "Step 29275 | Loss: 0.00203 | LR: 1.67e-05\n",
      "test loss: 0.0021\n",
      "Step 29300 | Loss: 0.00198 | LR: 1.64e-05\n",
      "Step 29325 | Loss: 0.00202 | LR: 1.61e-05\n",
      "Step 29350 | Loss: 0.00211 | LR: 1.58e-05\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0000.npz\n",
      "Step 29375 | Loss: 0.00195 | LR: 1.54e-05\n",
      "test loss: 0.0020\n",
      "Step 29400 | Loss: 0.00206 | LR: 1.51e-05\n",
      "Step 29425 | Loss: 0.00202 | LR: 1.48e-05\n",
      "Step 29450 | Loss: 0.00206 | LR: 1.45e-05\n",
      "Step 29475 | Loss: 0.00206 | LR: 1.42e-05\n",
      "test loss: 0.0020\n",
      "Step 29500 | Loss: 0.00202 | LR: 1.39e-05\n",
      "Step 29525 | Loss: 0.00205 | LR: 1.36e-05\n",
      "Step 29550 | Loss: 0.00205 | LR: 1.33e-05\n",
      "Step 29575 | Loss: 0.00216 | LR: 1.30e-05\n",
      "test loss: 0.0020\n",
      "Step 29600 | Loss: 0.00204 | LR: 1.27e-05\n",
      "Step 29625 | Loss: 0.00204 | LR: 1.24e-05\n",
      "Step 29650 | Loss: 0.00198 | LR: 1.21e-05\n",
      "Step 29675 | Loss: 0.00203 | LR: 1.18e-05\n",
      "test loss: 0.0020\n",
      "Step 29700 | Loss: 0.00197 | LR: 1.15e-05\n",
      "Step 29725 | Loss: 0.00205 | LR: 1.13e-05\n",
      "Step 29750 | Loss: 0.00213 | LR: 1.10e-05\n",
      "Step 29775 | Loss: 0.00207 | LR: 1.07e-05\n",
      "test loss: 0.0020\n",
      "Step 29800 | Loss: 0.00206 | LR: 1.05e-05\n",
      "Step 29825 | Loss: 0.00219 | LR: 1.02e-05\n",
      "Step 29850 | Loss: 0.00204 | LR: 9.94e-06\n",
      "Step 29875 | Loss: 0.00218 | LR: 9.68e-06\n",
      "test loss: 0.0020\n",
      "Step 29900 | Loss: 0.00207 | LR: 9.43e-06\n",
      "Step 29925 | Loss: 0.00211 | LR: 9.18e-06\n",
      "Step 29950 | Loss: 0.00209 | LR: 8.93e-06\n",
      "Step 29975 | Loss: 0.00198 | LR: 8.69e-06\n",
      "test loss: 0.0021\n",
      "Step 30000 | Loss: 0.00210 | LR: 8.45e-06\n",
      "Step 30025 | Loss: 0.00204 | LR: 8.21e-06\n",
      "Step 30050 | Loss: 0.00211 | LR: 7.98e-06\n",
      "Step 30075 | Loss: 0.00200 | LR: 7.75e-06\n",
      "test loss: 0.0020\n",
      "Step 30100 | Loss: 0.00217 | LR: 7.52e-06\n",
      "Step 30125 | Loss: 0.00208 | LR: 7.30e-06\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0002.npz\n",
      "Step 30150 | Loss: 0.00210 | LR: 7.08e-06\n",
      "Step 30175 | Loss: 0.00200 | LR: 6.86e-06\n",
      "test loss: 0.0020\n",
      "Step 30200 | Loss: 0.00206 | LR: 6.65e-06\n",
      "Step 30225 | Loss: 0.00215 | LR: 6.44e-06\n",
      "Step 30250 | Loss: 0.00206 | LR: 6.23e-06\n",
      "Step 30275 | Loss: 0.00208 | LR: 6.03e-06\n",
      "test loss: 0.0020\n",
      "Step 30300 | Loss: 0.00195 | LR: 5.83e-06\n",
      "Step 30325 | Loss: 0.00198 | LR: 5.63e-06\n",
      "Step 30350 | Loss: 0.00204 | LR: 5.44e-06\n",
      "Step 30375 | Loss: 0.00207 | LR: 5.25e-06\n",
      "test loss: 0.0020\n",
      "Step 30400 | Loss: 0.00206 | LR: 5.06e-06\n",
      "Step 30425 | Loss: 0.00194 | LR: 4.88e-06\n",
      "Step 30450 | Loss: 0.00210 | LR: 4.70e-06\n",
      "Step 30475 | Loss: 0.00211 | LR: 4.53e-06\n",
      "test loss: 0.0021\n",
      "Step 30500 | Loss: 0.00208 | LR: 4.35e-06\n",
      "Step 30525 | Loss: 0.00203 | LR: 4.18e-06\n",
      "Step 30550 | Loss: 0.00207 | LR: 4.02e-06\n",
      "Step 30575 | Loss: 0.00207 | LR: 3.85e-06\n",
      "test loss: 0.0020\n",
      "Step 30600 | Loss: 0.00198 | LR: 3.69e-06\n",
      "Step 30625 | Loss: 0.00209 | LR: 3.54e-06\n",
      "Step 30650 | Loss: 0.00199 | LR: 3.39e-06\n",
      "Step 30675 | Loss: 0.00208 | LR: 3.24e-06\n",
      "test loss: 0.0021\n",
      "Step 30700 | Loss: 0.00212 | LR: 3.09e-06\n",
      "Step 30725 | Loss: 0.00202 | LR: 2.95e-06\n",
      "Step 30750 | Loss: 0.00210 | LR: 2.81e-06\n",
      "Step 30775 | Loss: 0.00208 | LR: 2.67e-06\n",
      "test loss: 0.0020\n",
      "Step 30800 | Loss: 0.00200 | LR: 2.54e-06\n",
      "Step 30825 | Loss: 0.00203 | LR: 2.41e-06\n",
      "Step 30850 | Loss: 0.00199 | LR: 2.28e-06\n",
      "Step 30875 | Loss: 0.00202 | LR: 2.16e-06\n",
      "test loss: 0.0020\n",
      "Step 30900 | Loss: 0.00206 | LR: 2.04e-06\n",
      "Step 30925 | Loss: 0.00202 | LR: 1.93e-06\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0004.npz\n",
      "Step 30950 | Loss: 0.00200 | LR: 1.82e-06\n",
      "Step 30975 | Loss: 0.00203 | LR: 1.71e-06\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0001.npz\n",
      "loading /Users/djemec/data/jepa/v0_2/training/val/shard_k562e_val_0000.npz\n",
      "test loss: 0.0020\n",
      "Step 31000 | Loss: 0.00201 | LR: 1.60e-06\n",
      "Step 31025 | Loss: 0.00201 | LR: 1.50e-06\n",
      "Step 31050 | Loss: 0.00212 | LR: 1.40e-06\n",
      "Step 31075 | Loss: 0.00208 | LR: 1.30e-06\n",
      "test loss: 0.0020\n",
      "Step 31100 | Loss: 0.00208 | LR: 1.21e-06\n",
      "Step 31125 | Loss: 0.00206 | LR: 1.12e-06\n",
      "Step 31150 | Loss: 0.00206 | LR: 1.04e-06\n",
      "Step 31175 | Loss: 0.00202 | LR: 9.56e-07\n",
      "test loss: 0.0020\n",
      "Step 31200 | Loss: 0.00207 | LR: 8.78e-07\n",
      "Step 31225 | Loss: 0.00211 | LR: 8.02e-07\n",
      "Step 31250 | Loss: 0.00198 | LR: 7.31e-07\n",
      "Step 31275 | Loss: 0.00204 | LR: 6.62e-07\n",
      "test loss: 0.0020\n",
      "Step 31300 | Loss: 0.00200 | LR: 5.97e-07\n",
      "Step 31325 | Loss: 0.00197 | LR: 5.35e-07\n",
      "Step 31350 | Loss: 0.00208 | LR: 4.77e-07\n",
      "Step 31375 | Loss: 0.00205 | LR: 4.22e-07\n",
      "test loss: 0.0020\n",
      "Step 31400 | Loss: 0.00213 | LR: 3.71e-07\n",
      "Step 31425 | Loss: 0.00207 | LR: 3.23e-07\n",
      "Step 31450 | Loss: 0.00203 | LR: 2.78e-07\n",
      "Step 31475 | Loss: 0.00204 | LR: 2.37e-07\n",
      "test loss: 0.0020\n",
      "Step 31500 | Loss: 0.00207 | LR: 1.99e-07\n",
      "Step 31525 | Loss: 0.00201 | LR: 1.64e-07\n",
      "Step 31550 | Loss: 0.00199 | LR: 1.33e-07\n",
      "Step 31575 | Loss: 0.00198 | LR: 1.05e-07\n",
      "test loss: 0.0020\n",
      "Step 31600 | Loss: 0.00201 | LR: 8.04e-08\n",
      "Step 31625 | Loss: 0.00205 | LR: 5.94e-08\n",
      "Step 31650 | Loss: 0.00202 | LR: 4.17e-08\n",
      "Step 31675 | Loss: 0.00205 | LR: 2.74e-08\n",
      "test loss: 0.0021\n",
      "Step 31700 | Loss: 0.00204 | LR: 1.65e-08\n",
      "Step 31725 | Loss: 0.00200 | LR: 9.01e-09\n",
      "Step 31750 | Loss: 0.00198 | LR: 4.88e-09\n",
      "loading /Users/djemec/data/jepa/v0_2/training/train/shard_k562e_train_0004.npz\n",
      "test loss: 0.0020\n",
      "=== Step 31769 Done. Avg Loss: 0.00204 ===\n"
     ]
    }
   ],
   "source": [
    "for step in range(max_steps):\n",
    "\n",
    "    last_step = (step == max_steps - 1)\n",
    "\n",
    "    # once in a while evaluate our validation set loss\n",
    "    if step % 100 == 0 or last_step:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_loss_accum = 0.0\n",
    "            test_loss_steps = 10\n",
    "            for i in range(test_loss_steps):\n",
    "                xc, xct, xt, xtt, aid = val_loader.next_batch()\n",
    "                xc, xct, xt, xtt, aid = xc.to(DEVICE), xct.to(DEVICE), xt.to(DEVICE), xtt.to(DEVICE), aid.to(DEVICE)\n",
    "                loss = model(xc, xct, xt, xtt, aid)\n",
    "                loss = loss / test_loss_steps\n",
    "                test_loss_accum += loss.detach()\n",
    "\n",
    "        print(f'test loss: {test_loss_accum.item():.4f}')\n",
    "        # with open(log_file, \"a\") as f:\n",
    "        #    f.write(f'{step} test {test_loss_accum.item():.4f}\\n')\n",
    "\n",
    "\n",
    "    if step > 0 and (step+1) % steps_per_epoch ==0 and not last_step:\n",
    "        # Save Checkpoint\n",
    "        torch.save({\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'step': step\n",
    "        }, checkpoint_dir / f'bio_jepa_ckpt_{step}.pt')\n",
    "\n",
    "\n",
    "    model.train()\n",
    "    xc, xct, xt, xtt, aid = train_loader.next_batch()\n",
    "    xc, xct, xt, xtt, aid = xc.to(DEVICE), xct.to(DEVICE), xt.to(DEVICE), xtt.to(DEVICE), aid.to(DEVICE)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss = model(xc, xct, xt, xtt, aid)\n",
    "    loss.backward()\n",
    "\n",
    "    # gradient clipping\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Update Teacher (V-JEPA Momentum)\n",
    "    model.update_teacher()\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    lossi.append(loss.item())\n",
    "    total_epoch_loss += loss.item()\n",
    "    \n",
    "    if step % 25 == 0:\n",
    "        print(f\"Step {step} | Loss: {loss.item():.5f} | LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "    \n",
    "    \n",
    "    if step > 0 and (step+1) % steps_per_epoch == 0:   \n",
    "        avg_loss = total_epoch_loss / steps_per_epoch\n",
    "        print(f\"=== Step {step} Done. Avg Loss: {avg_loss:.5f} ===\")\n",
    "        total_epoch_loss = 0\n",
    "    \n",
    "    \n",
    "    if last_step:\n",
    "        # Save Checkpoint\n",
    "        torch.save({\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'step': step\n",
    "        }, checkpoint_dir / f'bio_jepa_ckpt_{step}_final.pt')\n",
    "\n",
    "    step += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4972cdbf-7fd0-4b3b-a541-16135d079c2c",
   "metadata": {},
   "source": [
    "#### Training Loss Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "43c293fa-2067-45b9-ad55-e6127515bb37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lossi[:])\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861fe89a-0447-48d3-9252-9153efcab902",
   "metadata": {},
   "source": [
    "## Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45916c9b-5f07-4aec-a9ac-e0635bae92a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dir = tok_dir / 'val'\n",
    "pathway_names_path = data_dir / 'pathway_names.json'\n",
    "gene_names_path = data_dir / 'gene_names.json'\n",
    "metadata_path = pert_dir / 'perturbation_map.json'\n",
    "pathway_names_path, gene_names_path, metadata_path, val_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c35355-2ff6-486d-9ce8-e4cbbbee1599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load Map (ID -> Name)\n",
    "with open(metadata_path, \"r\") as f:\n",
    "    pert_map = json.load(f)\n",
    "id_to_pert = {v: k for k, v in pert_map.items()}\n",
    "\n",
    "# with open(pathway_names_path, \"r\") as f:\n",
    "#     pathway_names = json.load(f) # List of 1024 names\n",
    "\n",
    "with open(data_dir / 'gene_names.json', \"r\") as f:\n",
    "    gene_names = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ae854c-4e74-42f5-9cfd-0c487ff22549",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09550430-309d-436c-a140-2bda3fb8bde3",
   "metadata": {},
   "source": [
    "**Get random data sample**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189729ef-8852-466c-b05b-455948aa6cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_test_pair(shard_dir):\n",
    "    '''Grab a single real pair from a random shard with Context'''\n",
    "    files = sorted(shard_dir.glob('*.npz'))\n",
    "        \n",
    "    file_path = files[np.random.randint(len(files))]\n",
    "    \n",
    "    with np.load(file_path) as data:\n",
    "        idx = np.random.randint(data['action_ids'].shape[0])\n",
    "        \n",
    "        # Extract Items (Now 5 items instead of 3)\n",
    "        c_raw = data['control'][idx]         # [2000]\n",
    "        ct_raw = data['control_total'][idx]  # Scalar\n",
    "        case_raw = data['case'][idx]         # [2000]\n",
    "        caset_raw = data['case_total'][idx]  # Scalar\n",
    "        act_id = data['action_ids'][idx]     # Scalar\n",
    "        \n",
    "    # Convert to Tensor & Add Batch Dim [1, ...]\n",
    "    xc = torch.tensor(c_raw).float().unsqueeze(0).to(DEVICE)\n",
    "    xct = torch.tensor(ct_raw).float().unsqueeze(0).to(DEVICE)\n",
    "    xt = torch.tensor(case_raw).float().unsqueeze(0).to(DEVICE)\n",
    "    xtt = torch.tensor(caset_raw).float().unsqueeze(0).to(DEVICE)\n",
    "    aid = torch.tensor([act_id]).long().to(DEVICE)\n",
    "    \n",
    "    return xc, xct, xt, xtt, aid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793361fe-baee-42b7-982d-72e604d28013",
   "metadata": {},
   "outputs": [],
   "source": [
    "xc, xct, xt, xtt, aid = get_random_test_pair(val_dir)\n",
    "pert_name = id_to_pert[aid.item()]\n",
    "pert_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e32d24-4c56-40fc-9d76-3048536eddb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Teacher sees the FUTURE (Real Treated Case)\n",
    "    # Output: [1, 1024, 384]\n",
    "    z_case = model.teacher(xt, xtt)       \n",
    "    \n",
    "    # Student sees the PAST (Control)\n",
    "    # Output: [1, 1024, 384]\n",
    "    z_context = model.student(xc, xct)\n",
    "    \n",
    "    # Control Baseline (Where we started in Teacher Space)\n",
    "    # We run control through teacher to get the \"Anchor\"\n",
    "    z_control = model.teacher(xc, xct) \n",
    "\n",
    "    # Predictor tries to guess Real from Context + Action\n",
    "    z_predicted = model.predictor(z_context, aid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5360c5-2215-4fc8-858a-d1488cf54ab6",
   "metadata": {},
   "source": [
    "**Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e38d09-e796-4d5f-882b-f6fcfe150974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Latent Movement\n",
    "delta_latent = (z_predicted - z_control).squeeze(0) # [1024, 384]\n",
    "delta_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0658ae95-4083-430e-8e7b-cf9d329f0b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Project \"Pathway Energy\" back to Genes ---\n",
    "# We want to score every gene based on how much its parent pathways moved.\n",
    "# Logic: Gene_Score = Sum(Mask_gp * Magnitude(Pathway_p))\n",
    "# If a gene is in 5 pathways that all moved violently, that gene is highly implicated.\n",
    "pathway_magnitudes = delta_latent.norm(dim=1) \n",
    "pathway_magnitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cae5596-43cc-4b2b-b01c-1f9bc7807c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Access the Learnable Weights\n",
    "# Shape in model: [1024 Pathways, 2000 Genes]\n",
    "# We use .detach() because we are in eval mode and don't want gradients\n",
    "weights = model.student.pathway_weights.detach()\n",
    "\n",
    "# 2. Transpose back to [Genes, Pathways] for the projection\n",
    "# We use .abs() because a strong negative weight (-5.0) implies \n",
    "# just as much biological \"impact\" as a strong positive one.\n",
    "routing_matrix = weights.T.abs() # Shape: [2000, 1024]\n",
    "routing_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcfdb61-d960-4b42-a8e9-83660d1b4035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project: [ Genes, 1024 Pathways] @ [1024 Magnitudes] -> [ Gene Scores]\n",
    "# This tells us the \"Implied Impact\" on each gene\n",
    "gene_impact_scores = torch.mv(routing_matrix, pathway_magnitudes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d91232c-aaa3-473c-aa75-bbf6953b4850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Get Top Predicted Genes ---\n",
    "top_k = 10\n",
    "pred_values, pred_indices = torch.topk(gene_impact_scores, top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43289f5c-461b-4d98-ab5e-b6ceda99d24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Based on the pathway changes, the model predicts these genes are most affected:')\n",
    "for val, idx in zip(pred_values, pred_indices):\n",
    "    g_name = gene_names[idx.item()]\n",
    "    print(f\"   {g_name:<10} (Score: {val.item():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df811e83-621e-4538-a44a-65a4f7b94d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. REALITY CHECK (The most important part) ---\n",
    "# Did these genes ACTUALLY change in the real data?\n",
    "# Let's compare the Model's \"Top Genes\" vs the Reality's \"Top Genes\"\n",
    "# Calculate Real Gene Change (Log Space)\n",
    "# Shape: [1, 2000]\n",
    "real_gene_delta = (xt - xc).abs().squeeze(0) \n",
    "# Get the Top 10 Genes that REALLY changed\n",
    "real_values, real_indices = torch.topk(real_gene_delta, top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef3b5d1-9b08-4653-b922-15db8938f291",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Ground Truth')\n",
    "for val, idx in zip(real_values, real_indices):\n",
    "    g_name = gene_names[idx.item()]\n",
    "    \n",
    "    # Check if our model also listed this gene in its top 10\n",
    "    match_icon = \"\" if idx in pred_indices else \"\"\n",
    "    \n",
    "    print(f\"  {match_icon} {g_name:<10} (Delta: {val.item():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1832c3-d09e-4e23-a1aa-934986ccb0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Precision@10 Metric ---\n",
    "# How many of the Top 10 real genes were in the Top 10 predicted genes?\n",
    "# (Set intersection)\n",
    "set_pred = set(pred_indices.tolist())\n",
    "set_real = set(real_indices.tolist())\n",
    "overlap = len(set_pred.intersection(set_real))\n",
    "\n",
    "print(f\"\\n[Precision @ 10]\")\n",
    "print(f\"The model correctly identified {overlap}/10 of the top driver genes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc7a4bc-88e5-4903-bbad-6ffa382db26a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30288334-6f6f-48a3-96e3-4f9144215288",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
