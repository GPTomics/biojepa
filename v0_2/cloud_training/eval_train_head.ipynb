{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71522d3b-8805-48e6-bb36-1c9a7bd15482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import pearsonr\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "\n",
    "import biojepa_ac_model as model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe98e32-54b3-4da6-adf9-8b3e76e3bc0b",
   "metadata": {},
   "source": [
    "## BioJEPA Model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bae5c1c-d7c4-4216-ab99-42a26b7aee75",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f67335c-63ea-4f80-9c10-96b4d3e09c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    device = 'cpu'\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(1337)\n",
    "        device = 'cuda'\n",
    "    # elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    #     device = 'mps'\n",
    "    print(f'using {device}')\n",
    "    return device\n",
    "\n",
    "DEVICE = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5ec64e1-0d7a-4dee-b478-b99f901ce9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "n_embd = 256\n",
    "action_n_embd = 256\n",
    "n_pathways = 1024\n",
    "training_file_chunk = 25000\n",
    "pretraining_file_chunk = 50000\n",
    "n_heads = 4\n",
    "n_layers = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "025d6281-f460-46eb-89d9-842d1ca6fa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('/home/ubuntu')\n",
    "train_dir = data_dir / 'training'\n",
    "pretrain_dir = data_dir / 'pretraining'\n",
    "mask_path = data_dir / 'binary_pathway_mask.npy'\n",
    "checkpoint_dir = data_dir / 'checkpoint'\n",
    "pert_dir = data_dir / 'pert_embd'\n",
    "pert_embd_path = pert_dir / 'action_embeddings_esm2.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9671701-5cc4-4016-a354-b71238c98947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Pathway Mask...\n",
      "Mask Loaded: 5000 Genes -> 1024 Pathways\n",
      "Loading Action Embedding ...\n",
      "Bank Loaded. Shape: (1087, 320)\n"
     ]
    }
   ],
   "source": [
    "print('Loading Pathway Mask...')\n",
    "binary_mask = np.load(mask_path)\n",
    "N_GENES, N_PATHWAYS = binary_mask.shape\n",
    "print(f'Mask Loaded: {N_GENES} Genes -> {N_PATHWAYS} Pathways')\n",
    "\n",
    "print('Loading Action Embedding ...')\n",
    "pert_embd = np.load(pert_embd_path)\n",
    "print(f'Bank Loaded. Shape: {pert_embd.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fe46b1b-aff7-4355-909f-9f71761f11c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high')\n",
    "config = model.BioJepaConfig(\n",
    "    mask_matrix=binary_mask, \n",
    "    num_genes=N_GENES,\n",
    "    num_pathways=N_PATHWAYS,\n",
    "    embed_dim=n_embd,\n",
    "    action_embed_dim= action_n_embd,\n",
    "    n_layer=n_layers,\n",
    "    heads=n_heads,\n",
    "    n_pre_layer = n_layers\n",
    ")\n",
    "model = model.BioJepa(config, pert_embd=pert_embd).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a609e628-943e-41c2-a4d7-a71b9075a640",
   "metadata": {},
   "source": [
    "**Load Checkpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4aac628e-6ab7-4768-955a-3f99e79f58ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = checkpoint_dir / 'bio_jepa_ckpt_7939_final.pt'\n",
    "checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "\n",
    "keys = model.load_state_dict(checkpoint['model'])\n",
    "keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8287a64f-9d0d-4fbe-8c9b-a03c463a1487",
   "metadata": {},
   "source": [
    "**Freeze Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63812672-368f-483a-be2a-8b65cc849a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c98b4a-5df8-4894-9c56-ae0dbcbc3acd",
   "metadata": {},
   "source": [
    "## Build Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3dec1f3-d9cc-4849-aaa8-6a7d60199432",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BenchmarkDecoderConfig:\n",
    "    embed_dim: int = 384\n",
    "    num_pathways: int = 1024\n",
    "    num_genes: int = 4096\n",
    "    \n",
    "class BenchmarkDecoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # Step 1: Collapse the embedding dimension (384 -> 1)\n",
    "        # This asks: \"How active is this pathway overall?\"\n",
    "        self.pool = nn.Linear(config.embed_dim, 1) \n",
    "        \n",
    "        # Step 2: Decode Pathway Activity -> Gene Expression\n",
    "        # This learns the specific contribution of each pathway to each gene\n",
    "        self.decode = nn.Linear(config.num_pathways, config.num_genes) \n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None: \n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        \n",
    "    def forward(self, latents):\n",
    "        # latents: [Batch, 1024, 384]\n",
    "        \n",
    "        # 1. Calculate Pathway Scores\n",
    "        # [B, 1024, 384] -> [B, 1024, 1] -> [B, 1024]\n",
    "        scores = self.pool(latents).squeeze(-1)\n",
    "        \n",
    "        # 2. Project to Genes\n",
    "        # [B, 1024] @ [1024, 2000] -> [B, 2000]\n",
    "        gene_preds = self.decode(scores)\n",
    "        \n",
    "        return gene_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c316fe59-e4aa-4d12-9db4-2e38781f53bc",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bfad6fe-82ef-4019-b0ae-0338089f11c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_shard(filename):\n",
    "    print(f'loading {filename}') # Optional: reduce noise\n",
    "    with np.load(filename) as data:\n",
    "        # Load all arrays into memory\n",
    "        # We convert to correct types immediately to save hassle later\n",
    "        control_x = data['control'].astype(np.float32)\n",
    "        control_tot = data['control_total'].astype(np.float32)\n",
    "        case_x = data['case'].astype(np.float32)\n",
    "        case_tot = data['case_total'].astype(np.float32)\n",
    "        action_ids = data['action_ids'].astype(np.int64)\n",
    "        \n",
    "    return control_x, control_tot, case_x, case_tot, action_ids\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, split, device):\n",
    "        self.B = B\n",
    "        self.split = split\n",
    "        self.device = device\n",
    "        \n",
    "        # 1. Find Shards\n",
    "        data_root = train_dir / f'{split}'\n",
    "        shards = list(data_root.glob('*.npz'))\n",
    "\n",
    "        self.total_files = len(shards)\n",
    "        self.shards = sorted(shards)\n",
    "\n",
    "        assert len(shards) > 0, f'no shards found for split {split}'\n",
    "        print(f'found {len(shards)} shards for split {split}')\n",
    "        \n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # Create a randomized queue of shards\n",
    "        self.remaining_shards = list(self.shards)\n",
    "        random.shuffle(self.remaining_shards)\n",
    "        \n",
    "        self.current_shard_idx = -1\n",
    "        self.load_next_shard()\n",
    "\n",
    "    def load_next_shard(self):\n",
    "        self.current_shard_idx += 1\n",
    "        \n",
    "        # If we ran out of shards, reset (Epoch done)\n",
    "        if self.current_shard_idx >= len(self.remaining_shards):\n",
    "            self.reset() # This resets shard_idx to -1 and reshuffles\n",
    "            return \n",
    "\n",
    "        # Load the file\n",
    "        filename = self.remaining_shards[self.current_shard_idx]\n",
    "        self.data_tuple = load_shard(filename)\n",
    "        \n",
    "        # Shuffle the items INSIDE the shard\n",
    "        # This is critical so we don't just memorize the sorted order of the shard\n",
    "        n_samples = len(self.data_tuple[0])\n",
    "        self.perm = np.random.permutation(n_samples)\n",
    "        self.current_position = 0\n",
    "        self.total_samples_in_shard = n_samples\n",
    "\n",
    "    def next_batch(self):\n",
    "        B = self.B\n",
    "        \n",
    "        # Check if we have enough data left in current shard\n",
    "        if self.current_position + B > self.total_samples_in_shard:\n",
    "            self.load_next_shard()\n",
    "            # Recursively call to get batch from the new shard\n",
    "            return self.next_batch()\n",
    "            \n",
    "        # Get indices for this batch\n",
    "        indices = self.perm[self.current_position : self.current_position + B]\n",
    "        self.current_position += B\n",
    "        \n",
    "        # Slice data using the shuffled indices\n",
    "        # data_tuple structure: (xc, xct, xt, xtt, aid)\n",
    "        batch_xc  = torch.from_numpy(self.data_tuple[0][indices]).to(self.device)\n",
    "        batch_xct = torch.from_numpy(self.data_tuple[1][indices]).to(self.device)\n",
    "        batch_xt  = torch.from_numpy(self.data_tuple[2][indices]).to(self.device)\n",
    "        batch_xtt = torch.from_numpy(self.data_tuple[3][indices]).to(self.device)\n",
    "        batch_aid = torch.from_numpy(self.data_tuple[4][indices]).to(self.device)\n",
    "        \n",
    "        return batch_xc, batch_xct, batch_xt, batch_xtt, batch_aid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7f7811-5727-45e3-8194-505072bb16eb",
   "metadata": {},
   "source": [
    "**Data Loader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e1d5000-f278-4f9b-9227-9554f31e9a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 3 shards for split train\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "found 1 shards for split val\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoaderLite(B=BATCH_SIZE, split='train', device=DEVICE)\n",
    "val_loader = DataLoaderLite(B=BATCH_SIZE, split='val', device=DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9025bf-05d7-4ff8-ae9a-c1b87471c3e5",
   "metadata": {},
   "source": [
    "## Training Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f484f1f2-2933-413d-80c0-f0c970656453",
   "metadata": {},
   "source": [
    "### Training Config/Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad700ee3-e710-43de-be89-4003c922a41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_decoder = 5e-3\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0494e7de-e96d-4989-b47b-9229c050a263",
   "metadata": {},
   "source": [
    "**Initialize Decoder** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74224c15-ff49-41d9-9552-b8ec9ed83dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BenchmarkDecoderConfig(\n",
    "    embed_dim= n_embd,\n",
    "    num_pathways= N_PATHWAYS,\n",
    "    num_genes= N_GENES\n",
    ")\n",
    "\n",
    "decoder = BenchmarkDecoder(config).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2da30fa-9c44-40be-921c-5184c764f0e5",
   "metadata": {},
   "source": [
    "**Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b197e689-8b98-4cf9-8382-6116bf2ce440",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(decoder.parameters(), lr=lr_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958f11cf-8c9f-49b9-b526-74be69d2126f",
   "metadata": {},
   "source": [
    "**Training Lenght**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "009526cc-7703-497d-87cb-4a50f9c1c1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_total_examples = 101682\n",
    "val_total_examples = 11044\n",
    "test_total_examples = 38829"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "766a254e-2fd7-4274-91c4-7ceab21d8d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(397, 7940)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch = train_total_examples // BATCH_SIZE\n",
    "max_steps = epochs * steps_per_epoch\n",
    "steps_per_epoch, max_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cf4803-b42e-4d1b-89db-c9b43d6e3558",
   "metadata": {},
   "source": [
    "**Scheduler**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c683627d-3da6-45e9-9827-06d8586a1f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=lr_decoder, total_steps=max_steps, pct_start=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15c5c7c-b741-486e-9ce2-99c2e253b05d",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82cb876c-e45d-43f7-94aa-2fa7f44b15c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossi = []\n",
    "step = 0\n",
    "total_epoch_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69468a01-365b-42b9-b041-5d6969e5b4ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss: 0.9122\n",
      "Step 0 | Loss: 0.92179 | LR: 2.00e-04\n",
      "Step 25 | Loss: 0.90524 | LR: 2.51e-04\n",
      "Step 50 | Loss: 0.90643 | LR: 3.94e-04\n",
      "Step 75 | Loss: 0.91473 | LR: 6.23e-04\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.9040\n",
      "Step 100 | Loss: 0.90751 | LR: 9.30e-04\n",
      "Step 125 | Loss: 0.90910 | LR: 1.30e-03\n",
      "Step 150 | Loss: 0.90989 | LR: 1.73e-03\n",
      "Step 175 | Loss: 0.91805 | LR: 2.18e-03\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "val loss: 0.9031\n",
      "Step 200 | Loss: 0.91066 | LR: 2.66e-03\n",
      "Step 225 | Loss: 0.92395 | LR: 3.13e-03\n",
      "Step 250 | Loss: 0.90408 | LR: 3.58e-03\n",
      "Step 275 | Loss: 0.90168 | LR: 3.99e-03\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.8973\n",
      "Step 300 | Loss: 0.90149 | LR: 4.35e-03\n",
      "Step 325 | Loss: 0.89856 | LR: 4.64e-03\n",
      "Step 350 | Loss: 0.88182 | LR: 4.85e-03\n",
      "Step 375 | Loss: 0.88831 | LR: 4.97e-03\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "=== Step 396 Done. Avg Loss: 0.90185 ===\n",
      "val loss: 0.8812\n",
      "Step 400 | Loss: 0.87310 | LR: 5.00e-03\n",
      "Step 425 | Loss: 0.86218 | LR: 5.00e-03\n",
      "Step 450 | Loss: 0.87141 | LR: 5.00e-03\n",
      "Step 475 | Loss: 0.85365 | LR: 5.00e-03\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.8627\n",
      "Step 500 | Loss: 0.84177 | LR: 5.00e-03\n",
      "Step 525 | Loss: 0.88138 | LR: 5.00e-03\n",
      "Step 550 | Loss: 0.85995 | LR: 4.99e-03\n",
      "Step 575 | Loss: 0.85747 | LR: 4.99e-03\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.8507\n",
      "Step 600 | Loss: 0.84286 | LR: 4.99e-03\n",
      "Step 625 | Loss: 0.85491 | LR: 4.99e-03\n",
      "Step 650 | Loss: 0.85687 | LR: 4.99e-03\n",
      "Step 675 | Loss: 0.84076 | LR: 4.98e-03\n",
      "val loss: 0.8410\n",
      "Step 700 | Loss: 0.82967 | LR: 4.98e-03\n",
      "Step 725 | Loss: 0.83350 | LR: 4.98e-03\n",
      "Step 750 | Loss: 0.82855 | LR: 4.97e-03\n",
      "Step 775 | Loss: 0.83496 | LR: 4.97e-03\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "=== Step 793 Done. Avg Loss: 0.85105 ===\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.8351\n",
      "Step 800 | Loss: 0.83183 | LR: 4.96e-03\n",
      "Step 825 | Loss: 0.82516 | LR: 4.96e-03\n",
      "Step 850 | Loss: 0.82039 | LR: 4.96e-03\n",
      "Step 875 | Loss: 0.82023 | LR: 4.95e-03\n",
      "val loss: 0.8338\n",
      "Step 900 | Loss: 0.81543 | LR: 4.94e-03\n",
      "Step 925 | Loss: 0.83249 | LR: 4.94e-03\n",
      "Step 950 | Loss: 0.82670 | LR: 4.93e-03\n",
      "Step 975 | Loss: 0.82663 | LR: 4.93e-03\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.8269\n",
      "Step 1000 | Loss: 0.81636 | LR: 4.92e-03\n",
      "Step 1025 | Loss: 0.80375 | LR: 4.91e-03\n",
      "Step 1050 | Loss: 0.81816 | LR: 4.91e-03\n",
      "Step 1075 | Loss: 0.82969 | LR: 4.90e-03\n",
      "val loss: 0.8239\n",
      "Step 1100 | Loss: 0.82046 | LR: 4.89e-03\n",
      "Step 1125 | Loss: 0.82097 | LR: 4.89e-03\n",
      "Step 1150 | Loss: 0.81189 | LR: 4.88e-03\n",
      "Step 1175 | Loss: 0.82473 | LR: 4.87e-03\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "=== Step 1190 Done. Avg Loss: 0.82188 ===\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.8207\n",
      "Step 1200 | Loss: 0.83696 | LR: 4.86e-03\n",
      "Step 1225 | Loss: 0.83091 | LR: 4.85e-03\n",
      "Step 1250 | Loss: 0.81278 | LR: 4.84e-03\n",
      "Step 1275 | Loss: 0.81854 | LR: 4.83e-03\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.8197\n",
      "Step 1300 | Loss: 0.81282 | LR: 4.82e-03\n",
      "Step 1325 | Loss: 0.80038 | LR: 4.81e-03\n",
      "Step 1350 | Loss: 0.81185 | LR: 4.80e-03\n",
      "Step 1375 | Loss: 0.80649 | LR: 4.79e-03\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "val loss: 0.8190\n",
      "Step 1400 | Loss: 0.81462 | LR: 4.78e-03\n",
      "Step 1425 | Loss: 0.81893 | LR: 4.77e-03\n",
      "Step 1450 | Loss: 0.80921 | LR: 4.76e-03\n",
      "Step 1475 | Loss: 0.80954 | LR: 4.75e-03\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.8133\n",
      "Step 1500 | Loss: 0.80275 | LR: 4.74e-03\n",
      "Step 1525 | Loss: 0.80541 | LR: 4.73e-03\n",
      "Step 1550 | Loss: 0.80042 | LR: 4.72e-03\n",
      "Step 1575 | Loss: 0.81301 | LR: 4.70e-03\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "=== Step 1587 Done. Avg Loss: 0.81109 ===\n",
      "val loss: 0.8159\n",
      "Step 1600 | Loss: 0.79990 | LR: 4.69e-03\n",
      "Step 1625 | Loss: 0.80618 | LR: 4.68e-03\n",
      "Step 1650 | Loss: 0.80508 | LR: 4.67e-03\n",
      "Step 1675 | Loss: 0.79746 | LR: 4.65e-03\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.8154\n",
      "Step 1700 | Loss: 0.79386 | LR: 4.64e-03\n",
      "Step 1725 | Loss: 0.81203 | LR: 4.63e-03\n",
      "Step 1750 | Loss: 0.81282 | LR: 4.61e-03\n",
      "Step 1775 | Loss: 0.80785 | LR: 4.60e-03\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.8116\n",
      "Step 1800 | Loss: 0.79196 | LR: 4.58e-03\n",
      "Step 1825 | Loss: 0.81484 | LR: 4.57e-03\n",
      "Step 1850 | Loss: 0.80243 | LR: 4.55e-03\n",
      "Step 1875 | Loss: 0.79748 | LR: 4.54e-03\n",
      "val loss: 0.8118\n",
      "Step 1900 | Loss: 0.82085 | LR: 4.52e-03\n",
      "Step 1925 | Loss: 0.80668 | LR: 4.51e-03\n",
      "Step 1950 | Loss: 0.80521 | LR: 4.49e-03\n",
      "Step 1975 | Loss: 0.80643 | LR: 4.48e-03\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "=== Step 1984 Done. Avg Loss: 0.80450 ===\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.8099\n",
      "Step 2000 | Loss: 0.79930 | LR: 4.46e-03\n",
      "Step 2025 | Loss: 0.80256 | LR: 4.45e-03\n",
      "Step 2050 | Loss: 0.80344 | LR: 4.43e-03\n",
      "Step 2075 | Loss: 0.78952 | LR: 4.41e-03\n",
      "val loss: 0.8101\n",
      "Step 2100 | Loss: 0.79612 | LR: 4.40e-03\n",
      "Step 2125 | Loss: 0.79300 | LR: 4.38e-03\n",
      "Step 2150 | Loss: 0.80226 | LR: 4.36e-03\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 2175 | Loss: 0.82422 | LR: 4.34e-03\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.8084\n",
      "Step 2200 | Loss: 0.79998 | LR: 4.33e-03\n",
      "Step 2225 | Loss: 0.80606 | LR: 4.31e-03\n",
      "Step 2250 | Loss: 0.80811 | LR: 4.29e-03\n",
      "Step 2275 | Loss: 0.80421 | LR: 4.27e-03\n",
      "val loss: 0.8087\n",
      "Step 2300 | Loss: 0.80007 | LR: 4.25e-03\n",
      "Step 2325 | Loss: 0.78659 | LR: 4.23e-03\n",
      "Step 2350 | Loss: 0.80572 | LR: 4.22e-03\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "Step 2375 | Loss: 0.80399 | LR: 4.20e-03\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "=== Step 2381 Done. Avg Loss: 0.79978 ===\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.8038\n",
      "Step 2400 | Loss: 0.80127 | LR: 4.18e-03\n",
      "Step 2425 | Loss: 0.79001 | LR: 4.16e-03\n",
      "Step 2450 | Loss: 0.79457 | LR: 4.14e-03\n",
      "Step 2475 | Loss: 0.79082 | LR: 4.12e-03\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.8096\n",
      "Step 2500 | Loss: 0.80030 | LR: 4.10e-03\n",
      "Step 2525 | Loss: 0.80706 | LR: 4.08e-03\n",
      "Step 2550 | Loss: 0.79723 | LR: 4.06e-03\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "Step 2575 | Loss: 0.79323 | LR: 4.04e-03\n",
      "val loss: 0.8072\n",
      "Step 2600 | Loss: 0.79318 | LR: 4.02e-03\n",
      "Step 2625 | Loss: 0.80192 | LR: 4.00e-03\n",
      "Step 2650 | Loss: 0.78752 | LR: 3.98e-03\n",
      "Step 2675 | Loss: 0.78460 | LR: 3.96e-03\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.8066\n",
      "Step 2700 | Loss: 0.78579 | LR: 3.93e-03\n",
      "Step 2725 | Loss: 0.79146 | LR: 3.91e-03\n",
      "Step 2750 | Loss: 0.79201 | LR: 3.89e-03\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "Step 2775 | Loss: 0.77505 | LR: 3.87e-03\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "=== Step 2778 Done. Avg Loss: 0.79645 ===\n",
      "val loss: 0.8040\n",
      "Step 2800 | Loss: 0.79343 | LR: 3.85e-03\n",
      "Step 2825 | Loss: 0.79404 | LR: 3.83e-03\n",
      "Step 2850 | Loss: 0.79610 | LR: 3.80e-03\n",
      "Step 2875 | Loss: 0.79969 | LR: 3.78e-03\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.8046\n",
      "Step 2900 | Loss: 0.78544 | LR: 3.76e-03\n",
      "Step 2925 | Loss: 0.79194 | LR: 3.74e-03\n",
      "Step 2950 | Loss: 0.77831 | LR: 3.71e-03\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 2975 | Loss: 0.79549 | LR: 3.69e-03\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.8028\n",
      "Step 3000 | Loss: 0.79136 | LR: 3.67e-03\n",
      "Step 3025 | Loss: 0.78945 | LR: 3.64e-03\n",
      "Step 3050 | Loss: 0.79355 | LR: 3.62e-03\n",
      "Step 3075 | Loss: 0.79411 | LR: 3.60e-03\n",
      "val loss: 0.8025\n",
      "Step 3100 | Loss: 0.77960 | LR: 3.57e-03\n",
      "Step 3125 | Loss: 0.80489 | LR: 3.55e-03\n",
      "Step 3150 | Loss: 0.80215 | LR: 3.53e-03\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 3175 | Loss: 0.78123 | LR: 3.50e-03\n",
      "=== Step 3175 Done. Avg Loss: 0.79370 ===\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.8025\n",
      "Step 3200 | Loss: 0.80511 | LR: 3.48e-03\n",
      "Step 3225 | Loss: 0.78675 | LR: 3.46e-03\n",
      "Step 3250 | Loss: 0.78224 | LR: 3.43e-03\n",
      "Step 3275 | Loss: 0.79149 | LR: 3.41e-03\n",
      "val loss: 0.8016\n",
      "Step 3300 | Loss: 0.79503 | LR: 3.38e-03\n",
      "Step 3325 | Loss: 0.79493 | LR: 3.36e-03\n",
      "Step 3350 | Loss: 0.79402 | LR: 3.33e-03\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "Step 3375 | Loss: 0.79649 | LR: 3.31e-03\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.8025\n",
      "Step 3400 | Loss: 0.77876 | LR: 3.28e-03\n",
      "Step 3425 | Loss: 0.79391 | LR: 3.26e-03\n",
      "Step 3450 | Loss: 0.78771 | LR: 3.24e-03\n",
      "Step 3475 | Loss: 0.79252 | LR: 3.21e-03\n",
      "val loss: 0.8011\n",
      "Step 3500 | Loss: 0.77975 | LR: 3.19e-03\n",
      "Step 3525 | Loss: 0.80182 | LR: 3.16e-03\n",
      "Step 3550 | Loss: 0.79518 | LR: 3.13e-03\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "=== Step 3572 Done. Avg Loss: 0.79118 ===\n",
      "Step 3575 | Loss: 0.78822 | LR: 3.11e-03\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.8000\n",
      "Step 3600 | Loss: 0.78810 | LR: 3.08e-03\n",
      "Step 3625 | Loss: 0.79004 | LR: 3.06e-03\n",
      "Step 3650 | Loss: 0.79525 | LR: 3.03e-03\n",
      "Step 3675 | Loss: 0.78853 | LR: 3.01e-03\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.7999\n",
      "Step 3700 | Loss: 0.78205 | LR: 2.98e-03\n",
      "Step 3725 | Loss: 0.79113 | LR: 2.96e-03\n",
      "Step 3750 | Loss: 0.79006 | LR: 2.93e-03\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 3775 | Loss: 0.79328 | LR: 2.91e-03\n",
      "val loss: 0.7999\n",
      "Step 3800 | Loss: 0.78986 | LR: 2.88e-03\n",
      "Step 3825 | Loss: 0.78883 | LR: 2.85e-03\n",
      "Step 3850 | Loss: 0.79934 | LR: 2.83e-03\n",
      "Step 3875 | Loss: 0.80291 | LR: 2.80e-03\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.7976\n",
      "Step 3900 | Loss: 0.78711 | LR: 2.78e-03\n",
      "Step 3925 | Loss: 0.78354 | LR: 2.75e-03\n",
      "Step 3950 | Loss: 0.79432 | LR: 2.73e-03\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "=== Step 3969 Done. Avg Loss: 0.78900 ===\n",
      "Step 3975 | Loss: 0.78456 | LR: 2.70e-03\n",
      "val loss: 0.7994\n",
      "Step 4000 | Loss: 0.78309 | LR: 2.67e-03\n",
      "Step 4025 | Loss: 0.78039 | LR: 2.65e-03\n",
      "Step 4050 | Loss: 0.78961 | LR: 2.62e-03\n",
      "Step 4075 | Loss: 0.79701 | LR: 2.60e-03\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.7997\n",
      "Step 4100 | Loss: 0.78072 | LR: 2.57e-03\n",
      "Step 4125 | Loss: 0.77959 | LR: 2.54e-03\n",
      "Step 4150 | Loss: 0.79021 | LR: 2.52e-03\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "Step 4175 | Loss: 0.78647 | LR: 2.49e-03\n",
      "val loss: 0.7975\n",
      "Step 4200 | Loss: 0.78520 | LR: 2.47e-03\n",
      "Step 4225 | Loss: 0.78236 | LR: 2.44e-03\n",
      "Step 4250 | Loss: 0.78306 | LR: 2.41e-03\n",
      "Step 4275 | Loss: 0.77965 | LR: 2.39e-03\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.7988\n",
      "Step 4300 | Loss: 0.79621 | LR: 2.36e-03\n",
      "Step 4325 | Loss: 0.79079 | LR: 2.34e-03\n",
      "Step 4350 | Loss: 0.78422 | LR: 2.31e-03\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "=== Step 4366 Done. Avg Loss: 0.78698 ===\n",
      "Step 4375 | Loss: 0.77212 | LR: 2.28e-03\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.7952\n",
      "Step 4400 | Loss: 0.77537 | LR: 2.26e-03\n",
      "Step 4425 | Loss: 0.78801 | LR: 2.23e-03\n",
      "Step 4450 | Loss: 0.78869 | LR: 2.21e-03\n",
      "Step 4475 | Loss: 0.78083 | LR: 2.18e-03\n",
      "val loss: 0.7977\n",
      "Step 4500 | Loss: 0.78686 | LR: 2.15e-03\n",
      "Step 4525 | Loss: 0.77079 | LR: 2.13e-03\n",
      "Step 4550 | Loss: 0.78354 | LR: 2.10e-03\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "Step 4575 | Loss: 0.78520 | LR: 2.08e-03\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.7962\n",
      "Step 4600 | Loss: 0.78923 | LR: 2.05e-03\n",
      "Step 4625 | Loss: 0.76790 | LR: 2.03e-03\n",
      "Step 4650 | Loss: 0.77929 | LR: 2.00e-03\n",
      "Step 4675 | Loss: 0.78115 | LR: 1.97e-03\n",
      "val loss: 0.7962\n",
      "Step 4700 | Loss: 0.78640 | LR: 1.95e-03\n",
      "Step 4725 | Loss: 0.79395 | LR: 1.92e-03\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "Step 4750 | Loss: 0.78504 | LR: 1.90e-03\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "=== Step 4763 Done. Avg Loss: 0.78509 ===\n",
      "Step 4775 | Loss: 0.77565 | LR: 1.87e-03\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.7957\n",
      "Step 4800 | Loss: 0.78162 | LR: 1.85e-03\n",
      "Step 4825 | Loss: 0.77715 | LR: 1.82e-03\n",
      "Step 4850 | Loss: 0.78803 | LR: 1.80e-03\n",
      "Step 4875 | Loss: 0.78821 | LR: 1.77e-03\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.7956\n",
      "Step 4900 | Loss: 0.77735 | LR: 1.75e-03\n",
      "Step 4925 | Loss: 0.78404 | LR: 1.72e-03\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 4950 | Loss: 0.77789 | LR: 1.70e-03\n",
      "Step 4975 | Loss: 0.77359 | LR: 1.67e-03\n",
      "val loss: 0.7935\n",
      "Step 5000 | Loss: 0.78865 | LR: 1.65e-03\n",
      "Step 5025 | Loss: 0.77799 | LR: 1.63e-03\n",
      "Step 5050 | Loss: 0.78152 | LR: 1.60e-03\n",
      "Step 5075 | Loss: 0.77784 | LR: 1.58e-03\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.7975\n",
      "Step 5100 | Loss: 0.78303 | LR: 1.55e-03\n",
      "Step 5125 | Loss: 0.77668 | LR: 1.53e-03\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "Step 5150 | Loss: 0.77837 | LR: 1.50e-03\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "=== Step 5160 Done. Avg Loss: 0.78335 ===\n",
      "Step 5175 | Loss: 0.77016 | LR: 1.48e-03\n",
      "val loss: 0.7940\n",
      "Step 5200 | Loss: 0.78005 | LR: 1.46e-03\n",
      "Step 5225 | Loss: 0.77380 | LR: 1.43e-03\n",
      "Step 5250 | Loss: 0.77566 | LR: 1.41e-03\n",
      "Step 5275 | Loss: 0.78028 | LR: 1.39e-03\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.7945\n",
      "Step 5300 | Loss: 0.79157 | LR: 1.36e-03\n",
      "Step 5325 | Loss: 0.78380 | LR: 1.34e-03\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "Step 5350 | Loss: 0.78876 | LR: 1.32e-03\n",
      "Step 5375 | Loss: 0.77839 | LR: 1.29e-03\n",
      "val loss: 0.7935\n",
      "Step 5400 | Loss: 0.77337 | LR: 1.27e-03\n",
      "Step 5425 | Loss: 0.77852 | LR: 1.25e-03\n",
      "Step 5450 | Loss: 0.77787 | LR: 1.23e-03\n",
      "Step 5475 | Loss: 0.78788 | LR: 1.20e-03\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.7937\n",
      "Step 5500 | Loss: 0.78928 | LR: 1.18e-03\n",
      "Step 5525 | Loss: 0.77433 | LR: 1.16e-03\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 5550 | Loss: 0.77743 | LR: 1.14e-03\n",
      "=== Step 5557 Done. Avg Loss: 0.78150 ===\n",
      "Step 5575 | Loss: 0.77589 | LR: 1.12e-03\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.7922\n",
      "Step 5600 | Loss: 0.77846 | LR: 1.09e-03\n",
      "Step 5625 | Loss: 0.77091 | LR: 1.07e-03\n",
      "Step 5650 | Loss: 0.76376 | LR: 1.05e-03\n",
      "Step 5675 | Loss: 0.77871 | LR: 1.03e-03\n",
      "val loss: 0.7944\n",
      "Step 5700 | Loss: 0.78462 | LR: 1.01e-03\n",
      "Step 5725 | Loss: 0.76587 | LR: 9.89e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "Step 5750 | Loss: 0.77126 | LR: 9.68e-04\n",
      "Step 5775 | Loss: 0.77875 | LR: 9.48e-04\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.7919\n",
      "Step 5800 | Loss: 0.78228 | LR: 9.27e-04\n",
      "Step 5825 | Loss: 0.77128 | LR: 9.07e-04\n",
      "Step 5850 | Loss: 0.78605 | LR: 8.87e-04\n",
      "Step 5875 | Loss: 0.77395 | LR: 8.67e-04\n",
      "val loss: 0.7929\n",
      "Step 5900 | Loss: 0.78741 | LR: 8.48e-04\n",
      "Step 5925 | Loss: 0.78221 | LR: 8.28e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "Step 5950 | Loss: 0.78293 | LR: 8.09e-04\n",
      "=== Step 5954 Done. Avg Loss: 0.78017 ===\n",
      "Step 5975 | Loss: 0.78075 | LR: 7.90e-04\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.7904\n",
      "Step 6000 | Loss: 0.79302 | LR: 7.71e-04\n",
      "Step 6025 | Loss: 0.77929 | LR: 7.52e-04\n",
      "Step 6050 | Loss: 0.77078 | LR: 7.34e-04\n",
      "Step 6075 | Loss: 0.77442 | LR: 7.16e-04\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.7938\n",
      "Step 6100 | Loss: 0.78496 | LR: 6.97e-04\n",
      "Step 6125 | Loss: 0.77218 | LR: 6.80e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 6150 | Loss: 0.77612 | LR: 6.62e-04\n",
      "Step 6175 | Loss: 0.77648 | LR: 6.44e-04\n",
      "val loss: 0.7909\n",
      "Step 6200 | Loss: 0.77100 | LR: 6.27e-04\n",
      "Step 6225 | Loss: 0.78279 | LR: 6.10e-04\n",
      "Step 6250 | Loss: 0.77564 | LR: 5.93e-04\n",
      "Step 6275 | Loss: 0.77367 | LR: 5.76e-04\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.7916\n",
      "Step 6300 | Loss: 0.76426 | LR: 5.60e-04\n",
      "Step 6325 | Loss: 0.77538 | LR: 5.43e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 6350 | Loss: 0.78544 | LR: 5.27e-04\n",
      "=== Step 6351 Done. Avg Loss: 0.77846 ===\n",
      "Step 6375 | Loss: 0.77722 | LR: 5.11e-04\n",
      "val loss: 0.7917\n",
      "Step 6400 | Loss: 0.76852 | LR: 4.96e-04\n",
      "Step 6425 | Loss: 0.78839 | LR: 4.80e-04\n",
      "Step 6450 | Loss: 0.78173 | LR: 4.65e-04\n",
      "Step 6475 | Loss: 0.77994 | LR: 4.50e-04\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.7917\n",
      "Step 6500 | Loss: 0.78165 | LR: 4.35e-04\n",
      "Step 6525 | Loss: 0.76400 | LR: 4.21e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "Step 6550 | Loss: 0.76622 | LR: 4.06e-04\n",
      "Step 6575 | Loss: 0.77597 | LR: 3.92e-04\n",
      "val loss: 0.7920\n",
      "Step 6600 | Loss: 0.78078 | LR: 3.78e-04\n",
      "Step 6625 | Loss: 0.78083 | LR: 3.65e-04\n",
      "Step 6650 | Loss: 0.76562 | LR: 3.51e-04\n",
      "Step 6675 | Loss: 0.78105 | LR: 3.38e-04\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.7900\n",
      "Step 6700 | Loss: 0.76472 | LR: 3.25e-04\n",
      "Step 6725 | Loss: 0.76947 | LR: 3.12e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "=== Step 6748 Done. Avg Loss: 0.77750 ===\n",
      "Step 6750 | Loss: 0.76826 | LR: 3.00e-04\n",
      "Step 6775 | Loss: 0.77195 | LR: 2.88e-04\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.7915\n",
      "Step 6800 | Loss: 0.78100 | LR: 2.76e-04\n",
      "Step 6825 | Loss: 0.77519 | LR: 2.64e-04\n",
      "Step 6850 | Loss: 0.77733 | LR: 2.52e-04\n",
      "Step 6875 | Loss: 0.76953 | LR: 2.41e-04\n",
      "val loss: 0.7910\n",
      "Step 6900 | Loss: 0.77472 | LR: 2.30e-04\n",
      "Step 6925 | Loss: 0.77268 | LR: 2.19e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "Step 6950 | Loss: 0.77229 | LR: 2.09e-04\n",
      "Step 6975 | Loss: 0.78356 | LR: 1.98e-04\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.7898\n",
      "Step 7000 | Loss: 0.78994 | LR: 1.88e-04\n",
      "Step 7025 | Loss: 0.78540 | LR: 1.79e-04\n",
      "Step 7050 | Loss: 0.77080 | LR: 1.69e-04\n",
      "Step 7075 | Loss: 0.76557 | LR: 1.60e-04\n",
      "val loss: 0.7911\n",
      "Step 7100 | Loss: 0.77667 | LR: 1.51e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "Step 7125 | Loss: 0.78007 | LR: 1.42e-04\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "=== Step 7145 Done. Avg Loss: 0.77669 ===\n",
      "Step 7150 | Loss: 0.77114 | LR: 1.33e-04\n",
      "Step 7175 | Loss: 0.76895 | LR: 1.25e-04\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.7901\n",
      "Step 7200 | Loss: 0.77373 | LR: 1.17e-04\n",
      "Step 7225 | Loss: 0.76894 | LR: 1.09e-04\n",
      "Step 7250 | Loss: 0.77660 | LR: 1.02e-04\n",
      "Step 7275 | Loss: 0.77263 | LR: 9.47e-05\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.7905\n",
      "Step 7300 | Loss: 0.78414 | LR: 8.78e-05\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "Step 7325 | Loss: 0.77773 | LR: 8.11e-05\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 7350 | Loss: 0.77862 | LR: 7.46e-05\n",
      "Step 7375 | Loss: 0.78046 | LR: 6.84e-05\n",
      "val loss: 0.7904\n",
      "Step 7400 | Loss: 0.77363 | LR: 6.25e-05\n",
      "Step 7425 | Loss: 0.77198 | LR: 5.69e-05\n",
      "Step 7450 | Loss: 0.78961 | LR: 5.15e-05\n",
      "Step 7475 | Loss: 0.77393 | LR: 4.64e-05\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.7899\n",
      "Step 7500 | Loss: 0.77818 | LR: 4.15e-05\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "Step 7525 | Loss: 0.78369 | LR: 3.69e-05\n",
      "=== Step 7542 Done. Avg Loss: 0.77596 ===\n",
      "Step 7550 | Loss: 0.78261 | LR: 3.26e-05\n",
      "Step 7575 | Loss: 0.78432 | LR: 2.85e-05\n",
      "val loss: 0.7908\n",
      "Step 7600 | Loss: 0.76557 | LR: 2.48e-05\n",
      "Step 7625 | Loss: 0.78173 | LR: 2.12e-05\n",
      "Step 7650 | Loss: 0.78151 | LR: 1.80e-05\n",
      "Step 7675 | Loss: 0.76722 | LR: 1.50e-05\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.7909\n",
      "Step 7700 | Loss: 0.77921 | LR: 1.23e-05\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0000.npz\n",
      "Step 7725 | Loss: 0.77587 | LR: 9.85e-06\n",
      "Step 7750 | Loss: 0.77897 | LR: 7.68e-06\n",
      "Step 7775 | Loss: 0.79004 | LR: 5.78e-06\n",
      "val loss: 0.7893\n",
      "Step 7800 | Loss: 0.77265 | LR: 4.15e-06\n",
      "Step 7825 | Loss: 0.77839 | LR: 2.79e-06\n",
      "Step 7850 | Loss: 0.76537 | LR: 1.70e-06\n",
      "Step 7875 | Loss: 0.77922 | LR: 8.81e-07\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.7893\n",
      "Step 7900 | Loss: 0.77760 | LR: 3.33e-07\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0002.npz\n",
      "Step 7925 | Loss: 0.78563 | LR: 5.66e-08\n",
      "loading /home/ubuntu/training/train/shard_k562e_train_0001.npz\n",
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n",
      "val loss: 0.7910\n",
      "=== Step 7939 Done. Avg Loss: 0.77560 ===\n"
     ]
    }
   ],
   "source": [
    "for step in range(max_steps):\n",
    "\n",
    "    last_step = (step == max_steps - 1)\n",
    "\n",
    "    # once in a while evaluate our validation set loss\n",
    "    if step % 100 == 0 or last_step:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss_accum = 0.0\n",
    "            val_loss_steps = 25\n",
    "            for i in range(val_loss_steps):\n",
    "                cont_x, cont_tot, case_x, case_tot, act_id = val_loader.next_batch()\n",
    "\n",
    "                # run BioJEPA\n",
    "                with torch.no_grad():\n",
    "                    z_context = model.student(cont_x, cont_tot)\n",
    "                    z_pred = model.predictor(z_context, act_id)\n",
    "\n",
    "                # run new decoder\n",
    "                pred_delta = decoder(z_pred) - decoder(z_context)\n",
    "                real_delta = case_x - cont_x\n",
    "\n",
    "                loss = F.mse_loss(pred_delta, real_delta)\n",
    "                loss = loss / val_loss_steps\n",
    "                val_loss_accum += loss.detach()\n",
    "\n",
    "        print(f'val loss: {val_loss_accum.item():.4f}')\n",
    "\n",
    "\n",
    "    # periodically save checkpoint\n",
    "    if step > 0 and  (step+1) % steps_per_epoch ==0 and not last_step:\n",
    "        # Save Checkpoint\n",
    "        torch.save({\n",
    "            'model': decoder.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'step': step\n",
    "        }, checkpoint_dir / f'biojepa_decoder_ckpt_{step}.pt')\n",
    "\n",
    "    # actual training\n",
    "    decoder.train\n",
    "    cont_x, cont_tot, case_x, case_tot, act_id = train_loader.next_batch()\n",
    "\n",
    "    # run frozen BioJEPA\n",
    "    with torch.no_grad():\n",
    "        z_context = model.student(cont_x, cont_tot)\n",
    "        z_pred = model.predictor(z_context, act_id)\n",
    "\n",
    "    # run decoder\n",
    "    pred_delta = decoder(z_pred) - decoder(z_context)\n",
    "    real_delta = case_x - cont_x\n",
    "    real_delta = case_x - cont_x\n",
    "\n",
    "    # loss\n",
    "    pred_delta = decoder(z_pred) - decoder(z_context)\n",
    "    real_delta = case_x - cont_x\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss = F.mse_loss(pred_delta, real_delta)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    # loss caching\n",
    "    lossi.append(loss.item())\n",
    "    total_epoch_loss += loss.item()\n",
    "\n",
    "    if step % 25 == 0:\n",
    "        print(f\"Step {step} | Loss: {loss.item():.5f} | LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "    \n",
    "    if step > 0 and (step+1) % steps_per_epoch == 0:   \n",
    "        avg_loss = total_epoch_loss / steps_per_epoch\n",
    "        print(f\"=== Step {step} Done. Avg Loss: {avg_loss:.5f} ===\")\n",
    "        total_epoch_loss = 0\n",
    "    \n",
    "    if last_step:\n",
    "        # Save Checkpoint\n",
    "        torch.save({\n",
    "            'model': decoder.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'step': step\n",
    "        }, checkpoint_dir / f'biojepa_decoder_ckpt_{step}_final.pt')\n",
    "\n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c924050-b2d8-4973-a3c1-f9dd0a3d1f03",
   "metadata": {},
   "source": [
    "**Training Loss Plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b11fde37-0cac-429d-be0c-351390bac22d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lossi)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8aaa78-e607-4bc4-8cde-49a529984c1b",
   "metadata": {},
   "source": [
    "## Trained Decoder Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "974730ff-1cda-43e0-a72f-df0986bb9daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from scipy.stats import ConstantInputWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94fd5355-31c8-47fc-a2fd-9b1fa9f91576",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.eval()\n",
    "correlations = []\n",
    "mses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "417a9438-c9f1-4cb0-8da2-e9fe7ec831a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_steps_per_epoch = val_total_examples // BATCH_SIZE\n",
    "test_steps_per_epoch = test_total_examples // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27c89be7-508c-4e06-9dfc-9f4809807077",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  91%|     | 39/43 [00:16<00:01,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading /home/ubuntu/training/val/shard_k562e_val_0000.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|| 43/43 [00:18<00:00,  2.32it/s]\n"
     ]
    }
   ],
   "source": [
    "for step in tqdm(range(val_steps_per_epoch), desc=\"Evaluating\"):\n",
    "    \n",
    "    # Custom Loader Call\n",
    "    cont_x, cont_tot, case_x, case_tot, act_id = val_loader.next_batch()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        z_context = model.student(cont_x, cont_tot)\n",
    "        z_pred = model.predictor(z_context, act_id)\n",
    "        \n",
    "        pred_delta = decoder(z_pred) - decoder(z_context)\n",
    "        real_delta = case_x - cont_x\n",
    "        \n",
    "    # Per-Sample Metrics\n",
    "    for i in range(len(pred_delta)):\n",
    "        p = pred_delta[i].cpu().numpy()\n",
    "        t = real_delta[i].cpu().numpy()\n",
    "\n",
    "        # Metric: Pearson Correlation on Top 20 DEGs\n",
    "        top_20_idx = np.argsort(np.abs(t))[-20:]\n",
    "\n",
    "        p_top = p[top_20_idx]\n",
    "        t_top = t[top_20_idx]\n",
    "        \n",
    "        if np.std(p_top) > 1e-9 and np.std(t_top) > 1e-9:\n",
    "            corr, _ = pearsonr(p_top, t_top)\n",
    "            if not np.isnan(corr):\n",
    "                correlations.append(corr)\n",
    "            else:\n",
    "                # Penalize undefined correlation as 0\n",
    "                correlations.append(0.0)\n",
    "        else:\n",
    "            # Variance is 0 (constant prediction or target) treated as 0 correlation for benchmarks\n",
    "            correlations.append(0.0)\n",
    "            \n",
    "        mses.append(np.mean((p - t)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "750afdb9-ba92-4a7a-874d-0cc221525de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global MSE: 0.7900\n",
      "Top-20 Pearson R: 0.6097\n"
     ]
    }
   ],
   "source": [
    "mean_mse = np.mean(mses)\n",
    "mean_corr = np.mean(correlations)\n",
    "print(f'Global MSE: {mean_mse:.4f}')\n",
    "print(f'Top-20 Pearson R: {mean_corr:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "44680d47-980a-4b0e-b910-471b65efd220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " FUNCTIONAL (Better than random)\n"
     ]
    }
   ],
   "source": [
    "if mean_corr > 0.75:\n",
    "    print(' SOTA COMPETITIVE (Matches GEARS)')\n",
    "elif mean_corr > 0.40:\n",
    "    print(' FUNCTIONAL (Better than random)')\n",
    "else:\n",
    "    print(' NEEDS IMPROVEMENT')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6b2741-9067-4cee-b37f-2ee471d3c7e6",
   "metadata": {},
   "source": [
    "## Trained Decoder Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ecd5cb77-d9a4-43d8-ad77-b6d102d00caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_correlations = []\n",
    "eval_mses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "17c58931-04dc-4c75-a84d-23fbf3f7abc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 1 shards for split test\n",
      "loading /home/ubuntu/training/test/shard_k562e_test_0000.npz\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoaderLite(B=BATCH_SIZE, split='test', device=DEVICE)\n",
    "test_steps_per_epoch = test_total_examples // BATCH_SIZE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9022a3d0-68f1-4de8-bb53-33e5bd05e212",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   7%|                                                         | 11/151 [00:04<00:59,  2.34it/s]/tmp/ipykernel_28544/2469069358.py:25: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  corr, _ = pearsonr(p_top, t_top)\n",
      "Evaluating: 100%|| 151/151 [01:04<00:00,  2.34it/s]\n"
     ]
    }
   ],
   "source": [
    "for step in tqdm(range(test_steps_per_epoch), desc=\"Evaluating\"):\n",
    "    \n",
    "    # Custom Loader Call\n",
    "    cont_x, cont_tot, case_x, case_tot, act_id = test_loader.next_batch()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        z_context = model.student(cont_x, cont_tot)\n",
    "        z_pred = model.predictor(z_context, act_id)\n",
    "        \n",
    "        pred_delta = decoder(z_pred) - decoder(z_context)\n",
    "        real_delta = case_x - cont_x\n",
    "        \n",
    "    # Per-Sample Metrics\n",
    "    for i in range(len(pred_delta)):\n",
    "        p = pred_delta[i].cpu().numpy()\n",
    "        t = real_delta[i].cpu().numpy()\n",
    "\n",
    "        # Metric: Pearson Correlation on Top 20 DEGs\n",
    "        top_20_idx = np.argsort(np.abs(t))[-20:]\n",
    "\n",
    "        p_top = p[top_20_idx]\n",
    "        t_top = t[top_20_idx]\n",
    "        \n",
    "        if np.std(p_top) > 1e-9 and np.std(t_top) > 1e-9:\n",
    "            corr, _ = pearsonr(p_top, t_top)\n",
    "            if not np.isnan(corr):\n",
    "                eval_correlations.append(corr)\n",
    "            else:\n",
    "                # Penalize undefined correlation as 0\n",
    "                eval_correlations.append(0.0)\n",
    "        else:\n",
    "            # Variance is 0 (constant prediction or target) treated as 0 correlation for benchmarks\n",
    "            eval_correlations.append(0.0)\n",
    "            \n",
    "        eval_mses.append(np.mean((p - t)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "40ee16bc-f9f8-4ab6-bc12-9e72fafe1b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global MSE: 0.7896\n",
      "Top-20 Pearson R: 0.6046\n"
     ]
    }
   ],
   "source": [
    "eval_mean_mse = np.mean(eval_mses)\n",
    "eval_mean_corr = np.mean(eval_correlations)\n",
    "print(f'Global MSE: {eval_mean_mse:.4f}')\n",
    "print(f'Top-20 Pearson R: {eval_mean_corr:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4370f0b7-232f-4fe4-8777-3d08ab4519be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
