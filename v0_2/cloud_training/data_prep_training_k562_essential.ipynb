{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd7d3448-7cd2-476a-991f-543a571e152a",
   "metadata": {},
   "source": [
    "## V0_2 Updates\n",
    "1. Properly handling the test/train/val split based on gears\n",
    "2. Adding in ESM-2 based embeddings for the actions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4b4a182-be86-4c90-93cf-8008d5f38e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import gseapy as gp\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from gears import PertData, GEARS\n",
    "from pathlib import Path\n",
    "from Bio import Entrez, SeqIO\n",
    "import mygene\n",
    "import gzip\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfe00742-3648-45b2-ac04-b8b203574b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('/home/ubuntu')\n",
    "tok_dir = data_dir / 'training'\n",
    "splits= ['train','val','test']\n",
    "\n",
    "protein_dir = data_dir / 'uniprot'\n",
    "pert_dir = data_dir / 'pert_embd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75b03165-2beb-4c31-bf32-85d101560db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 50000        # How many cells per file\n",
    "n_pathways = 1024          # Number of pathway \"tokens\" per cell\n",
    "n_genes = 8192 # 2**13\n",
    "count_normalize_target = 1e4 # normalize each cell to this count\n",
    "dataset_name = 'k562e'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fa2532-aa8a-4cdf-b9d0-c055e5481e73",
   "metadata": {},
   "source": [
    "## Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c03df92b-cd7b-43c8-8ba9-dc82f4b3f411",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "100%|██████████████████████████████████████████████████████████████████████| 9.46M/9.46M [00:00<00:00, 134MiB/s]\n",
      "Downloading...\n",
      "100%|███████████████████████████████████████████████████████████████████████| 670M/670M [00:25<00:00, 26.5MiB/s]\n",
      "Extracting zip file...\n",
      "Done!\n",
      "Downloading...\n",
      "100%|███████████████████████████████████████████████████████████████████████| 559k/559k [00:00<00:00, 30.2MiB/s]\n",
      "These perturbations are not in the GO graph and their perturbation can thus not be predicted\n",
      "['C7orf26+ctrl' 'C14orf178+ctrl' 'RPS10-NUDT3+ctrl' 'SEM1+ctrl' 'FAU+ctrl']\n",
      "Creating pyg object for each cell in the data...\n",
      "Creating dataset file...\n",
      "100%|███████████████████████████████████████████████████████████████████████| 1088/1088 [07:54<00:00,  2.29it/s]\n",
      "Done!\n",
      "Saving new dataset pyg object at /home/ubuntu/k562e/replogle_k562_essential/data_pyg/cell_graphs.pkl\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "pert_data = PertData(data_dir / dataset_name) \n",
    "pert_data.load(data_name='replogle_k562_essential')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b065529f-7e52-4946-9355-9108d93b968c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating new splits....\n",
      "Saving new splits at /home/ubuntu/k562e/replogle_k562_essential/splits/replogle_k562_essential_simulation_1_0.75.pkl\n",
      "Simulation split test composition:\n",
      "combo_seen0:0\n",
      "combo_seen1:0\n",
      "combo_seen2:0\n",
      "unseen_single:272\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "pert_data.prepare_split(split='simulation', seed=1) \n",
    "adata = pert_data.adata "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5dd4fb-12a6-4fc6-aeb9-68b746178c5c",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8914679e-7281-4dae-8925-bfd108d4a86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_counts = np.array(adata.X.sum(axis=1)).flatten()\n",
    "adata.obs['log_total_counts'] = np.log1p(total_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2462943-bceb-45ef-ba68-780c1d63f435",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.normalize_total(adata, target_sum=count_normalize_target)\n",
    "sc.pp.log1p(adata)\n",
    "sc.pp.highly_variable_genes(adata, n_top_genes=n_genes, subset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f77ea077-3f31-4a5c-b744-27798bb0092e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Feature Space: 5000 Genes\n"
     ]
    }
   ],
   "source": [
    "genes = adata.var.gene_name.tolist()\n",
    "print(f'Final Feature Space: {len(genes)} Genes')\n",
    "\n",
    "with open(data_dir / 'gene_names.json', 'w') as f:\n",
    "    json.dump(genes, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7dc725-39bf-496c-8970-e14d76cb40c5",
   "metadata": {},
   "source": [
    "## Perturbation Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d6261c6-ed48-4a7e-a805-d329c2463215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_gears_name(name):\n",
    "    # GEARS format is 'Gene+ctrl' -> We want 'Gene'\n",
    "    if name.endswith('+ctrl'):\n",
    "        return name.replace('+ctrl', '')\n",
    "    elif name == 'ctrl':\n",
    "        return 'control'\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca05552c-3614-45e2-812f-b7f088b9a251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1087, 1087)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_perts = adata.obs['condition'].unique()\n",
    "all_perts = [clean_gears_name(p) for p in all_perts if p != 'ctrl']\n",
    "unique_perts = sorted(list(set(all_perts)))\n",
    "len(all_perts), len(unique_perts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e51ffc-5266-4e37-b352-761abe7b7b21",
   "metadata": {},
   "source": [
    "**Uniprot Cache**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "906c7074-5d78-49b8-98b5-c451161aa25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_prots = 'https://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/reference_proteomes/Eukaryota/UP000005640/UP000005640_9606.fasta.gz'\n",
    "#! wget -P {protein_dir} {all_prots}\n",
    "fasta = [i for i in protein_dir.iterdir()][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d30ff3e-3286-43d6-b72d-62b013e18c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniprot_db = {}\n",
    "with gzip.open(fasta, 'rt') as handle:\n",
    "    for record in SeqIO.parse(handle, 'fasta'):\n",
    "        # Header: >sp|P12345|GENE_HUMAN Description GN=Symbol PE=1 ...\n",
    "        desc = record.description\n",
    "        \n",
    "        # Robustly extract Gene Name (GN)\n",
    "        if 'GN=' in desc:\n",
    "            parts = desc.split('GN=')\n",
    "            if len(parts) > 1:\n",
    "                # Symbol is usually the first string after GN=, before the next space\n",
    "                symbol = parts[1].split(' ')[0]\n",
    "                uniprot_db[symbol] = str(record.seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be523cd9-31fd-4d46-8733-3598c5037c0f",
   "metadata": {},
   "source": [
    "**Normalize Gene Perturbation Names**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8413cc63-9406-419e-8213-8cb99782bf09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input sequence provided is already in string format. No operation performed\n",
      "Input sequence provided is already in string format. No operation performed\n",
      "85 input query terms found dup hits:\t[('ACTB', 2), ('ALDOA', 2), ('ALG2', 2), ('ARPC4', 2), ('ATP1A1', 3), ('ATR', 4), ('BAP1', 3), ('BDP\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1184,\n",
       " [{'query': 'AAMP', '_id': '14', '_score': 17.595827, 'symbol': 'AAMP'},\n",
       "  {'query': 'AARS', '_id': '16', '_score': 16.989502, 'symbol': 'AARS1'}])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mg = mygene.MyGeneInfo()\n",
    "mg_results = mg.querymany(unique_perts, scopes='symbol,alias', fields='symbol', species='human')\n",
    "len(mg_results),mg_results[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f15fea3-1110-46fd-96cc-15198a5e5b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_map = {}\n",
    "missing_genes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8b0bb13-a8e7-4af5-a81f-bff92aa18035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NEPRO', 'SMN2']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for res in mg_results:\n",
    "    original_query = res['query']\n",
    "    \n",
    "    if 'symbol' in res:\n",
    "        modern_symbol = res['symbol']\n",
    "    else:\n",
    "        print(f'failed to find {res}')\n",
    "        modern_symbol = original_query\n",
    "\n",
    "    if modern_symbol in uniprot_db:\n",
    "        final_map[original_query] = uniprot_db[modern_symbol]\n",
    "    else:\n",
    "        missing_genes.append(original_query)\n",
    "\n",
    "# final cleanup of duplicates\n",
    "missing_genes = [g for g in missing_genes if not final_map.get(g)]\n",
    "\n",
    "missing_genes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2350f3-1266-4664-bc14-2b4a1b002c59",
   "metadata": {},
   "source": [
    "**Search NCBI for missing protein sequences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b73b64ad-9ff2-49c9-a769-b768b619dd21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.53it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Entrez.email = 'test@test.com'\n",
    "results = {}\n",
    "for gene in tqdm(missing_genes.copy()):\n",
    "    try:\n",
    "        # Specific search for RefSeq proteins in Humans\n",
    "        term = f'{gene}[Gene Name] AND Homo sapiens[Organism] AND srcdb_refseq[PROP]'\n",
    "        \n",
    "        # 1. Search\n",
    "        search_handle = Entrez.esearch(db=\"protein\", term=term, retmax=1)\n",
    "        record = Entrez.read(search_handle)\n",
    "        \n",
    "        if record['IdList']:\n",
    "            # 2. Fetch\n",
    "            uid = record['IdList'][0]\n",
    "            fetch_handle = Entrez.efetch(db=\"protein\", id=uid, rettype=\"fasta\", retmode=\"text\")\n",
    "            seq_record = SeqIO.read(fetch_handle, \"fasta\")\n",
    "            results[gene] = str(seq_record.seq)\n",
    "            missing_genes.remove(gene)\n",
    "        else:\n",
    "            results[gene] = \"M\" # True Ghost Gene\n",
    "    except Exception as e:\n",
    "        print(f'Failed Entrez lookup for {gene}: {e}')\n",
    "        results[gene] = \"M\"\n",
    "        \n",
    "final_map.update(results)\n",
    "\n",
    "missing_genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79fd7eed-d2bc-4353-9046-6f371217ea38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Initial: 1087 | Found 1087 | Missing 0'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "found = len([v for v in final_map.values() if v != 'M'])\n",
    "total_perts = len(unique_perts)\n",
    "f'Initial: {total_perts} | Found {found} | Missing {total_perts - found}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77640c76-00db-4999-a8e1-4e13fc092c8d",
   "metadata": {},
   "source": [
    "**Saving Perturbations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f83160c-de36-4aac-b407-00b0d5330cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pert_dir /'perturbation_seq.json', 'w') as f:\n",
    "    json.dump({str(k): str(v) for k, v in final_map.items()}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda5602a-8d3d-4d57-ba5e-61df9214b274",
   "metadata": {},
   "source": [
    "**Get Model For Embeddings**\n",
    "for protein embeddings, we'll use the [ESM-2](https://huggingface.co/facebook/esm2_t6_8M_UR50D) for now given it's been a common workhorse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e647c60-1427-4c9e-a769-54be5db97a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda\n"
     ]
    }
   ],
   "source": [
    "model_name = 'facebook/esm2_t6_8M_UR50D' \n",
    "def get_device():\n",
    "    device = 'cpu'\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(1337)\n",
    "        device = 'cuda'\n",
    "    print(f'using {device}')\n",
    "    return device\n",
    "\n",
    "DEVICE = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c2b3a3f-466b-47df-a038-16c681914cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94fb506236f04156b436f0eafa74d264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/95.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6595e1389f5747a8ad6a39f53deafcb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/93.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e3d3fdcbdc146eebaa1bc26ca0a5ad3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62490a78b6cd49be9c341627f76c41ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/775 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f020525da2ee42959893d69769b70295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/31.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78e7ebc0-aeb6-4a71-a66a-c35d39d95247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EsmModel(\n",
       "  (embeddings): EsmEmbeddings(\n",
       "    (word_embeddings): Embedding(33, 320, padding_idx=1)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (encoder): EsmEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x EsmLayer(\n",
       "        (attention): EsmAttention(\n",
       "          (self): EsmSelfAttention(\n",
       "            (query): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (key): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (value): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (rotary_embeddings): RotaryEmbedding()\n",
       "          )\n",
       "          (output): EsmSelfOutput(\n",
       "            (dense): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (intermediate): EsmIntermediate(\n",
       "          (dense): Linear(in_features=320, out_features=1280, bias=True)\n",
       "        )\n",
       "        (output): EsmOutput(\n",
       "          (dense): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (emb_layer_norm_after): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (pooler): EsmPooler(\n",
       "    (dense): Linear(in_features=320, out_features=320, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "  (contact_head): EsmContactPredictionHead(\n",
       "    (regression): Linear(in_features=120, out_features=1, bias=True)\n",
       "    (activation): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320dd1ff-2b66-40dd-87f1-ac1aa09681c4",
   "metadata": {},
   "source": [
    "**Get Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20bb1f09-0322-4b05-a1a3-fabe037bbbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_genes = sorted(list(final_map.keys()))\n",
    "    \n",
    "embeddings = []\n",
    "pert_to_id = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c3511c5-fce8-4bcf-8376-19127a615f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 1087/1087 [00:08<00:00, 133.17it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for idx, gene in enumerate(tqdm(sorted_genes)):\n",
    "        seq = final_map[gene]\n",
    "        \n",
    "        # Map Gene Name -> Integer ID\n",
    "        pert_to_id[gene] = idx\n",
    "        \n",
    "        # Tokenize\n",
    "        # Truncate to 1024 AA (covers 95% of human proteins)\n",
    "        inputs = tokenizer(seq, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "        \n",
    "        # Forward Pass\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        # Mean Pooling: Compress [Seq_Len, 320] -> [320]\n",
    "        # We take the mean of all amino acids to get the \"whole protein\" vector\n",
    "        phys_vector = outputs.last_hidden_state[0].mean(dim=0).cpu().numpy()\n",
    "        embeddings.append(phys_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "64b1d86f-734a-46f8-8ae8-f7809ec4821b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pert_to_id['control'] = len(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bc3dbd-4183-4ea2-812e-0d47996788ab",
   "metadata": {},
   "source": [
    "**Save Pert Mapping/Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e81a88a-e1c6-48ac-960d-52326d793686",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_bank = np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5075aa69-ecb0-41f0-ac2d-8aa127c60567",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(pert_dir / 'action_embeddings_esm2.npy', final_bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0017ab82-40ed-4fa0-abf7-17b9469a8a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pert_dir / 'pert_to_id.json', 'w') as f:\n",
    "        json.dump(pert_to_id, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea22ac77-b220-4661-82d8-0f1a1fe5616e",
   "metadata": {},
   "source": [
    "## Pathway Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "715518e0-2c17-4adc-821e-70a9fb4674e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_res = gp.get_library(name='DSigDB', organism='Human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc3766c8-840a-45cb-94d0-89da76ec82f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pathways = {k: v for k, v in gs_res.items() if 80 <= len(v) <= 1400}\n",
    "pathway_names = list(valid_pathways.keys())[:n_pathways]\n",
    "# Save Pathway Names\n",
    "with open(data_dir / 'pathway_names.json', 'w') as f:\n",
    "    json.dump(pathway_names, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb1cffba-a0d6-4317-b0bd-58c540f664ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_mask = np.zeros((len(genes), len(pathway_names)), dtype=np.float32)\n",
    "gene_to_idx = {gene: i for i, gene in enumerate(genes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b090ba65-1aaa-4d7f-bd4b-c8f61bf18529",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p_idx, p_name in enumerate(pathway_names):\n",
    "    hit_count = 0\n",
    "    genes_in_pathway = valid_pathways[p_name]\n",
    "    for g in genes_in_pathway:\n",
    "        if g in gene_to_idx:\n",
    "            binary_mask[gene_to_idx[g], p_idx] = 1.0\n",
    "            hit_count += 1\n",
    "\n",
    "    if hit_count <= 1:\n",
    "    \tprint(f'pathway {p_name} had {hit_count} gene hits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a0c7cecd-2663-43bb-b47d-beb42294f7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(data_dir / 'binary_pathway_mask.npy', binary_mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be255d96-9645-496a-8f55-68225562445f",
   "metadata": {},
   "source": [
    "## Prepare Controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b86f5eef-8b0a-4471-ac50-2e4f97803237",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_mask = adata.obs['condition'] == 'ctrl'\n",
    "control_indices = np.where(control_mask)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "880f10c5-209e-4889-8ead-9ae6894c0d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format: List of (Gene_Vector, Total_Count_Scalar)\n",
    "control_bank = {\n",
    "    'X': adata.X[control_indices].toarray().astype(np.float32),\n",
    "    'total': adata.obs['log_total_counts'].values[control_indices].astype(np.float32)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0d5a2762-35dd-4921-a265-39073ad1dbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10691 control cells.\n"
     ]
    }
   ],
   "source": [
    "print(f'Found {len(control_indices)} control cells.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6e4054-d83e-43c7-86b9-974cd1358fe6",
   "metadata": {},
   "source": [
    "## Shard Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f2772821-a8da-49ae-86dc-5ddf31e102b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_map = pert_data.set2conditions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e346c31c-bc57-4467-94be-cc56022e6a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_shards(split_name, condition_list, ds_name):\n",
    "    \"\"\"\n",
    "    Iterates through cells belonging to the given conditions, \n",
    "    pairs them with random controls, and saves .npz shards.\n",
    "    \"\"\"\n",
    "    print(f'Split: {split_name.upper()}')\n",
    "    \n",
    "    # Filter cells belonging to these perturbations\n",
    "    # Note: We exclude 'ctrl' from the 'Treated' side of the pair\n",
    "    mask = adata.obs['condition'].isin(condition_list) & (adata.obs['condition'] != 'ctrl')\n",
    "    indices = np.where(mask)[0]\n",
    "    \n",
    "    # Shuffle for randomness\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    # Buffer for current shard\n",
    "    buffer = {\n",
    "        'control_x': [], \n",
    "        'control_total': [],\n",
    "        'case_x': [], \n",
    "        'case_total': [],\n",
    "        'action_ids': []\n",
    "    }\n",
    "    \n",
    "    shard_count = 0\n",
    "    save_path = tok_dir / split_name\n",
    "    \n",
    "    for idx in tqdm(indices):\n",
    "        # 1. Get Case Data\n",
    "        case_x = adata.X[idx].toarray().flatten().astype(np.float32)\n",
    "        case_tot = adata.obs['log_total_counts'].iloc[idx].astype(np.float32)\n",
    "        pert_name = adata.obs['condition'].iloc[idx]\n",
    "        \n",
    "        # 2. Get Random Control Pair\n",
    "        # Ideally we match batch, but Replogle K562 is often batch-corrected or single batch.\n",
    "        # For simplicity/speed here, we sample global control.\n",
    "        # (Improvement: dictionary mapping batch_id -> control_indices)\n",
    "        rand_idx = np.random.randint(len(control_bank['X']))\n",
    "        ctrl_x = control_bank['X'][rand_idx]\n",
    "        ctrl_tot = control_bank['total'][rand_idx]\n",
    "        \n",
    "        # 3. Add to Buffer\n",
    "        buffer['control_x'].append(ctrl_x)\n",
    "        buffer['control_total'].append(ctrl_tot)\n",
    "        buffer['case_x'].append(case_x)\n",
    "        buffer['case_total'].append(case_tot)\n",
    "        buffer['action_ids'].append(pert_to_id[clean_gears_name(pert_name)])\n",
    "        \n",
    "        # 4. Save if buffer full\n",
    "        if len(buffer['case_x']) >= chunk_size:\n",
    "            np.savez(\n",
    "                save_path / f'shard_{ds_name}_{split_name}_{shard_count:04d}.npz',\n",
    "                control=np.array(buffer['control_x']),\n",
    "                control_total=np.array(buffer['control_total']),\n",
    "                case=np.array(buffer['case_x']),\n",
    "                case_total=np.array(buffer['case_total']),\n",
    "                action_ids=np.array(buffer['action_ids'], dtype=np.int16)\n",
    "            )\n",
    "            # Reset\n",
    "            buffer = {k: [] for k in buffer}\n",
    "            shard_count += 1\n",
    "            \n",
    "    # Save leftovers\n",
    "    if len(buffer['case_x']) > 0:\n",
    "        np.savez(\n",
    "            save_path / f'shard_{ds_name}_{split_name}_{shard_count:04d}.npz',\n",
    "            control=np.array(buffer['control_x']),\n",
    "            control_total=np.array(buffer['control_total']),\n",
    "            case=np.array(buffer['case_x']),\n",
    "            case_total=np.array(buffer['case_total']),\n",
    "            action_ids=np.array(buffer['action_ids'], dtype=np.int16)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6a0bf447-aecf-4c4b-8ef1-6cbe398156b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split: TRAIN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 101682/101682 [00:06<00:00, 14758.92it/s]\n"
     ]
    }
   ],
   "source": [
    "write_shards('train', split_map['train'], dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6f659adb-efce-44e0-a628-2c4eb6f75d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split: VAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 11044/11044 [00:00<00:00, 21644.71it/s]\n"
     ]
    }
   ],
   "source": [
    "write_shards('val', split_map['val'], dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "21054db1-434c-4c50-a38d-b6094ba5a952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split: TEST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 38829/38829 [00:01<00:00, 21438.19it/s]\n"
     ]
    }
   ],
   "source": [
    "write_shards('test', split_map['test'], dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1eec9b-934f-4fd9-bad4-fffdf3abf5d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
