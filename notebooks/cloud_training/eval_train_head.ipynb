{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71522d3b-8805-48e6-bb36-1c9a7bd15482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import pearsonr\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "\n",
    "import biojepa_ac_model as model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe98e32-54b3-4da6-adf9-8b3e76e3bc0b",
   "metadata": {},
   "source": [
    "## BioJEPA Model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bae5c1c-d7c4-4216-ab99-42a26b7aee75",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f67335c-63ea-4f80-9c10-96b4d3e09c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    device = 'cpu'\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(1337)\n",
    "        device = 'cuda'\n",
    "    # elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    #     device = 'mps'\n",
    "    print(f'using {device}')\n",
    "    return device\n",
    "\n",
    "DEVICE = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5ec64e1-0d7a-4dee-b478-b99f901ce9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "n_embd = 128\n",
    "n_heads = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "025d6281-f460-46eb-89d9-842d1ca6fa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('/home/ubuntu/data/decoder')\n",
    "eval_dir = data_dir / 'tokenized' \n",
    "\n",
    "mask_path = eval_dir / 'binary_pathway_mask.npy'\n",
    "metadata_path = eval_dir / 'perturbation_map.json'\n",
    "checkpoint_dir = Path('/home/ubuntu/data/ac_model') / 'checkpoint'\n",
    "gene_names_path = eval_dir / 'gene_names.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9671701-5cc4-4016-a354-b71238c98947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Pathway Mask...\n",
      "Mask Loaded: 4096 Genes -> 1024 Pathways\n",
      "Loaded 2058 perturbations\n",
      "Loaded 4096 genes\n"
     ]
    }
   ],
   "source": [
    "print('Loading Pathway Mask...')\n",
    "binary_mask = np.load(mask_path)\n",
    "n_genes, n_pathways = binary_mask.shape\n",
    "print(f'Mask Loaded: {n_genes} Genes -> {n_pathways} Pathways')\n",
    "\n",
    "with open(metadata_path, 'r') as f:\n",
    "    pert_map = json.load(f)\n",
    "id_to_pert = {v: k for k, v in pert_map.items()}\n",
    "print(f'Loaded {len(id_to_pert.keys())} perturbations')\n",
    "\n",
    "with open(gene_names_path, 'r') as f:\n",
    "    gene_names = json.load(f)\n",
    "print(f'Loaded {len(gene_names)} genes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fe46b1b-aff7-4355-909f-9f71761f11c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = model.BioJepaConfig(\n",
    "    mask_matrix=binary_mask, \n",
    "    num_genes=n_genes,\n",
    "    num_pathways=n_pathways,\n",
    "    embed_dim=n_embd,\n",
    "    heads=n_heads\n",
    ")\n",
    "model = model.BioJepa(config).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a609e628-943e-41c2-a4d7-a71b9075a640",
   "metadata": {},
   "source": [
    "**Load Checkpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4aac628e-6ab7-4768-955a-3f99e79f58ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = checkpoint_dir / 'bio_jepa_ckpt_11699_final.pt'\n",
    "checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "\n",
    "keys = model.load_state_dict(checkpoint['model'])\n",
    "keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8287a64f-9d0d-4fbe-8c9b-a03c463a1487",
   "metadata": {},
   "source": [
    "**Freeze Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63812672-368f-483a-be2a-8b65cc849a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c98b4a-5df8-4894-9c56-ae0dbcbc3acd",
   "metadata": {},
   "source": [
    "## Build Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3dec1f3-d9cc-4849-aaa8-6a7d60199432",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BenchmarkDecoderConfig:\n",
    "    embed_dim: int = 384\n",
    "    num_pathways: int = 1024\n",
    "    num_genes: int = 4096\n",
    "    \n",
    "class BenchmarkDecoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # Step 1: Collapse the embedding dimension (384 -> 1)\n",
    "        # This asks: \"How active is this pathway overall?\"\n",
    "        self.pool = nn.Linear(config.embed_dim, 1) \n",
    "        \n",
    "        # Step 2: Decode Pathway Activity -> Gene Expression\n",
    "        # This learns the specific contribution of each pathway to each gene\n",
    "        self.decode = nn.Linear(config.num_pathways, config.num_genes) \n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None: \n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        \n",
    "    def forward(self, latents):\n",
    "        # latents: [Batch, 1024, 384]\n",
    "        \n",
    "        # 1. Calculate Pathway Scores\n",
    "        # [B, 1024, 384] -> [B, 1024, 1] -> [B, 1024]\n",
    "        scores = self.pool(latents).squeeze(-1)\n",
    "        \n",
    "        # 2. Project to Genes\n",
    "        # [B, 1024] @ [1024, 2000] -> [B, 2000]\n",
    "        gene_preds = self.decode(scores)\n",
    "        \n",
    "        return gene_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c316fe59-e4aa-4d12-9db4-2e38781f53bc",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bfad6fe-82ef-4019-b0ae-0338089f11c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_shard(filename):\n",
    "    print(f'loading {filename}') # Optional: reduce noise\n",
    "    with np.load(filename) as data:\n",
    "        # Load all arrays into memory\n",
    "        # We convert to correct types immediately to save hassle later\n",
    "        control_x = data['control'].astype(np.float32)\n",
    "        control_tot = data['control_total'].astype(np.float32)\n",
    "        case_x = data['case'].astype(np.float32)\n",
    "        case_tot = data['case_total'].astype(np.float32)\n",
    "        action_ids = data['action_ids'].astype(np.int64)\n",
    "        \n",
    "    return control_x, control_tot, case_x, case_tot, action_ids\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, batch, split, device, tok_dir):\n",
    "        self.batch = batch\n",
    "        self.split = split\n",
    "        self.device = device\n",
    "        \n",
    "        # 1. Find Shards\n",
    "        data_root = tok_dir / f'{split}'\n",
    "        shards = list(data_root.glob('*.npz'))\n",
    "\n",
    "        self.total_files = len(shards)\n",
    "        self.shards = sorted(shards)\n",
    "\n",
    "        assert len(shards) > 0, f'no shards found for split {split}'\n",
    "        print(f'found {len(shards)} shards for split {split}')\n",
    "        \n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # Create a randomized queue of shards\n",
    "        self.remaining_shards = list(self.shards)\n",
    "        random.shuffle(self.remaining_shards)\n",
    "        \n",
    "        self.current_shard_idx = -1\n",
    "        self.load_next_shard()\n",
    "\n",
    "    def load_next_shard(self):\n",
    "        self.current_shard_idx += 1\n",
    "        \n",
    "        # If we ran out of shards, reset (Epoch done)\n",
    "        if self.current_shard_idx >= len(self.remaining_shards):\n",
    "            self.reset() # This resets shard_idx to -1 and reshuffles\n",
    "            return \n",
    "\n",
    "        # Load the file\n",
    "        filename = self.remaining_shards[self.current_shard_idx]\n",
    "        self.data_tuple = load_shard(filename)\n",
    "        \n",
    "        # Shuffle the items INSIDE the shard\n",
    "        # This is critical so we don't just memorize the sorted order of the shard\n",
    "        n_samples = len(self.data_tuple[0])\n",
    "        self.perm = np.random.permutation(n_samples)\n",
    "        self.current_position = 0\n",
    "        self.total_samples_in_shard = n_samples\n",
    "\n",
    "    def next_batch(self):\n",
    "        batch = self.batch\n",
    "        \n",
    "        # Check if we have enough data left in current shard\n",
    "        if self.current_position + batch > self.total_samples_in_shard:\n",
    "            self.load_next_shard()\n",
    "            # Recursively call to get batch from the new shard\n",
    "            return self.next_batch()\n",
    "            \n",
    "        # Get indices for this batch\n",
    "        indices = self.perm[self.current_position : self.current_position + batch]\n",
    "        self.current_position += batch\n",
    "        \n",
    "        # Slice data using the shuffled indices\n",
    "        # data_tuple structure: (xc, xct, xt, xtt, aid)\n",
    "        batch_cont_x  = torch.from_numpy(self.data_tuple[0][indices]).to(self.device)\n",
    "        batch_cont_tot = torch.from_numpy(self.data_tuple[1][indices]).to(self.device)\n",
    "        batch_case_x  = torch.from_numpy(self.data_tuple[2][indices]).to(self.device)\n",
    "        batch_case_t = torch.from_numpy(self.data_tuple[3][indices]).to(self.device)\n",
    "        batch_aid = torch.from_numpy(self.data_tuple[4][indices]).to(self.device)\n",
    "        \n",
    "        return batch_cont_x, batch_cont_tot, batch_case_x, batch_case_t, batch_aid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7f7811-5727-45e3-8194-505072bb16eb",
   "metadata": {},
   "source": [
    "**Data Loader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e1d5000-f278-4f9b-9227-9554f31e9a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 6 shards for split train\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0001.npz\n",
      "found 1 shards for split val\n",
      "loading /home/ubuntu/data/decoder/tokenized/val/shard_val_0000.npz\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoaderLite(batch=batch_size, split='train', device=DEVICE, tok_dir=eval_dir)\n",
    "val_loader = DataLoaderLite(batch=batch_size, split='val', device=DEVICE, tok_dir=eval_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9025bf-05d7-4ff8-ae9a-c1b87471c3e5",
   "metadata": {},
   "source": [
    "## Training Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f484f1f2-2933-413d-80c0-f0c970656453",
   "metadata": {},
   "source": [
    "### Training Config/Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad700ee3-e710-43de-be89-4003c922a41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_decoder = 1e-2\n",
    "epochs = 10\n",
    "tok_file_chunk_size = 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0494e7de-e96d-4989-b47b-9229c050a263",
   "metadata": {},
   "source": [
    "**Initialize Decoder** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74224c15-ff49-41d9-9552-b8ec9ed83dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BenchmarkDecoderConfig(\n",
    "    embed_dim= n_embd,\n",
    "    num_pathways= n_pathways,\n",
    "    num_genes= n_genes\n",
    ")\n",
    "\n",
    "decoder = BenchmarkDecoder(config).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2da30fa-9c44-40be-921c-5184c764f0e5",
   "metadata": {},
   "source": [
    "**Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b197e689-8b98-4cf9-8382-6116bf2ce440",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(decoder.parameters(), lr=lr_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958f11cf-8c9f-49b9-b526-74be69d2126f",
   "metadata": {},
   "source": [
    "**Training Lenght**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "009526cc-7703-497d-87cb-4a50f9c1c1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_total_examples = 101682\n",
    "val_total_examples = 11044\n",
    "test_total_examples = 38829"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "766a254e-2fd7-4274-91c4-7ceab21d8d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(397, 3970)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch = train_total_examples // batch_size\n",
    "max_steps = epochs * steps_per_epoch\n",
    "steps_per_epoch, max_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cf4803-b42e-4d1b-89db-c9b43d6e3558",
   "metadata": {},
   "source": [
    "**Scheduler**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c683627d-3da6-45e9-9827-06d8586a1f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=lr_decoder, total_steps=max_steps, pct_start=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15c5c7c-b741-486e-9ce2-99c2e253b05d",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82cb876c-e45d-43f7-94aa-2fa7f44b15c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossi = []\n",
    "step = 0\n",
    "total_epoch_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69468a01-365b-42b9-b041-5d6969e5b4ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.9044\n",
      "Step 0 | Loss: 0.91788 | LR: 4.01e-04\n",
      "Step 25 | Loss: 0.90334 | LR: 8.05e-04\n",
      "Step 50 | Loss: 0.91142 | LR: 1.89e-03\n",
      "Step 75 | Loss: 0.91070 | LR: 3.50e-03\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0000.npz\n",
      "loading /home/ubuntu/data/decoder/tokenized/val/shard_val_0000.npz\n",
      "test loss: 0.8946\n",
      "Step 100 | Loss: 0.90052 | LR: 5.37e-03\n",
      "Step 125 | Loss: 0.89411 | LR: 7.22e-03\n",
      "Step 150 | Loss: 0.87945 | LR: 8.75e-03\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0003.npz\n",
      "Step 175 | Loss: 0.87421 | LR: 9.72e-03\n",
      "test loss: 0.8561\n",
      "Step 200 | Loss: 0.84630 | LR: 1.00e-02\n",
      "Step 225 | Loss: 0.83501 | LR: 1.00e-02\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0002.npz\n",
      "Step 250 | Loss: 0.85298 | LR: 1.00e-02\n",
      "Step 275 | Loss: 0.83646 | LR: 9.99e-03\n",
      "loading /home/ubuntu/data/decoder/tokenized/val/shard_val_0000.npz\n",
      "test loss: 0.8328\n",
      "Step 300 | Loss: 0.84419 | LR: 9.98e-03\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0005.npz\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0004.npz\n",
      "Step 325 | Loss: 0.83732 | LR: 9.97e-03\n",
      "Step 350 | Loss: 0.81735 | LR: 9.96e-03\n",
      "Step 375 | Loss: 0.82157 | LR: 9.94e-03\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0000.npz\n",
      "=== Step 397 Done. Avg Loss: 0.86841 ===\n",
      "test loss: 0.8138\n",
      "Step 400 | Loss: 0.81193 | LR: 9.93e-03\n",
      "Step 425 | Loss: 0.82432 | LR: 9.91e-03\n",
      "Step 450 | Loss: 0.80991 | LR: 9.89e-03\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0004.npz\n",
      "Step 475 | Loss: 0.81130 | LR: 9.87e-03\n",
      "loading /home/ubuntu/data/decoder/tokenized/val/shard_val_0000.npz\n",
      "test loss: 0.8052\n",
      "Step 500 | Loss: 0.80646 | LR: 9.84e-03\n",
      "Step 525 | Loss: 0.80852 | LR: 9.81e-03\n",
      "Step 550 | Loss: 0.78757 | LR: 9.78e-03\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0001.npz\n",
      "Step 575 | Loss: 0.80392 | LR: 9.75e-03\n",
      "loading /home/ubuntu/data/decoder/tokenized/val/shard_val_0000.npz\n",
      "test loss: 0.7992\n",
      "Step 600 | Loss: 0.81158 | LR: 9.72e-03\n",
      "Step 625 | Loss: 0.81201 | LR: 9.68e-03\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0005.npz\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0003.npz\n",
      "Step 650 | Loss: 0.79255 | LR: 9.65e-03\n",
      "Step 675 | Loss: 0.78978 | LR: 9.61e-03\n",
      "test loss: 0.7945\n",
      "Step 700 | Loss: 0.78625 | LR: 9.57e-03\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0002.npz\n",
      "Step 725 | Loss: 0.79782 | LR: 9.52e-03\n",
      "Step 750 | Loss: 0.79240 | LR: 9.48e-03\n",
      "Step 775 | Loss: 0.80910 | LR: 9.43e-03\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0003.npz\n",
      "=== Step 794 Done. Avg Loss: 0.80157 ===\n",
      "loading /home/ubuntu/data/decoder/tokenized/val/shard_val_0000.npz\n",
      "test loss: 0.7902\n",
      "Step 800 | Loss: 0.77501 | LR: 9.38e-03\n",
      "Step 825 | Loss: 0.78379 | LR: 9.33e-03\n",
      "Step 850 | Loss: 0.79108 | LR: 9.28e-03\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0004.npz\n",
      "Step 875 | Loss: 0.79553 | LR: 9.22e-03\n",
      "test loss: 0.7881\n",
      "Step 900 | Loss: 0.78466 | LR: 9.17e-03\n",
      "Step 925 | Loss: 0.78265 | LR: 9.11e-03\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0000.npz\n",
      "Step 950 | Loss: 0.78147 | LR: 9.05e-03\n",
      "Step 975 | Loss: 0.78251 | LR: 8.99e-03\n",
      "loading /home/ubuntu/data/decoder/tokenized/val/shard_val_0000.npz\n",
      "test loss: 0.7873\n",
      "Step 1000 | Loss: 0.77666 | LR: 8.92e-03\n",
      "Step 1025 | Loss: 0.77927 | LR: 8.86e-03\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0002.npz\n",
      "Step 1050 | Loss: 0.77933 | LR: 8.79e-03\n",
      "Step 1075 | Loss: 0.78610 | LR: 8.72e-03\n",
      "test loss: 0.7812\n",
      "Step 1100 | Loss: 0.78228 | LR: 8.65e-03\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0001.npz\n",
      "Step 1125 | Loss: 0.78742 | LR: 8.58e-03\n",
      "Step 1150 | Loss: 0.78392 | LR: 8.50e-03\n",
      "Step 1175 | Loss: 0.78898 | LR: 8.43e-03\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0005.npz\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0004.npz\n",
      "=== Step 1191 Done. Avg Loss: 0.78492 ===\n",
      "loading /home/ubuntu/data/decoder/tokenized/val/shard_val_0000.npz\n",
      "test loss: 0.7805\n",
      "Step 1200 | Loss: 0.77155 | LR: 8.35e-03\n",
      "Step 1225 | Loss: 0.78381 | LR: 8.27e-03\n",
      "Step 1250 | Loss: 0.78441 | LR: 8.20e-03\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0001.npz\n",
      "Step 1275 | Loss: 0.76052 | LR: 8.11e-03\n",
      "loading /home/ubuntu/data/decoder/tokenized/val/shard_val_0000.npz\n",
      "test loss: 0.7797\n",
      "Step 1300 | Loss: 0.76783 | LR: 8.03e-03\n",
      "Step 1325 | Loss: 0.78377 | LR: 7.95e-03\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0002.npz\n",
      "Step 1350 | Loss: 0.77112 | LR: 7.86e-03\n",
      "Step 1375 | Loss: 0.77369 | LR: 7.78e-03\n",
      "test loss: 0.7791\n",
      "Step 1400 | Loss: 0.78081 | LR: 7.69e-03\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0000.npz\n",
      "Step 1425 | Loss: 0.76662 | LR: 7.60e-03\n",
      "Step 1450 | Loss: 0.76861 | LR: 7.51e-03\n",
      "Step 1475 | Loss: 0.77666 | LR: 7.42e-03\n",
      "loading /home/ubuntu/data/decoder/tokenized/val/shard_val_0000.npz\n",
      "test loss: 0.7761\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0003.npz\n",
      "Step 1500 | Loss: 0.77559 | LR: 7.33e-03\n",
      "Step 1525 | Loss: 0.77113 | LR: 7.24e-03\n",
      "Step 1550 | Loss: 0.78627 | LR: 7.14e-03\n",
      "Step 1575 | Loss: 0.76033 | LR: 7.05e-03\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0005.npz\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0001.npz\n",
      "=== Step 1588 Done. Avg Loss: 0.77623 ===\n",
      "test loss: 0.7747\n",
      "Step 1600 | Loss: 0.75962 | LR: 6.95e-03\n",
      "Step 1625 | Loss: 0.76495 | LR: 6.86e-03\n",
      "Step 1650 | Loss: 0.76246 | LR: 6.76e-03\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0000.npz\n",
      "Step 1675 | Loss: 0.76940 | LR: 6.66e-03\n",
      "loading /home/ubuntu/data/decoder/tokenized/val/shard_val_0000.npz\n",
      "test loss: 0.7744\n",
      "Step 1700 | Loss: 0.76455 | LR: 6.57e-03\n",
      "Step 1725 | Loss: 0.78256 | LR: 6.47e-03\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0004.npz\n",
      "Step 1750 | Loss: 0.76673 | LR: 6.37e-03\n",
      "Step 1775 | Loss: 0.77466 | LR: 6.27e-03\n",
      "loading /home/ubuntu/data/decoder/tokenized/val/shard_val_0000.npz\n",
      "test loss: 0.7720\n",
      "Step 1800 | Loss: 0.76846 | LR: 6.16e-03\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0002.npz\n",
      "Step 1825 | Loss: 0.77415 | LR: 6.06e-03\n",
      "Step 1850 | Loss: 0.76862 | LR: 5.96e-03\n",
      "Step 1875 | Loss: 0.77458 | LR: 5.86e-03\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0005.npz\n",
      "test loss: 0.7723\n",
      "Step 1900 | Loss: 0.76616 | LR: 5.76e-03\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0003.npz\n",
      "Step 1925 | Loss: 0.76254 | LR: 5.65e-03\n",
      "Step 1950 | Loss: 0.76683 | LR: 5.55e-03\n",
      "Step 1975 | Loss: 0.75769 | LR: 5.45e-03\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0005.npz\n",
      "=== Step 1985 Done. Avg Loss: 0.76989 ===\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0003.npz\n",
      "loading /home/ubuntu/data/decoder/tokenized/val/shard_val_0000.npz\n",
      "test loss: 0.7706\n",
      "Step 2000 | Loss: 0.75779 | LR: 5.34e-03\n",
      "Step 2025 | Loss: 0.75506 | LR: 5.24e-03\n",
      "Step 2050 | Loss: 0.75465 | LR: 5.13e-03\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0001.npz\n",
      "Step 2075 | Loss: 0.75906 | LR: 5.03e-03\n",
      "test loss: 0.7691\n",
      "Step 2100 | Loss: 0.75764 | LR: 4.93e-03\n",
      "Step 2125 | Loss: 0.76975 | LR: 4.82e-03\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0000.npz\n",
      "Step 2150 | Loss: 0.75831 | LR: 4.72e-03\n",
      "Step 2175 | Loss: 0.76788 | LR: 4.61e-03\n",
      "loading /home/ubuntu/data/decoder/tokenized/val/shard_val_0000.npz\n",
      "test loss: 0.7686\n",
      "Step 2200 | Loss: 0.76987 | LR: 4.51e-03\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0004.npz\n",
      "Step 2225 | Loss: 0.75525 | LR: 4.41e-03\n",
      "Step 2250 | Loss: 0.75803 | LR: 4.30e-03\n",
      "Step 2275 | Loss: 0.76686 | LR: 4.20e-03\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0002.npz\n",
      "test loss: 0.7676\n",
      "Step 2300 | Loss: 0.76213 | LR: 4.10e-03\n",
      "Step 2325 | Loss: 0.77902 | LR: 4.00e-03\n",
      "Step 2350 | Loss: 0.75573 | LR: 3.89e-03\n",
      "Step 2375 | Loss: 0.77367 | LR: 3.79e-03\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0000.npz\n",
      "=== Step 2382 Done. Avg Loss: 0.76430 ===\n",
      "loading /home/ubuntu/data/decoder/tokenized/val/shard_val_0000.npz\n",
      "test loss: 0.7666\n",
      "Step 2400 | Loss: 0.76287 | LR: 3.69e-03\n",
      "Step 2425 | Loss: 0.77283 | LR: 3.59e-03\n",
      "Step 2450 | Loss: 0.74543 | LR: 3.49e-03\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0005.npz\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0004.npz\n",
      "Step 2475 | Loss: 0.76155 | LR: 3.39e-03\n",
      "loading /home/ubuntu/data/decoder/tokenized/val/shard_val_0000.npz\n",
      "test loss: 0.7654\n",
      "Step 2500 | Loss: 0.76317 | LR: 3.30e-03\n",
      "Step 2525 | Loss: 0.76265 | LR: 3.20e-03\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0003.npz\n",
      "Step 2550 | Loss: 0.76150 | LR: 3.10e-03\n",
      "Step 2575 | Loss: 0.76989 | LR: 3.00e-03\n",
      "test loss: 0.7642\n",
      "Step 2600 | Loss: 0.76099 | LR: 2.91e-03\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0002.npz\n",
      "Step 2625 | Loss: 0.75293 | LR: 2.82e-03\n",
      "Step 2650 | Loss: 0.75930 | LR: 2.72e-03\n",
      "Step 2675 | Loss: 0.74263 | LR: 2.63e-03\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0001.npz\n",
      "loading /home/ubuntu/data/decoder/tokenized/val/shard_val_0000.npz\n",
      "test loss: 0.7649\n",
      "Step 2700 | Loss: 0.76274 | LR: 2.54e-03\n",
      "Step 2725 | Loss: 0.77104 | LR: 2.45e-03\n",
      "Step 2750 | Loss: 0.77270 | LR: 2.36e-03\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0005.npz\n",
      "Step 2775 | Loss: 0.74895 | LR: 2.27e-03\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0003.npz\n",
      "=== Step 2779 Done. Avg Loss: 0.75964 ===\n",
      "test loss: 0.7632\n",
      "Step 2800 | Loss: 0.75820 | LR: 2.19e-03\n",
      "Step 2825 | Loss: 0.74681 | LR: 2.10e-03\n",
      "Step 2850 | Loss: 0.75292 | LR: 2.02e-03\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0002.npz\n",
      "Step 2875 | Loss: 0.76095 | LR: 1.93e-03\n",
      "loading /home/ubuntu/data/decoder/tokenized/val/shard_val_0000.npz\n",
      "test loss: 0.7642\n",
      "Step 2900 | Loss: 0.75898 | LR: 1.85e-03\n",
      "Step 2925 | Loss: 0.76055 | LR: 1.77e-03\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0000.npz\n",
      "Step 2950 | Loss: 0.75886 | LR: 1.69e-03\n",
      "Step 2975 | Loss: 0.75906 | LR: 1.62e-03\n",
      "loading /home/ubuntu/data/decoder/tokenized/val/shard_val_0000.npz\n",
      "test loss: 0.7611\n",
      "Step 3000 | Loss: 0.75641 | LR: 1.54e-03\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0004.npz\n",
      "Step 3025 | Loss: 0.76570 | LR: 1.46e-03\n",
      "Step 3050 | Loss: 0.75972 | LR: 1.39e-03\n",
      "Step 3075 | Loss: 0.74714 | LR: 1.32e-03\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0001.npz\n",
      "test loss: 0.7610\n",
      "Step 3100 | Loss: 0.76492 | LR: 1.25e-03\n",
      "Step 3125 | Loss: 0.75838 | LR: 1.18e-03\n",
      "Step 3150 | Loss: 0.75863 | LR: 1.12e-03\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0001.npz\n",
      "Step 3175 | Loss: 0.76467 | LR: 1.05e-03\n",
      "=== Step 3176 Done. Avg Loss: 0.75555 ===\n",
      "loading /home/ubuntu/data/decoder/tokenized/val/shard_val_0000.npz\n",
      "test loss: 0.7619\n",
      "Step 3200 | Loss: 0.75109 | LR: 9.89e-04\n",
      "Step 3225 | Loss: 0.74719 | LR: 9.27e-04\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0004.npz\n",
      "Step 3250 | Loss: 0.75547 | LR: 8.68e-04\n",
      "Step 3275 | Loss: 0.75568 | LR: 8.10e-04\n",
      "test loss: 0.7609\n",
      "Step 3300 | Loss: 0.74539 | LR: 7.54e-04\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0002.npz\n",
      "Step 3325 | Loss: 0.74564 | LR: 7.00e-04\n",
      "Step 3350 | Loss: 0.76518 | LR: 6.48e-04\n",
      "Step 3375 | Loss: 0.75539 | LR: 5.98e-04\n",
      "loading /home/ubuntu/data/decoder/tokenized/val/shard_val_0000.npz\n",
      "test loss: 0.7611\n",
      "Step 3400 | Loss: 0.75528 | LR: 5.49e-04\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0005.npz\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0003.npz\n",
      "Step 3425 | Loss: 0.75472 | LR: 5.03e-04\n",
      "Step 3450 | Loss: 0.75528 | LR: 4.58e-04\n",
      "Step 3475 | Loss: 0.73741 | LR: 4.16e-04\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0000.npz\n",
      "test loss: 0.7587\n",
      "Step 3500 | Loss: 0.75803 | LR: 3.75e-04\n",
      "Step 3525 | Loss: 0.74696 | LR: 3.37e-04\n",
      "Step 3550 | Loss: 0.74449 | LR: 3.00e-04\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0002.npz\n",
      "=== Step 3573 Done. Avg Loss: 0.75203 ===\n",
      "Step 3575 | Loss: 0.75346 | LR: 2.66e-04\n",
      "loading /home/ubuntu/data/decoder/tokenized/val/shard_val_0000.npz\n",
      "test loss: 0.7593\n",
      "Step 3600 | Loss: 0.73804 | LR: 2.33e-04\n",
      "Step 3625 | Loss: 0.75555 | LR: 2.03e-04\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0003.npz\n",
      "Step 3650 | Loss: 0.73823 | LR: 1.74e-04\n",
      "Step 3675 | Loss: 0.75120 | LR: 1.48e-04\n",
      "loading /home/ubuntu/data/decoder/tokenized/val/shard_val_0000.npz\n",
      "test loss: 0.7596\n",
      "Step 3700 | Loss: 0.75508 | LR: 1.24e-04\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0000.npz\n",
      "Step 3725 | Loss: 0.75501 | LR: 1.02e-04\n",
      "Step 3750 | Loss: 0.75087 | LR: 8.23e-05\n",
      "Step 3775 | Loss: 0.74288 | LR: 6.45e-05\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0004.npz\n",
      "test loss: 0.7595\n",
      "Step 3800 | Loss: 0.75421 | LR: 4.89e-05\n",
      "Step 3825 | Loss: 0.75335 | LR: 3.55e-05\n",
      "Step 3850 | Loss: 0.74283 | LR: 2.42e-05\n",
      "Step 3875 | Loss: 0.74301 | LR: 1.50e-05\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0001.npz\n",
      "loading /home/ubuntu/data/decoder/tokenized/val/shard_val_0000.npz\n",
      "test loss: 0.7597\n",
      "Step 3900 | Loss: 0.73913 | LR: 8.06e-06\n",
      "Step 3925 | Loss: 0.75595 | LR: 3.25e-06\n",
      "Step 3950 | Loss: 0.76065 | LR: 6.02e-07\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0005.npz\n",
      "loading /home/ubuntu/data/decoder/tokenized/train/shard_train_0004.npz\n",
      "test loss: 0.7581\n"
     ]
    }
   ],
   "source": [
    "for step in range(max_steps):\n",
    "\n",
    "    last_step = (step == max_steps - 1)\n",
    "\n",
    "    # once in a while evaluate our validation set loss\n",
    "    if step % 100 == 0 or last_step:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss_accum = 0.0\n",
    "            val_loss_steps = 25\n",
    "            for i in range(val_loss_steps):\n",
    "                cont_x, cont_tot, case_x, case_tot, act_id = val_loader.next_batch()\n",
    "\n",
    "                # run BioJEPA\n",
    "                with torch.no_grad():\n",
    "                    z_context = model.student(cont_x, cont_tot)\n",
    "                    z_pred = model.predictor(z_context, act_id)\n",
    "\n",
    "                # run new decoder\n",
    "                pred_delta = decoder(z_pred) - decoder(z_context)\n",
    "                real_delta = case_x - cont_x\n",
    "\n",
    "                loss = F.mse_loss(pred_delta, real_delta)\n",
    "                loss = loss / val_loss_steps\n",
    "                val_loss_accum += loss.detach()\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "        print(f'test loss: {val_loss_accum.item():.4f}')\n",
    "\n",
    "\n",
    "    # periodically save checkpoint\n",
    "    if step > 0 and (step % 1000 == 0 or step % steps_per_epoch ==0) and not last_step:\n",
    "        # Save Checkpoint\n",
    "        torch.save({\n",
    "            'model': decoder.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'step': step\n",
    "        }, checkpoint_dir / f'biojepa_decoder_ckpt_{step}.pt')\n",
    "\n",
    "    # actual training\n",
    "    decoder.train\n",
    "    cont_x, cont_tot, case_x, case_tot, act_id = train_loader.next_batch()\n",
    "\n",
    "    # run frozen BioJEPA\n",
    "    with torch.no_grad():\n",
    "        z_context = model.student(cont_x, cont_tot)\n",
    "        z_pred = model.predictor(z_context, act_id)\n",
    "\n",
    "    # run decoder\n",
    "    pred_delta = decoder(z_pred) - decoder(z_context)\n",
    "    real_delta = case_x - cont_x\n",
    "    real_delta = case_x - cont_x\n",
    "\n",
    "    # loss\n",
    "    pred_delta = decoder(z_pred) - decoder(z_context)\n",
    "    real_delta = case_x - cont_x\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss = F.mse_loss(pred_delta, real_delta)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    # loss caching\n",
    "    lossi.append(loss.item())\n",
    "    total_epoch_loss += loss.item()\n",
    "\n",
    "    if step % 25 == 0:\n",
    "        print(f\"Step {step} | Loss: {loss.item():.5f} | LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "    \n",
    "    \n",
    "    if step > 0 and step % steps_per_epoch == 0:   \n",
    "        avg_loss = total_epoch_loss / steps_per_epoch\n",
    "        print(f\"=== Step {step} Done. Avg Loss: {avg_loss:.5f} ===\")\n",
    "        total_epoch_loss = 0\n",
    "    \n",
    "    if last_step:\n",
    "        # Save Checkpoint\n",
    "        torch.save({\n",
    "            'model': decoder.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'step': step\n",
    "        }, checkpoint_dir / f'biojepa_decoder_ckpt_{step}_final.pt')\n",
    "\n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c924050-b2d8-4973-a3c1-f9dd0a3d1f03",
   "metadata": {},
   "source": [
    "**Training Loss Plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b11fde37-0cac-429d-be0c-351390bac22d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lossi)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8aaa78-e607-4bc4-8cde-49a529984c1b",
   "metadata": {},
   "source": [
    "## Trained Decoder Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "974730ff-1cda-43e0-a72f-df0986bb9daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from scipy.stats import ConstantInputWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "94fd5355-31c8-47fc-a2fd-9b1fa9f91576",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.eval()\n",
    "correlations = []\n",
    "mses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "417a9438-c9f1-4cb0-8da2-e9fe7ec831a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_steps_per_epoch = val_total_examples // batch_size\n",
    "test_steps_per_epoch = test_total_examples // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "27c89be7-508c-4e06-9dfc-9f4809807077",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  72%|                 | 31/43 [00:17<00:06,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading /home/ubuntu/data/decoder/tokenized/val/shard_val_0000.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|| 43/43 [00:24<00:00,  1.74it/s]\n"
     ]
    }
   ],
   "source": [
    "for step in tqdm(range(val_steps_per_epoch), desc=\"Evaluating\"):\n",
    "    \n",
    "    # Custom Loader Call\n",
    "    cont_x, cont_tot, case_x, case_tot, act_id = val_loader.next_batch()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        z_context = model.student(cont_x, cont_tot)\n",
    "        z_pred = model.predictor(z_context, act_id)\n",
    "        \n",
    "        pred_delta = decoder(z_pred) - decoder(z_context)\n",
    "        real_delta = case_x - cont_x\n",
    "        \n",
    "    # Per-Sample Metrics\n",
    "    for i in range(len(pred_delta)):\n",
    "        p = pred_delta[i].cpu().numpy()\n",
    "        t = real_delta[i].cpu().numpy()\n",
    "        \n",
    "        # Metric: Pearson Correlation on Top 20 DEGs\n",
    "        top_20_idx = np.argsort(np.abs(t))[-20:]\n",
    "        \n",
    "        if np.std(p[top_20_idx]) > 1e-9 and np.std(t[top_20_idx]) > 1e-9:\n",
    "            \n",
    "            try:\n",
    "                corr, _ = pearsonr(p[top_20_idx], t[top_20_idx])\n",
    "                correlations.append(corr)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        mses.append(np.mean((p - t)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "750afdb9-ba92-4a7a-874d-0cc221525de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global MSE: 0.7589\n",
      "Top-20 Pearson R: 0.6744\n"
     ]
    }
   ],
   "source": [
    "mean_mse = np.mean(mses)\n",
    "mean_corr = np.mean(correlations)\n",
    "print(f'Global MSE: {mean_mse:.4f}')\n",
    "print(f'Top-20 Pearson R: {mean_corr:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "44680d47-980a-4b0e-b910-471b65efd220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " FUNCTIONAL (Better than random)\n"
     ]
    }
   ],
   "source": [
    "if mean_corr > 0.75:\n",
    "    print(' SOTA COMPETITIVE (Matches GEARS)')\n",
    "elif mean_corr > 0.40:\n",
    "    print(' FUNCTIONAL (Better than random)')\n",
    "else:\n",
    "    print(' NEEDS IMPROVEMENT')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6b2741-9067-4cee-b37f-2ee471d3c7e6",
   "metadata": {},
   "source": [
    "## Trained Decoder Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ecd5cb77-d9a4-43d8-ad77-b6d102d00caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_correlations = []\n",
    "eval_mses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "17c58931-04dc-4c75-a84d-23fbf3f7abc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 2 shards for split test\n",
      "loading /home/ubuntu/data/decoder/tokenized/test/shard_test_0001.npz\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoaderLite(batch=batch_size, split='test', device=DEVICE, tok_dir=eval_dir)\n",
    "test_steps_per_epoch = test_total_examples // batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9022a3d0-68f1-4de8-bb53-33e5bd05e212",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  48%|                                | 73/151 [00:41<00:44,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading /home/ubuntu/data/decoder/tokenized/test/shard_test_0000.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|| 151/151 [01:25<00:00,  1.76it/s]\n"
     ]
    }
   ],
   "source": [
    "for step in tqdm(range(test_steps_per_epoch), desc=\"Evaluating\"):\n",
    "    \n",
    "    # Custom Loader Call\n",
    "    cont_x, cont_tot, case_x, case_tot, act_id = test_loader.next_batch()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        z_context = model.student(cont_x, cont_tot)\n",
    "        z_pred = model.predictor(z_context, act_id)\n",
    "        \n",
    "        pred_delta = decoder(z_pred) - decoder(z_context)\n",
    "        real_delta = case_x - cont_x\n",
    "        \n",
    "    # Per-Sample Metrics\n",
    "    for i in range(len(pred_delta)):\n",
    "        p = pred_delta[i].cpu().numpy()\n",
    "        t = real_delta[i].cpu().numpy()\n",
    "        \n",
    "        # Metric: Pearson Correlation on Top 20 DEGs\n",
    "        top_20_idx = np.argsort(np.abs(t))[-20:]\n",
    "        \n",
    "        if np.std(p[top_20_idx]) > 1e-9 and np.std(t[top_20_idx]) > 1e-9:\n",
    "            corr, _ = pearsonr(p[top_20_idx], t[top_20_idx])\n",
    "            eval_correlations.append(corr)\n",
    "            \n",
    "        eval_mses.append(np.mean((p - t)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "40ee16bc-f9f8-4ab6-bc12-9e72fafe1b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global MSE: 0.7576\n",
      "Top-20 Pearson R: 0.6751\n"
     ]
    }
   ],
   "source": [
    "eval_mean_mse = np.mean(eval_mses)\n",
    "eval_mean_corr = np.mean(eval_correlations)\n",
    "print(f'Global MSE: {eval_mean_mse:.4f}')\n",
    "print(f'Top-20 Pearson R: {eval_mean_corr:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
