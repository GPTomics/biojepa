{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfe968dc-a937-469a-89d8-7ba179a2e01a",
   "metadata": {},
   "source": [
    "# Bio-JEPA AC \n",
    "\n",
    "Based on [V-JEPA 2 AC](https://arxiv.org/abs/2506.09985)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b67dcd-a768-447b-ad7b-77e8cddd59c0",
   "metadata": {},
   "source": [
    "**Goal**\n",
    "Our goal is to build a World Model for cell biology. In it's current configuration, our model calculates that hitting a cell with a specific gene knockout, or drug if we had it in the dataset, causes specific biological pathways to activate or shut down. Unlike an LLM, this is a predictive simulation that operates entirely within a compressed mathematical space to understand cause and effect.\n",
    "\n",
    "The process begins with the inputs, which feed the model three distinct pieces of information for every training step. \n",
    "\tFirst, it receives the **Before** state, which is data representing a healthy control cell. Instead of a messy list of 20,000 raw gene counts, the tokenizer has already compressed this into a structured set of pathway scores (using [Reactome Pathway 2024 data](https://maayanlab.cloud/Harmonizome/dataset/Reactome+Pathways+2024)). This essentially tells the model that the cell currently has high energy, low stress, and normal growth. \n",
    "\tSecond, the model receives the **Action**, which is the specific perturbation performed in the lab, in our case a CRISPR knockdown of a specific gene. This could also be a drug application or protein introduction if we had the data. This is converted into a learnable \"Action Embedding,\" effectively serving as the command that tells the simulation what event just occurred. \n",
    "\tThird, the model is given the **After** state, which is the actual knockdown cell observed in the experiment. This third input serves purely as the target or \"ground truth\"; the model is not allowed to see it while making its prediction, but uses it afterwards in the loss calculation and backprop.\n",
    "\n",
    "Inside the model, a three-step simulation plays out to process these inputs. It starts with the **Student Encoder**, or the *Perception* module, which looks at the healthy **Before** cell input  and compresses it into a Latent State. At this stage, the model is simply understanding the baseline biological status of the cell. This latent representation is then passed to the **Action-Conditioned Predictor**, which acts as the *Physics Engine.* This component combines the cell's current state with the Action vector. Using a mechanism called Adaptive Layer Normalization (AdaLN), the action actually modulates the internal weights of the neural network, effectively shifting the physical rules of the simulation to match the drug's effects. The **Predictor** then tries to predict, or hellucinate, what the future state of the cell will be, calculating a new vector that represents the cell's condition after the knockout, or drug impact. \n",
    "\n",
    "To validate the predictor, simultaneously the **Teacher Encoder** looks at the real **After** data from the lab and encodes it into that same latent space to serve as the judge. To learn, the model compares the predictor output against the Teacher's reality. With backprop, we attempt to minimize the difference between the prediction and the actual outcome, trying to minimize this error over millions of examples. \n",
    "\n",
    "In theory, by forcing its predictions to match reality, the model moves beyond simply memorizing data and begins to learn the underlying causal rules of biology. It figures out gene regulatory logic—understanding that if Gene A is knocked down, Pathway B must functionally fail. It learns how different pathways, like inflammation and cell death, are causally linked. Ultimately, we hope it learns the \"physics\" of how perturbations work, allowing us to eventually simulate the effects of gene mutations, drugs or genetic interventions on cells without having to perform the physical experiment in a wet lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a357f2-d497-4dfc-9920-568422b5403a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import copy\n",
    "import math\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9242cf00-c812-41d0-ae92-9c7dd0af6f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "random.seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f158bffc-f24a-4471-a463-7241c2ad6784",
   "metadata": {},
   "source": [
    "## Utils\n",
    "\n",
    "### ROTARY POSITIONAL EMBEDDINGS (RoPE)\n",
    "V-JEPA 2 uses 3D-RoPE. We adapt this to 1D-RoPE for our list of Pathway Tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ffafca-b99d-4cde-884f-a6347c5eafed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_seq_len=2048):\n",
    "        super().__init__()\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        t = torch.arange(max_seq_len).type_as(inv_freq)\n",
    "        freqs = torch.einsum('i,j->ij', t, inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.register_buffer('emb', emb)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [Batch, Seq, Dim]\n",
    "        # returns cos, sin for the sequence length of x\n",
    "        n = x.shape[1]\n",
    "        return self.emb[:n, :].cos(), self.emb[:n, :].sin()\n",
    "\n",
    "def apply_rotary_pos_emb(x, cos, sin):\n",
    "    # Standard RoPE rotation\n",
    "    # split x into half\n",
    "    d = x.shape[-1] // 2\n",
    "    x1, x2 = x[..., :d], x[..., d:]\n",
    "    rotated = torch.cat((-x2, x1), dim=-1)\n",
    "    return (x * cos) + (rotated * sin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b51720-0b30-466c-b5ea-5cac69175e1e",
   "metadata": {},
   "source": [
    "## Components\n",
    "\n",
    "### COMPONENT 1: THE ENCODER (STUDENT/TEACHER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602e56c8-779a-4cab-a23e-a360682aa4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioEncoderBlock(nn.Module):\n",
    "    def __init__(self, dim, heads, mlp_ratio=4.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = nn.MultiheadAttention(dim, heads, batch_first=True)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, int(dim * mlp_ratio)),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(int(dim * mlp_ratio), dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, cos, sin):\n",
    "        # 1. Attention with RoPE\n",
    "        x_norm = self.norm1(x)\n",
    "        \n",
    "        # Apply RoPE to queries and keys inside attention? \n",
    "        # For simplicity in standard PyTorch MHA, we apply it to x before attention\n",
    "        # (Technically RoPE is applied to Q and K inside, but this approximation works \n",
    "        # if we treat x as the carrier of position).\n",
    "        # STRICT IMPLEMENTATION: manually project Q,K,V, apply RoPE to Q,K, then Attn.\n",
    "        # We will use the simplified \"Inject Position\" approach for readability here.\n",
    "        x_rope = apply_rotary_pos_emb(x_norm, cos, sin)\n",
    "        \n",
    "        attn_out, _ = self.attn(x_rope, x_rope, x_norm) # V is not rotated usually\n",
    "        x = x + attn_out\n",
    "        \n",
    "        # 2. MLP\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32aadea7-3614-4d4e-afe7-812fb0e21551",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PathwayEncoder(nn.Module):\n",
    "    def __init__(self, num_pathways=1024, embed_dim=384, depth=12, heads=2):\n",
    "        super().__init__()\n",
    "        # Input: [Batch, Pathways] (Float) -> Project to [Batch, Pathways, Dim]\n",
    "        self.input_proj = nn.Linear(1, embed_dim)\n",
    "        \n",
    "        self.rope = RotaryEmbedding(embed_dim // heads * heads) # Ensure divisibility\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            BioEncoderBlock(embed_dim, heads) for _ in range(depth)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [Batch, Num_Pathways]\n",
    "        x = x.unsqueeze(-1) # [B, N, 1]\n",
    "        x = self.input_proj(x) # [B, N, Dim]\n",
    "        \n",
    "        # Generate RoPE cache\n",
    "        cos, sin = self.rope(x)\n",
    "        cos, sin = cos.to(x.device), sin.to(x.device)\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x, cos, sin)\n",
    "            \n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64874bc-9e76-4715-bee3-08fd86598823",
   "metadata": {},
   "source": [
    "### COMPONENT 2: ACTION-CONDITIONED PREDICTOR (AdaLN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce1c88f-fb19-4084-96fa-520202fe783e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaLN(nn.Module):\n",
    "    \"\"\"\n",
    "    V-JEPA 2 / DiT Style Conditioning.\n",
    "    The action vector regresses the Scale (gamma) and Shift (beta) of the normalization.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(embed_dim, elementwise_affine=False)\n",
    "        self.action_mlp = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(action_dim, 2 * embed_dim)\n",
    "        )\n",
    "        # Initialize to identity (gamma=0, beta=0 originally, effectively gamma=1 after logic)\n",
    "        # Standard practice: zero-init the last layer so action starts as \"no-op\"\n",
    "        nn.init.zeros_(self.action_mlp[1].weight)\n",
    "        nn.init.zeros_(self.action_mlp[1].bias)\n",
    "\n",
    "    def forward(self, x, action_emb):\n",
    "        # action_emb: [Batch, Action_Dim]\n",
    "        style = self.action_mlp(action_emb).unsqueeze(1) # [B, 1, 2*D]\n",
    "        gamma, beta = style.chunk(2, dim=-1)\n",
    "        return self.norm(x) * (1 + gamma) + beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec41393d-a173-4ce6-9894-4c8684dd913d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictorBlock(nn.Module):\n",
    "    def __init__(self, dim, heads, action_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(dim, heads, batch_first=True)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, 4 * dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * dim, dim)\n",
    "        )\n",
    "        self.ada_ln1 = AdaLN(dim, action_dim)\n",
    "        self.ada_ln2 = AdaLN(dim, action_dim)\n",
    "\n",
    "    def forward(self, x, action_emb, cos, sin):\n",
    "        # AdaLN -> Attn (with RoPE) -> Residual\n",
    "        x_norm = self.ada_ln1(x, action_emb)\n",
    "        \n",
    "        # Apply RoPE to Q, K\n",
    "        x_rope = apply_rotary_pos_emb(x_norm, cos, sin)\n",
    "        \n",
    "        attn_out, _ = self.attn(x_rope, x_rope, x_norm) \n",
    "        x = x + attn_out\n",
    "        \n",
    "        # AdaLN -> MLP -> Residual\n",
    "        x_norm = self.ada_ln2(x, action_emb)\n",
    "        x = x + self.mlp(x_norm)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53d09da-e83a-480a-9ccb-1c6159b567a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACPredictor(nn.Module):\n",
    "    def __init__(self, embed_dim=384, action_dim=256, depth=6, heads=2, num_pathways=1024):\n",
    "        super().__init__()\n",
    "        self.num_pathways = num_pathways\n",
    "        \n",
    "        # Action Embedding (Discrete ID -> Vector)\n",
    "        # We assume action_ids are passed in\n",
    "        self.action_embed = nn.Embedding(3000, action_dim) # 3000 max perturbations\n",
    "        \n",
    "        # Learnable Queries (The \"Mask Tokens\" for the future state)\n",
    "        # We learn one query vector per pathway position\n",
    "        self.mask_queries = nn.Parameter(torch.randn(1, num_pathways, embed_dim) * 0.02)\n",
    "        \n",
    "        self.rope = RotaryEmbedding(embed_dim)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            PredictorBlock(embed_dim, heads, action_dim) for _ in range(depth)\n",
    "        ])\n",
    "        \n",
    "        self.final_norm = AdaLN(embed_dim, action_dim)\n",
    "\n",
    "    def forward(self, context_latents, action_ids):\n",
    "        \"\"\"\n",
    "        context_latents: [Batch, N, Dim] (From Student Encoder)\n",
    "        action_ids: [Batch] (Ints)\n",
    "        \"\"\"\n",
    "        B, N, D = context_latents.shape\n",
    "        \n",
    "        # 1. Embed Action\n",
    "        action_emb = self.action_embed(action_ids) # [B, Action_Dim]\n",
    "        \n",
    "        # 2. Construct Input: Context + Mask Queries\n",
    "        # In V-JEPA, the predictor takes the context AND the queries for what to predict.\n",
    "        # Since we are predicting the *entire* next state (Treated), we concatenate:\n",
    "        # [Context_State (RoPE pos 0..N), Mask_Queries (RoPE pos N..2N)]\n",
    "        # However, for biological simplicity, we can just treat this as a transformation:\n",
    "        # We want to transform Context -> Predicted Target.\n",
    "        \n",
    "        # STRICT V-JEPA APPROACH:\n",
    "        # The predictor runs on the Queries, attending to the Context.\n",
    "        # It does NOT process the context deeply again.\n",
    "        \n",
    "        queries = self.mask_queries.repeat(B, 1, 1) # [B, N, D]\n",
    "        \n",
    "        # We concat for attention purposes (Cross-attention is cleaner, but V-JEPA uses single stream)\n",
    "        # Input Sequence: [Context, Queries]\n",
    "        sequence = torch.cat([context_latents, queries], dim=1)\n",
    "        \n",
    "        # RoPE: Need to handle the extended sequence length (2N)\n",
    "        cos, sin = self.rope(sequence)\n",
    "        cos, sin = cos.to(sequence.device), sin.to(sequence.device)\n",
    "        \n",
    "        # 3. Pass through AdaLN Blocks\n",
    "        for block in self.blocks:\n",
    "            sequence = block(sequence, action_emb, cos, sin)\n",
    "            \n",
    "        sequence = self.final_norm(sequence, action_emb)\n",
    "        \n",
    "        # 4. Return only the predicted part (The Queries)\n",
    "        # We discard the processed context\n",
    "        predictions = sequence[:, N:, :] \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb48919-caa3-4d81-93d0-dca2d4d30f96",
   "metadata": {},
   "source": [
    "## Bio-JEPA AC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16297aa1-3670-4b89-9e3a-19cfab546ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioVJepa2(nn.Module):\n",
    "    def __init__(self, num_pathways=1024, embed_dim=384):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.student = PathwayEncoder(num_pathways, embed_dim)\n",
    "        self.teacher = copy.deepcopy(self.student)\n",
    "        \n",
    "        # Freeze teacher\n",
    "        for p in self.teacher.parameters():\n",
    "            p.requires_grad = False\n",
    "            \n",
    "        self.predictor = ACPredictor(embed_dim=embed_dim, action_dim=256, num_pathways=num_pathways)\n",
    "        \n",
    "    def forward(self, x_control, x_treated, action_id):\n",
    "        # 1. Teacher encodes the Target (Treated)\n",
    "        with torch.no_grad():\n",
    "            target_latents = self.teacher(x_treated)\n",
    "            \n",
    "        # 2. Student encodes the Context (Control)\n",
    "        # (Optional: Add masking here if you want extra difficulty, \n",
    "        # but the task Control->Treated is already hard enough)\n",
    "        context_latents = self.student(x_control)\n",
    "        \n",
    "        # 3. Predictor tries to guess Target given Context + Action\n",
    "        predicted_latents = self.predictor(context_latents, action_id)\n",
    "        \n",
    "        # 4. Latent Loss (L1)\n",
    "        loss = F.l1_loss(predicted_latents, target_latents)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_teacher(self, m=0.996):\n",
    "        for param_s, param_t in zip(self.student.parameters(), self.teacher.parameters()):\n",
    "            param_t.data.mul_(m).add_((1 - m) * param_s.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdbd6fb-ec72-4af9-ba48-13d7731c3343",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96184936-3410-4d44-8be1-e01ed3c7cc9e",
   "metadata": {},
   "source": [
    "#### Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dfbb5f-2a56-4709-a9a2-9ab3fe0973a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('/Users/djemec/data/jepa')\n",
    "shard_dir = data_dir / 'tokenized'\n",
    "metadata_path = data_dir / 'perturbation_map.json'\n",
    "checkpoint_dir = data_dir / 'checkpoint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7885c628-45b4-4c23-97e5-e3ee6f0400e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "# Steps/epoch = 29 (files) *10000 (chunk size) /8 (batch size) \n",
    "epoch_steps = 900\n",
    "n_embd = 8\n",
    "n_pathways = 1024\n",
    "LR = 1e-3\n",
    "EPOCHS = 2\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "QUANTIZATION_MAX = 20.0 # Must match tokenizer\n",
    "tok_file_chunk_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a1967a-562d-4772-904a-d1092ba2cf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairedShardDataset(Dataset):\n",
    "    def __init__(self, shard_dir):\n",
    "        self.files = sorted(shard_dir.glob('*.npz'))\n",
    "        print(f'Found {len(self.files)} shards.')\n",
    "        \n",
    "    def __len__(self):\n",
    "        # Approx length for progress bar (10k per shard)\n",
    "        return len(self.files) * tok_file_chunk_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Stochastic loading: Pick random shard, then random row\n",
    "        # This avoids loading 100MB files for just one item in strict order\n",
    "        file_path = self.files[np.random.randint(len(self.files))]\n",
    "        \n",
    "        # Load shard (fast if on SSD)\n",
    "        try:\n",
    "            with np.load(file_path) as data:\n",
    "                # Keys: 'control', 'treated', 'action_ids'\n",
    "                n_rows = data['action_ids'].shape[0]\n",
    "                row_idx = np.random.randint(n_rows)\n",
    "                \n",
    "                c_raw = data['control'][row_idx]\n",
    "                t_raw = data['treated'][row_idx]\n",
    "                act_id = data['action_ids'][row_idx]\n",
    "        except Exception as e:\n",
    "            # Fallback for corrupt shard\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "            return self.__getitem__(0)\n",
    "\n",
    "        # Dequantize\n",
    "        scale = QUANTIZATION_MAX / (2**32 - 1)\n",
    "        x_control = torch.tensor(c_raw.astype(np.float32) * scale)\n",
    "        x_treated = torch.tensor(t_raw.astype(np.float32) * scale)\n",
    "        action_id = torch.tensor(act_id, dtype=torch.long)\n",
    "        \n",
    "        return x_control, x_treated, action_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0202749-f533-40fd-a891-08f9c9ff8edb",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723366c9-4869-408a-b1bc-f2dba2f913ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BioVJepa2(num_pathways=n_pathways, embed_dim=n_embd).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ff27eb-93fa-439c-a31d-366dff143d7d",
   "metadata": {},
   "source": [
    "#### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff8ba83-7ff9-4a99-92c7-1fec94759781",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57196ead-30ec-4098-a645-9ec7bdf3a300",
   "metadata": {},
   "source": [
    "#### Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95819c7-d216-4a1b-93cc-467a20311ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=LR, steps_per_epoch=epoch_steps, epochs=EPOCHS, pct_start=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f022bc1-b281-4ba4-a38a-5c245ce354df",
   "metadata": {},
   "source": [
    "#### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037e931a-d57a-4e2a-b7e1-349126321bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PairedShardDataset(shard_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bfd331-4e87-4957-8ff9-94fe5d1233c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89952bd9-d632-44db-ae1e-0063389ff31f",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bc4b56-b682-4b56-922e-ef040ab8d29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossi = []\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cee680-aa5a-464f-99ef-27b5c758d21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    # We manually limit steps per epoch to avoid iterating infinite stochastic loader\n",
    "    # or just iterate a fixed number of batches\n",
    "    for i, (xc, xt, aid) in enumerate(loader):\n",
    "        xc, xt, aid = xc.to(DEVICE), xt.to(DEVICE), aid.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = model(xc, xt, aid)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update Teacher (V-JEPA Momentum)\n",
    "        model.update_teacher(m=0.996)\n",
    "\n",
    "        if scheduler.last_epoch < scheduler.total_steps:\n",
    "            scheduler.step()\n",
    "\n",
    "        lossi.append(loss.item())\n",
    "        total_loss += loss.item()\n",
    "        step += 1\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(f\"Epoch {epoch} | Step {i} | Loss: {loss.item():.5f} | LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "        \n",
    "        if i >= epoch_steps: # End epoch after 2000 batches\n",
    "            break\n",
    "            \n",
    "    avg_loss = total_loss / epoch_steps\n",
    "    \n",
    "    print(f\"=== Epoch {epoch} Done. Avg Loss: {avg_loss:.5f} ===\")\n",
    "    \n",
    "    # Save Checkpoint\n",
    "    torch.save({\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'epoch': epoch\n",
    "    }, checkpoint_dir / f'bio_jepa_ckpt_{epoch}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4972cdbf-7fd0-4b3b-a541-16135d079c2c",
   "metadata": {},
   "source": [
    "#### Training Loss Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c293fa-2067-45b9-ad55-e6127515bb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lossi)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861fe89a-0447-48d3-9252-9153efcab902",
   "metadata": {},
   "source": [
    "## Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45916c9b-5f07-4aec-a9ac-e0635bae92a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path, shard_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c35355-2ff6-486d-9ce8-e4cbbbee1599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load Map (ID -> Name)\n",
    "with open(metadata_path, \"r\") as f:\n",
    "    pert_map = json.load(f)\n",
    "# Invert map to: ID -> Name\n",
    "id_to_name = {v: k for k, v in pert_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ae854c-4e74-42f5-9cfd-0c487ff22549",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09550430-309d-436c-a140-2bda3fb8bde3",
   "metadata": {},
   "source": [
    "**Get random data sample**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189729ef-8852-466c-b05b-455948aa6cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_test_pair(shard_dir):\n",
    "    '''Grab a single real pair from a random shard'''\n",
    "    files = sorted(shard_dir.glob('*.npz'))\n",
    "    file_path = files[np.random.randint(len(files))]\n",
    "    \n",
    "    with np.load(file_path) as data:\n",
    "        idx = np.random.randint(data['action_ids'].shape[0])\n",
    "        \n",
    "        # Extract raw uint32\n",
    "        c_raw = data['control'][idx]\n",
    "        t_raw = data['treated'][idx]\n",
    "        act_id = data['action_ids'][idx]\n",
    "        \n",
    "    # Dequantize\n",
    "    scale = QUANTIZATION_MAX / (2**32 - 1)\n",
    "    x_control = torch.tensor(c_raw.astype(np.float32) * scale).unsqueeze(0).to(DEVICE) # [1, 1024]\n",
    "    x_treated = torch.tensor(t_raw.astype(np.float32) * scale).unsqueeze(0).to(DEVICE)\n",
    "    action_id = torch.tensor([act_id], dtype=torch.long).to(DEVICE)\n",
    "    \n",
    "    return x_control, x_treated, action_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793361fe-baee-42b7-982d-72e604d28013",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_control, x_real_treated, action_id = get_random_test_pair(shard_dir)\n",
    "pert_name = id_to_name[action_id.item()]\n",
    "pert_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e32d24-4c56-40fc-9d76-3048536eddb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Get Baselines (Teacher View)\n",
    "# We need the Teacher to tell us where the \"Control\" and \"Real Treated\" \n",
    "# sit in the abstract latent space.\n",
    "with torch.no_grad():\n",
    "    z_control = model.teacher(x_control)       # Where the cell started\n",
    "    z_real = model.teacher(x_real_treated)     # Where the cell actually went"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054228e3-73d0-48da-9b7f-2f80be87cc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Run The Physics Engine (Predictor)\n",
    "# Student encodes context -> Predictor adds Action -> Output\n",
    "with torch.no_grad():\n",
    "    z_context = model.student(x_control)\n",
    "    z_predicted = model.predictor(z_context, action_id) # Where the model thinks it went"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5360c5-2215-4fc8-858a-d1488cf54ab6",
   "metadata": {},
   "source": [
    "**Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49851bf2-12f7-4de4-9b3f-efbd1bac2c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric 1: Baseline Drift (How much did the drug actually change the cell?)\n",
    "# If this is 0, the drug did nothing, so prediction is trivial.\n",
    "drift = F.l1_loss(z_control, z_real).item()\n",
    "drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a65de9-8f27-4f96-b304-07272e5367f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric 2: Prediction Error (How close is our guess to the real result?)\n",
    "error = F.l1_loss(z_predicted, z_real).item()\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cbe219-8cfa-4d15-a09f-999de53f87bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric 3: Simulation Magnitude (How much did our model decide to move the cell?)\n",
    "sim_move = F.l1_loss(z_predicted, z_control).item()\n",
    "sim_move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb7a8ae-c7ac-4bf5-8aca-ca82af1f57fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- INTERPRETATION ---\n",
    "print(f\"[Result Interpretation]\")\n",
    "if drift < 0.01:\n",
    "    print(f\"⚠️  WEAK SIGNAL: This perturbation didn't change the cell much in reality.\")\n",
    "elif error < drift:\n",
    "    improvement = (1 - (error / drift)) * 100\n",
    "    print(f\"✅ SUCCESS: The model predicted the state shift!\")\n",
    "    print(f\"   The prediction is {improvement:.1f}% closer to the truth than the Control state was.\")\n",
    "else:\n",
    "    print(f\"❌ FAILURE: The model failed to capture the dynamics.\")\n",
    "    print(f\"   It would have been better to just guess 'Nothing Happened'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a7c686-6b68-4975-add4-f8178fdf3f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which latent dimensions changed the most?\n",
    "# This gives us a \"fingerprint\" of the predicted change\n",
    "diff_vector = (z_predicted - z_control).abs().mean(dim=0) # [Embed_Dim]\n",
    "top_dims = torch.topk(diff_vector, 5).indices.tolist()\n",
    "\n",
    "print(f\"\\n[Top Active Latent Dimensions]\")\n",
    "print(f\"Dimensions {top_dims} showed the highest activity during this simulation.\")\n",
    "print(\"(In a full analysis, you would regress these dimensions back to Pathways).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc7a4bc-88e5-4903-bbad-6ffa382db26a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30288334-6f6f-48a3-96e3-4f9144215288",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
