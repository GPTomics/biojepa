{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfe968dc-a937-469a-89d8-7ba179a2e01a",
   "metadata": {},
   "source": [
    "# Bio-JEPA AC \n",
    "\n",
    "Based on [V-JEPA 2 AC](https://arxiv.org/abs/2506.09985)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b67dcd-a768-447b-ad7b-77e8cddd59c0",
   "metadata": {},
   "source": [
    "**Goal**\n",
    "Our goal is to build a World Model for cell biology. In it's current configuration, our model calculates an latent space representation of the gene expression and network impact to a cell given a specific gene knockout, or drug if we had it in the dataset. Unlike an LLM, this is a predictive simulation that operates entirely within a compressed mathematical space to understand cause and effect.\n",
    "\n",
    "The process begins with the inputs, which feed the model three distinct pieces of information for every training step. \n",
    "\tFirst, it receives the **Before** state, which is data representing a healthy control cell. Based on the dataset, the model can learn across the top 4096 expressed genes. Additionally, the model has access to a set of gene to pathway mappings based on the [Drug Signatures Database (DSigDB)](https://academic.oup.com/bioinformatics/article/31/18/3069/241009) dataset. This essentially lets the model learn from gene expression and a small cheat sheet on the networks so that the model can learn what cell looks like to have high energy, low stress, and normal growth (or whatever the control state is).  This state is predicted/learned by the **Student** model.  \n",
    "\tSecond, the model receives the **Action**, which is the specific perturbation performed in the lab, in our case a CRISPR knockdown of a specific gene. This could also be a drug application or protein introduction if we had the data. This is converted into a learnable \"Action Embedding,\" effectively serving as the command that tells the simulation what event just occurred. This adjustment is learned by the **Action-Conditioned Predictor** model. \n",
    "\tThird, the model is given the **After** state, which is the actual knockdown cell observed in the experiment. This third input serves purely as the target or \"ground truth\"; the model is not allowed to see it while making its prediction, but uses it afterwards in the loss calculation and backprop. This is encoded by the **Teacher** model. \n",
    "\n",
    "Inside the model, a three-step simulation plays out to process these inputs. It starts with the **Student Encoder** which looks at the healthy before cell input  and compresses it into a Latent State representing the *control* state. At this stage, the model is simply understanding the baseline biological status of the cell. This latent representation is then passed to the **Action-Conditioned Predictor**, which acts as the *Physics Engine*. This component combines the cell's current state with the learned Action vector. Using a mechanism called Adaptive Layer Normalization (AdaLN), the action actually modulates the internal weights of the neural network, effectively shifting the physical rules of the simulation to match the drug's effects. The **Predictor** then tries to predict, or hellucinate, what the future state of the cell will be, calculating a new vector that represents the cell's condition after the knockout, or drug impact. \n",
    "\n",
    "To validate the predictor, simultaneously the **Teacher Encoder** looks at the real perturbed data from the lab and encodes it into that same latent space to serve as the judge representing the *case* state. To learn, the model compares the predictor output against the Teacher's reality. With backprop, we update the predictor and student in an attempt to minimize the difference between the prediction and the actual outcome, trying to minimize this error over millions of examples. \n",
    "\n",
    "In theory, by forcing its predictions to match reality, the model moves beyond simply memorizing data and begins to learn the underlying causal rules of biology. It figures out gene regulatory logic, understanding that if Gene A is knocked down, Pathway B must functionally fail. It learns how different pathways, like inflammation and cell death, are causally linked. Ultimately, we hope it learns the \"physics\" of how perturbations work, allowing us to eventually simulate the effects of gene mutations, drugs or genetic interventions on cells without having to perform the physical experiment in a wet lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33a357f2-d497-4dfc-9920-568422b5403a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import copy\n",
    "import math\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9242cf00-c812-41d0-ae92-9c7dd0af6f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d513e0d-33a2-400c-a40a-194b365bdd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: nvidia-smi: command not found\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a66afb3-044f-44b3-bf5a-3b34e37e2f95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3396f99a-eb28-4ea3-a10d-4e64f777fa08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cpu\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    device = 'cpu'\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(1337)\n",
    "        device = 'cuda'\n",
    "    print(f'using {device}')\n",
    "    return device\n",
    "\n",
    "DEVICE = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d6fc33-8c5c-43bf-960a-2f83fea1601e",
   "metadata": {},
   "source": [
    "## Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358918d9-36ba-48e5-aade-398109b4d660",
   "metadata": {},
   "source": [
    "### Multi-Headed Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d1ff083-ffb8-4102-a8ea-e1f6f17bee12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioMultiHeadAttention(nn.Module):\n",
    "    # mirrors nn.MultiheadAttention(dim, heads, batch_first=True) \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        assert config.embed_dim % config.heads == 0\n",
    "        \n",
    "        self.head_dim = config.embed_dim // config.heads\n",
    "        self.heads = config.heads\n",
    "        \n",
    "        # Projections\n",
    "        self.q_proj = nn.Linear(config.embed_dim, config.embed_dim)\n",
    "        self.k_proj = nn.Linear(config.embed_dim, config.embed_dim)\n",
    "        self.v_proj = nn.Linear(config.embed_dim, config.embed_dim)\n",
    "        \n",
    "        self.c_proj = nn.Linear(config.embed_dim, config.embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # Batch, Seq, Embed Dim\n",
    "        \n",
    "        # 1. Project\n",
    "        q = self.q_proj(x).view(B, T, self.heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(x).view(B, T, self.heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(x).view(B, T, self.heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # 2. Standard Scaled Dot Product Attention (Permutation Invariant)\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=False)\n",
    "        \n",
    "        # 5. Reassemble\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.c_proj(y)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715a67a5-efb2-4b22-8ec0-54a0294e01e2",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bb01e77-936e-4235-a962-9cb7656f889b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.c_fc = nn.Linear(config.embed_dim, int(config.mlp_ratio * config.embed_dim))\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "        self.c_proj = nn.Linear(int(config.mlp_ratio * config.embed_dim), config.embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5be00e5-ce0a-4335-95c5-2519e8157824",
   "metadata": {},
   "source": [
    "### Hidden Transfomer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02208352-b690-47af-8c90-06210718b5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CellStateBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.ln_1 = nn.LayerNorm(config.embed_dim)\n",
    "        self.attn = BioMultiHeadAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.embed_dim)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. Attention \n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "\n",
    "        # 2. MLP\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69942da-c18f-4585-9041-911fed8107f7",
   "metadata": {},
   "source": [
    "### Cell State Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e45f71db-0836-4c3b-bc8b-3957f4c02630",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CellStateEncoderConfig:\n",
    "    num_genes: int = 4096\n",
    "    num_pathways: int = 1024 \n",
    "    n_layer: int = 24 \n",
    "    heads: int = 12\n",
    "    embed_dim: int = 768\n",
    "    mlp_ratio: float = 4.0 # Changed to float for precision\n",
    "    mask_matrix: np.ndarray = None \n",
    "\n",
    "class CellStateEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        assert config.mask_matrix is not None, \"Must provide binary_pathway_mask!\"\n",
    "        self.register_buffer(\"mask\", torch.tensor(config.mask_matrix).float()) \n",
    "        \n",
    "        # Learnable Gene Embeddings [num_genes, Dim]\n",
    "        self.gene_embeddings = nn.Parameter(torch.randn(config.num_genes, config.embed_dim) * 0.02)\n",
    "        \n",
    "        # Context Injector\n",
    "        self.total_count_proj = nn.Linear(1, config.embed_dim)\n",
    "\n",
    "        # Transfomer\n",
    "        self.blocks = nn.ModuleList([CellStateBlock(config) for _ in range(config.n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(config.embed_dim)\n",
    "\n",
    "        # Initiation \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None: \n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        \n",
    "    def forward(self, x_genes, x_total_ct):\n",
    "        # 1. Project Genes\n",
    "        x_genes = x_genes.unsqueeze(-1) \n",
    "        gene_repr = x_genes * self.gene_embeddings.unsqueeze(0)\n",
    "\n",
    "        # 2. Aggregate (Routing)\n",
    "        x_pathway = self.mask.T @ gene_repr\n",
    "\n",
    "        # 3. Context Injection\n",
    "        x_total_ct = x_total_ct.unsqueeze(-1)\n",
    "        x_total_ct = self.total_count_proj(x_total_ct)\n",
    "        x_total_ct = x_total_ct.unsqueeze(1)\n",
    "        x = x_pathway + x_total_ct\n",
    "\n",
    "        # 4. Set Transformer\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # 5. Layer Norm\n",
    "        x = self.ln_f(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e165225-f747-4be2-a0a8-cc7b8b1b6d1f",
   "metadata": {},
   "source": [
    "### Adaptive Layer Normalization AdaLN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab2556a4-03b2-47e6-9392-eb2dbb648284",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaLN(nn.Module):\n",
    "    '''\n",
    "    Adaptive Layer Norm for conditioning the predictor on action embeddings.\n",
    "    The action vector regresses the Scale (gamma) and Shift (beta) of the normalization.\n",
    "    '''\n",
    "    def __init__(self, embed_dim, action_embed_dim):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(embed_dim, elementwise_affine=False)\n",
    "        self.action_mlp = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(action_embed_dim, 2 * embed_dim)\n",
    "        )\n",
    "        # Initialize to identity (gamma=0, beta=0 originally, effectively gamma=1 after logic)\n",
    "        # Zero-init the last layer so the action starts as a \"no-op\" (identity)\n",
    "        nn.init.zeros_(self.action_mlp[1].weight)\n",
    "        nn.init.zeros_(self.action_mlp[1].bias)\n",
    "\n",
    "    def forward(self, x, action_emb):\n",
    "        # x: [Batch, Seq, Dim]\n",
    "        # action_emb: [Batch, action_embed_dim]\n",
    "        \n",
    "        # Project action to style: [B, 2*D] -> [B, 1, 2*D]\n",
    "        style = self.action_mlp(action_emb).unsqueeze(1) \n",
    "        gamma, beta = style.chunk(2, dim=-1)\n",
    "        \n",
    "        # Apply affine transformation based on action\n",
    "        return self.norm(x) * (1 + gamma) + beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d966f7-8f5d-4c06-b6db-d02aaba87de8",
   "metadata": {},
   "source": [
    "### Predictor Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc8a2a58-7549-4bd1-908f-d82608063959",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictorBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # 1. Conditioning (AdaLN) replaces standard LayerNorm\n",
    "        self.ada_ln1 = AdaLN(config.embed_dim, config.action_embed_dim)\n",
    "        \n",
    "        # 2. Attention (Using the shared BioMultiHeadAttention)\n",
    "        self.attn = BioMultiHeadAttention(config)\n",
    "        \n",
    "        # 3. Conditioning (AdaLN) for the MLP block\n",
    "        self.ada_ln2 = AdaLN(config.embed_dim, config.action_embed_dim)\n",
    "        \n",
    "        # 4. MLP (Using the shared MLP)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x, action_emb):\n",
    "        # 1. AdaLN -> Attention  -> Residual\n",
    "        x_norm = self.ada_ln1(x, action_emb)\n",
    "        x = x + self.attn(x_norm)\n",
    "        \n",
    "        # 2. AdaLN -> MLP -> Residual\n",
    "        x_norm = self.ada_ln2(x, action_emb)\n",
    "        x = x + self.mlp(x_norm)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b7340a-5225-4131-83d2-614631a55a10",
   "metadata": {},
   "source": [
    "### Main Predictor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b641afb7-7d8d-4b4b-833c-72400e48610d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ACPredictorConfig:\n",
    "    num_pathways: int = 1024\n",
    "    n_layer: int = 6 \n",
    "    heads: int = 4\n",
    "    embed_dim: int = 384\n",
    "    action_embed_dim: int = 256 \n",
    "    mlp_ratio: float = 4.0\n",
    "    max_perturb: int = 2058 ## eventually try to get to a 2**N power\n",
    "\n",
    "class ACPredictor(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Action Embedding (Discrete ID -> Vector)\n",
    "        self.action_embed = nn.Embedding(config.max_perturb, config.action_embed_dim)\n",
    "        \n",
    "        # Learnable Queries (\"Mask Tokens\") for the future state\n",
    "        # One query vector per pathway position\n",
    "        self.mask_queries = nn.Parameter(torch.randn(1, config.num_pathways, config.embed_dim) * 0.02)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            PredictorBlock(config) for _ in range(config.n_layer)\n",
    "        ])\n",
    "        \n",
    "        self.final_norm = AdaLN(config.embed_dim, config.action_embed_dim)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, context_latents, action_ids):\n",
    "        \"\"\"\n",
    "        context_latents: [Batch, N, Dim] (From Student Encoder)\n",
    "        action_ids: [Batch] (Ints)\n",
    "        \"\"\"\n",
    "        B, N, D = context_latents.shape\n",
    "        \n",
    "        # 1. Embed Action\n",
    "        action_emb = self.action_embed(action_ids) # [B, action_embed_dim]\n",
    "        \n",
    "        # 2. Construct Input: [Context, Mask_Queries]\n",
    "        # We concatenate the learned queries to the context. \n",
    "        # The predictor will attend to the context to update the queries.\n",
    "        queries = self.mask_queries.repeat(B, 1, 1) # [B, N, D]\n",
    "        sequence = torch.cat([context_latents, queries], dim=1) # [B, 2N, D]     \n",
    "        \n",
    "        # 3. Pass through AdaLN Blocks\n",
    "        for block in self.blocks:\n",
    "            sequence = block(sequence, action_emb)\n",
    "            \n",
    "        sequence = self.final_norm(sequence, action_emb)\n",
    "        \n",
    "        # 4. Return only the predicted part (The Queries corresponding to N..2N)\n",
    "        predictions = sequence[:, N:, :] \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb48919-caa3-4d81-93d0-dca2d4d30f96",
   "metadata": {},
   "source": [
    "## Bio-JEPA AC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16297aa1-3670-4b89-9e3a-19cfab546ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BioJepaConfig:\n",
    "    mask_matrix: np.ndarray\n",
    "    num_genes: int = 4096\n",
    "    num_pathways: int = 1024\n",
    "    n_layer: int = 6 \n",
    "    heads: int = 4\n",
    "    embed_dim: int = 256\n",
    "    action_embed_dim: int=256 \n",
    "    mlp_ratio: float = 4.0\n",
    "    max_perturb: int= 2058 ## eventually try to get to a 2**N power\n",
    "    \n",
    "class BioJepa(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        enc_conf = CellStateEncoderConfig(\n",
    "            num_genes=config.num_genes,\n",
    "            num_pathways=config.num_pathways,\n",
    "            n_layer=config.n_layer,\n",
    "            heads=config.heads,\n",
    "            embed_dim=config.embed_dim,\n",
    "            mlp_ratio=config.mlp_ratio,\n",
    "            mask_matrix=config.mask_matrix\n",
    "        )\n",
    "        \n",
    "        self.student = CellStateEncoder(enc_conf)   \n",
    "        self.teacher = copy.deepcopy(self.student)\n",
    "        \n",
    "        # Freeze teacher\n",
    "        for p in self.teacher.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "\n",
    "        pred_conf = ACPredictorConfig(\n",
    "            num_pathways=config.num_pathways,\n",
    "            n_layer=config.n_layer,\n",
    "            heads=config.heads,\n",
    "            embed_dim=config.embed_dim,\n",
    "            action_embed_dim=config.action_embed_dim,\n",
    "            mlp_ratio=config.mlp_ratio,\n",
    "            max_perturb=config.max_perturb\n",
    "        )\n",
    "        self.predictor = ACPredictor(pred_conf)\n",
    "\n",
    "    def forward(self, x_control, total_control, x_treated, total_treated, action_id):\n",
    "        # 1. Teacher\n",
    "        with torch.no_grad():\n",
    "            target_latents = self.teacher(x_treated, total_treated)\n",
    "            \n",
    "        # 2. Student \n",
    "        context_latents = self.student(x_control, total_control)\n",
    "        \n",
    "        # 3. Predictor \n",
    "        predicted_latents = self.predictor(context_latents, action_id)\n",
    "        \n",
    "        # 4. Latent Loss (L1)\n",
    "        loss = F.l1_loss(predicted_latents, target_latents)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_teacher(self, m=0.996):\n",
    "        for param_s, param_t in zip(self.student.parameters(), self.teacher.parameters()):\n",
    "            param_t.data.mul_(m).add_((1 - m) * param_s.data)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdbd6fb-ec72-4af9-ba48-13d7731c3343",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96184936-3410-4d44-8be1-e01ed3c7cc9e",
   "metadata": {},
   "source": [
    "#### Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dfbb5f-2a56-4709-a9a2-9ab3fe0973a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('/Users/djemec/data/jepa')\n",
    "tok_dir = data_dir / 'tokenized'\n",
    "mask_path = data_dir / 'binary_pathway_mask.npy'\n",
    "metadata_path = data_dir / 'perturbation_map.json'\n",
    "checkpoint_dir = data_dir / 'checkpoint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a287ede-b205-4377-9ffe-646694b977dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Pathway Mask...\n",
      "Mask Loaded: 4096 Genes -> 1024 Pathways\n"
     ]
    }
   ],
   "source": [
    "print('Loading Pathway Mask...')\n",
    "binary_mask = np.load(mask_path)\n",
    "N_GENES, N_PATHWAYS = binary_mask.shape\n",
    "print(f'Mask Loaded: {N_GENES} Genes -> {N_PATHWAYS} Pathways')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7885c628-45b4-4c23-97e5-e3ee6f0400e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "n_embd = 8\n",
    "n_pathways = 1024\n",
    "LR = 3e-4\n",
    "EPOCHS = 2\n",
    "tok_file_chunk_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39a1967a-562d-4772-904a-d1092ba2cf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_shard(filename):\n",
    "    print(f'loading {filename}') # Optional: reduce noise\n",
    "    with np.load(filename) as data:\n",
    "        # Load all arrays into memory\n",
    "        # We convert to correct types immediately to save hassle later\n",
    "        control_x = data['control'].astype(np.float32)\n",
    "        control_tot = data['control_total'].astype(np.float32)\n",
    "        case_x = data['case'].astype(np.float32)\n",
    "        case_tot = data['case_total'].astype(np.float32)\n",
    "        action_ids = data['action_ids'].astype(np.int64)\n",
    "        \n",
    "    return control_x, control_tot, case_x, case_tot, action_ids\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, split, device):\n",
    "        self.B = B\n",
    "        self.split = split\n",
    "        self.device = device\n",
    "        \n",
    "        # 1. Find Shards\n",
    "        data_root = tok_dir / f'{split}'\n",
    "        shards = list(data_root.glob('*.npz'))\n",
    "\n",
    "        self.total_files = len(shards)\n",
    "        self.shards = sorted(shards)\n",
    "\n",
    "        assert len(shards) > 0, f'no shards found for split {split}'\n",
    "        print(f'found {len(shards)} shards for split {split}')\n",
    "        \n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # Create a randomized queue of shards\n",
    "        self.remaining_shards = list(self.shards)\n",
    "        random.shuffle(self.remaining_shards)\n",
    "        \n",
    "        self.current_shard_idx = -1\n",
    "        self.load_next_shard()\n",
    "\n",
    "    def load_next_shard(self):\n",
    "        self.current_shard_idx += 1\n",
    "        \n",
    "        # If we ran out of shards, reset (Epoch done)\n",
    "        if self.current_shard_idx >= len(self.remaining_shards):\n",
    "            self.reset() # This resets shard_idx to -1 and reshuffles\n",
    "            return \n",
    "\n",
    "        # Load the file\n",
    "        filename = self.remaining_shards[self.current_shard_idx]\n",
    "        self.data_tuple = load_shard(filename)\n",
    "        \n",
    "        # Shuffle the items INSIDE the shard\n",
    "        # This is critical so we don't just memorize the sorted order of the shard\n",
    "        n_samples = len(self.data_tuple[0])\n",
    "        self.perm = np.random.permutation(n_samples)\n",
    "        self.current_position = 0\n",
    "        self.total_samples_in_shard = n_samples\n",
    "\n",
    "    def next_batch(self):\n",
    "        B = self.B\n",
    "        \n",
    "        # Check if we have enough data left in current shard\n",
    "        if self.current_position + B > self.total_samples_in_shard:\n",
    "            self.load_next_shard()\n",
    "            # Recursively call to get batch from the new shard\n",
    "            return self.next_batch()\n",
    "            \n",
    "        # Get indices for this batch\n",
    "        indices = self.perm[self.current_position : self.current_position + B]\n",
    "        self.current_position += B\n",
    "        \n",
    "        # Slice data using the shuffled indices\n",
    "        # data_tuple structure: (xc, xct, xt, xtt, aid)\n",
    "        batch_xc  = torch.from_numpy(self.data_tuple[0][indices]).to(self.device)\n",
    "        batch_xct = torch.from_numpy(self.data_tuple[1][indices]).to(self.device)\n",
    "        batch_xt  = torch.from_numpy(self.data_tuple[2][indices]).to(self.device)\n",
    "        batch_xtt = torch.from_numpy(self.data_tuple[3][indices]).to(self.device)\n",
    "        batch_aid = torch.from_numpy(self.data_tuple[4][indices]).to(self.device)\n",
    "        \n",
    "        return batch_xc, batch_xct, batch_xt, batch_xtt, batch_aid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cc07f0-eb9e-4e6a-8784-0e02e5eb0faf",
   "metadata": {},
   "source": [
    "#### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c02da536-0088-4376-8d89-37839eebb8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 29 shards for split train\n",
      "loading /Users/djemec/data/jepa/tokenized/train/shard_0005.npz\n",
      "found 1 shards for split val\n",
      "loading /Users/djemec/data/jepa/tokenized/val/shard_0000.npz\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoaderLite(B=BATCH_SIZE, split='train', device=DEVICE)\n",
    "val_loader = DataLoaderLite(B=BATCH_SIZE, split='val', device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0202749-f533-40fd-a891-08f9c9ff8edb",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57369cd2-cdad-48d8-ac29-4ccb25e605a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "723366c9-4869-408a-b1bc-f2dba2f913ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BioJepaConfig(\n",
    "    mask_matrix=binary_mask, \n",
    "    num_genes=N_GENES,\n",
    "    num_pathways=N_PATHWAYS,\n",
    "    embed_dim=n_embd,\n",
    "    heads=1\n",
    ")\n",
    "model = BioJepa(config).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ff27eb-93fa-439c-a31d-366dff143d7d",
   "metadata": {},
   "source": [
    "#### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ff8ba83-7ff9-4a99-92c7-1fec94759781",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57196ead-30ec-4098-a645-9ec7bdf3a300",
   "metadata": {},
   "source": [
    "#### Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b66ffc0d-d4f0-4f3d-af4c-390acfd7d5ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18730, 37460)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch = 299694 // BATCH_SIZE\n",
    "max_steps = EPOCHS * steps_per_epoch\n",
    "steps_per_epoch, max_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e95819c7-d216-4a1b-93cc-467a20311ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=LR, total_steps=max_steps, pct_start=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89952bd9-d632-44db-ae1e-0063389ff31f",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62bc4b56-b682-4b56-922e-ef040ab8d29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossi = []\n",
    "step = 0\n",
    "total_epoch_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96cee680-aa5a-464f-99ef-27b5c758d21c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 1.2855\n",
      "Step 0 | Loss: 1.29129 | LR: 1.20e-05\n",
      "=== Step 0 Done. Avg Loss: 0.00007 ===\n",
      "test loss: 1.2666\n",
      "test loss: 1.2437\n",
      "Step 25 | Loss: 1.22832 | LR: 1.21e-05\n",
      "test loss: 1.2213\n",
      "test loss: 1.1974\n",
      "test loss: 1.1750\n",
      "Step 50 | Loss: 1.16804 | LR: 1.25e-05\n",
      "test loss: 1.1523\n",
      "test loss: 1.1269\n",
      "Step 75 | Loss: 1.11049 | LR: 1.32e-05\n",
      "test loss: 1.1007\n",
      "test loss: 1.0806\n",
      "test loss: 1.0590\n",
      "Step 100 | Loss: 1.05879 | LR: 1.41e-05\n",
      "test loss: 1.0355\n",
      "test loss: 1.0084\n",
      "Step 125 | Loss: 0.99119 | LR: 1.52e-05\n",
      "test loss: 0.9863\n",
      "test loss: 0.9599\n",
      "test loss: 0.9375\n",
      "Step 150 | Loss: 0.94303 | LR: 1.66e-05\n",
      "test loss: 0.9112\n",
      "test loss: 0.8857\n",
      "Step 175 | Loss: 0.87093 | LR: 1.82e-05\n",
      "test loss: 0.8574\n",
      "test loss: 0.8376\n",
      "test loss: 0.8076\n",
      "Step 200 | Loss: 0.81033 | LR: 2.01e-05\n",
      "test loss: 0.7857\n",
      "test loss: 0.7627\n",
      "Step 225 | Loss: 0.75731 | LR: 2.22e-05\n",
      "test loss: 0.7400\n",
      "test loss: 0.7146\n",
      "test loss: 0.7008\n",
      "Step 250 | Loss: 0.70138 | LR: 2.46e-05\n",
      "test loss: 0.6842\n",
      "test loss: 0.6642\n",
      "Step 275 | Loss: 0.65077 | LR: 2.72e-05\n",
      "test loss: 0.6408\n",
      "test loss: 0.6179\n",
      "test loss: 0.5989\n",
      "Step 300 | Loss: 0.59829 | LR: 3.00e-05\n",
      "test loss: 0.5788\n",
      "test loss: 0.5555\n",
      "Step 325 | Loss: 0.54587 | LR: 3.30e-05\n",
      "test loss: 0.5268\n",
      "test loss: 0.5022\n",
      "test loss: 0.4763\n",
      "Step 350 | Loss: 0.46664 | LR: 3.63e-05\n",
      "test loss: 0.4386\n",
      "test loss: 0.4089\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     34\u001b[39m xc, xct, xt, xtt, aid = xc.to(DEVICE), xct.to(DEVICE), xt.to(DEVICE), xtt.to(DEVICE), aid.to(DEVICE)\n\u001b[32m     36\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m loss = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxct\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxtt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m loss.backward()\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# gradient clipping\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/general/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/general/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 49\u001b[39m, in \u001b[36mBioJepa.forward\u001b[39m\u001b[34m(self, x_control, total_control, x_treated, total_treated, action_id)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_control, total_control, x_treated, total_treated, action_id):\n\u001b[32m     47\u001b[39m     \u001b[38;5;66;03m# 1. Teacher\u001b[39;00m\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m         target_latents = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mteacher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_treated\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_treated\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;66;03m# 2. Student \u001b[39;00m\n\u001b[32m     52\u001b[39m     context_latents = \u001b[38;5;28mself\u001b[39m.student(x_control, total_control)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/general/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/general/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36mCellStateEncoder.forward\u001b[39m\u001b[34m(self, x_genes, x_total_ct)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# 4. Set Transformer\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks:\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m     x = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# 5. Layer Norm\u001b[39;00m\n\u001b[32m     58\u001b[39m x = \u001b[38;5;28mself\u001b[39m.ln_f(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/general/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/general/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mCellStateBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# 1. Attention \u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     x = x + \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mln_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# 2. MLP\u001b[39;00m\n\u001b[32m     16\u001b[39m     x = x + \u001b[38;5;28mself\u001b[39m.mlp(\u001b[38;5;28mself\u001b[39m.ln_2(x))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/general/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/general/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mBioMultiHeadAttention.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     25\u001b[39m v = \u001b[38;5;28mself\u001b[39m.v_proj(x).view(B, T, \u001b[38;5;28mself\u001b[39m.heads, \u001b[38;5;28mself\u001b[39m.head_dim).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# 2. Standard Scaled Dot Product Attention (Permutation Invariant)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m y = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# 5. Reassemble\u001b[39;00m\n\u001b[32m     31\u001b[39m y = y.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m).contiguous().view(B, T, C)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for step in range(max_steps):\n",
    "\n",
    "    last_step = (step == max_steps - 1)\n",
    "\n",
    "    # once in a while evaluate our validation set loss\n",
    "    if step % 10 == 0 or last_step:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_loss_accum = 0.0\n",
    "            test_loss_steps = 5\n",
    "            for i in range(test_loss_steps):\n",
    "                xc, xct, xt, xtt, aid = val_loader.next_batch()\n",
    "                xc, xct, xt, xtt, aid = xc.to(DEVICE), xct.to(DEVICE), xt.to(DEVICE), xtt.to(DEVICE), aid.to(DEVICE)\n",
    "                loss = model(xc, xct, xt, xtt, aid)\n",
    "                loss = loss / test_loss_steps\n",
    "                test_loss_accum += loss.detach()\n",
    "\n",
    "        print(f'test loss: {test_loss_accum.item():.4f}')\n",
    "        # with open(log_file, \"a\") as f:\n",
    "        #    f.write(f'{step} test {test_loss_accum.item():.4f}\\n')\n",
    "\n",
    "\n",
    "    if step > 0 and (step % 1000 == 0 or step % steps_per_epoch ==0) and not last_step:\n",
    "        # Save Checkpoint\n",
    "        torch.save({\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'step': step\n",
    "        }, checkpoint_dir / f'bio_jepa_ckpt_{step}.pt')\n",
    "\n",
    "\n",
    "    model.train()\n",
    "    xc, xct, xt, xtt, aid = val_loader.next_batch()\n",
    "    xc, xct, xt, xtt, aid = xc.to(DEVICE), xct.to(DEVICE), xt.to(DEVICE), xtt.to(DEVICE), aid.to(DEVICE)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss = model(xc, xct, xt, xtt, aid)\n",
    "    loss.backward()\n",
    "\n",
    "    # gradient clipping\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Update Teacher (V-JEPA Momentum)\n",
    "    model.update_teacher()\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    lossi.append(loss.item())\n",
    "    total_epoch_loss += loss.item()\n",
    "    \n",
    "    if step % 25 == 0:\n",
    "        print(f\"Step {step} | Loss: {loss.item():.5f} | LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "    \n",
    "    \n",
    "    if step % steps_per_epoch == 0:   \n",
    "        avg_loss = total_epoch_loss / steps_per_epoch\n",
    "        print(f\"=== Step {step} Done. Avg Loss: {avg_loss:.5f} ===\")\n",
    "        total_epoch_loss = 0\n",
    "    \n",
    "    \n",
    "    if last_step:\n",
    "        # Save Checkpoint\n",
    "        torch.save({\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'step': step\n",
    "        }, checkpoint_dir / f'bio_jepa_ckpt_{step}_final.pt')\n",
    "\n",
    "    step += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4972cdbf-7fd0-4b3b-a541-16135d079c2c",
   "metadata": {},
   "source": [
    "#### Training Loss Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "43c293fa-2067-45b9-ad55-e6127515bb37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lossi)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861fe89a-0447-48d3-9252-9153efcab902",
   "metadata": {},
   "source": [
    "## Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "45916c9b-5f07-4aec-a9ac-e0635bae92a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('/Users/djemec/data/jepa/pathway_names.json'),\n",
       " PosixPath('/Users/djemec/data/jepa/gene_names.json'),\n",
       " PosixPath('/Users/djemec/data/jepa/perturbation_map.json'),\n",
       " PosixPath('/Users/djemec/data/jepa/tokenized/val'))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dir = tok_dir / 'val'\n",
    "pathway_names_path = data_dir / 'pathway_names.json'\n",
    "gene_names_path = data_dir / 'gene_names.json'\n",
    "pathway_names_path, gene_names_path, metadata_path, val_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "54c35355-2ff6-486d-9ce8-e4cbbbee1599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load Map (ID -> Name)\n",
    "with open(metadata_path, \"r\") as f:\n",
    "    pert_map = json.load(f)\n",
    "id_to_pert = {v: k for k, v in pert_map.items()}\n",
    "\n",
    "with open(pathway_names_path, \"r\") as f:\n",
    "    pathway_names = json.load(f) # List of 1024 names\n",
    "\n",
    "with open(data_dir / 'gene_names.json', \"r\") as f:\n",
    "    gene_names = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "24ae854c-4e74-42f5-9cfd-0c487ff22549",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BioJepa(\n",
       "  (student): CellStateEncoder(\n",
       "    (total_count_proj): Linear(in_features=1, out_features=8, bias=True)\n",
       "    (blocks): ModuleList(\n",
       "      (0-5): 6 x CellStateBlock(\n",
       "        (ln_1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): BioMultiHeadAttention(\n",
       "          (q_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "          (k_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "          (v_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "          (c_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=8, out_features=32, bias=True)\n",
       "          (gelu): GELU(approximate='tanh')\n",
       "          (c_proj): Linear(in_features=32, out_features=8, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (teacher): CellStateEncoder(\n",
       "    (total_count_proj): Linear(in_features=1, out_features=8, bias=True)\n",
       "    (blocks): ModuleList(\n",
       "      (0-5): 6 x CellStateBlock(\n",
       "        (ln_1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): BioMultiHeadAttention(\n",
       "          (q_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "          (k_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "          (v_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "          (c_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=8, out_features=32, bias=True)\n",
       "          (gelu): GELU(approximate='tanh')\n",
       "          (c_proj): Linear(in_features=32, out_features=8, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (predictor): ACPredictor(\n",
       "    (action_embed): Embedding(2058, 256)\n",
       "    (blocks): ModuleList(\n",
       "      (0-5): 6 x PredictorBlock(\n",
       "        (ada_ln1): AdaLN(\n",
       "          (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=False)\n",
       "          (action_mlp): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=256, out_features=16, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn): BioMultiHeadAttention(\n",
       "          (q_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "          (k_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "          (v_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "          (c_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "        )\n",
       "        (ada_ln2): AdaLN(\n",
       "          (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=False)\n",
       "          (action_mlp): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=256, out_features=16, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=8, out_features=32, bias=True)\n",
       "          (gelu): GELU(approximate='tanh')\n",
       "          (c_proj): Linear(in_features=32, out_features=8, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_norm): AdaLN(\n",
       "      (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=False)\n",
       "      (action_mlp): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=256, out_features=16, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09550430-309d-436c-a140-2bda3fb8bde3",
   "metadata": {},
   "source": [
    "**Get random data sample**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "189729ef-8852-466c-b05b-455948aa6cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_test_pair(shard_dir):\n",
    "    '''Grab a single real pair from a random shard with Context'''\n",
    "    files = sorted(shard_dir.glob('*.npz'))\n",
    "        \n",
    "    file_path = files[np.random.randint(len(files))]\n",
    "    \n",
    "    with np.load(file_path) as data:\n",
    "        idx = np.random.randint(data['action_ids'].shape[0])\n",
    "        \n",
    "        # Extract Items (Now 5 items instead of 3)\n",
    "        c_raw = data['control'][idx]         # [2000]\n",
    "        ct_raw = data['control_total'][idx]  # Scalar\n",
    "        case_raw = data['case'][idx]         # [2000]\n",
    "        caset_raw = data['case_total'][idx]  # Scalar\n",
    "        act_id = data['action_ids'][idx]     # Scalar\n",
    "        \n",
    "    # Convert to Tensor & Add Batch Dim [1, ...]\n",
    "    xc = torch.tensor(c_raw).float().unsqueeze(0).to(DEVICE)\n",
    "    xct = torch.tensor(ct_raw).float().unsqueeze(0).to(DEVICE)\n",
    "    xt = torch.tensor(case_raw).float().unsqueeze(0).to(DEVICE)\n",
    "    xtt = torch.tensor(caset_raw).float().unsqueeze(0).to(DEVICE)\n",
    "    aid = torch.tensor([act_id]).long().to(DEVICE)\n",
    "    \n",
    "    return xc, xct, xt, xtt, aid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "793361fe-baee-42b7-982d-72e604d28013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HSPA8'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xc, xct, xt, xtt, aid = get_random_test_pair(val_dir)\n",
    "pert_name = id_to_pert[aid.item()]\n",
    "pert_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "75e32d24-4c56-40fc-9d76-3048536eddb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Teacher sees the FUTURE (Real Treated Case)\n",
    "    # Output: [1, 1024, 384]\n",
    "    z_case = model.teacher(xt, xtt)       \n",
    "    \n",
    "    # Student sees the PAST (Control)\n",
    "    # Output: [1, 1024, 384]\n",
    "    z_context = model.student(xc, xct)\n",
    "    \n",
    "    # Control Baseline (Where we started in Teacher Space)\n",
    "    # We run control through teacher to get the \"Anchor\"\n",
    "    z_control = model.teacher(xc, xct) \n",
    "\n",
    "    # Predictor tries to guess Real from Context + Action\n",
    "    z_predicted = model.predictor(z_context, aid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5360c5-2215-4fc8-858a-d1488cf54ab6",
   "metadata": {},
   "source": [
    "**Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f5e38d09-e796-4d5f-882b-f6fcfe150974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5329, -0.3398,  0.5301,  ..., -0.2015, -0.2472, -0.5073],\n",
       "        [ 0.5523, -0.9346, -0.4710,  ...,  0.3838, -1.3744,  0.4791],\n",
       "        [-0.0195,  0.0485,  0.4043,  ...,  0.5261, -0.8577, -0.4143],\n",
       "        ...,\n",
       "        [-0.4131,  0.0534, -0.1567,  ...,  1.9780, -0.5056, -1.0708],\n",
       "        [-0.0732,  0.5548, -0.1782,  ..., -0.1769, -0.7081, -0.0992],\n",
       "        [ 0.0480, -0.8392,  0.2991,  ...,  0.5657, -0.3735, -0.1615]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate Latent Movement\n",
    "delta_latent = (z_predicted - z_control).squeeze(0) # [1024, 384]\n",
    "delta_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0658ae95-4083-430e-8e7b-cf9d329f0b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0891, 2.1517, 1.1839,  ..., 2.3555, 1.0771, 1.1752])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 3. Project \"Pathway Energy\" back to Genes ---\n",
    "# We want to score every gene based on how much its parent pathways moved.\n",
    "# Logic: Gene_Score = Sum(Mask_gp * Magnitude(Pathway_p))\n",
    "# If a gene is in 5 pathways that all moved violently, that gene is highly implicated.\n",
    "pathway_magnitudes = delta_latent.norm(dim=1) \n",
    "pathway_magnitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7cae5596-43cc-4b2b-b01c-1f9bc7807c0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the mask (ensure it's on the same device)\n",
    "# Shape: [2000, 1024]\n",
    "mask_tensor = model.student.mask \n",
    "mask_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8bcfdb61-d960-4b42-a8e9-83660d1b4035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project: [2000 Genes, 1024 Pathways] @ [1024 Magnitudes] -> [2000 Gene Scores]\n",
    "# This tells us the \"Implied Impact\" on each gene\n",
    "gene_impact_scores = torch.mv(mask_tensor, pathway_magnitudes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9d91232c-aaa3-473c-aa75-bbf6953b4850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Get Top Predicted Genes ---\n",
    "top_k = 20\n",
    "pred_values, pred_indices = torch.topk(gene_impact_scores, top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "43289f5c-461b-4d98-ab5e-b6ceda99d24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the pathway changes, the model predicts these genes are most affected:\n",
      "   CASP3      (Score: 630.9901)\n",
      "   VEGFA      (Score: 562.6682)\n",
      "   AURKB      (Score: 427.5754)\n",
      "   MYC        (Score: 420.9684)\n",
      "   SLC7A1     (Score: 393.1136)\n",
      "   MCL1       (Score: 379.5205)\n",
      "   JUN        (Score: 374.8224)\n",
      "   ICAM1      (Score: 361.8789)\n",
      "   NR3C1      (Score: 353.1133)\n",
      "   SOD1       (Score: 344.0250)\n",
      "   NFKB1      (Score: 334.8659)\n",
      "   NFKBIA     (Score: 333.9523)\n",
      "   NFE2L2     (Score: 333.3680)\n",
      "   PRDX2      (Score: 323.3816)\n",
      "   CYCS       (Score: 320.3308)\n",
      "   LDHA       (Score: 317.0228)\n",
      "   BCL2L1     (Score: 315.9322)\n",
      "   HSPA1A     (Score: 312.3195)\n",
      "   DDIT3      (Score: 310.0233)\n",
      "   MAPK1      (Score: 307.0726)\n"
     ]
    }
   ],
   "source": [
    "print(f'Based on the pathway changes, the model predicts these genes are most affected:')\n",
    "for val, idx in zip(pred_values, pred_indices):\n",
    "    g_name = gene_names[idx.item()]\n",
    "    print(f\"   {g_name:<10} (Score: {val.item():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a81bbc5-ca1e-49fe-a31e-43cb5adb3f30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "df811e83-621e-4538-a44a-65a4f7b94d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. REALITY CHECK (The most important part) ---\n",
    "# Did these genes ACTUALLY change in the real data?\n",
    "# Let's compare the Model's \"Top Genes\" vs the Reality's \"Top Genes\"\n",
    "# Calculate Real Gene Change (Log Space)\n",
    "# Shape: [1, 2000]\n",
    "real_gene_delta = (xt - xc).abs().squeeze(0) \n",
    "# Get the Top 10 Genes that REALLY changed\n",
    "real_values, real_indices = torch.topk(real_gene_delta, top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aef3b5d1-9b08-4653-b922-15db8938f291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "   HBZ        (Delta: 3.7502)\n",
      "   ACTB       (Delta: 3.3753)\n",
      "   CHI3L2     (Delta: 3.1257)\n",
      "   SLC25A37   (Delta: 2.9807)\n",
      "   HBG2       (Delta: 2.5640)\n",
      "   CYCS       (Delta: 2.5466)\n",
      "   LDHA       (Delta: 2.4499)\n",
      "   SRM        (Delta: 2.3428)\n",
      "   APOE       (Delta: 2.2907)\n",
      "   PRDX5      (Delta: 2.2381)\n",
      "   RESF1      (Delta: 2.2381)\n",
      "   POLR2L     (Delta: 2.2229)\n",
      "   TAF15      (Delta: 2.2229)\n",
      "   HSPA8      (Delta: 2.1150)\n",
      "   HSP90B1    (Delta: 2.0867)\n",
      "   SYNCRIP    (Delta: 2.0867)\n",
      "   TCP1       (Delta: 2.0867)\n",
      "   MTRNR2L8   (Delta: 2.0867)\n",
      "   EIF3B      (Delta: 2.0867)\n",
      "   CRYM       (Delta: 2.0412)\n"
     ]
    }
   ],
   "source": [
    "print(f'Ground Truth')\n",
    "for val, idx in zip(real_values, real_indices):\n",
    "    g_name = gene_names[idx.item()]\n",
    "    \n",
    "    # Check if our model also listed this gene in its top 10\n",
    "    match_icon = \"\" if idx in pred_indices else \"\"\n",
    "    \n",
    "    print(f\"  {match_icon} {g_name:<10} (Delta: {val.item():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3b1832c3-d09e-4e23-a1aa-934986ccb0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Precision @ 10]\n",
      "The model correctly identified 2/10 of the top driver genes.\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Precision@10 Metric ---\n",
    "# How many of the Top 10 real genes were in the Top 10 predicted genes?\n",
    "# (Set intersection)\n",
    "set_pred = set(pred_indices.tolist())\n",
    "set_real = set(real_indices.tolist())\n",
    "overlap = len(set_pred.intersection(set_real))\n",
    "\n",
    "print(f\"\\n[Precision @ 10]\")\n",
    "print(f\"The model correctly identified {overlap}/10 of the top driver genes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc7a4bc-88e5-4903-bbad-6ffa382db26a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30288334-6f6f-48a3-96e3-4f9144215288",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
