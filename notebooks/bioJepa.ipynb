{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfe968dc-a937-469a-89d8-7ba179a2e01a",
   "metadata": {},
   "source": [
    "# Bio-JEPA AC \n",
    "\n",
    "Based on [V-JEPA 2 AC](https://arxiv.org/abs/2506.09985)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b67dcd-a768-447b-ad7b-77e8cddd59c0",
   "metadata": {},
   "source": [
    "**Goal**\n",
    "Our goal is to build a World Model for cell biology. In it's current configuration, our model calculates that hitting a cell with a specific gene knockout, or drug if we had it in the dataset, causes specific biological pathways to activate or shut down. Unlike an LLM, this is a predictive simulation that operates entirely within a compressed mathematical space to understand cause and effect.\n",
    "\n",
    "The process begins with the inputs, which feed the model three distinct pieces of information for every training step. \n",
    "\tFirst, it receives the **Before** state, which is data representing a healthy control cell. Instead of a messy list of 20,000 raw gene counts, the tokenizer has already compressed this into a structured set of pathway scores (using [Reactome Pathway 2024 data](https://maayanlab.cloud/Harmonizome/dataset/Reactome+Pathways+2024)). This essentially tells the model that the cell currently has high energy, low stress, and normal growth. \n",
    "\tSecond, the model receives the **Action**, which is the specific perturbation performed in the lab, in our case a CRISPR knockdown of a specific gene. This could also be a drug application or protein introduction if we had the data. This is converted into a learnable \"Action Embedding,\" effectively serving as the command that tells the simulation what event just occurred. \n",
    "\tThird, the model is given the **After** state, which is the actual knockdown cell observed in the experiment. This third input serves purely as the target or \"ground truth\"; the model is not allowed to see it while making its prediction, but uses it afterwards in the loss calculation and backprop.\n",
    "\n",
    "Inside the model, a three-step simulation plays out to process these inputs. It starts with the **Student Encoder**, or the *Perception* module, which looks at the healthy **Before** cell input  and compresses it into a Latent State. At this stage, the model is simply understanding the baseline biological status of the cell. This latent representation is then passed to the **Action-Conditioned Predictor**, which acts as the *Physics Engine.* This component combines the cell's current state with the Action vector. Using a mechanism called Adaptive Layer Normalization (AdaLN), the action actually modulates the internal weights of the neural network, effectively shifting the physical rules of the simulation to match the drug's effects. The **Predictor** then tries to predict, or hellucinate, what the future state of the cell will be, calculating a new vector that represents the cell's condition after the knockout, or drug impact. \n",
    "\n",
    "To validate the predictor, simultaneously the **Teacher Encoder** looks at the real **After** data from the lab and encodes it into that same latent space to serve as the judge. To learn, the model compares the predictor output against the Teacher's reality. With backprop, we attempt to minimize the difference between the prediction and the actual outcome, trying to minimize this error over millions of examples. \n",
    "\n",
    "In theory, by forcing its predictions to match reality, the model moves beyond simply memorizing data and begins to learn the underlying causal rules of biology. It figures out gene regulatory logicâ€”understanding that if Gene A is knocked down, Pathway B must functionally fail. It learns how different pathways, like inflammation and cell death, are causally linked. Ultimately, we hope it learns the \"physics\" of how perturbations work, allowing us to eventually simulate the effects of gene mutations, drugs or genetic interventions on cells without having to perform the physical experiment in a wet lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33a357f2-d497-4dfc-9920-568422b5403a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import copy\n",
    "import math\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9242cf00-c812-41d0-ae92-9c7dd0af6f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "random.seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d6fc33-8c5c-43bf-960a-2f83fea1601e",
   "metadata": {},
   "source": [
    "## Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f158bffc-f24a-4471-a463-7241c2ad6784",
   "metadata": {},
   "source": [
    "### ROTARY POSITIONAL EMBEDDINGS (RoPE)\n",
    "V-JEPA 2 uses 3D-RoPE. We adapt this to 1D-RoPE for our list of Pathway Tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38ffafca-b99d-4cde-884f-a6347c5eafed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_seq_len=2048):\n",
    "        super().__init__()\n",
    "        # Ensure dim is the head dimension, not the full embedding dimension\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        t = torch.arange(max_seq_len).type_as(inv_freq)\n",
    "        freqs = torch.outer(t, inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.register_buffer('emb', emb)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [Batch, Seq, Dim]\n",
    "        # We only care about the sequence length here\n",
    "        n = x.shape[1]\n",
    "        # Returns [Seq, Head_Dim]\n",
    "        return self.emb[:n, :].cos(), self.emb[:n, :].sin()\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin):\n",
    "    # q, k: [Batch, Heads, Seq, Head_Dim]\n",
    "    # cos, sin: [Seq, Head_Dim] -> reshape to [1, 1, Seq, Head_Dim]\n",
    "    \n",
    "    cos = cos.unsqueeze(0).unsqueeze(0)\n",
    "    sin = sin.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # Standard RoPE rotation logic\n",
    "    # split last dim into half\n",
    "    q_d = q.shape[-1] // 2\n",
    "    k_d = k.shape[-1] // 2\n",
    "    \n",
    "    q1, q2 = q[..., :q_d], q[..., q_d:]\n",
    "    k1, k2 = k[..., :k_d], k[..., k_d:]\n",
    "    \n",
    "    q_rotated = torch.cat((-q2, q1), dim=-1)\n",
    "    k_rotated = torch.cat((-k2, k1), dim=-1)\n",
    "    \n",
    "    q_out = (q * cos) + (q_rotated * sin)\n",
    "    k_out = (k * cos) + (k_rotated * sin)\n",
    "    \n",
    "    return q_out, k_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358918d9-36ba-48e5-aade-398109b4d660",
   "metadata": {},
   "source": [
    "### Multi-Headed Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d1ff083-ffb8-4102-a8ea-e1f6f17bee12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioMultiHeadAttention(nn.Module):\n",
    "    # mirrors nn.MultiheadAttention(dim, heads, batch_first=True) \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.embed_dim % config.heads == 0\n",
    "        \n",
    "        self.head_dim = config.embed_dim // config.heads\n",
    "        self.heads = config.heads\n",
    "        self.embed_dim = config.embed_dim\n",
    "        \n",
    "        # Projections\n",
    "        self.q_proj = nn.Linear(config.embed_dim, config.embed_dim)\n",
    "        self.k_proj = nn.Linear(config.embed_dim, config.embed_dim)\n",
    "        self.v_proj = nn.Linear(config.embed_dim, config.embed_dim)\n",
    "        \n",
    "        self.c_proj = nn.Linear(config.embed_dim, config.embed_dim)\n",
    "\n",
    "    def forward(self, x, cos, sin):\n",
    "        B, T, C = x.size() # Batch, Seq, Embed Dim\n",
    "        \n",
    "        # 1. Project\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "\n",
    "        # 2. Reshape for multi-head attention\n",
    "        # (B, T, nh, hs) -> (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, T, self.heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # 3. Apply RoPE to Q and K (Rotary is applied per head)\n",
    "        q, k = apply_rotary_pos_emb(q, k, cos, sin)\n",
    "\n",
    "        # 4. Attention\n",
    "        # is_causal=False because this is a bidirectional encoder\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=False)\n",
    "        \n",
    "        # 5. Reassemble\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.c_proj(y)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715a67a5-efb2-4b22-8ec0-54a0294e01e2",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bb01e77-936e-4235-a962-9cb7656f889b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.embed_dim, int(config.mlp_ratio * config.embed_dim))\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "        self.c_proj = nn.Linear(int(config.mlp_ratio * config.embed_dim), config.embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5be00e5-ce0a-4335-95c5-2519e8157824",
   "metadata": {},
   "source": [
    "### Hidden Transfomer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02208352-b690-47af-8c90-06210718b5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PathwayBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.embed_dim)\n",
    "        self.attn = BioMultiHeadAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.embed_dim)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x, cos, sin):\n",
    "        # 1. Attention with RoPE\n",
    "        x_norm = self.ln_1(x)\n",
    "        # Pass cos/sin into attn to be applied to Q/K\n",
    "        attn_out = self.attn(x_norm, cos, sin)\n",
    "        x = x + attn_out\n",
    "\n",
    "        # 2. MLP\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69942da-c18f-4585-9041-911fed8107f7",
   "metadata": {},
   "source": [
    "### Pathway Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e45f71db-0836-4c3b-bc8b-3957f4c02630",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PathwayEncoderConfig:\n",
    "    num_pathways: int = 1024 \n",
    "    n_layer: int = 24 \n",
    "    heads: int = 12\n",
    "    embed_dim: int = 768\n",
    "    mlp_ratio: float = 4.0 # Changed to float for precision\n",
    "\n",
    "class PathwayEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            # input projects to embedding\n",
    "            input_proj = nn.Linear(1, config.embed_dim),\n",
    "            \n",
    "            # RoPE needs the HEAD dimension, not the full embedding dimension\n",
    "            rope = RotaryEmbedding(config.embed_dim // config.heads),\n",
    "            \n",
    "            # transformer block\n",
    "            blocks = nn.ModuleList([PathwayBlock(config) for _ in range(config.n_layer)]),\n",
    "            \n",
    "            # final layer norm\n",
    "            ln_f = nn.LayerNorm(config.embed_dim) \n",
    "        ))\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        std = 0.02\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [Batch, Num_Pathways]\n",
    "        x = x.unsqueeze(-1) # [B, N, 1]\n",
    "        x = self.transformer.input_proj(x) # [B, N, Dim]\n",
    "        \n",
    "        # Generate RoPE cache\n",
    "        # cos, sin are [Seq, Head_Dim]\n",
    "        cos, sin = self.transformer.rope(x)\n",
    "        cos, sin = cos.to(x.device), sin.to(x.device)\n",
    "        \n",
    "        for block in self.transformer.blocks:\n",
    "            x = block(x, cos, sin)\n",
    "            \n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e165225-f747-4be2-a0a8-cc7b8b1b6d1f",
   "metadata": {},
   "source": [
    "### Adaptive Layer Normalization AdaLN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab2556a4-03b2-47e6-9392-eb2dbb648284",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaLN(nn.Module):\n",
    "    '''\n",
    "    Adaptive Layer Norm for conditioning the predictor on action embeddings.\n",
    "    The action vector regresses the Scale (gamma) and Shift (beta) of the normalization.\n",
    "    '''\n",
    "    def __init__(self, embed_dim, action_embed_dim):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(embed_dim, elementwise_affine=False)\n",
    "        self.action_mlp = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(action_embed_dim, 2 * embed_dim)\n",
    "        )\n",
    "        # Initialize to identity (gamma=0, beta=0 originally, effectively gamma=1 after logic)\n",
    "        # Zero-init the last layer so the action starts as a \"no-op\" (identity)\n",
    "        nn.init.zeros_(self.action_mlp[1].weight)\n",
    "        nn.init.zeros_(self.action_mlp[1].bias)\n",
    "\n",
    "    def forward(self, x, action_emb):\n",
    "        # x: [Batch, Seq, Dim]\n",
    "        # action_emb: [Batch, action_embed_dim]\n",
    "        \n",
    "        # Project action to style: [B, 2*D] -> [B, 1, 2*D]\n",
    "        style = self.action_mlp(action_emb).unsqueeze(1) \n",
    "        gamma, beta = style.chunk(2, dim=-1)\n",
    "        \n",
    "        # Apply affine transformation based on action\n",
    "        return self.norm(x) * (1 + gamma) + beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d966f7-8f5d-4c06-b6db-d02aaba87de8",
   "metadata": {},
   "source": [
    "### Predictor Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc8a2a58-7549-4bd1-908f-d82608063959",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictorBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # 1. Conditioning (AdaLN) replaces standard LayerNorm\n",
    "        self.ada_ln1 = AdaLN(config.embed_dim, config.action_embed_dim)\n",
    "        \n",
    "        # 2. Attention (Using the shared BioMultiHeadAttention)\n",
    "        self.attn = BioMultiHeadAttention(config)\n",
    "        \n",
    "        # 3. Conditioning (AdaLN) for the MLP block\n",
    "        self.ada_ln2 = AdaLN(config.embed_dim, config.action_embed_dim)\n",
    "        \n",
    "        # 4. MLP (Using the shared MLP)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x, action_emb, cos, sin):\n",
    "        # 1. AdaLN -> Attention (with internal RoPE) -> Residual\n",
    "        x_norm = self.ada_ln1(x, action_emb)\n",
    "        \n",
    "        # Note: BioMultiHeadAttention handles q/k/v projection and apply_rotary_pos_emb internally\n",
    "        attn_out = self.attn(x_norm, cos, sin)\n",
    "        x = x + attn_out\n",
    "        \n",
    "        # 2. AdaLN -> MLP -> Residual\n",
    "        x_norm = self.ada_ln2(x, action_emb)\n",
    "        x = x + self.mlp(x_norm)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b7340a-5225-4131-83d2-614631a55a10",
   "metadata": {},
   "source": [
    "### Main Predictor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b641afb7-7d8d-4b4b-833c-72400e48610d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ACPredictorConfig:\n",
    "    num_pathways: int = 1024\n",
    "    n_layer: int = 6 \n",
    "    heads: int = 4\n",
    "    embed_dim: int = 384\n",
    "    action_embed_dim: int=256 \n",
    "    mlp_ratio: float = 4.0\n",
    "    max_perturb: int= 2058 ## eventually try to get to a 2**N power\n",
    "\n",
    "class ACPredictor(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Action Embedding (Discrete ID -> Vector)\n",
    "        self.action_embed = nn.Embedding(config.max_perturb, config.action_embed_dim)\n",
    "        \n",
    "        # Learnable Queries (\"Mask Tokens\") for the future state\n",
    "        # One query vector per pathway position\n",
    "        self.mask_queries = nn.Parameter(torch.randn(1, config.num_pathways, config.embed_dim) * 0.02)\n",
    "        \n",
    "        # RoPE: initialized with HEAD dimension (dim // heads)\n",
    "        head_dim = config.embed_dim // config.heads\n",
    "        self.rope = RotaryEmbedding(head_dim)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            PredictorBlock(config) for _ in range(config.n_layer)\n",
    "        ])\n",
    "        \n",
    "        self.final_norm = AdaLN(config.embed_dim, config.action_embed_dim)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        std = 0.02\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "\n",
    "    def forward(self, context_latents, action_ids):\n",
    "        \"\"\"\n",
    "        context_latents: [Batch, N, Dim] (From Student Encoder)\n",
    "        action_ids: [Batch] (Ints)\n",
    "        \"\"\"\n",
    "        B, N, D = context_latents.shape\n",
    "        \n",
    "        # 1. Embed Action\n",
    "        action_emb = self.action_embed(action_ids) # [B, action_embed_dim]\n",
    "        \n",
    "        # 2. Construct Input: [Context, Mask_Queries]\n",
    "        # We concatenate the learned queries to the context. \n",
    "        # The predictor will attend to the context to update the queries.\n",
    "        queries = self.mask_queries.repeat(B, 1, 1) # [B, N, D]\n",
    "        sequence = torch.cat([context_latents, queries], dim=1) # [B, 2N, D]\n",
    "        \n",
    "        # 3. Generate RoPE for the full sequence (2N)\n",
    "        # cos, sin are [2N, Head_Dim]\n",
    "        cos, sin = self.rope(sequence)\n",
    "        cos, sin = cos.to(sequence.device), sin.to(sequence.device)\n",
    "        \n",
    "        # 4. Pass through AdaLN Blocks\n",
    "        for block in self.blocks:\n",
    "            sequence = block(sequence, action_emb, cos, sin)\n",
    "            \n",
    "        sequence = self.final_norm(sequence, action_emb)\n",
    "        \n",
    "        # 5. Return only the predicted part (The Queries corresponding to N..2N)\n",
    "        predictions = sequence[:, N:, :] \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb48919-caa3-4d81-93d0-dca2d4d30f96",
   "metadata": {},
   "source": [
    "## Bio-JEPA AC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16297aa1-3670-4b89-9e3a-19cfab546ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BioJepaConfig:\n",
    "    num_pathways: int = 1024\n",
    "    n_layer: int = 6 \n",
    "    heads: int = 4\n",
    "    embed_dim: int = 384\n",
    "    action_embed_dim: int=256 \n",
    "    mlp_ratio: float = 4.0\n",
    "    max_perturb: int= 2058 ## eventually try to get to a 2**N power\n",
    "    \n",
    "class BioJepa(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.student = PathwayEncoder(PathwayEncoderConfig(\n",
    "            num_pathways = config.num_pathways,\n",
    "            n_layer= config.n_layer, \n",
    "            heads=config.heads, \n",
    "            embed_dim= config.embed_dim\n",
    "        ))\n",
    "                                      \n",
    "        self.teacher = copy.deepcopy(self.student)\n",
    "        \n",
    "        # Freeze teacher\n",
    "        for p in self.teacher.parameters():\n",
    "            p.requires_grad = False\n",
    "            \n",
    "        self.predictor = ACPredictor(ACPredictorConfig(\n",
    "            n_layer=config.n_layer, \n",
    "            heads=config.heads, \n",
    "            embed_dim=config.embed_dim, \n",
    "            action_embed_dim=config.action_embed_dim,\n",
    "            num_pathways=config.num_pathways\n",
    "        ))\n",
    "        \n",
    "    def forward(self, x_control, x_treated, action_id):\n",
    "        # 1. Teacher\n",
    "        with torch.no_grad():\n",
    "            target_latents = self.teacher(x_treated)\n",
    "            \n",
    "        # 2. Student \n",
    "        context_latents = self.student(x_control)\n",
    "        \n",
    "        # 3. Predictor \n",
    "        predicted_latents = self.predictor(context_latents, action_id)\n",
    "        \n",
    "        # 4. Latent Loss (L1)\n",
    "        loss = F.l1_loss(predicted_latents, target_latents)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_teacher(self, m=0.996):\n",
    "        for param_s, param_t in zip(self.student.parameters(), self.teacher.parameters()):\n",
    "            param_t.data.mul_(m).add_((1 - m) * param_s.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdbd6fb-ec72-4af9-ba48-13d7731c3343",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96184936-3410-4d44-8be1-e01ed3c7cc9e",
   "metadata": {},
   "source": [
    "#### Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6dfbb5f-2a56-4709-a9a2-9ab3fe0973a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('/Users/djemec/data/jepa')\n",
    "shard_dir = data_dir / 'tokenized'\n",
    "train_dir = shard_dir / 'train'\n",
    "val_dir =  shard_dir / 'val'\n",
    "metadata_path = data_dir / 'perturbation_map.json'\n",
    "checkpoint_dir = data_dir / 'checkpoint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7885c628-45b4-4c23-97e5-e3ee6f0400e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "n_embd = 8\n",
    "n_pathways = 1024\n",
    "LR = 1e-3\n",
    "EPOCHS = 2\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tok_file_chunk_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39a1967a-562d-4772-904a-d1092ba2cf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairedShardDataset(Dataset):\n",
    "    def __init__(self, shard_dir):\n",
    "        self.files = sorted(shard_dir.glob('*.npz'))\n",
    "        print(f'Found {len(self.files)} shards.')\n",
    "        \n",
    "    def __len__(self):\n",
    "        # Approx length for progress bar (10k per shard)\n",
    "        return len(self.files) * tok_file_chunk_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Stochastic loading: Pick random shard, then random row\n",
    "        # This avoids loading 100MB files for just one item in strict order\n",
    "        file_path = self.files[np.random.randint(len(self.files))]\n",
    "        \n",
    "        # Load shard (fast if on SSD)\n",
    "        try:\n",
    "            with np.load(file_path) as data:\n",
    "                # Keys: 'control', 'case', 'action_ids'\n",
    "                n_rows = data['action_ids'].shape[0]\n",
    "                row_idx = np.random.randint(n_rows)\n",
    "                \n",
    "                control_raw = data['control'][row_idx]\n",
    "                case_raw = data['case'][row_idx]\n",
    "                act_id = data['action_ids'][row_idx]\n",
    "        except Exception as e:\n",
    "            # Fallback for corrupt shard\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "            return self.__getitem__(0)\n",
    "\n",
    "        # Dequantize\n",
    "        x_control = torch.tensor(control_raw)\n",
    "        x_case = torch.tensor(case_raw)\n",
    "        action_id = torch.tensor(act_id, dtype=torch.long)\n",
    "        \n",
    "        return x_control, x_case, action_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cc07f0-eb9e-4e6a-8784-0e02e5eb0faf",
   "metadata": {},
   "source": [
    "#### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c02da536-0088-4376-8d89-37839eebb8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 29 shards.\n",
      "Found 1 shards.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = PairedShardDataset(train_dir)\n",
    "val_dataset = PairedShardDataset(val_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67101eee-4628-4d8a-b327-c7bf3de3e3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "cycle_train_loader = itertools.cycle(train_loader)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "cycle_val_loader = itertools.cycle(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0202749-f533-40fd-a891-08f9c9ff8edb",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "723366c9-4869-408a-b1bc-f2dba2f913ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BioJepa(BioJepaConfig(\n",
    "    num_pathways=n_pathways,\n",
    "    embed_dim = n_embd,\n",
    "    n_layer = 1\n",
    ")).to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ff27eb-93fa-439c-a31d-366dff143d7d",
   "metadata": {},
   "source": [
    "#### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ff8ba83-7ff9-4a99-92c7-1fec94759781",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57196ead-30ec-4098-a645-9ec7bdf3a300",
   "metadata": {},
   "source": [
    "#### Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b66ffc0d-d4f0-4f3d-af4c-390acfd7d5ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18125, 36250)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch = len(train_loader)\n",
    "max_steps = EPOCHS * len(train_loader)\n",
    "steps_per_epoch, max_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e95819c7-d216-4a1b-93cc-467a20311ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=LR, total_steps=max_steps, pct_start=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89952bd9-d632-44db-ae1e-0063389ff31f",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62bc4b56-b682-4b56-922e-ef040ab8d29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossi = []\n",
    "step = 0\n",
    "total_epoch_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96cee680-aa5a-464f-99ef-27b5c758d21c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 1.0457\n",
      "Step 25 | Loss: 0.94716 | LR: 4.05e-05\n",
      "Step 50 | Loss: 0.84752 | LR: 4.18e-05\n",
      "Step 75 | Loss: 0.74294 | LR: 4.41e-05\n",
      "Step 100 | Loss: 0.62373 | LR: 4.72e-05\n",
      "test loss: 0.6193\n",
      "Step 125 | Loss: 0.49959 | LR: 5.12e-05\n",
      "Step 150 | Loss: 0.37433 | LR: 5.61e-05\n",
      "Step 175 | Loss: 0.26647 | LR: 6.19e-05\n",
      "Step 200 | Loss: 0.17888 | LR: 6.86e-05\n",
      "test loss: 0.1760\n",
      "Step 225 | Loss: 0.11973 | LR: 7.61e-05\n",
      "Step 250 | Loss: 0.08708 | LR: 8.44e-05\n",
      "Step 275 | Loss: 0.06388 | LR: 9.36e-05\n",
      "Step 300 | Loss: 0.04639 | LR: 1.04e-04\n",
      "test loss: 0.0453\n",
      "Step 325 | Loss: 0.03795 | LR: 1.14e-04\n",
      "Step 350 | Loss: 0.03324 | LR: 1.26e-04\n",
      "Step 375 | Loss: 0.03872 | LR: 1.38e-04\n",
      "Step 400 | Loss: 0.03589 | LR: 1.51e-04\n",
      "test loss: 0.0342\n",
      "Step 425 | Loss: 0.03117 | LR: 1.65e-04\n",
      "Step 450 | Loss: 0.03456 | LR: 1.79e-04\n",
      "Step 475 | Loss: 0.03909 | LR: 1.94e-04\n",
      "Step 500 | Loss: 0.04497 | LR: 2.09e-04\n",
      "test loss: 0.0413\n",
      "Step 525 | Loss: 0.04290 | LR: 2.26e-04\n",
      "Step 550 | Loss: 0.04512 | LR: 2.42e-04\n",
      "Step 575 | Loss: 0.04362 | LR: 2.60e-04\n",
      "Step 600 | Loss: 0.05669 | LR: 2.77e-04\n",
      "test loss: 0.0449\n",
      "Step 625 | Loss: 0.03751 | LR: 2.95e-04\n",
      "Step 650 | Loss: 0.03943 | LR: 3.14e-04\n",
      "Step 675 | Loss: 0.03370 | LR: 3.33e-04\n",
      "Step 700 | Loss: 0.03493 | LR: 3.52e-04\n",
      "test loss: 0.0342\n",
      "Step 725 | Loss: 0.02962 | LR: 3.72e-04\n",
      "Step 750 | Loss: 0.03077 | LR: 3.92e-04\n",
      "Step 775 | Loss: 0.02674 | LR: 4.12e-04\n",
      "Step 800 | Loss: 0.03033 | LR: 4.32e-04\n",
      "test loss: 0.0246\n",
      "Step 825 | Loss: 0.02406 | LR: 4.53e-04\n",
      "Step 850 | Loss: 0.03202 | LR: 4.74e-04\n",
      "Step 875 | Loss: 0.02767 | LR: 4.94e-04\n",
      "Step 900 | Loss: 0.02540 | LR: 5.15e-04\n",
      "test loss: 0.0227\n",
      "Step 925 | Loss: 0.02353 | LR: 5.36e-04\n",
      "Step 950 | Loss: 0.02522 | LR: 5.57e-04\n",
      "Step 975 | Loss: 0.02523 | LR: 5.78e-04\n",
      "Step 1000 | Loss: 0.01956 | LR: 5.98e-04\n",
      "test loss: 0.0216\n",
      "Step 1025 | Loss: 0.02531 | LR: 6.19e-04\n",
      "Step 1050 | Loss: 0.02330 | LR: 6.39e-04\n",
      "Step 1075 | Loss: 0.02061 | LR: 6.59e-04\n",
      "Step 1100 | Loss: 0.01489 | LR: 6.79e-04\n",
      "test loss: 0.0180\n",
      "Step 1125 | Loss: 0.01799 | LR: 6.98e-04\n",
      "Step 1150 | Loss: 0.01826 | LR: 7.17e-04\n",
      "Step 1175 | Loss: 0.02095 | LR: 7.36e-04\n",
      "Step 1200 | Loss: 0.01685 | LR: 7.54e-04\n",
      "test loss: 0.0178\n",
      "Step 1225 | Loss: 0.01653 | LR: 7.72e-04\n",
      "Step 1250 | Loss: 0.01579 | LR: 7.90e-04\n",
      "Step 1275 | Loss: 0.01782 | LR: 8.07e-04\n",
      "Step 1300 | Loss: 0.01601 | LR: 8.23e-04\n",
      "test loss: 0.0160\n",
      "Step 1325 | Loss: 0.01837 | LR: 8.39e-04\n",
      "Step 1350 | Loss: 0.01457 | LR: 8.54e-04\n",
      "Step 1375 | Loss: 0.01397 | LR: 8.69e-04\n",
      "Step 1400 | Loss: 0.01625 | LR: 8.83e-04\n",
      "test loss: 0.0144\n",
      "Step 1425 | Loss: 0.01469 | LR: 8.96e-04\n",
      "Step 1450 | Loss: 0.01743 | LR: 9.09e-04\n",
      "Step 1475 | Loss: 0.01302 | LR: 9.21e-04\n",
      "Step 1500 | Loss: 0.01833 | LR: 9.32e-04\n",
      "test loss: 0.0160\n",
      "Step 1525 | Loss: 0.01502 | LR: 9.42e-04\n",
      "Step 1550 | Loss: 0.01673 | LR: 9.51e-04\n",
      "Step 1575 | Loss: 0.01808 | LR: 9.60e-04\n",
      "Step 1600 | Loss: 0.01632 | LR: 9.68e-04\n",
      "test loss: 0.0159\n",
      "Step 1625 | Loss: 0.01599 | LR: 9.75e-04\n",
      "Step 1650 | Loss: 0.02216 | LR: 9.81e-04\n",
      "Step 1675 | Loss: 0.01888 | LR: 9.87e-04\n",
      "Step 1700 | Loss: 0.01450 | LR: 9.91e-04\n",
      "test loss: 0.0173\n",
      "Step 1725 | Loss: 0.01852 | LR: 9.95e-04\n",
      "Step 1750 | Loss: 0.01703 | LR: 9.97e-04\n",
      "Step 1775 | Loss: 0.01892 | LR: 9.99e-04\n",
      "Step 1800 | Loss: 0.01754 | LR: 1.00e-03\n",
      "test loss: 0.0172\n",
      "Step 1825 | Loss: 0.01903 | LR: 1.00e-03\n",
      "Step 1850 | Loss: 0.02096 | LR: 1.00e-03\n",
      "Step 1875 | Loss: 0.01777 | LR: 1.00e-03\n",
      "Step 1900 | Loss: 0.02253 | LR: 1.00e-03\n",
      "test loss: 0.0186\n",
      "Step 1925 | Loss: 0.02158 | LR: 1.00e-03\n",
      "Step 1950 | Loss: 0.01645 | LR: 1.00e-03\n",
      "Step 1975 | Loss: 0.01881 | LR: 1.00e-03\n",
      "Step 2000 | Loss: 0.01916 | LR: 1.00e-03\n",
      "test loss: 0.0179\n",
      "Step 2025 | Loss: 0.01589 | LR: 1.00e-03\n",
      "Step 2050 | Loss: 0.02021 | LR: 1.00e-03\n",
      "Step 2075 | Loss: 0.02066 | LR: 1.00e-03\n",
      "Step 2100 | Loss: 0.01828 | LR: 1.00e-03\n",
      "test loss: 0.0198\n",
      "Step 2125 | Loss: 0.01602 | LR: 1.00e-03\n",
      "Step 2150 | Loss: 0.01855 | LR: 1.00e-03\n",
      "Step 2175 | Loss: 0.02587 | LR: 1.00e-03\n",
      "Step 2200 | Loss: 0.01914 | LR: 1.00e-03\n",
      "test loss: 0.0202\n",
      "Step 2225 | Loss: 0.01736 | LR: 1.00e-03\n",
      "Step 2250 | Loss: 0.02599 | LR: 1.00e-03\n",
      "Step 2275 | Loss: 0.02294 | LR: 1.00e-03\n",
      "Step 2300 | Loss: 0.03010 | LR: 1.00e-03\n",
      "test loss: 0.0192\n",
      "Step 2325 | Loss: 0.01807 | LR: 9.99e-04\n",
      "Step 2350 | Loss: 0.02894 | LR: 9.99e-04\n",
      "Step 2375 | Loss: 0.02526 | LR: 9.99e-04\n",
      "Step 2400 | Loss: 0.02406 | LR: 9.99e-04\n",
      "test loss: 0.0224\n",
      "Step 2425 | Loss: 0.02263 | LR: 9.99e-04\n",
      "Step 2450 | Loss: 0.02890 | LR: 9.99e-04\n",
      "Step 2475 | Loss: 0.01959 | LR: 9.99e-04\n",
      "Step 2500 | Loss: 0.02972 | LR: 9.99e-04\n",
      "test loss: 0.0277\n",
      "Step 2525 | Loss: 0.02468 | LR: 9.99e-04\n",
      "Step 2550 | Loss: 0.02490 | LR: 9.99e-04\n",
      "Step 2575 | Loss: 0.01987 | LR: 9.99e-04\n",
      "Step 2600 | Loss: 0.02256 | LR: 9.99e-04\n",
      "test loss: 0.0263\n",
      "Step 2625 | Loss: 0.02950 | LR: 9.99e-04\n",
      "Step 2650 | Loss: 0.02971 | LR: 9.99e-04\n",
      "Step 2675 | Loss: 0.03062 | LR: 9.98e-04\n",
      "Step 2700 | Loss: 0.02572 | LR: 9.98e-04\n",
      "test loss: 0.0317\n",
      "Step 2725 | Loss: 0.03487 | LR: 9.98e-04\n",
      "Step 2750 | Loss: 0.03278 | LR: 9.98e-04\n",
      "Step 2775 | Loss: 0.04671 | LR: 9.98e-04\n",
      "Step 2800 | Loss: 0.02680 | LR: 9.98e-04\n",
      "test loss: 0.0324\n",
      "Step 2825 | Loss: 0.03469 | LR: 9.98e-04\n",
      "Step 2850 | Loss: 0.03633 | LR: 9.98e-04\n",
      "Step 2875 | Loss: 0.02577 | LR: 9.98e-04\n",
      "Step 2900 | Loss: 0.04070 | LR: 9.98e-04\n",
      "test loss: 0.0369\n",
      "Step 2925 | Loss: 0.03815 | LR: 9.97e-04\n",
      "Step 2950 | Loss: 0.03439 | LR: 9.97e-04\n",
      "Step 2975 | Loss: 0.05698 | LR: 9.97e-04\n",
      "Step 3000 | Loss: 0.03612 | LR: 9.97e-04\n",
      "test loss: 0.0395\n",
      "Step 3025 | Loss: 0.04076 | LR: 9.97e-04\n",
      "Step 3050 | Loss: 0.04557 | LR: 9.97e-04\n",
      "Step 3075 | Loss: 0.03409 | LR: 9.97e-04\n",
      "Step 3100 | Loss: 0.04172 | LR: 9.97e-04\n",
      "test loss: 0.0465\n",
      "Step 3125 | Loss: 0.05651 | LR: 9.96e-04\n",
      "Step 3150 | Loss: 0.04295 | LR: 9.96e-04\n",
      "Step 3175 | Loss: 0.04565 | LR: 9.96e-04\n",
      "Step 3200 | Loss: 0.04084 | LR: 9.96e-04\n",
      "test loss: 0.0537\n",
      "Step 3225 | Loss: 0.05998 | LR: 9.96e-04\n",
      "Step 3250 | Loss: 0.04951 | LR: 9.96e-04\n",
      "Step 3275 | Loss: 0.04436 | LR: 9.96e-04\n",
      "Step 3300 | Loss: 0.06733 | LR: 9.95e-04\n",
      "test loss: 0.0567\n",
      "Step 3325 | Loss: 0.04107 | LR: 9.95e-04\n",
      "Step 3350 | Loss: 0.05089 | LR: 9.95e-04\n",
      "Step 3375 | Loss: 0.04142 | LR: 9.95e-04\n",
      "Step 3400 | Loss: 0.04442 | LR: 9.95e-04\n",
      "test loss: 0.0541\n",
      "Step 3425 | Loss: 0.05042 | LR: 9.95e-04\n",
      "Step 3450 | Loss: 0.05332 | LR: 9.94e-04\n",
      "Step 3475 | Loss: 0.04810 | LR: 9.94e-04\n",
      "Step 3500 | Loss: 0.06334 | LR: 9.94e-04\n",
      "test loss: 0.0699\n",
      "Step 3525 | Loss: 0.05956 | LR: 9.94e-04\n",
      "Step 3550 | Loss: 0.06507 | LR: 9.94e-04\n",
      "Step 3575 | Loss: 0.07566 | LR: 9.94e-04\n",
      "Step 3600 | Loss: 0.11174 | LR: 9.93e-04\n",
      "test loss: 0.0908\n",
      "Step 3625 | Loss: 0.12531 | LR: 9.93e-04\n",
      "Step 3650 | Loss: 0.10503 | LR: 9.93e-04\n",
      "Step 3675 | Loss: 0.08330 | LR: 9.93e-04\n",
      "Step 3700 | Loss: 0.07319 | LR: 9.93e-04\n",
      "test loss: 0.0944\n",
      "Step 3725 | Loss: 0.08558 | LR: 9.92e-04\n",
      "Step 3750 | Loss: 0.07058 | LR: 9.92e-04\n",
      "Step 3775 | Loss: 0.08211 | LR: 9.92e-04\n",
      "Step 3800 | Loss: 0.10265 | LR: 9.92e-04\n",
      "test loss: 0.0939\n",
      "Step 3825 | Loss: 0.07998 | LR: 9.92e-04\n",
      "Step 3850 | Loss: 0.06373 | LR: 9.91e-04\n",
      "Step 3875 | Loss: 0.11377 | LR: 9.91e-04\n",
      "Step 3900 | Loss: 0.09358 | LR: 9.91e-04\n",
      "test loss: 0.0828\n",
      "Step 3925 | Loss: 0.09356 | LR: 9.91e-04\n",
      "Step 3950 | Loss: 0.11809 | LR: 9.91e-04\n",
      "Step 3975 | Loss: 0.14009 | LR: 9.90e-04\n",
      "Step 4000 | Loss: 0.20450 | LR: 9.90e-04\n",
      "test loss: 0.1256\n",
      "Step 4025 | Loss: 0.13530 | LR: 9.90e-04\n",
      "Step 4050 | Loss: 0.12689 | LR: 9.90e-04\n",
      "Step 4075 | Loss: 0.15573 | LR: 9.89e-04\n",
      "Step 4100 | Loss: 0.15960 | LR: 9.89e-04\n",
      "test loss: 0.1042\n",
      "Step 4125 | Loss: 0.15580 | LR: 9.89e-04\n",
      "Step 4150 | Loss: 0.11222 | LR: 9.89e-04\n",
      "Step 4175 | Loss: 0.07863 | LR: 9.88e-04\n",
      "Step 4200 | Loss: 0.16838 | LR: 9.88e-04\n",
      "test loss: 0.1063\n",
      "Step 4225 | Loss: 0.09685 | LR: 9.88e-04\n",
      "Step 4250 | Loss: 0.20432 | LR: 9.88e-04\n",
      "Step 4275 | Loss: 0.14956 | LR: 9.87e-04\n",
      "Step 4300 | Loss: 0.23765 | LR: 9.87e-04\n",
      "test loss: 0.1782\n",
      "Step 4325 | Loss: 0.16558 | LR: 9.87e-04\n",
      "Step 4350 | Loss: 0.18631 | LR: 9.87e-04\n",
      "Step 4375 | Loss: 0.16099 | LR: 9.86e-04\n",
      "Step 4400 | Loss: 0.20754 | LR: 9.86e-04\n",
      "test loss: 0.1901\n",
      "Step 4425 | Loss: 0.26117 | LR: 9.86e-04\n",
      "Step 4450 | Loss: 0.13121 | LR: 9.86e-04\n",
      "Step 4475 | Loss: 0.19268 | LR: 9.85e-04\n",
      "Step 4500 | Loss: 0.09595 | LR: 9.85e-04\n",
      "test loss: 0.1202\n",
      "Step 4525 | Loss: 0.12204 | LR: 9.85e-04\n",
      "Step 4550 | Loss: 0.07528 | LR: 9.84e-04\n",
      "Step 4575 | Loss: 0.09212 | LR: 9.84e-04\n",
      "Step 4600 | Loss: 0.11829 | LR: 9.84e-04\n",
      "test loss: 0.0906\n",
      "Step 4625 | Loss: 0.06798 | LR: 9.84e-04\n",
      "Step 4650 | Loss: 0.09042 | LR: 9.83e-04\n",
      "Step 4675 | Loss: 0.10443 | LR: 9.83e-04\n",
      "Step 4700 | Loss: 0.15001 | LR: 9.83e-04\n",
      "test loss: 0.0927\n",
      "Step 4725 | Loss: 0.05928 | LR: 9.82e-04\n",
      "Step 4750 | Loss: 0.06164 | LR: 9.82e-04\n",
      "Step 4775 | Loss: 0.09046 | LR: 9.82e-04\n",
      "Step 4800 | Loss: 0.07033 | LR: 9.82e-04\n",
      "test loss: 0.0749\n",
      "Step 4825 | Loss: 0.07313 | LR: 9.81e-04\n",
      "Step 4850 | Loss: 0.08332 | LR: 9.81e-04\n",
      "Step 4875 | Loss: 0.11985 | LR: 9.81e-04\n",
      "Step 4900 | Loss: 0.07207 | LR: 9.80e-04\n",
      "test loss: 0.0840\n",
      "Step 4925 | Loss: 0.07440 | LR: 9.80e-04\n",
      "Step 4950 | Loss: 0.08110 | LR: 9.80e-04\n",
      "Step 4975 | Loss: 0.08835 | LR: 9.79e-04\n",
      "Step 5000 | Loss: 0.09336 | LR: 9.79e-04\n",
      "test loss: 0.0959\n",
      "Step 5025 | Loss: 0.08036 | LR: 9.79e-04\n",
      "Step 5050 | Loss: 0.09400 | LR: 9.78e-04\n",
      "Step 5075 | Loss: 0.11815 | LR: 9.78e-04\n",
      "Step 5100 | Loss: 0.13680 | LR: 9.78e-04\n",
      "test loss: 0.1161\n",
      "Step 5125 | Loss: 0.10887 | LR: 9.77e-04\n",
      "Step 5150 | Loss: 0.10126 | LR: 9.77e-04\n",
      "Step 5175 | Loss: 0.12020 | LR: 9.77e-04\n",
      "Step 5200 | Loss: 0.08137 | LR: 9.76e-04\n",
      "test loss: 0.1159\n",
      "Step 5225 | Loss: 0.09044 | LR: 9.76e-04\n",
      "Step 5250 | Loss: 0.12295 | LR: 9.76e-04\n",
      "Step 5275 | Loss: 0.10307 | LR: 9.75e-04\n",
      "Step 5300 | Loss: 0.12344 | LR: 9.75e-04\n",
      "test loss: 0.1099\n",
      "Step 5325 | Loss: 0.12018 | LR: 9.75e-04\n",
      "Step 5350 | Loss: 0.16132 | LR: 9.74e-04\n",
      "Step 5375 | Loss: 0.11941 | LR: 9.74e-04\n",
      "Step 5400 | Loss: 0.11927 | LR: 9.73e-04\n",
      "test loss: 0.1158\n",
      "Step 5425 | Loss: 0.14058 | LR: 9.73e-04\n",
      "Step 5450 | Loss: 0.13004 | LR: 9.73e-04\n",
      "Step 5475 | Loss: 0.10537 | LR: 9.72e-04\n",
      "Step 5500 | Loss: 0.16089 | LR: 9.72e-04\n",
      "test loss: 0.1253\n",
      "Step 5525 | Loss: 0.10835 | LR: 9.72e-04\n",
      "Step 5550 | Loss: 0.12375 | LR: 9.71e-04\n",
      "Step 5575 | Loss: 0.15372 | LR: 9.71e-04\n",
      "Step 5600 | Loss: 0.12614 | LR: 9.70e-04\n",
      "test loss: 0.1306\n",
      "Step 5625 | Loss: 0.12239 | LR: 9.70e-04\n",
      "Step 5650 | Loss: 0.11922 | LR: 9.70e-04\n",
      "Step 5675 | Loss: 0.10665 | LR: 9.69e-04\n",
      "Step 5700 | Loss: 0.15858 | LR: 9.69e-04\n",
      "test loss: 0.1355\n",
      "Step 5725 | Loss: 0.16193 | LR: 9.68e-04\n",
      "Step 5750 | Loss: 0.16001 | LR: 9.68e-04\n",
      "Step 5775 | Loss: 0.10081 | LR: 9.68e-04\n",
      "Step 5800 | Loss: 0.18889 | LR: 9.67e-04\n",
      "test loss: 0.1499\n",
      "Step 5825 | Loss: 0.17263 | LR: 9.67e-04\n",
      "Step 5850 | Loss: 0.17815 | LR: 9.66e-04\n",
      "Step 5875 | Loss: 0.24982 | LR: 9.66e-04\n",
      "Step 5900 | Loss: 0.24092 | LR: 9.66e-04\n",
      "test loss: 0.1904\n",
      "Step 5925 | Loss: 0.23434 | LR: 9.65e-04\n",
      "Step 5950 | Loss: 0.19184 | LR: 9.65e-04\n",
      "Step 5975 | Loss: 0.18728 | LR: 9.64e-04\n",
      "Step 6000 | Loss: 0.09942 | LR: 9.64e-04\n",
      "test loss: 0.2061\n",
      "Step 6025 | Loss: 0.30521 | LR: 9.64e-04\n",
      "Step 6050 | Loss: 0.24262 | LR: 9.63e-04\n",
      "Step 6075 | Loss: 0.29135 | LR: 9.63e-04\n",
      "Step 6100 | Loss: 0.16223 | LR: 9.62e-04\n",
      "test loss: 0.2030\n",
      "Step 6125 | Loss: 0.25336 | LR: 9.62e-04\n",
      "Step 6150 | Loss: 0.21393 | LR: 9.61e-04\n",
      "Step 6175 | Loss: 0.22106 | LR: 9.61e-04\n",
      "Step 6200 | Loss: 0.20658 | LR: 9.60e-04\n",
      "test loss: 0.2126\n",
      "Step 6225 | Loss: 0.16354 | LR: 9.60e-04\n",
      "Step 6250 | Loss: 0.18345 | LR: 9.60e-04\n",
      "Step 6275 | Loss: 0.20301 | LR: 9.59e-04\n",
      "Step 6300 | Loss: 0.27522 | LR: 9.59e-04\n",
      "test loss: 0.2369\n",
      "Step 6325 | Loss: 0.26148 | LR: 9.58e-04\n",
      "Step 6350 | Loss: 0.22367 | LR: 9.58e-04\n",
      "Step 6375 | Loss: 0.16731 | LR: 9.57e-04\n",
      "Step 6400 | Loss: 0.20411 | LR: 9.57e-04\n",
      "test loss: 0.1762\n",
      "Step 6425 | Loss: 0.20721 | LR: 9.56e-04\n",
      "Step 6450 | Loss: 0.13693 | LR: 9.56e-04\n",
      "Step 6475 | Loss: 0.19759 | LR: 9.55e-04\n",
      "Step 6500 | Loss: 0.11266 | LR: 9.55e-04\n",
      "test loss: 0.1427\n",
      "Step 6525 | Loss: 0.18187 | LR: 9.54e-04\n",
      "Step 6550 | Loss: 0.12495 | LR: 9.54e-04\n",
      "Step 6575 | Loss: 0.06866 | LR: 9.54e-04\n",
      "Step 6600 | Loss: 0.11216 | LR: 9.53e-04\n",
      "test loss: 0.1068\n",
      "Step 6625 | Loss: 0.09076 | LR: 9.53e-04\n",
      "Step 6650 | Loss: 0.09377 | LR: 9.52e-04\n",
      "Step 6675 | Loss: 0.09880 | LR: 9.52e-04\n",
      "Step 6700 | Loss: 0.10975 | LR: 9.51e-04\n",
      "test loss: 0.1064\n",
      "Step 6725 | Loss: 0.10632 | LR: 9.51e-04\n",
      "Step 6750 | Loss: 0.10746 | LR: 9.50e-04\n",
      "Step 6775 | Loss: 0.07693 | LR: 9.50e-04\n",
      "Step 6800 | Loss: 0.09427 | LR: 9.49e-04\n",
      "test loss: 0.1050\n",
      "Step 6825 | Loss: 0.13737 | LR: 9.49e-04\n",
      "Step 6850 | Loss: 0.11790 | LR: 9.48e-04\n",
      "Step 6875 | Loss: 0.15471 | LR: 9.48e-04\n",
      "Step 6900 | Loss: 0.12458 | LR: 9.47e-04\n",
      "test loss: 0.1086\n",
      "Step 6925 | Loss: 0.15011 | LR: 9.47e-04\n",
      "Step 6950 | Loss: 0.12379 | LR: 9.46e-04\n",
      "Step 6975 | Loss: 0.16586 | LR: 9.46e-04\n",
      "Step 7000 | Loss: 0.13744 | LR: 9.45e-04\n",
      "test loss: 0.1069\n",
      "Step 7025 | Loss: 0.09684 | LR: 9.45e-04\n",
      "Step 7050 | Loss: 0.13091 | LR: 9.44e-04\n",
      "Step 7075 | Loss: 0.12480 | LR: 9.43e-04\n",
      "Step 7100 | Loss: 0.09221 | LR: 9.43e-04\n",
      "test loss: 0.1432\n",
      "Step 7125 | Loss: 0.12671 | LR: 9.42e-04\n",
      "Step 7150 | Loss: 0.11856 | LR: 9.42e-04\n",
      "Step 7175 | Loss: 0.16304 | LR: 9.41e-04\n",
      "Step 7200 | Loss: 0.17579 | LR: 9.41e-04\n",
      "test loss: 0.1262\n",
      "Step 7225 | Loss: 0.11938 | LR: 9.40e-04\n",
      "Step 7250 | Loss: 0.15957 | LR: 9.40e-04\n",
      "Step 7275 | Loss: 0.15316 | LR: 9.39e-04\n",
      "Step 7300 | Loss: 0.12242 | LR: 9.39e-04\n",
      "test loss: 0.1330\n",
      "Step 7325 | Loss: 0.20098 | LR: 9.38e-04\n",
      "Step 7350 | Loss: 0.12382 | LR: 9.38e-04\n",
      "Step 7375 | Loss: 0.21007 | LR: 9.37e-04\n",
      "Step 7400 | Loss: 0.11515 | LR: 9.36e-04\n",
      "test loss: 0.1474\n",
      "Step 7425 | Loss: 0.25362 | LR: 9.36e-04\n",
      "Step 7450 | Loss: 0.23996 | LR: 9.35e-04\n",
      "Step 7475 | Loss: 0.16451 | LR: 9.35e-04\n",
      "Step 7500 | Loss: 0.17156 | LR: 9.34e-04\n",
      "test loss: 0.1607\n",
      "Step 7525 | Loss: 0.22566 | LR: 9.34e-04\n",
      "Step 7550 | Loss: 0.16617 | LR: 9.33e-04\n",
      "Step 7575 | Loss: 0.23710 | LR: 9.32e-04\n",
      "Step 7600 | Loss: 0.15743 | LR: 9.32e-04\n",
      "test loss: 0.1594\n",
      "Step 7625 | Loss: 0.20700 | LR: 9.31e-04\n",
      "Step 7650 | Loss: 0.16988 | LR: 9.31e-04\n",
      "Step 7675 | Loss: 0.16460 | LR: 9.30e-04\n",
      "Step 7700 | Loss: 0.30977 | LR: 9.30e-04\n",
      "test loss: 0.1738\n",
      "Step 7725 | Loss: 0.18892 | LR: 9.29e-04\n",
      "Step 7750 | Loss: 0.20236 | LR: 9.28e-04\n",
      "Step 7775 | Loss: 0.27636 | LR: 9.28e-04\n",
      "Step 7800 | Loss: 0.15664 | LR: 9.27e-04\n",
      "test loss: 0.2014\n",
      "Step 7825 | Loss: 0.20489 | LR: 9.27e-04\n",
      "Step 7850 | Loss: 0.20987 | LR: 9.26e-04\n",
      "Step 7875 | Loss: 0.25355 | LR: 9.25e-04\n",
      "Step 7900 | Loss: 0.29835 | LR: 9.25e-04\n",
      "test loss: 0.2020\n",
      "Step 7925 | Loss: 0.20516 | LR: 9.24e-04\n",
      "Step 7950 | Loss: 0.26244 | LR: 9.24e-04\n",
      "Step 7975 | Loss: 0.19760 | LR: 9.23e-04\n",
      "Step 8000 | Loss: 0.21511 | LR: 9.22e-04\n",
      "test loss: 0.2482\n",
      "Step 8025 | Loss: 0.23809 | LR: 9.22e-04\n",
      "Step 8050 | Loss: 0.28554 | LR: 9.21e-04\n",
      "Step 8075 | Loss: 0.13992 | LR: 9.21e-04\n",
      "Step 8100 | Loss: 0.31230 | LR: 9.20e-04\n",
      "test loss: 0.1992\n",
      "Step 8125 | Loss: 0.21898 | LR: 9.19e-04\n",
      "Step 8150 | Loss: 0.25917 | LR: 9.19e-04\n",
      "Step 8175 | Loss: 0.25117 | LR: 9.18e-04\n",
      "Step 8200 | Loss: 0.23469 | LR: 9.17e-04\n",
      "test loss: 0.2283\n",
      "Step 8225 | Loss: 0.18479 | LR: 9.17e-04\n",
      "Step 8250 | Loss: 0.24589 | LR: 9.16e-04\n",
      "Step 8275 | Loss: 0.27804 | LR: 9.16e-04\n",
      "Step 8300 | Loss: 0.18692 | LR: 9.15e-04\n",
      "test loss: 0.2143\n",
      "Step 8325 | Loss: 0.26661 | LR: 9.14e-04\n",
      "Step 8350 | Loss: 0.16517 | LR: 9.14e-04\n",
      "Step 8375 | Loss: 0.26245 | LR: 9.13e-04\n",
      "Step 8400 | Loss: 0.19109 | LR: 9.12e-04\n",
      "test loss: 0.2047\n",
      "Step 8425 | Loss: 0.18326 | LR: 9.12e-04\n",
      "Step 8450 | Loss: 0.29731 | LR: 9.11e-04\n",
      "Step 8475 | Loss: 0.25394 | LR: 9.10e-04\n",
      "Step 8500 | Loss: 0.26227 | LR: 9.10e-04\n",
      "test loss: 0.2056\n",
      "Step 8525 | Loss: 0.25150 | LR: 9.09e-04\n",
      "Step 8550 | Loss: 0.16239 | LR: 9.08e-04\n",
      "Step 8575 | Loss: 0.18826 | LR: 9.08e-04\n",
      "Step 8600 | Loss: 0.20279 | LR: 9.07e-04\n",
      "test loss: 0.1788\n",
      "Step 8625 | Loss: 0.22921 | LR: 9.06e-04\n",
      "Step 8650 | Loss: 0.28591 | LR: 9.06e-04\n",
      "Step 8675 | Loss: 0.26092 | LR: 9.05e-04\n",
      "Step 8700 | Loss: 0.19408 | LR: 9.04e-04\n",
      "test loss: 0.2070\n",
      "Step 8725 | Loss: 0.26647 | LR: 9.04e-04\n",
      "Step 8750 | Loss: 0.13975 | LR: 9.03e-04\n",
      "Step 8775 | Loss: 0.12701 | LR: 9.02e-04\n",
      "Step 8800 | Loss: 0.20669 | LR: 9.02e-04\n",
      "test loss: 0.1953\n",
      "Step 8825 | Loss: 0.25201 | LR: 9.01e-04\n",
      "Step 8850 | Loss: 0.24779 | LR: 9.00e-04\n",
      "Step 8875 | Loss: 0.22557 | LR: 9.00e-04\n",
      "Step 8900 | Loss: 0.27787 | LR: 8.99e-04\n",
      "test loss: 0.1991\n",
      "Step 8925 | Loss: 0.22091 | LR: 8.98e-04\n",
      "Step 8950 | Loss: 0.26299 | LR: 8.98e-04\n",
      "Step 8975 | Loss: 0.19195 | LR: 8.97e-04\n",
      "Step 9000 | Loss: 0.17967 | LR: 8.96e-04\n",
      "test loss: 0.2027\n",
      "Step 9025 | Loss: 0.26044 | LR: 8.96e-04\n",
      "Step 9050 | Loss: 0.17574 | LR: 8.95e-04\n",
      "Step 9075 | Loss: 0.23521 | LR: 8.94e-04\n",
      "Step 9100 | Loss: 0.13698 | LR: 8.93e-04\n",
      "test loss: 0.1878\n",
      "Step 9125 | Loss: 0.22936 | LR: 8.93e-04\n",
      "Step 9150 | Loss: 0.30232 | LR: 8.92e-04\n",
      "Step 9175 | Loss: 0.17548 | LR: 8.91e-04\n",
      "Step 9200 | Loss: 0.19982 | LR: 8.91e-04\n",
      "test loss: 0.1661\n",
      "Step 9225 | Loss: 0.22746 | LR: 8.90e-04\n",
      "Step 9250 | Loss: 0.19262 | LR: 8.89e-04\n",
      "Step 9275 | Loss: 0.20745 | LR: 8.89e-04\n",
      "Step 9300 | Loss: 0.16671 | LR: 8.88e-04\n",
      "test loss: 0.1769\n",
      "Step 9325 | Loss: 0.19508 | LR: 8.87e-04\n",
      "Step 9350 | Loss: 0.18886 | LR: 8.86e-04\n",
      "Step 9375 | Loss: 0.16750 | LR: 8.86e-04\n",
      "Step 9400 | Loss: 0.16562 | LR: 8.85e-04\n",
      "test loss: 0.1945\n",
      "Step 9425 | Loss: 0.24827 | LR: 8.84e-04\n",
      "Step 9450 | Loss: 0.17802 | LR: 8.83e-04\n",
      "Step 9475 | Loss: 0.19098 | LR: 8.83e-04\n",
      "Step 9500 | Loss: 0.13447 | LR: 8.82e-04\n",
      "test loss: 0.1623\n",
      "Step 9525 | Loss: 0.13876 | LR: 8.81e-04\n",
      "Step 9550 | Loss: 0.17151 | LR: 8.80e-04\n",
      "Step 9575 | Loss: 0.12245 | LR: 8.80e-04\n",
      "Step 9600 | Loss: 0.18274 | LR: 8.79e-04\n",
      "test loss: 0.1504\n",
      "Step 9625 | Loss: 0.16241 | LR: 8.78e-04\n",
      "Step 9650 | Loss: 0.19257 | LR: 8.78e-04\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     31\u001b[39m model.train()\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# We manually limit steps per epoch to avoid iterating infinite stochastic loader\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# or just iterate a fixed number of batches\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m xc, xt, aid = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcycle_train_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m xc, xt, aid = xc.to(DEVICE), xt.to(DEVICE), aid.to(DEVICE)\n\u001b[32m     40\u001b[39m optimizer.zero_grad()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/general/lib/python3.13/site-packages/torch/utils/data/dataloader.py:734\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    732\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    733\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    737\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    739\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    740\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/general/lib/python3.13/site-packages/torch/utils/data/dataloader.py:790\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    789\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    791\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    792\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/general/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mPairedShardDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     19\u001b[39m n_rows = data[\u001b[33m'\u001b[39m\u001b[33maction_ids\u001b[39m\u001b[33m'\u001b[39m].shape[\u001b[32m0\u001b[39m]\n\u001b[32m     20\u001b[39m row_idx = np.random.randint(n_rows)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m control_raw = \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcontrol\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m[row_idx]\n\u001b[32m     23\u001b[39m case_raw = data[\u001b[33m'\u001b[39m\u001b[33mcase\u001b[39m\u001b[33m'\u001b[39m][row_idx]\n\u001b[32m     24\u001b[39m act_id = data[\u001b[33m'\u001b[39m\u001b[33maction_ids\u001b[39m\u001b[33m'\u001b[39m][row_idx]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/general/lib/python3.13/site-packages/numpy/lib/_npyio_impl.py:257\u001b[39m, in \u001b[36mNpzFile.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;28mbytes\u001b[39m.seek(\u001b[32m0\u001b[39m)\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m magic == \u001b[38;5;28mformat\u001b[39m.MAGIC_PREFIX:\n\u001b[32m    249\u001b[39m     \u001b[38;5;66;03m# FIXME: This seems like it will copy strings around\u001b[39;00m\n\u001b[32m    250\u001b[39m     \u001b[38;5;66;03m#   more than is strictly necessary.  The zipfile\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    255\u001b[39m     \u001b[38;5;66;03m#   (or at least uncompress) the data\u001b[39;00m\n\u001b[32m    256\u001b[39m     \u001b[38;5;66;03m#   directly into the array memory.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_header_size\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    264\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbytes\u001b[39m.read()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/general/lib/python3.13/site-packages/numpy/lib/_format_impl.py:869\u001b[39m, in \u001b[36mread_array\u001b[39m\u001b[34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[39m\n\u001b[32m    867\u001b[39m             read_count = \u001b[38;5;28mmin\u001b[39m(max_read_count, count - i)\n\u001b[32m    868\u001b[39m             read_size = \u001b[38;5;28mint\u001b[39m(read_count * dtype.itemsize)\n\u001b[32m--> \u001b[39m\u001b[32m869\u001b[39m             data = \u001b[43m_read_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43marray data\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    870\u001b[39m             array[i:i + read_count] = numpy.frombuffer(data, dtype=dtype,\n\u001b[32m    871\u001b[39m                                                      count=read_count)\n\u001b[32m    873\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m array.size != count:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/general/lib/python3.13/site-packages/numpy/lib/_format_impl.py:1013\u001b[39m, in \u001b[36m_read_bytes\u001b[39m\u001b[34m(fp, size, error_template)\u001b[39m\n\u001b[32m   1008\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m   1009\u001b[39m     \u001b[38;5;66;03m# io files (default in python3) return None or raise on\u001b[39;00m\n\u001b[32m   1010\u001b[39m     \u001b[38;5;66;03m# would-block, python2 file will truncate, probably nothing can be\u001b[39;00m\n\u001b[32m   1011\u001b[39m     \u001b[38;5;66;03m# done about that.  note that regular files can't be non-blocking\u001b[39;00m\n\u001b[32m   1012\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1013\u001b[39m         r = \u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1014\u001b[39m         data += r\n\u001b[32m   1015\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(r) == \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) == size:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/general/lib/python3.13/zipfile/__init__.py:1015\u001b[39m, in \u001b[36mZipExtFile.read\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m   1013\u001b[39m \u001b[38;5;28mself\u001b[39m._offset = \u001b[32m0\u001b[39m\n\u001b[32m   1014\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m n > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._eof:\n\u001b[32m-> \u001b[39m\u001b[32m1015\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n < \u001b[38;5;28mlen\u001b[39m(data):\n\u001b[32m   1017\u001b[39m         \u001b[38;5;28mself\u001b[39m._readbuffer = data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/general/lib/python3.13/zipfile/__init__.py:1091\u001b[39m, in \u001b[36mZipExtFile._read1\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m   1089\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compress_type == ZIP_DEFLATED:\n\u001b[32m   1090\u001b[39m     n = \u001b[38;5;28mmax\u001b[39m(n, \u001b[38;5;28mself\u001b[39m.MIN_READ_SIZE)\n\u001b[32m-> \u001b[39m\u001b[32m1091\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_decompressor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecompress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1092\u001b[39m     \u001b[38;5;28mself\u001b[39m._eof = (\u001b[38;5;28mself\u001b[39m._decompressor.eof \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m   1093\u001b[39m                  \u001b[38;5;28mself\u001b[39m._compress_left <= \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m   1094\u001b[39m                  \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._decompressor.unconsumed_tail)\n\u001b[32m   1095\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._eof:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for step in range(max_steps):\n",
    "    last_step = (step == max_steps - 1)\n",
    "\n",
    "    # once in a while evaluate our validation set loss\n",
    "    if step % 100 == 0 or last_step:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_loss_accum = 0.0\n",
    "            test_loss_steps = 10\n",
    "            for i in range(test_loss_steps):\n",
    "                xc, xt, aid = next(cycle_val_loader)\n",
    "                xc, xt, aid = xc.to(DEVICE), xt.to(DEVICE), aid.to(DEVICE)\n",
    "                loss = model(xc, xt, aid)\n",
    "                loss = loss / test_loss_steps\n",
    "                test_loss_accum += loss.detach()\n",
    "\n",
    "        print(f'test loss: {test_loss_accum.item():.4f}')\n",
    "        # with open(log_file, \"a\") as f:\n",
    "        #    f.write(f'{step} test {test_loss_accum.item():.4f}\\n')\n",
    "\n",
    "\n",
    "    if step > 0 and (step % 1000 == 0 or step % steps_per_epoch ==0) and not last_step:\n",
    "        # Save Checkpoint\n",
    "        torch.save({\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'step': step\n",
    "        }, checkpoint_dir / f'bio_jepa_ckpt_{step}.pt')\n",
    "\n",
    "\n",
    "    model.train()\n",
    "    xc, xt, aid = next(cycle_train_loader)\n",
    "    xc, xt, aid = xc.to(DEVICE), xt.to(DEVICE), aid.to(DEVICE)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss = model(xc, xt, aid)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Update Teacher (V-JEPA Momentum)\n",
    "    model.update_teacher(m=0.996)\n",
    "    \n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    lossi.append(loss.item())\n",
    "    total_epoch_loss += loss.item()\n",
    "    step += 1\n",
    "    \n",
    "    if step % 25 == 0:\n",
    "        print(f\"Step {step} | Loss: {loss.item():.5f} | LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "    \n",
    "    \n",
    "    if step % steps_per_epoch == 0:   \n",
    "        avg_loss = total_epoch_loss / steps_per_epoch\n",
    "        print(f\"=== Step {step} Done. Avg Loss: {avg_loss:.5f} ===\")\n",
    "        total_epoch_loss = 0\n",
    "    \n",
    "    \n",
    "    if last_step:\n",
    "        # Save Checkpoint\n",
    "        torch.save({\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'step': step\n",
    "        })\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4972cdbf-7fd0-4b3b-a541-16135d079c2c",
   "metadata": {},
   "source": [
    "#### Training Loss Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "43c293fa-2067-45b9-ad55-e6127515bb37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x168edb610>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lossi)\n",
    "#plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861fe89a-0447-48d3-9252-9153efcab902",
   "metadata": {},
   "source": [
    "## Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45916c9b-5f07-4aec-a9ac-e0635bae92a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('/Users/djemec/data/jepa/perturbation_map.json'),\n",
       " PosixPath('/Users/djemec/data/jepa/tokenized/val'))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_path, val_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "54c35355-2ff6-486d-9ce8-e4cbbbee1599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load Map (ID -> Name)\n",
    "with open(metadata_path, \"r\") as f:\n",
    "    pert_map = json.load(f)\n",
    "# Invert map to: ID -> Name\n",
    "id_to_name = {v: k for k, v in pert_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "24ae854c-4e74-42f5-9cfd-0c487ff22549",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BioJepa(\n",
       "  (student): PathwayEncoder(\n",
       "    (transformer): ModuleDict(\n",
       "      (input_proj): Linear(in_features=1, out_features=8, bias=True)\n",
       "      (rope): RotaryEmbedding()\n",
       "      (blocks): ModuleList(\n",
       "        (0): PathwayBlock(\n",
       "          (ln_1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): BioMultiHeadAttention(\n",
       "            (q_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "            (k_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "            (v_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "            (c_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Linear(in_features=8, out_features=32, bias=True)\n",
       "            (gelu): GELU(approximate='tanh')\n",
       "            (c_proj): Linear(in_features=32, out_features=8, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (teacher): PathwayEncoder(\n",
       "    (transformer): ModuleDict(\n",
       "      (input_proj): Linear(in_features=1, out_features=8, bias=True)\n",
       "      (rope): RotaryEmbedding()\n",
       "      (blocks): ModuleList(\n",
       "        (0): PathwayBlock(\n",
       "          (ln_1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): BioMultiHeadAttention(\n",
       "            (q_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "            (k_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "            (v_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "            (c_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Linear(in_features=8, out_features=32, bias=True)\n",
       "            (gelu): GELU(approximate='tanh')\n",
       "            (c_proj): Linear(in_features=32, out_features=8, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (predictor): ACPredictor(\n",
       "    (action_embed): Embedding(2058, 256)\n",
       "    (rope): RotaryEmbedding()\n",
       "    (blocks): ModuleList(\n",
       "      (0): PredictorBlock(\n",
       "        (ada_ln1): AdaLN(\n",
       "          (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=False)\n",
       "          (action_mlp): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=256, out_features=16, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn): BioMultiHeadAttention(\n",
       "          (q_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "          (k_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "          (v_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "          (c_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "        )\n",
       "        (ada_ln2): AdaLN(\n",
       "          (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=False)\n",
       "          (action_mlp): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=256, out_features=16, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=8, out_features=32, bias=True)\n",
       "          (gelu): GELU(approximate='tanh')\n",
       "          (c_proj): Linear(in_features=32, out_features=8, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_norm): AdaLN(\n",
       "      (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=False)\n",
       "      (action_mlp): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=256, out_features=16, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09550430-309d-436c-a140-2bda3fb8bde3",
   "metadata": {},
   "source": [
    "**Get random data sample**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "189729ef-8852-466c-b05b-455948aa6cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_test_pair(shard_dir):\n",
    "    '''Grab a single real pair from a random shard'''\n",
    "    files = sorted(shard_dir.glob('*.npz'))\n",
    "    file_path = files[np.random.randint(len(files))]\n",
    "    \n",
    "    with np.load(file_path) as data:\n",
    "        idx = np.random.randint(data['action_ids'].shape[0])\n",
    "        \n",
    "        # Extract raw uint32\n",
    "        control_raw = data['control'][idx]\n",
    "        case_raw = data['case'][idx]\n",
    "        act_id = data['action_ids'][idx]\n",
    "        \n",
    "    # Dequantize\n",
    "    x_control = torch.tensor(control_raw.astype(np.float32)).unsqueeze(0).to(DEVICE) # [1, 1024]\n",
    "    x_case = torch.tensor(case_raw.astype(np.float32)).unsqueeze(0).to(DEVICE)\n",
    "    action_id = torch.tensor([act_id], dtype=torch.long).to(DEVICE)\n",
    "    \n",
    "    return x_control, x_case, action_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "793361fe-baee-42b7-982d-72e604d28013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ORC4'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_control, x_case, action_id = get_random_test_pair(val_dir)\n",
    "pert_name = id_to_name[action_id.item()]\n",
    "pert_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "75e32d24-4c56-40fc-9d76-3048536eddb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Get Baselines (Teacher View)\n",
    "# We need the Teacher to tell us where the \"Control\" and \"Real Treated\" \n",
    "# sit in the abstract latent space.\n",
    "with torch.no_grad():\n",
    "    z_control = model.teacher(x_control)       # Where the cell started\n",
    "    z_case = model.teacher(x_case)     # Where the cell actually went"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "054228e3-73d0-48da-9b7f-2f80be87cc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Run The Physics Engine (Predictor)\n",
    "# Student encodes context -> Predictor adds Action -> Output\n",
    "with torch.no_grad():\n",
    "    z_context = model.student(x_control)\n",
    "    z_predicted = model.predictor(z_context, action_id) # Where the model thinks it went"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5360c5-2215-4fc8-858a-d1488cf54ab6",
   "metadata": {},
   "source": [
    "**Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "49851bf2-12f7-4de4-9b3f-efbd1bac2c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08322122693061829"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Metric 1: Baseline Drift (How much did the drug actually change the cell?)\n",
    "# If this is 0, the drug did nothing, so prediction is trivial.\n",
    "drift = F.l1_loss(z_control, z_case).item()\n",
    "drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "19a65de9-8f27-4f96-b304-07272e5367f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03847771883010864"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Metric 2: Prediction Error (How close is our guess to the real result?)\n",
    "error = F.l1_loss(z_predicted, z_case).item()\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "00cbe219-8cfa-4d15-a09f-999de53f87bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06297487765550613"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Metric 3: Simulation Magnitude (How much did our model decide to move the cell?)\n",
    "sim_move = F.l1_loss(z_control,z_predicted).item()\n",
    "sim_move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cdb7a8ae-c7ac-4bf5-8aca-ca82af1f57fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Result Interpretation]\n",
      "âœ… SUCCESS: The model predicted the state shift!\n",
      "   The prediction is 53.8% closer to the truth than the Control state was.\n"
     ]
    }
   ],
   "source": [
    "# --- INTERPRETATION ---\n",
    "print(f\"[Result Interpretation]\")\n",
    "if drift < 0.01:\n",
    "    print(f\"âš ï¸  WEAK SIGNAL: This perturbation didn't change the cell much in reality.\")\n",
    "elif error < drift:\n",
    "    improvement = (1 - (error / drift)) * 100\n",
    "    print(f\"âœ… SUCCESS: The model predicted the state shift!\")\n",
    "    print(f\"   The prediction is {improvement:.1f}% closer to the truth than the Control state was.\")\n",
    "else:\n",
    "    print(f\"âŒ FAILURE: The model failed to capture the dynamics.\")\n",
    "    print(f\"   It would have been better to just guess 'Nothing Happened'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "03a7c686-6b68-4975-add4-f8178fdf3f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Top Active Latent Dimensions]\n",
      "Dimensions [[3, 2, 7, 4, 0], [3, 2, 7, 0, 1], [3, 0, 1, 2, 7], [1, 0, 7, 2, 4], [3, 2, 7, 5, 6], [3, 0, 1, 2, 7], [3, 2, 7, 5, 6], [3, 2, 7, 0, 1], [3, 2, 1, 0, 5], [2, 3, 1, 0, 4], [3, 2, 5, 1, 0], [3, 2, 5, 7, 1], [3, 2, 1, 0, 5], [3, 2, 1, 0, 4], [2, 3, 1, 0, 5], [2, 3, 0, 1, 5], [3, 2, 1, 0, 5], [3, 2, 1, 0, 5], [1, 0, 7, 5, 3], [3, 2, 7, 5, 0], [3, 2, 1, 0, 5], [3, 2, 1, 0, 5], [3, 2, 1, 0, 4], [3, 2, 7, 1, 5], [3, 2, 7, 5, 6], [3, 2, 7, 1, 0], [2, 3, 1, 0, 4], [3, 2, 1, 0, 4], [3, 2, 1, 0, 5], [3, 2, 0, 1, 5], [3, 2, 7, 1, 5], [3, 2, 1, 0, 4], [3, 2, 1, 0, 5], [3, 2, 1, 0, 5], [2, 3, 1, 5, 0], [3, 2, 1, 0, 4], [3, 2, 1, 0, 5], [3, 2, 1, 0, 4], [3, 2, 1, 0, 5], [2, 3, 1, 0, 5], [3, 2, 1, 0, 5], [3, 2, 0, 1, 5], [3, 2, 1, 0, 4], [3, 2, 5, 7, 0], [3, 2, 1, 0, 4], [2, 3, 1, 0, 7], [3, 2, 1, 0, 5], [3, 2, 5, 7, 0], [3, 2, 1, 0, 5], [3, 2, 1, 0, 5], [3, 2, 1, 0, 4], [3, 2, 0, 1, 5], [3, 2, 1, 0, 4], [3, 2, 1, 0, 4], [3, 2, 5, 7, 0], [2, 3, 1, 0, 5], [3, 2, 1, 0, 7], [3, 2, 7, 5, 1], [3, 2, 1, 0, 5], [2, 3, 1, 0, 5], [3, 2, 1, 0, 4], [3, 7, 1, 0, 2], [3, 1, 7, 2, 0], [3, 2, 0, 5, 1], [3, 2, 1, 0, 5], [3, 2, 5, 7, 0], [2, 3, 1, 0, 5], [3, 2, 7, 5, 6], [2, 3, 1, 0, 4], [3, 2, 7, 5, 6], [3, 2, 1, 0, 5], [3, 2, 7, 1, 5], [3, 2, 7, 1, 5], [3, 2, 5, 7, 1], [2, 3, 5, 7, 1], [2, 3, 5, 1, 0], [2, 3, 1, 0, 5], [3, 2, 1, 0, 5], [2, 3, 1, 0, 5], [3, 2, 1, 0, 5], [3, 0, 1, 2, 7], [3, 2, 7, 5, 1], [3, 2, 7, 5, 0], [2, 3, 1, 0, 6], [2, 3, 1, 0, 5], [2, 3, 1, 0, 4], [3, 2, 1, 0, 4], [3, 2, 1, 0, 5], [3, 2, 5, 7, 1], [3, 2, 5, 7, 1], [3, 2, 5, 7, 1], [2, 3, 1, 0, 5], [3, 2, 7, 4, 0], [3, 2, 0, 1, 7], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [3, 2, 1, 0, 5], [3, 2, 1, 0, 4], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [3, 2, 1, 0, 5], [3, 2, 1, 0, 4], [2, 3, 7, 5, 1], [2, 3, 5, 7, 1], [2, 3, 1, 0, 5], [3, 2, 5, 7, 1], [2, 3, 5, 7, 1], [3, 2, 7, 1, 0], [3, 0, 1, 5, 6], [2, 3, 5, 7, 1], [2, 3, 1, 0, 5], [3, 2, 1, 0, 4], [3, 2, 1, 0, 4], [2, 3, 1, 0, 4], [3, 2, 7, 5, 1], [2, 3, 1, 5, 0], [3, 2, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [3, 2, 7, 4, 5], [2, 3, 1, 5, 7], [2, 3, 5, 1, 7], [2, 3, 7, 1, 5], [2, 3, 5, 1, 0], [2, 3, 1, 0, 4], [2, 3, 5, 7, 1], [2, 3, 1, 0, 5], [3, 2, 7, 0, 1], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 5, 0], [2, 3, 1, 0, 5], [2, 3, 5, 7, 1], [2, 3, 5, 7, 0], [3, 2, 1, 0, 4], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 4], [2, 3, 1, 0, 5], [2, 3, 1, 0, 4], [2, 3, 7, 5, 1], [2, 3, 1, 0, 4], [3, 2, 1, 0, 4], [3, 2, 7, 5, 1], [2, 3, 7, 1, 5], [2, 3, 1, 0, 5], [2, 3, 5, 7, 1], [2, 3, 5, 1, 0], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [3, 2, 7, 5, 4], [3, 2, 7, 5, 0], [0, 1, 3, 6, 4], [2, 3, 7, 5, 1], [2, 3, 1, 0, 4], [2, 3, 1, 0, 5], [2, 3, 5, 1, 7], [2, 3, 5, 7, 1], [3, 2, 7, 5, 1], [3, 2, 5, 0, 1], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [3, 2, 7, 5, 4], [3, 2, 7, 5, 1], [2, 3, 1, 0, 6], [2, 3, 7, 5, 4], [3, 2, 5, 1, 7], [3, 2, 0, 1, 7], [3, 2, 0, 1, 5], [3, 2, 1, 0, 7], [3, 2, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 5, 0], [2, 3, 1, 0, 5], [3, 2, 0, 4, 1], [2, 3, 7, 5, 1], [2, 3, 1, 0, 5], [2, 3, 5, 7, 1], [2, 3, 1, 0, 5], [3, 2, 5, 7, 1], [3, 2, 1, 0, 5], [2, 3, 5, 7, 1], [2, 3, 1, 0, 4], [2, 3, 1, 0, 5], [2, 3, 1, 0, 4], [3, 2, 1, 0, 4], [2, 3, 5, 7, 1], [2, 3, 5, 7, 1], [3, 2, 5, 7, 1], [2, 3, 1, 0, 5], [3, 0, 1, 2, 7], [2, 3, 1, 0, 5], [2, 3, 1, 5, 0], [2, 3, 1, 0, 5], [2, 3, 5, 7, 1], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 5, 0, 1], [2, 3, 7, 5, 1], [2, 3, 1, 0, 4], [2, 3, 1, 0, 5], [3, 2, 7, 5, 6], [2, 3, 1, 0, 5], [2, 3, 5, 7, 1], [2, 3, 1, 0, 4], [2, 3, 1, 0, 5], [3, 2, 1, 0, 4], [2, 3, 5, 7, 1], [2, 3, 1, 0, 5], [2, 3, 5, 7, 1], [2, 3, 7, 1, 5], [2, 3, 7, 5, 1], [2, 3, 5, 7, 1], [2, 3, 1, 0, 5], [3, 2, 5, 7, 1], [2, 3, 1, 0, 5], [2, 3, 5, 1, 0], [2, 3, 1, 0, 5], [2, 3, 7, 5, 1], [2, 3, 5, 7, 1], [3, 2, 5, 7, 6], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 5, 1, 0], [2, 3, 5, 7, 1], [3, 2, 7, 5, 6], [2, 3, 1, 0, 5], [2, 3, 1, 5, 0], [2, 3, 1, 0, 5], [2, 3, 1, 5, 7], [2, 3, 7, 5, 1], [2, 3, 1, 5, 0], [3, 2, 7, 5, 0], [2, 3, 1, 5, 0], [3, 2, 7, 5, 0], [2, 3, 1, 0, 5], [3, 2, 7, 0, 1], [3, 2, 0, 1, 7], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [3, 2, 5, 7, 1], [3, 0, 1, 2, 7], [2, 3, 5, 7, 1], [2, 3, 1, 0, 5], [2, 3, 5, 7, 1], [3, 2, 7, 5, 6], [3, 2, 7, 4, 0], [3, 2, 1, 0, 5], [3, 2, 7, 0, 1], [3, 2, 0, 1, 5], [2, 3, 7, 5, 1], [3, 2, 1, 0, 5], [2, 3, 7, 5, 1], [3, 2, 7, 4, 0], [2, 3, 5, 1, 7], [2, 3, 5, 7, 1], [3, 2, 1, 0, 5], [3, 2, 1, 0, 7], [3, 2, 0, 7, 1], [2, 3, 1, 0, 5], [3, 2, 7, 5, 1], [2, 3, 5, 1, 7], [2, 3, 7, 5, 1], [3, 2, 0, 1, 7], [3, 2, 7, 5, 0], [3, 2, 7, 5, 4], [3, 2, 7, 5, 4], [2, 3, 1, 0, 5], [2, 3, 7, 5, 4], [2, 3, 1, 0, 5], [3, 2, 7, 5, 0], [2, 3, 1, 0, 5], [3, 0, 1, 7, 2], [3, 2, 7, 5, 1], [2, 3, 5, 0, 1], [3, 2, 7, 5, 6], [2, 3, 1, 0, 5], [3, 2, 7, 1, 0], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 5, 7], [2, 3, 7, 5, 1], [3, 2, 1, 0, 4], [3, 2, 7, 4, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 7, 5, 1], [2, 3, 5, 1, 0], [2, 3, 5, 7, 1], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 5, 7, 1], [3, 2, 5, 1, 0], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [3, 2, 5, 7, 1], [3, 2, 5, 7, 1], [3, 2, 1, 5, 0], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 5, 1, 0], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 5, 7, 1], [2, 3, 1, 0, 4], [3, 2, 1, 0, 4], [3, 2, 7, 0, 5], [2, 3, 5, 7, 1], [2, 3, 1, 0, 5], [3, 2, 7, 5, 0], [2, 3, 1, 5, 0], [3, 2, 7, 5, 0], [2, 3, 1, 0, 5], [2, 3, 1, 5, 0], [2, 3, 1, 5, 7], [3, 2, 7, 4, 0], [3, 2, 7, 5, 4], [2, 3, 1, 5, 0], [2, 3, 1, 0, 5], [2, 3, 5, 1, 7], [2, 3, 5, 1, 0], [3, 2, 7, 5, 0], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 5, 0], [2, 3, 5, 1, 7], [2, 3, 7, 5, 1], [2, 3, 5, 7, 1], [2, 3, 1, 0, 6], [2, 3, 1, 0, 5], [2, 3, 7, 5, 1], [2, 3, 1, 5, 0], [2, 3, 1, 0, 5], [2, 3, 5, 7, 1], [2, 3, 1, 0, 5], [3, 2, 7, 5, 0], [2, 3, 1, 0, 5], [3, 2, 5, 7, 1], [2, 3, 1, 0, 5], [2, 3, 1, 5, 7], [2, 3, 7, 5, 4], [2, 3, 5, 7, 1], [2, 3, 5, 1, 0], [2, 3, 5, 7, 1], [2, 3, 1, 0, 4], [3, 2, 7, 5, 4], [3, 2, 5, 7, 0], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [3, 2, 1, 0, 4], [2, 3, 1, 0, 5], [3, 2, 7, 5, 4], [2, 3, 1, 0, 5], [2, 3, 7, 5, 1], [3, 2, 7, 5, 4], [2, 3, 1, 0, 5], [3, 2, 1, 0, 7], [2, 3, 1, 0, 5], [2, 3, 7, 5, 1], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [3, 2, 1, 0, 5], [2, 3, 5, 7, 1], [2, 3, 5, 7, 1], [2, 3, 1, 0, 4], [2, 3, 1, 0, 6], [3, 2, 7, 5, 4], [2, 3, 1, 5, 0], [2, 3, 5, 7, 0], [2, 3, 5, 0, 1], [3, 2, 7, 5, 4], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [3, 2, 7, 5, 4], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 5, 0], [2, 3, 1, 0, 5], [3, 2, 7, 5, 1], [3, 2, 1, 0, 5], [2, 3, 1, 0, 5], [3, 2, 7, 5, 0], [3, 2, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [3, 2, 7, 5, 1], [3, 2, 1, 0, 5], [2, 3, 1, 0, 4], [3, 2, 1, 0, 4], [2, 3, 5, 7, 1], [2, 3, 1, 5, 0], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 7, 5, 0], [3, 2, 7, 5, 1], [2, 3, 1, 0, 5], [3, 2, 1, 0, 4], [2, 3, 1, 0, 5], [3, 2, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [3, 2, 7, 5, 1], [2, 3, 1, 5, 0], [2, 3, 1, 0, 5], [3, 2, 7, 5, 6], [2, 3, 1, 0, 5], [3, 2, 7, 5, 4], [2, 3, 1, 0, 5], [3, 2, 7, 5, 4], [3, 2, 7, 5, 6], [3, 2, 1, 0, 4], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [3, 2, 0, 1, 4], [3, 2, 1, 0, 4], [2, 3, 1, 0, 5], [2, 3, 5, 1, 7], [2, 3, 5, 7, 0], [2, 3, 1, 0, 5], [3, 2, 5, 7, 1], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 6], [2, 3, 1, 0, 5], [2, 3, 5, 1, 7], [2, 3, 5, 1, 7], [2, 3, 1, 0, 4], [3, 2, 0, 7, 1], [2, 3, 1, 5, 0], [3, 2, 7, 5, 4], [2, 3, 1, 0, 5], [2, 3, 1, 0, 4], [2, 3, 1, 0, 5], [3, 2, 1, 0, 4], [2, 3, 1, 0, 5], [3, 2, 1, 0, 5], [2, 3, 1, 0, 5], [3, 2, 1, 0, 4], [2, 3, 1, 0, 5], [3, 2, 1, 0, 5], [3, 2, 7, 5, 4], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [3, 2, 1, 0, 4], [3, 2, 7, 0, 1], [3, 2, 1, 0, 5], [3, 2, 0, 1, 7], [3, 2, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [3, 2, 1, 0, 5], [2, 3, 1, 0, 5], [3, 2, 1, 0, 4], [2, 3, 1, 0, 5], [3, 2, 1, 0, 5], [2, 3, 1, 0, 5], [3, 2, 1, 0, 5], [2, 3, 1, 0, 4], [2, 3, 5, 7, 0], [3, 2, 5, 1, 0], [2, 3, 1, 0, 5], [3, 0, 1, 7, 2], [2, 3, 7, 5, 1], [3, 2, 7, 0, 1], [2, 3, 1, 5, 0], [2, 3, 7, 5, 1], [2, 3, 7, 5, 1], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [3, 2, 0, 7, 1], [3, 2, 7, 5, 0], [2, 3, 5, 7, 1], [3, 2, 7, 5, 4], [2, 3, 5, 7, 1], [2, 3, 7, 5, 1], [1, 0, 3, 4, 6], [3, 0, 2, 1, 7], [2, 3, 5, 7, 1], [2, 3, 5, 1, 7], [2, 3, 1, 0, 5], [2, 3, 7, 5, 4], [2, 3, 5, 7, 1], [3, 2, 7, 5, 4], [3, 2, 0, 1, 4], [2, 3, 5, 1, 7], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 7, 5, 1], [3, 2, 7, 0, 4], [2, 3, 1, 0, 5], [2, 3, 7, 5, 1], [2, 3, 5, 7, 1], [2, 3, 5, 1, 7], [2, 3, 1, 0, 5], [2, 3, 1, 0, 6], [3, 2, 7, 0, 1], [2, 3, 5, 7, 1], [3, 2, 7, 5, 4], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [3, 2, 7, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [3, 2, 7, 5, 1], [2, 3, 1, 5, 0], [2, 3, 1, 0, 5], [3, 2, 0, 1, 7], [1, 0, 2, 5, 6], [2, 3, 1, 5, 0], [3, 2, 0, 7, 1], [2, 3, 7, 5, 1], [2, 3, 1, 5, 0], [3, 2, 7, 4, 0], [2, 3, 1, 0, 5], [0, 1, 3, 6, 7], [2, 3, 7, 5, 1], [3, 2, 7, 5, 4], [0, 1, 3, 7, 4], [3, 2, 7, 5, 4], [2, 3, 1, 0, 5], [2, 3, 5, 1, 7], [2, 3, 5, 1, 7], [2, 3, 1, 0, 5], [2, 3, 7, 5, 1], [3, 2, 7, 0, 1], [1, 0, 6, 7, 3], [2, 3, 1, 5, 0], [2, 3, 1, 5, 0], [2, 3, 1, 0, 5], [0, 1, 3, 4, 7], [2, 3, 5, 7, 1], [1, 0, 2, 6, 5], [1, 0, 7, 3, 4], [2, 3, 1, 0, 5], [2, 3, 5, 1, 7], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [3, 2, 7, 5, 4], [2, 3, 1, 0, 5], [3, 2, 7, 0, 4], [3, 2, 7, 5, 4], [2, 3, 1, 0, 6], [3, 2, 7, 5, 0], [2, 3, 5, 7, 1], [3, 0, 2, 1, 7], [2, 3, 5, 7, 1], [3, 2, 7, 5, 0], [3, 2, 7, 5, 1], [2, 3, 5, 7, 1], [0, 1, 7, 3, 4], [3, 0, 1, 2, 7], [2, 3, 1, 0, 5], [3, 2, 7, 0, 1], [2, 3, 5, 7, 1], [2, 3, 1, 0, 5], [2, 3, 1, 5, 0], [2, 3, 1, 5, 0], [1, 0, 3, 7, 6], [2, 3, 1, 0, 5], [3, 2, 7, 5, 4], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 5, 0], [2, 3, 1, 0, 5], [3, 2, 7, 5, 1], [3, 2, 7, 0, 1], [2, 3, 7, 5, 1], [3, 2, 7, 5, 0], [2, 3, 1, 0, 5], [3, 2, 0, 1, 7], [3, 2, 7, 5, 0], [3, 2, 7, 5, 4], [3, 2, 7, 5, 4], [2, 3, 7, 5, 1], [2, 3, 1, 5, 7], [2, 3, 1, 0, 5], [2, 3, 1, 5, 0], [3, 2, 7, 5, 1], [3, 0, 1, 2, 7], [3, 0, 1, 2, 7], [3, 2, 0, 7, 1], [3, 2, 7, 5, 4], [1, 0, 2, 6, 5], [3, 2, 7, 0, 1], [3, 0, 1, 7, 2], [2, 3, 1, 0, 5], [2, 3, 7, 5, 1], [2, 3, 1, 0, 5], [3, 2, 7, 0, 5], [3, 2, 0, 7, 1], [3, 2, 7, 0, 1], [1, 0, 6, 2, 5], [2, 3, 1, 5, 0], [3, 2, 7, 5, 4], [2, 3, 1, 0, 5], [3, 2, 7, 0, 1], [3, 2, 0, 7, 1], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [3, 0, 1, 2, 7], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [3, 2, 7, 5, 4], [3, 2, 5, 7, 1], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 7, 5, 1], [2, 3, 7, 5, 1], [3, 2, 0, 1, 7], [2, 3, 1, 0, 5], [2, 3, 7, 5, 1], [2, 3, 5, 7, 1], [2, 3, 1, 5, 0], [2, 3, 7, 5, 1], [2, 3, 1, 0, 5], [0, 1, 3, 7, 4], [2, 3, 5, 7, 1], [2, 3, 5, 7, 1], [3, 2, 7, 5, 4], [2, 3, 5, 7, 1], [2, 3, 5, 7, 1], [2, 3, 1, 5, 0], [0, 1, 3, 7, 4], [3, 2, 7, 5, 1], [3, 2, 7, 0, 1], [3, 2, 7, 5, 4], [2, 3, 1, 0, 5], [1, 0, 2, 6, 5], [2, 3, 7, 5, 1], [3, 2, 7, 5, 0], [2, 3, 1, 0, 5], [3, 2, 0, 1, 7], [3, 2, 7, 4, 0], [3, 2, 7, 0, 5], [2, 3, 1, 5, 0], [3, 2, 7, 5, 0], [2, 3, 7, 5, 4], [2, 3, 1, 5, 0], [2, 3, 1, 0, 5], [2, 3, 5, 1, 7], [2, 3, 7, 5, 1], [2, 3, 1, 0, 5], [0, 1, 7, 3, 6], [2, 3, 7, 5, 1], [3, 2, 7, 5, 4], [3, 2, 7, 5, 4], [2, 3, 1, 5, 7], [2, 3, 1, 5, 7], [3, 0, 1, 7, 2], [2, 3, 5, 7, 1], [2, 3, 1, 0, 5], [3, 2, 7, 5, 4], [2, 3, 1, 5, 0], [3, 2, 7, 5, 0], [3, 2, 7, 5, 0], [2, 3, 1, 0, 5], [3, 2, 7, 5, 4], [3, 2, 5, 7, 1], [3, 2, 0, 1, 4], [2, 3, 7, 5, 1], [3, 2, 7, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 5, 7], [2, 3, 5, 1, 7], [2, 3, 1, 0, 5], [3, 2, 7, 0, 4], [2, 3, 1, 0, 5], [2, 3, 1, 5, 7], [2, 3, 7, 5, 4], [1, 0, 3, 7, 4], [1, 0, 5, 2, 6], [3, 0, 1, 2, 7], [3, 2, 7, 5, 0], [3, 2, 7, 5, 4], [3, 2, 7, 5, 0], [2, 3, 7, 5, 4], [2, 3, 7, 5, 4], [0, 1, 3, 4, 6], [3, 2, 7, 5, 0], [3, 2, 7, 5, 0], [2, 3, 1, 0, 5], [2, 3, 5, 7, 1], [2, 3, 1, 0, 5], [2, 3, 7, 5, 1], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [3, 2, 7, 0, 4], [2, 3, 1, 5, 0], [2, 3, 5, 7, 1], [3, 2, 7, 0, 4], [2, 3, 1, 0, 5], [3, 0, 1, 2, 7], [2, 3, 5, 1, 7], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [3, 2, 7, 0, 1], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 5, 7, 1], [3, 2, 7, 5, 1], [3, 0, 1, 5, 6], [0, 1, 6, 4, 3], [1, 0, 6, 3, 5], [2, 3, 1, 5, 7], [3, 2, 7, 5, 1], [3, 2, 7, 0, 1], [3, 2, 7, 5, 4], [2, 3, 1, 5, 0], [3, 2, 7, 5, 4], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [3, 2, 7, 5, 0], [3, 2, 7, 5, 1], [3, 2, 0, 7, 1], [2, 3, 7, 5, 1], [2, 3, 5, 7, 1], [2, 3, 1, 0, 5], [3, 2, 7, 5, 4], [3, 2, 4, 7, 0], [0, 1, 7, 4, 3], [3, 2, 4, 0, 7], [2, 3, 4, 1, 6], [3, 2, 4, 7, 6], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 5, 1, 0], [2, 3, 1, 5, 0], [2, 3, 7, 5, 1], [3, 2, 7, 5, 0], [3, 2, 7, 5, 4], [2, 3, 5, 1, 7], [2, 3, 1, 0, 5], [1, 0, 3, 7, 6], [3, 2, 7, 5, 0], [2, 3, 7, 5, 4], [2, 3, 7, 5, 1], [2, 3, 1, 5, 0], [2, 3, 5, 7, 1], [2, 3, 1, 0, 5], [2, 3, 1, 0, 4], [2, 3, 1, 5, 0], [3, 2, 7, 0, 5], [2, 3, 5, 1, 7], [2, 3, 1, 0, 5], [3, 2, 1, 5, 0], [2, 3, 1, 5, 0], [0, 1, 7, 3, 4], [2, 3, 7, 5, 1], [2, 3, 5, 1, 7], [2, 3, 5, 7, 1], [2, 3, 1, 0, 5], [2, 3, 5, 7, 1], [3, 2, 5, 7, 1], [2, 3, 1, 5, 7], [2, 3, 1, 0, 5], [1, 0, 3, 7, 6], [2, 3, 1, 0, 5], [2, 3, 1, 5, 7], [0, 1, 3, 7, 4], [3, 2, 5, 7, 6], [3, 2, 1, 5, 7], [2, 3, 1, 0, 5], [2, 3, 5, 7, 1], [2, 3, 1, 5, 0], [2, 3, 1, 5, 0], [3, 2, 0, 1, 4], [2, 3, 5, 1, 7], [2, 3, 5, 7, 1], [2, 3, 1, 5, 7], [2, 3, 1, 5, 7], [3, 2, 7, 5, 0], [3, 2, 7, 5, 4], [2, 3, 5, 7, 1], [3, 2, 0, 1, 7], [2, 3, 5, 7, 1], [5, 3, 1, 0, 6], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 5, 7, 1], [2, 3, 1, 0, 5], [3, 2, 7, 5, 4], [2, 3, 5, 7, 1], [2, 3, 5, 7, 1], [2, 3, 1, 0, 4], [2, 3, 1, 5, 7], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [3, 2, 7, 5, 0], [2, 3, 1, 0, 5], [3, 2, 7, 5, 1], [2, 3, 1, 0, 4], [2, 3, 7, 5, 1], [2, 3, 1, 0, 5], [2, 3, 7, 4, 1], [2, 3, 1, 0, 5], [3, 2, 0, 1, 4], [2, 3, 1, 0, 5], [3, 2, 7, 4, 0], [3, 2, 7, 5, 1], [2, 3, 5, 7, 1], [3, 2, 7, 5, 0], [2, 3, 1, 5, 0], [3, 2, 7, 0, 5], [2, 3, 1, 5, 0], [2, 3, 1, 0, 5], [0, 1, 3, 7, 4], [2, 3, 7, 5, 4], [2, 3, 1, 5, 7], [0, 1, 3, 7, 4], [2, 3, 7, 5, 1], [2, 3, 5, 1, 7], [2, 3, 1, 5, 0], [3, 2, 7, 5, 0], [2, 3, 5, 7, 1], [3, 2, 7, 5, 1], [2, 3, 7, 5, 1], [2, 3, 5, 7, 1], [2, 3, 1, 5, 0], [2, 3, 1, 0, 5], [2, 3, 5, 1, 7], [3, 2, 7, 5, 0], [2, 3, 1, 0, 5], [2, 3, 5, 7, 1], [2, 3, 5, 1, 7], [3, 2, 0, 1, 7], [2, 3, 5, 7, 1], [0, 1, 7, 3, 5], [2, 3, 5, 7, 1], [3, 2, 7, 0, 1], [3, 2, 7, 0, 1], [2, 3, 5, 7, 1], [2, 3, 1, 5, 0], [2, 3, 5, 7, 1], [3, 2, 5, 7, 1], [3, 2, 7, 0, 1], [2, 3, 5, 1, 7], [2, 3, 5, 7, 1], [2, 3, 5, 7, 1], [3, 0, 1, 7, 2], [3, 2, 5, 7, 1], [2, 3, 1, 5, 0], [1, 0, 2, 5, 6], [2, 3, 1, 5, 0], [3, 2, 7, 5, 4], [2, 3, 7, 5, 1], [2, 3, 1, 0, 5], [2, 3, 5, 7, 1], [1, 0, 2, 6, 4], [2, 3, 1, 0, 5], [3, 2, 7, 5, 4], [2, 3, 1, 0, 5], [3, 2, 7, 5, 0], [2, 3, 5, 1, 0], [2, 3, 1, 0, 5], [2, 3, 1, 0, 6], [2, 3, 7, 5, 4], [3, 2, 7, 0, 5], [3, 2, 7, 5, 0], [2, 3, 1, 5, 0], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 5, 7, 1], [2, 3, 5, 7, 1], [2, 3, 5, 1, 7], [3, 0, 2, 1, 7], [3, 2, 7, 0, 1], [3, 0, 1, 2, 7], [3, 0, 1, 2, 7], [2, 3, 5, 1, 7], [3, 2, 7, 5, 0], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [3, 2, 7, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 7, 5, 4], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [2, 3, 5, 1, 7], [3, 2, 7, 5, 1], [3, 2, 7, 5, 0], [2, 3, 1, 5, 0], [3, 2, 7, 5, 4], [3, 2, 5, 7, 0], [2, 3, 5, 1, 7], [2, 3, 5, 7, 1], [2, 3, 1, 0, 5], [2, 3, 7, 5, 1], [2, 3, 1, 0, 5], [2, 3, 1, 0, 5], [3, 2, 7, 0, 1], [2, 3, 7, 5, 1], [0, 3, 1, 4, 7], [3, 2, 7, 5, 0], [2, 3, 1, 5, 0], [3, 2, 7, 5, 1], [3, 2, 7, 5, 1], [2, 3, 7, 1, 6], [2, 3, 1, 0, 5], [2, 3, 1, 5, 7], [3, 0, 2, 1, 7], [2, 3, 1, 5, 0], [2, 3, 1, 5, 0], [2, 3, 5, 1, 7], [2, 3, 1, 0, 4], [2, 3, 1, 0, 5], [3, 2, 0, 1, 7], [2, 3, 5, 1, 7], [2, 3, 1, 0, 5], [2, 3, 5, 7, 1], [3, 2, 1, 0, 4], [3, 2, 7, 0, 5], [3, 2, 7, 5, 4], [3, 2, 7, 5, 4], [2, 3, 1, 0, 6], [2, 3, 1, 0, 5], [3, 2, 7, 5, 6], [2, 3, 5, 1, 7], [2, 3, 5, 7, 1], [3, 0, 2, 1, 7], [2, 3, 5, 7, 1], [2, 3, 1, 0, 4], [2, 3, 1, 0, 5], [2, 3, 1, 5, 0], [2, 3, 1, 0, 5], [3, 2, 5, 7, 1], [3, 2, 7, 5, 4], [3, 2, 5, 7, 1], [2, 3, 5, 7, 1], [3, 2, 0, 1, 7], [2, 3, 5, 7, 1], [2, 3, 1, 5, 0], [2, 3, 5, 1, 0], [3, 5, 1, 0, 6], [2, 3, 5, 1, 0], [3, 2, 7, 5, 0], [2, 3, 1, 0, 5], [2, 3, 1, 0, 6], [3, 1, 0, 7, 2], [3, 2, 7, 5, 0], [3, 2, 4, 7, 0], [2, 3, 5, 7, 1], [2, 3, 5, 1, 7], [2, 3, 1, 5, 0], [2, 3, 1, 5, 7], [2, 3, 7, 5, 4], [2, 3, 1, 0, 5], [3, 2, 0, 1, 7], [2, 3, 5, 7, 1], [1, 0, 7, 3, 4], [3, 2, 7, 5, 0], [3, 2, 7, 5, 4], [2, 3, 7, 5, 1], [2, 3, 5, 1, 7], [2, 3, 5, 7, 1], [2, 3, 5, 7, 1], [2, 3, 5, 7, 1], [3, 2, 0, 1, 7], [2, 3, 7, 5, 1], [2, 3, 1, 0, 5], [3, 2, 0, 7, 4], [3, 2, 0, 7, 4], [2, 3, 1, 0, 5], [3, 2, 7, 5, 4], [3, 2, 4, 5, 0], [2, 3, 1, 0, 5], [2, 3, 1, 0, 4], [3, 2, 5, 4, 0], [0, 1, 2, 7, 6], [2, 3, 7, 5, 1], [2, 3, 5, 7, 1], [2, 3, 7, 5, 1], [2, 3, 1, 0, 5], [2, 3, 1, 0, 4], [0, 1, 3, 7, 4]] showed the highest activity during this simulation.\n",
      "(In a full analysis, you would regress these dimensions back to Pathways).\n"
     ]
    }
   ],
   "source": [
    "# Which latent dimensions changed the most?\n",
    "# This gives us a \"fingerprint\" of the predicted change\n",
    "diff_vector = (z_predicted - z_control).abs().mean(dim=0) # [Embed_Dim]\n",
    "top_dims = torch.topk(diff_vector, 5).indices.tolist()\n",
    "\n",
    "print(f\"\\n[Top Active Latent Dimensions]\")\n",
    "print(f\"Dimensions {top_dims} showed the highest activity during this simulation.\")\n",
    "print(\"(In a full analysis, you would regress these dimensions back to Pathways).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc7a4bc-88e5-4903-bbad-6ffa382db26a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30288334-6f6f-48a3-96e3-4f9144215288",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
