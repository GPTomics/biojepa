{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a394c59-337f-40d2-876c-2d5c06a84044",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import copy\n",
    "import math\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947366e6-bf14-43fa-8345-cf286325f4ba",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74ac7ff5-225d-45f6-9b2d-a6810fde4e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76a7c401-4337-47b6-86b7-6df85c66d3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cpu\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    device = 'cpu'\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(1337)\n",
    "        device = 'cuda'\n",
    "    # elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    #     device = 'mps'\n",
    "    print(f'using {device}')\n",
    "    return device\n",
    "\n",
    "DEVICE = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e20f1a9-1407-4ffc-a730-6afdd99d20c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_seq_len=2048):\n",
    "        super().__init__()\n",
    "        # Ensure dim is the head dimension, not the full embedding dimension\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        t = torch.arange(max_seq_len).type_as(inv_freq)\n",
    "        freqs = torch.outer(t, inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.register_buffer('emb', emb)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [Batch, Seq, Dim]\n",
    "        # We only care about the sequence length here\n",
    "        n = x.shape[1]\n",
    "        # Returns [Seq, Head_Dim]\n",
    "        return self.emb[:n, :].cos(), self.emb[:n, :].sin()\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin):\n",
    "    # q, k: [Batch, Heads, Seq, Head_Dim]\n",
    "    # cos, sin: [Seq, Head_Dim] -> reshape to [1, 1, Seq, Head_Dim]\n",
    "    \n",
    "    cos = cos.unsqueeze(0).unsqueeze(0)\n",
    "    sin = sin.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # Standard RoPE rotation logic\n",
    "    # split last dim into half\n",
    "    q_d = q.shape[-1] // 2\n",
    "    k_d = k.shape[-1] // 2\n",
    "    \n",
    "    q1, q2 = q[..., :q_d], q[..., q_d:]\n",
    "    k1, k2 = k[..., :k_d], k[..., k_d:]\n",
    "    \n",
    "    q_rotated = torch.cat((-q2, q1), dim=-1)\n",
    "    k_rotated = torch.cat((-k2, k1), dim=-1)\n",
    "    \n",
    "    q_out = (q * cos) + (q_rotated * sin)\n",
    "    k_out = (k * cos) + (k_rotated * sin)\n",
    "    \n",
    "    return q_out, k_out\n",
    "\n",
    "class BioMultiHeadAttention(nn.Module):\n",
    "    # mirrors nn.MultiheadAttention(dim, heads, batch_first=True) \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.embed_dim % config.heads == 0\n",
    "        \n",
    "        self.head_dim = config.embed_dim // config.heads\n",
    "        self.heads = config.heads\n",
    "        self.embed_dim = config.embed_dim\n",
    "        \n",
    "        # Projections\n",
    "        self.q_proj = nn.Linear(config.embed_dim, config.embed_dim)\n",
    "        self.k_proj = nn.Linear(config.embed_dim, config.embed_dim)\n",
    "        self.v_proj = nn.Linear(config.embed_dim, config.embed_dim)\n",
    "        \n",
    "        self.c_proj = nn.Linear(config.embed_dim, config.embed_dim)\n",
    "\n",
    "    def forward(self, x, cos, sin):\n",
    "        B, T, C = x.size() # Batch, Seq, Embed Dim\n",
    "        \n",
    "        # 1. Project\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "\n",
    "        # 2. Reshape for multi-head attention\n",
    "        # (B, T, nh, hs) -> (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, T, self.heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # 3. Apply RoPE to Q and K (Rotary is applied per head)\n",
    "        q, k = apply_rotary_pos_emb(q, k, cos, sin)\n",
    "\n",
    "        # 4. Attention\n",
    "        # is_causal=False because this is a bidirectional encoder\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=False)\n",
    "        \n",
    "        # 5. Reassemble\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.c_proj(y)\n",
    "        \n",
    "        return y\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.embed_dim, int(config.mlp_ratio * config.embed_dim))\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "        self.c_proj = nn.Linear(int(config.mlp_ratio * config.embed_dim), config.embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "class PathwayBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.embed_dim)\n",
    "        self.attn = BioMultiHeadAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.embed_dim)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x, cos, sin):\n",
    "        # 1. Attention with RoPE\n",
    "        x_norm = self.ln_1(x)\n",
    "        # Pass cos/sin into attn to be applied to Q/K\n",
    "        attn_out = self.attn(x_norm, cos, sin)\n",
    "        x = x + attn_out\n",
    "\n",
    "        # 2. MLP\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "@dataclass\n",
    "class PathwayEncoderConfig:\n",
    "    num_pathways: int = 1024 \n",
    "    n_layer: int = 24 \n",
    "    heads: int = 12\n",
    "    embed_dim: int = 768\n",
    "    mlp_ratio: float = 4.0 # Changed to float for precision\n",
    "\n",
    "class PathwayEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            # input projects to embedding\n",
    "            input_proj = nn.Linear(1, config.embed_dim),\n",
    "            \n",
    "            # RoPE needs the HEAD dimension, not the full embedding dimension\n",
    "            rope = RotaryEmbedding(config.embed_dim // config.heads),\n",
    "            \n",
    "            # transformer block\n",
    "            blocks = nn.ModuleList([PathwayBlock(config) for _ in range(config.n_layer)]),\n",
    "            \n",
    "            # final layer norm\n",
    "            ln_f = nn.LayerNorm(config.embed_dim) \n",
    "        ))\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        std = 0.02\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [Batch, Num_Pathways]\n",
    "        x = x.unsqueeze(-1) # [B, N, 1]\n",
    "        x = self.transformer.input_proj(x) # [B, N, Dim]\n",
    "        \n",
    "        # Generate RoPE cache\n",
    "        # cos, sin are [Seq, Head_Dim]\n",
    "        cos, sin = self.transformer.rope(x)\n",
    "        cos, sin = cos.to(x.device), sin.to(x.device)\n",
    "        \n",
    "        for block in self.transformer.blocks:\n",
    "            x = block(x, cos, sin)\n",
    "            \n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class AdaLN(nn.Module):\n",
    "    '''\n",
    "    Adaptive Layer Norm for conditioning the predictor on action embeddings.\n",
    "    The action vector regresses the Scale (gamma) and Shift (beta) of the normalization.\n",
    "    '''\n",
    "    def __init__(self, embed_dim, action_embed_dim):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(embed_dim, elementwise_affine=False)\n",
    "        self.action_mlp = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(action_embed_dim, 2 * embed_dim)\n",
    "        )\n",
    "        # Initialize to identity (gamma=0, beta=0 originally, effectively gamma=1 after logic)\n",
    "        # Zero-init the last layer so the action starts as a \"no-op\" (identity)\n",
    "        nn.init.zeros_(self.action_mlp[1].weight)\n",
    "        nn.init.zeros_(self.action_mlp[1].bias)\n",
    "\n",
    "    def forward(self, x, action_emb):\n",
    "        # x: [Batch, Seq, Dim]\n",
    "        # action_emb: [Batch, action_embed_dim]\n",
    "        \n",
    "        # Project action to style: [B, 2*D] -> [B, 1, 2*D]\n",
    "        style = self.action_mlp(action_emb).unsqueeze(1) \n",
    "        gamma, beta = style.chunk(2, dim=-1)\n",
    "        \n",
    "        # Apply affine transformation based on action\n",
    "        return self.norm(x) * (1 + gamma) + beta\n",
    "\n",
    "class PredictorBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # 1. Conditioning (AdaLN) replaces standard LayerNorm\n",
    "        self.ada_ln1 = AdaLN(config.embed_dim, config.action_embed_dim)\n",
    "        \n",
    "        # 2. Attention (Using the shared BioMultiHeadAttention)\n",
    "        self.attn = BioMultiHeadAttention(config)\n",
    "        \n",
    "        # 3. Conditioning (AdaLN) for the MLP block\n",
    "        self.ada_ln2 = AdaLN(config.embed_dim, config.action_embed_dim)\n",
    "        \n",
    "        # 4. MLP (Using the shared MLP)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x, action_emb, cos, sin):\n",
    "        # 1. AdaLN -> Attention (with internal RoPE) -> Residual\n",
    "        x_norm = self.ada_ln1(x, action_emb)\n",
    "        \n",
    "        # Note: BioMultiHeadAttention handles q/k/v projection and apply_rotary_pos_emb internally\n",
    "        attn_out = self.attn(x_norm, cos, sin)\n",
    "        x = x + attn_out\n",
    "        \n",
    "        # 2. AdaLN -> MLP -> Residual\n",
    "        x_norm = self.ada_ln2(x, action_emb)\n",
    "        x = x + self.mlp(x_norm)\n",
    "        \n",
    "        return x\n",
    "\n",
    "@dataclass\n",
    "class ACPredictorConfig:\n",
    "    num_pathways: int = 1024\n",
    "    n_layer: int = 6 \n",
    "    heads: int = 4\n",
    "    embed_dim: int = 384\n",
    "    action_embed_dim: int=256 \n",
    "    mlp_ratio: float = 4.0\n",
    "    max_perturb: int= 2058 ## eventually try to get to a 2**N power\n",
    "\n",
    "class ACPredictor(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Action Embedding (Discrete ID -> Vector)\n",
    "        self.action_embed = nn.Embedding(config.max_perturb, config.action_embed_dim)\n",
    "        \n",
    "        # Learnable Queries (\"Mask Tokens\") for the future state\n",
    "        # One query vector per pathway position\n",
    "        self.mask_queries = nn.Parameter(torch.randn(1, config.num_pathways, config.embed_dim) * 0.02)\n",
    "        \n",
    "        # RoPE: initialized with HEAD dimension (dim // heads)\n",
    "        head_dim = config.embed_dim // config.heads\n",
    "        self.rope = RotaryEmbedding(head_dim)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            PredictorBlock(config) for _ in range(config.n_layer)\n",
    "        ])\n",
    "        \n",
    "        self.final_norm = AdaLN(config.embed_dim, config.action_embed_dim)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        std = 0.02\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "\n",
    "    def forward(self, context_latents, action_ids):\n",
    "        \"\"\"\n",
    "        context_latents: [Batch, N, Dim] (From Student Encoder)\n",
    "        action_ids: [Batch] (Ints)\n",
    "        \"\"\"\n",
    "        B, N, D = context_latents.shape\n",
    "        \n",
    "        # 1. Embed Action\n",
    "        action_emb = self.action_embed(action_ids) # [B, action_embed_dim]\n",
    "        \n",
    "        # 2. Construct Input: [Context, Mask_Queries]\n",
    "        # We concatenate the learned queries to the context. \n",
    "        # The predictor will attend to the context to update the queries.\n",
    "        queries = self.mask_queries.repeat(B, 1, 1) # [B, N, D]\n",
    "        sequence = torch.cat([context_latents, queries], dim=1) # [B, 2N, D]\n",
    "        \n",
    "        # 3. Generate RoPE for the full sequence (2N)\n",
    "        # cos, sin are [2N, Head_Dim]\n",
    "        cos, sin = self.rope(sequence)\n",
    "        cos, sin = cos.to(sequence.device), sin.to(sequence.device)\n",
    "        \n",
    "        # 4. Pass through AdaLN Blocks\n",
    "        for block in self.blocks:\n",
    "            sequence = block(sequence, action_emb, cos, sin)\n",
    "            \n",
    "        sequence = self.final_norm(sequence, action_emb)\n",
    "        \n",
    "        # 5. Return only the predicted part (The Queries corresponding to N..2N)\n",
    "        predictions = sequence[:, N:, :] \n",
    "        return predictions\n",
    "\n",
    "@dataclass\n",
    "class BioJepaConfig:\n",
    "    num_pathways: int = 1024\n",
    "    n_layer: int = 6 \n",
    "    heads: int = 4\n",
    "    embed_dim: int = 256\n",
    "    action_embed_dim: int=256 \n",
    "    mlp_ratio: float = 4.0\n",
    "    max_perturb: int= 2058 ## eventually try to get to a 2**N power\n",
    "    \n",
    "class BioJepa(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.student = PathwayEncoder(PathwayEncoderConfig(\n",
    "            num_pathways = config.num_pathways,\n",
    "            n_layer= config.n_layer, \n",
    "            heads=config.heads, \n",
    "            embed_dim= config.embed_dim\n",
    "        ))\n",
    "                                      \n",
    "        self.teacher = copy.deepcopy(self.student)\n",
    "        \n",
    "        # Freeze teacher\n",
    "        for p in self.teacher.parameters():\n",
    "            p.requires_grad = False\n",
    "            \n",
    "        self.predictor = ACPredictor(ACPredictorConfig(\n",
    "            n_layer=config.n_layer, \n",
    "            heads=config.heads, \n",
    "            embed_dim=config.embed_dim, \n",
    "            action_embed_dim=config.action_embed_dim,\n",
    "            num_pathways=config.num_pathways\n",
    "        ))\n",
    "        \n",
    "    def forward(self, x_control, x_treated, action_id):\n",
    "        # 1. Teacher\n",
    "        with torch.no_grad():\n",
    "            target_latents = self.teacher(x_treated)\n",
    "            \n",
    "        # 2. Student \n",
    "        context_latents = self.student(x_control)\n",
    "        \n",
    "        # 3. Predictor \n",
    "        predicted_latents = self.predictor(context_latents, action_id)\n",
    "        \n",
    "        # 4. Latent Loss (L1)\n",
    "        loss = F.l1_loss(predicted_latents, target_latents)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_teacher(self, m=0.996):\n",
    "        for param_s, param_t in zip(self.student.parameters(), self.teacher.parameters()):\n",
    "            param_t.data.mul_(m).add_((1 - m) * param_s.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8faa224f-49fb-41e9-b47d-e89add8306b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "n_embd = 128\n",
    "n_pathways = 1024\n",
    "LR = 3e-4\n",
    "EPOCHS = 15\n",
    "tok_file_chunk_size = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb26a16e-f58d-4b7c-bed7-b6a97c04008f",
   "metadata": {},
   "source": [
    "## Data Loading and Model instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b95495b3-904b-403b-8a64-52d5f545c915",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('/Users/djemec/data/jepa')\n",
    "checkpoint_dir = data_dir / 'checkpoint'\n",
    "val_dir = data_dir / 'tokenized' / 'val'\n",
    "metadata_path = data_dir / 'perturbation_map.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fd5ca4c-1948-4bd4-ba70-75c38acc1d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BioJepa(BioJepaConfig(\n",
    "    num_pathways=n_pathways,\n",
    "    embed_dim = n_embd\n",
    ")).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3daf4c-b2f2-4876-9a38-b6222cd86b61",
   "metadata": {},
   "source": [
    "## Checkpoint Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2966b875-baa0-4064-8281-98a74c5b510c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load Checkpoint\n",
    "# Your script saves a dict: {'model': ..., 'optimizer': ..., 'step': ...}\n",
    "checkpoint_path = checkpoint_dir / 'bio_jepa_ckpt_17550_final.pt'\n",
    "checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "\n",
    "keys = model.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bde6936-d032-43c3-814f-7ed6ff7e8129",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(metadata_path, \"r\") as f:\n",
    "    pert_map = json.load(f)\n",
    "# Invert map to: ID -> Name\n",
    "id_to_name = {v: k for k, v in pert_map.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2c2827-7a77-4f0b-a6cd-50a7f5e84dee",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb294ebe-bddd-45c4-bedb-b382469f9722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/Users/djemec/data/jepa/tokenized/val/shard_0000.npz')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_file = sorted(val_dir.glob('*.npz'))\n",
    "val_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "30558a62-ad16-42f5-a04e-76c87a5c85da",
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load(val_file[0]) as data:\n",
    "    # Extract raw uint32\n",
    "    all_control_raw = data['control']\n",
    "    all_case_raw = data['case']\n",
    "    all_act_id = data['action_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "623d01ea-cb8d-43a7-9058-678109f07e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_to_validate = 100000\n",
    "validation = {}\n",
    "total_examples = len(all_act_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "40ca4805-882b-403b-839a-efcb8234ed9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0\n",
      "for id 3390, perturbation STAT5A prediction is 0.4% closer to the truth than the Control state was.\n",
      "step 10000\n",
      "for id 1648, perturbation EIF4G1 prediction is 0.3% closer to the truth than the Control state was.\n",
      "step 20000\n",
      "for id 9874, perturbation TRAPPC4 prediction is 0.1% closer to the truth than the Control state was.\n",
      "step 30000\n",
      "for id 9336, perturbation PLEKHN1 prediction is 0.3% closer to the truth than the Control state was.\n",
      "step 40000\n",
      "for id 1333, perturbation CDC16 prediction is 0.1% closer to the truth than the Control state was.\n",
      "step 50000\n",
      "for id 3593, perturbation CPSF3 prediction is 0.1% closer to the truth than the Control state was.\n",
      "step 60000\n",
      "for id 6132, perturbation HSPA9 prediction is 0.2% closer to the truth than the Control state was.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# run teacher\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     z_control = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mteacher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_control\u001b[49m\u001b[43m)\u001b[49m       \u001b[38;5;66;03m# Where the cell started\u001b[39;00m\n\u001b[32m     18\u001b[39m     z_case = model.teacher(x_case)     \u001b[38;5;66;03m# Where the cell actually went\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# run student and predictor\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/general/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/general/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 167\u001b[39m, in \u001b[36mPathwayEncoder.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    164\u001b[39m cos, sin = cos.to(x.device), sin.to(x.device)\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transformer.blocks:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     x = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m x = \u001b[38;5;28mself\u001b[39m.transformer.ln_f(x)\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/general/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/general/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 111\u001b[39m, in \u001b[36mPathwayBlock.forward\u001b[39m\u001b[34m(self, x, cos, sin)\u001b[39m\n\u001b[32m    109\u001b[39m x_norm = \u001b[38;5;28mself\u001b[39m.ln_1(x)\n\u001b[32m    110\u001b[39m \u001b[38;5;66;03m# Pass cos/sin into attn to be applied to Q/K\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m attn_out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m x = x + attn_out\n\u001b[32m    114\u001b[39m \u001b[38;5;66;03m# 2. MLP\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/general/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/general/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 64\u001b[39m, in \u001b[36mBioMultiHeadAttention.forward\u001b[39m\u001b[34m(self, x, cos, sin)\u001b[39m\n\u001b[32m     62\u001b[39m q = \u001b[38;5;28mself\u001b[39m.q_proj(x)\n\u001b[32m     63\u001b[39m k = \u001b[38;5;28mself\u001b[39m.k_proj(x)\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m v = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mv_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# 2. Reshape for multi-head attention\u001b[39;00m\n\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# (B, T, nh, hs) -> (B, nh, T, hs)\u001b[39;00m\n\u001b[32m     68\u001b[39m q = q.view(B, T, \u001b[38;5;28mself\u001b[39m.heads, \u001b[38;5;28mself\u001b[39m.head_dim).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/general/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/general/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/general/lib/python3.13/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "for i in range(examples_to_validate):\n",
    "    # pick a random index out \n",
    "    idx = np.random.randint(total_examples)\n",
    "\n",
    "    # extract inputs for the index\n",
    "    x_control = torch.tensor(all_control_raw[idx].astype(np.float32)).unsqueeze(0).to(DEVICE) # [1, 1024]\n",
    "    x_case = torch.tensor(all_case_raw[idx].astype(np.float32)).unsqueeze(0).to(DEVICE)\n",
    "    action_id = torch.tensor([all_act_id[idx]], dtype=torch.long).to(DEVICE)\n",
    "\n",
    "    # get perturbation name\n",
    "    pert_name = id_to_name[action_id.item()]\n",
    "\n",
    "    # run teacher\n",
    "    with torch.no_grad():\n",
    "        z_control = model.teacher(x_control)       # Where the cell started\n",
    "        z_case = model.teacher(x_case)     # Where the cell actually went\n",
    "\n",
    "    # run student and predictor\n",
    "    with torch.no_grad():\n",
    "        z_context = model.student(x_control)\n",
    "        z_predicted = model.predictor(z_context, action_id) # Where the model thinks it went\n",
    "\n",
    "    drift = F.l1_loss(z_control, z_case).item()\n",
    "    error = F.l1_loss(z_predicted, z_case).item()\n",
    "    sim_move = F.l1_loss(z_control,z_predicted).item()\n",
    "\n",
    "    improvement = (1 - (error / drift))\n",
    "    if i % int(examples_to_validate*.1) == 0:\n",
    "        print(f'step {i}')\n",
    "        #print(f'drift {drift} | error: {error} | sim_move {sim_move}')\n",
    "        print(f'for id {idx}, perturbation {pert_name} prediction is {improvement:.1f}% closer to the truth than the Control state was.')\n",
    "\n",
    "    validation[idx] = {\n",
    "        'perturbation': pert_name,\n",
    "        'error': error,\n",
    "        'drift': drift,\n",
    "        'improvement': improvement,\n",
    "        'sim_move': sim_move\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8c93aef6-4f21-434d-907d-c7f851d2129a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.000e+00, 0.000e+00, 1.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        2.000e+00, 0.000e+00, 0.000e+00, 1.000e+00, 1.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 1.000e+00, 0.000e+00, 5.000e+00, 4.000e+00,\n",
       "        2.000e+00, 4.000e+00, 3.000e+00, 1.100e+01, 7.000e+00, 1.400e+01,\n",
       "        1.300e+01, 2.300e+01, 2.800e+01, 3.700e+01, 6.100e+01, 6.600e+01,\n",
       "        1.200e+02, 1.500e+02, 2.890e+02, 4.410e+02, 6.930e+02, 8.220e+02,\n",
       "        9.510e+02, 1.083e+03, 1.011e+03, 9.070e+02, 7.850e+02, 6.950e+02,\n",
       "        5.370e+02, 4.420e+02, 3.090e+02, 2.160e+02, 1.250e+02, 8.100e+01,\n",
       "        2.900e+01, 9.000e+00]),\n",
       " array([-1.19290611, -1.15580531, -1.1187045 , -1.0816037 , -1.04450289,\n",
       "        -1.00740209, -0.97030128, -0.93320048, -0.89609968, -0.85899887,\n",
       "        -0.82189807, -0.78479726, -0.74769646, -0.71059565, -0.67349485,\n",
       "        -0.63639404, -0.59929324, -0.56219244, -0.52509163, -0.48799083,\n",
       "        -0.45089002, -0.41378922, -0.37668841, -0.33958761, -0.30248681,\n",
       "        -0.265386  , -0.2282852 , -0.19118439, -0.15408359, -0.11698278,\n",
       "        -0.07988198, -0.04278117, -0.00568037,  0.03142043,  0.06852124,\n",
       "         0.10562204,  0.14272285,  0.17982365,  0.21692446,  0.25402526,\n",
       "         0.29112606,  0.32822687,  0.36532767,  0.40242848,  0.43952928,\n",
       "         0.47663009,  0.51373089,  0.55083169,  0.5879325 ,  0.6250333 ,\n",
       "         0.66213411]),\n",
       " <BarContainer object of 50 artists>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_improves = [entry['improvement'] for entry in validation.values()]\n",
    "plt.hist(all_improves, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "199886ab-9c83-46ed-b60d-a1ef6633724f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 13., 139., 345., 650., 826., 867., 851., 747., 654., 589., 486.,\n",
       "        431., 353., 318., 269., 223., 192., 191., 170., 145., 135., 128.,\n",
       "        118.,  90.,  92.,  93.,  80.,  72.,  69.,  57.,  71.,  50.,  60.,\n",
       "         52.,  67.,  57.,  39.,  41.,  35.,  30.,  29.,  19.,  11.,  11.,\n",
       "          5.,   5.,   1.,   2.,   1.,   2.]),\n",
       " array([0.14732005, 0.16420249, 0.18108493, 0.19796737, 0.21484981,\n",
       "        0.23173225, 0.24861469, 0.26549713, 0.28237957, 0.29926201,\n",
       "        0.31614445, 0.3330269 , 0.34990934, 0.36679178, 0.38367422,\n",
       "        0.40055666, 0.4174391 , 0.43432154, 0.45120398, 0.46808642,\n",
       "        0.48496886, 0.5018513 , 0.51873374, 0.53561618, 0.55249862,\n",
       "        0.56938107, 0.58626351, 0.60314595, 0.62002839, 0.63691083,\n",
       "        0.65379327, 0.67067571, 0.68755815, 0.70444059, 0.72132303,\n",
       "        0.73820547, 0.75508791, 0.77197035, 0.7888528 , 0.80573524,\n",
       "        0.82261768, 0.83950012, 0.85638256, 0.873265  , 0.89014744,\n",
       "        0.90702988, 0.92391232, 0.94079476, 0.9576772 , 0.97455964,\n",
       "        0.99144208]),\n",
       " <BarContainer object of 50 artists>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_error = [entry['error'] for entry in validation.values()]\n",
    "plt.hist(all_error, bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccb04dc-2a5c-4aaa-ba0d-a1afc0681d92",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
