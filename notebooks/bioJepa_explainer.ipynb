{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cedd2e3-bcee-4a58-abcf-62858c07bd0f",
   "metadata": {},
   "source": [
    "# Bio-JEPA AC Explainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ba0ded7-81b3-4afd-819b-299f9db18255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import copy\n",
    "import math\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cbf54b7-a62d-4cf0-9e81-ef07aec56ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54ca0e60-1213-4640-a688-bb7377085320",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('/Users/djemec/data/jepa')\n",
    "shard_dir = data_dir / 'tokenized'\n",
    "metadata_path = data_dir / 'perturbation_map.json'\n",
    "checkpoint_dir = data_dir / 'checkpoint'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8e09de-cb6f-4922-a39d-32c9ef5e9383",
   "metadata": {},
   "source": [
    "## Text Prep/Tokenization\n",
    "\n",
    "We previously loaded and saved our data into a tokenized files.  For a full explainer, look at the data_prep_explainer.ipynb notebook.   In this step we'll pull the data from the files to use for training.  First we'll start by getting all the files in the tokenized directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "543054b1-a33c-4ccf-a20a-f1c5d783aad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30 shards.\n"
     ]
    }
   ],
   "source": [
    "data_files = sorted(shard_dir.glob('*.npz'))\n",
    "print(f'Found {len(data_files)} shards.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba37f0b-d520-485e-9495-077e03653d4e",
   "metadata": {},
   "source": [
    "Now that we have the files, we need just a single file. Because of how we tokanized our files, we do not have to load them in any particular order so we'll select a random file from the list.  In training, we'd want to remove this from the file list to ensure we go through the full epoch before restarting.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efe3192c-11ee-47b7-8631-fee7be5d32c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, PosixPath('/Users/djemec/data/jepa/tokenized/shard_0015.npz'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_file_idx = np.random.randint(len(data_files))\n",
    "file_path = data_files[random_file_idx]\n",
    "random_file_idx, file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952899be-68ad-4e37-8a9b-5b662d1d4482",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631425fd-ae6d-4b97-82cd-f307002812a2",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1043fd-c1c0-4e2f-a591-ac23bec2630f",
   "metadata": {},
   "source": [
    "\n",
    "To start, we need to get enough data to run the forward and backward passes.  Since our total dataset is likely too big to be held in memory all at once in real practice, we will read just enough file information into memory so that we can run the passes, leaving memory and compute to be used on the passes instead of static data holding. \n",
    "\n",
    "Recall that we saved the data in the following structure:\n",
    "\n",
    "```\n",
    "{\n",
    "    'control':[array of network expression information],\n",
    "    'case':[array of network expression information],\n",
    "    'action_ids':[array of ids of the perturbation]\n",
    "}\n",
    "```\n",
    "\n",
    "Because of this, taking the same index out of each array gives us our action_id, pertrubed data, and control data. For training we'll do more than 1 cell at a time.  We'll use a variable `BATCH_SIZE` to control how much data to train on at a time.  In this case we'll start with 2. During training we'll want a increase our batch size.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "687c858b-51ee-4b9d-8b11-4aeaf3389cda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1391, 2491]),\n",
       " tensor([1045,  802]),\n",
       " tensor([[0.7370, 0.4793, 0.5609,  ..., 0.4051, 0.2653, 0.4966],\n",
       "         [0.8382, 0.6693, 0.5777,  ..., 0.4235, 0.3463, 0.5699]]),\n",
       " tensor([[0.6636, 0.5720, 0.5646,  ..., 0.2728, 0.2980, 0.5147],\n",
       "         [0.7203, 0.6128, 0.6233,  ..., 0.3658, 0.2489, 0.5796]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 2\n",
    "with np.load(file_path) as data:\n",
    "    # Keys: 'control', 'treated', 'action_ids'\n",
    "    n_rows = data['action_ids'].shape[0]\n",
    "    row_idx = np.random.randint(n_rows, size=BATCH_SIZE)\n",
    "    \n",
    "    control_raw = data['control'][row_idx]\n",
    "    case_raw = data['case'][row_idx]\n",
    "    act_id = data['action_ids'][row_idx]\n",
    "\n",
    "x_control = torch.tensor(control_raw)\n",
    "x_case = torch.tensor(case_raw)\n",
    "action_id = torch.tensor(act_id, dtype=torch.long)\n",
    "\n",
    "row_idx, action_id, x_control, x_case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43148f73-be59-4bc8-9b53-3e9eca075646",
   "metadata": {},
   "source": [
    "## Forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3945b1b1-92c4-4be9-9554-2749e0f39367",
   "metadata": {},
   "source": [
    "![network_impact_math](../resources/biojepa_v1.png)\n",
    "\n",
    "Our current architecture has 3 models:  \n",
    "\n",
    "1. **The Student** - starts from the control cell and learns how to create a latent embedding of the cell state. \n",
    "2. **The Predictor** - learns how to adjust the student's embedded cell state based on different perturbations to hallucinate the future diseased state in the latent space shared with the teacher. \n",
    "3. **The Teacher** - starts from the actual perturbed cell and creates a latent space embedding for the pertured state to grade the predictors output against.\n",
    "\n",
    "\n",
    "We'll start by building the Pre-Norm Transformer Encoder block with Rotary Positional Embeddings (RoPE) that are the basis for both the Teacher and Student models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739cbb8b-5abb-4de4-9753-785e6fabc05f",
   "metadata": {},
   "source": [
    "### Teacher/Student Setup\n",
    "**RoPE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ae7b27e-170e-4c31-89da-701fecff22ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_seq_len=2048):\n",
    "        super().__init__()\n",
    "        # Ensure dim is the head dimension, not the full embedding dimension\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        t = torch.arange(max_seq_len).type_as(inv_freq)\n",
    "        freqs = torch.outer(t, inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.register_buffer('emb', emb)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [Batch, Seq, Dim]\n",
    "        # We only care about the sequence length here\n",
    "        n = x.shape[1]\n",
    "        # Returns [Seq, Head_Dim]\n",
    "        return self.emb[:n, :].cos(), self.emb[:n, :].sin()\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin):\n",
    "    # q, k: [Batch, Heads, Seq, Head_Dim]\n",
    "    # cos, sin: [Seq, Head_Dim] -> reshape to [1, 1, Seq, Head_Dim]\n",
    "    \n",
    "    cos = cos.unsqueeze(0).unsqueeze(0)\n",
    "    sin = sin.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # Standard RoPE rotation logic\n",
    "    # split last dim into half\n",
    "    q_d = q.shape[-1] // 2\n",
    "    k_d = k.shape[-1] // 2\n",
    "    \n",
    "    q1, q2 = q[..., :q_d], q[..., q_d:]\n",
    "    k1, k2 = k[..., :k_d], k[..., k_d:]\n",
    "    \n",
    "    q_rotated = torch.cat((-q2, q1), dim=-1)\n",
    "    k_rotated = torch.cat((-k2, k1), dim=-1)\n",
    "    \n",
    "    q_out = (q * cos) + (q_rotated * sin)\n",
    "    k_out = (k * cos) + (k_rotated * sin)\n",
    "    \n",
    "    return q_out, k_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993c4bd8-bfac-46ac-ab69-a2da56dccf6f",
   "metadata": {},
   "source": [
    "**Multi-Headed Attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41791397-532a-4572-bfb2-f1a08bfbc62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioMultiHeadAttention(nn.Module):\n",
    "    # mirrors nn.MultiheadAttention(dim, heads, batch_first=True) \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.embed_dim % config.heads == 0\n",
    "        \n",
    "        self.head_dim = config.embed_dim // config.heads\n",
    "        self.heads = config.heads\n",
    "        self.embed_dim = config.embed_dim\n",
    "        \n",
    "        # Projections\n",
    "        self.q_proj = nn.Linear(config.embed_dim, config.embed_dim)\n",
    "        self.k_proj = nn.Linear(config.embed_dim, config.embed_dim)\n",
    "        self.v_proj = nn.Linear(config.embed_dim, config.embed_dim)\n",
    "        \n",
    "        self.c_proj = nn.Linear(config.embed_dim, config.embed_dim)\n",
    "\n",
    "    def forward(self, x, cos, sin):\n",
    "        B, T, C = x.size() # Batch, Seq, Embed Dim\n",
    "        \n",
    "        # 1. Project\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "\n",
    "        # 2. Reshape for multi-head attention\n",
    "        # (B, T, nh, hs) -> (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, T, self.heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # 3. Apply RoPE to Q and K (Rotary is applied per head)\n",
    "        q, k = apply_rotary_pos_emb(q, k, cos, sin)\n",
    "\n",
    "        # 4. Attention\n",
    "        # is_causal=False because this is a bidirectional encoder\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=False)\n",
    "        \n",
    "        # 5. Reassemble\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.c_proj(y)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e754b8e-8536-44da-84cd-05f617f34af0",
   "metadata": {},
   "source": [
    "**MLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aeb98e8a-868c-4f8f-991c-84cdc4e887d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.embed_dim, int(config.mlp_ratio * config.embed_dim))\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "        self.c_proj = nn.Linear(int(config.mlp_ratio * config.embed_dim), config.embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0306eb8-042e-4dda-bc35-99a73d1e1ffb",
   "metadata": {},
   "source": [
    "**Hidden Transformer Block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90042ed3-5020-49be-8914-568d25a605c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PathwayBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.embed_dim)\n",
    "        self.attn = BioMultiHeadAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.embed_dim)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x, cos, sin):\n",
    "        # 1. Attention with RoPE\n",
    "        x_norm = self.ln_1(x)\n",
    "        # Pass cos/sin into attn to be applied to Q/K\n",
    "        attn_out = self.attn(x_norm, cos, sin)\n",
    "        x = x + attn_out\n",
    "\n",
    "        # 2. MLP\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f6508f-895c-45c9-ace7-04f3fbda30c5",
   "metadata": {},
   "source": [
    "**Main Pathway Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2151f336-a980-4466-a2b4-2b342f701e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PathwayEncoderConfig:\n",
    "    num_pathways: int = 1024 \n",
    "    n_layer: int = 24 \n",
    "    heads: int = 12\n",
    "    embed_dim: int = 768\n",
    "    mlp_ratio: float = 4.0 # Changed to float for precision\n",
    "\n",
    "class PathwayEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            # input projects to embedding\n",
    "            input_proj = nn.Linear(1, config.embed_dim),\n",
    "            \n",
    "            # RoPE needs the HEAD dimension, not the full embedding dimension\n",
    "            rope = RotaryEmbedding(config.embed_dim // config.heads),\n",
    "            \n",
    "            # transformer block\n",
    "            blocks = nn.ModuleList([PathwayBlock(config) for _ in range(config.n_layer)]),\n",
    "            \n",
    "            # final layer norm\n",
    "            ln_f = nn.LayerNorm(config.embed_dim) \n",
    "        ))\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        std = 0.02\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [Batch, Num_Pathways]\n",
    "        x = x.unsqueeze(-1) # [B, N, 1]\n",
    "        x = self.transformer.input_proj(x) # [B, N, Dim]\n",
    "        \n",
    "        # Generate RoPE cache\n",
    "        # cos, sin are [Seq, Head_Dim]\n",
    "        cos, sin = self.transformer.rope(x)\n",
    "        cos, sin = cos.to(x.device), sin.to(x.device)\n",
    "        \n",
    "        for block in self.transformer.blocks:\n",
    "            x = block(x, cos, sin)\n",
    "            \n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02752d4-46e3-4558-8468-d2e7843ba3ff",
   "metadata": {},
   "source": [
    "### Create The Student And Teacher Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "caa8f876-43cf-496f-a487-54da0f38f915",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 4\n",
    "n_heads = 2\n",
    "n_pathways = 1024\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f56c881d-e7be-428e-a09f-4da8f3b4135b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PathwayEncoder(\n",
       "  (transformer): ModuleDict(\n",
       "    (input_proj): Linear(in_features=1, out_features=4, bias=True)\n",
       "    (rope): RotaryEmbedding()\n",
       "    (blocks): ModuleList(\n",
       "      (0): PathwayBlock(\n",
       "        (ln_1): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): BioMultiHeadAttention(\n",
       "          (q_proj): Linear(in_features=4, out_features=4, bias=True)\n",
       "          (k_proj): Linear(in_features=4, out_features=4, bias=True)\n",
       "          (v_proj): Linear(in_features=4, out_features=4, bias=True)\n",
       "          (c_proj): Linear(in_features=4, out_features=4, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=4, out_features=16, bias=True)\n",
       "          (gelu): GELU(approximate='tanh')\n",
       "          (c_proj): Linear(in_features=16, out_features=4, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student = PathwayEncoder(PathwayEncoderConfig(n_layer= 1, heads=n_heads, embed_dim= n_embd))\n",
    "student"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09134ca7-76b4-47fd-a0b6-e9500da152f5",
   "metadata": {},
   "source": [
    "**Teacher** Now let's make a copy for the teacher. Since the teacher does not require updates, we can update the model to not require gradient updates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a257b3da-5334-46d2-9715-b3ec001efc56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PathwayEncoder(\n",
       "  (transformer): ModuleDict(\n",
       "    (input_proj): Linear(in_features=1, out_features=4, bias=True)\n",
       "    (rope): RotaryEmbedding()\n",
       "    (blocks): ModuleList(\n",
       "      (0): PathwayBlock(\n",
       "        (ln_1): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): BioMultiHeadAttention(\n",
       "          (q_proj): Linear(in_features=4, out_features=4, bias=True)\n",
       "          (k_proj): Linear(in_features=4, out_features=4, bias=True)\n",
       "          (v_proj): Linear(in_features=4, out_features=4, bias=True)\n",
       "          (c_proj): Linear(in_features=4, out_features=4, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=4, out_features=16, bias=True)\n",
       "          (gelu): GELU(approximate='tanh')\n",
       "          (c_proj): Linear(in_features=16, out_features=4, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher = copy.deepcopy(student)\n",
    "teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b34811d-ae3f-4d9a-97f9-b3d559a13191",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in teacher.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c22025e-cf3b-447d-a219-7562d1d5b71f",
   "metadata": {},
   "source": [
    "### Forward Pass: Teacher\n",
    "Teacher encodes the Target (Case). We'll make sure to run it without gradient accumulation.  This teacher creates an embedding of the case cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c02b1af1-fbbb-4533-af8d-8792d97347d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.7027,  0.8507, -1.1759, -0.3774],\n",
       "         [ 0.6592,  0.8491, -1.0908, -0.4175],\n",
       "         [ 0.6550,  0.8488, -1.0831, -0.4207],\n",
       "         ...,\n",
       "         [ 0.3765,  0.7773, -0.6491, -0.5047],\n",
       "         [ 0.4124,  0.7902, -0.6988, -0.5038],\n",
       "         [ 0.6244,  0.8451, -1.0278, -0.4418]],\n",
       "\n",
       "        [[ 0.7226,  0.8514, -1.2200, -0.3541],\n",
       "         [ 0.6788,  0.8525, -1.1302, -0.4011],\n",
       "         [ 0.6838,  0.8526, -1.1400, -0.3964],\n",
       "         ...,\n",
       "         [ 0.4938,  0.8193, -0.8186, -0.4945],\n",
       "         [ 0.3378,  0.7663, -0.5985, -0.5057],\n",
       "         [ 0.6618,  0.8516, -1.0978, -0.4156]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    target_latents = teacher(x_case)\n",
    "target_latents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1e472d-8fab-4cc5-b9c1-960e233b2615",
   "metadata": {},
   "source": [
    "### Forward Pass: Student\n",
    "Student encodes the Context (Control), With the student we need to accumulate gradients since we need to be able to train the model. \n",
    "> TO DO: Add masking here if you want extra difficulty, but the task Control->Treated is enough for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "143c7081-8409-4d8f-af6f-c22b05f04454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.7284,  0.8506, -1.2322, -0.3467],\n",
       "         [ 0.5979,  0.8431, -0.9838, -0.4573],\n",
       "         [ 0.6517,  0.8504, -1.0786, -0.4235],\n",
       "         ...,\n",
       "         [ 0.5346,  0.8296, -0.8807, -0.4835],\n",
       "         [ 0.3636,  0.7756, -0.6330, -0.5063],\n",
       "         [ 0.6106,  0.8452, -1.0054, -0.4504]],\n",
       "\n",
       "        [[ 0.7536,  0.8499, -1.2958, -0.3077],\n",
       "         [ 0.7005,  0.8572, -1.1785, -0.3792],\n",
       "         [ 0.6573,  0.8565, -1.0942, -0.4197],\n",
       "         ...,\n",
       "         [ 0.5476,  0.8399, -0.9058, -0.4816],\n",
       "         [ 0.4675,  0.8191, -0.7837, -0.5030],\n",
       "         [ 0.6530,  0.8562, -1.0861, -0.4231]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_latents = student(x_control)\n",
    "context_latents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f70a5f8-0f52-402e-8bd9-eeac1074f912",
   "metadata": {},
   "source": [
    "## Predictor Setup\n",
    "We need to create a predictor that tries to guess Target (from the teacher) given Context + Action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bb2ed4-6561-4b8e-9cb5-4bf857ddad91",
   "metadata": {},
   "source": [
    "**Adaptive Layer Normalization AdaLN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77dc0a71-29c4-423d-ba89-9595cdac0bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaLN(nn.Module):\n",
    "    '''\n",
    "    Adaptive Layer Norm for conditioning the predictor on action embeddings.\n",
    "    The action vector regresses the Scale (gamma) and Shift (beta) of the normalization.\n",
    "    '''\n",
    "    def __init__(self, embed_dim, action_embed_dim):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(embed_dim, elementwise_affine=False)\n",
    "        self.action_mlp = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(action_embed_dim, 2 * embed_dim)\n",
    "        )\n",
    "        # Initialize to identity (gamma=0, beta=0 originally, effectively gamma=1 after logic)\n",
    "        # Zero-init the last layer so the action starts as a \"no-op\" (identity)\n",
    "        nn.init.zeros_(self.action_mlp[1].weight)\n",
    "        nn.init.zeros_(self.action_mlp[1].bias)\n",
    "\n",
    "    def forward(self, x, action_emb):\n",
    "        # x: [Batch, Seq, Dim]\n",
    "        # action_emb: [Batch, action_embed_dim]\n",
    "        \n",
    "        # Project action to style: [B, 2*D] -> [B, 1, 2*D]\n",
    "        style = self.action_mlp(action_emb).unsqueeze(1) \n",
    "        gamma, beta = style.chunk(2, dim=-1)\n",
    "        \n",
    "        # Apply affine transformation based on action\n",
    "        return self.norm(x) * (1 + gamma) + beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93608ce7-5ba4-488e-849e-05c298bfc50b",
   "metadata": {},
   "source": [
    "**Predictor Block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "009a430a-202c-4eff-974e-d5dfc9beca30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictorBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # 1. Conditioning (AdaLN) replaces standard LayerNorm\n",
    "        self.ada_ln1 = AdaLN(config.embed_dim, config.action_embed_dim)\n",
    "        \n",
    "        # 2. Attention (Using the shared BioMultiHeadAttention)\n",
    "        self.attn = BioMultiHeadAttention(config)\n",
    "        \n",
    "        # 3. Conditioning (AdaLN) for the MLP block\n",
    "        self.ada_ln2 = AdaLN(config.embed_dim, config.action_embed_dim)\n",
    "        \n",
    "        # 4. MLP (Using the shared MLP)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x, action_emb, cos, sin):\n",
    "        # 1. AdaLN -> Attention (with internal RoPE) -> Residual\n",
    "        x_norm = self.ada_ln1(x, action_emb)\n",
    "        \n",
    "        # Note: BioMultiHeadAttention handles q/k/v projection and apply_rotary_pos_emb internally\n",
    "        attn_out = self.attn(x_norm, cos, sin)\n",
    "        x = x + attn_out\n",
    "        \n",
    "        # 2. AdaLN -> MLP -> Residual\n",
    "        x_norm = self.ada_ln2(x, action_emb)\n",
    "        x = x + self.mlp(x_norm)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27332d23-4dd4-4c4a-b8a2-24438e1bca6f",
   "metadata": {},
   "source": [
    "**Main Predictor Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d4e7df9-10e8-4e35-bc69-623f5b4dae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ACPredictorConfig:\n",
    "    num_pathways: int = 1024\n",
    "    n_layer: int = 6 \n",
    "    heads: int = 4\n",
    "    embed_dim: int = 384\n",
    "    action_embed_dim: int=256 \n",
    "    mlp_ratio: float = 4.0\n",
    "    max_perturb: int= 2058 ## eventually try to get to a 2**N power\n",
    "\n",
    "class ACPredictor(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Action Embedding (Discrete ID -> Vector)\n",
    "        self.action_embed = nn.Embedding(config.max_perturb, config.action_embed_dim)\n",
    "        \n",
    "        # Learnable Queries (\"Mask Tokens\") for the future state\n",
    "        # One query vector per pathway position\n",
    "        self.mask_queries = nn.Parameter(torch.randn(1, config.num_pathways, config.embed_dim) * 0.02)\n",
    "        \n",
    "        # RoPE: initialized with HEAD dimension (dim // heads)\n",
    "        head_dim = config.embed_dim // config.heads\n",
    "        self.rope = RotaryEmbedding(head_dim)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            PredictorBlock(config) for _ in range(config.n_layer)\n",
    "        ])\n",
    "        \n",
    "        self.final_norm = AdaLN(config.embed_dim, config.action_embed_dim)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        std = 0.02\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "\n",
    "    def forward(self, context_latents, action_ids):\n",
    "        \"\"\"\n",
    "        context_latents: [Batch, N, Dim] (From Student Encoder)\n",
    "        action_ids: [Batch] (Ints)\n",
    "        \"\"\"\n",
    "        B, N, D = context_latents.shape\n",
    "        \n",
    "        # 1. Embed Action\n",
    "        action_emb = self.action_embed(action_ids) # [B, action_embed_dim]\n",
    "        \n",
    "        # 2. Construct Input: [Context, Mask_Queries]\n",
    "        # We concatenate the learned queries to the context. \n",
    "        # The predictor will attend to the context to update the queries.\n",
    "        queries = self.mask_queries.repeat(B, 1, 1) # [B, N, D]\n",
    "        sequence = torch.cat([context_latents, queries], dim=1) # [B, 2N, D]\n",
    "        \n",
    "        # 3. Generate RoPE for the full sequence (2N)\n",
    "        # cos, sin are [2N, Head_Dim]\n",
    "        cos, sin = self.rope(sequence)\n",
    "        cos, sin = cos.to(sequence.device), sin.to(sequence.device)\n",
    "        \n",
    "        # 4. Pass through AdaLN Blocks\n",
    "        for block in self.blocks:\n",
    "            sequence = block(sequence, action_emb, cos, sin)\n",
    "            \n",
    "        sequence = self.final_norm(sequence, action_emb)\n",
    "        \n",
    "        # 5. Return only the predicted part (The Queries corresponding to N..2N)\n",
    "        predictions = sequence[:, N:, :] \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a087de0-e0a3-49bf-b09e-b917d492eb5c",
   "metadata": {},
   "source": [
    "### Create the Predictor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7f5531a-f3c3-4472-b638-5a3a17d18322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ACPredictor(\n",
       "  (action_embed): Embedding(2058, 4)\n",
       "  (rope): RotaryEmbedding()\n",
       "  (blocks): ModuleList(\n",
       "    (0): PredictorBlock(\n",
       "      (ada_ln1): AdaLN(\n",
       "        (norm): LayerNorm((4,), eps=1e-05, elementwise_affine=False)\n",
       "        (action_mlp): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=4, out_features=8, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (attn): BioMultiHeadAttention(\n",
       "        (q_proj): Linear(in_features=4, out_features=4, bias=True)\n",
       "        (k_proj): Linear(in_features=4, out_features=4, bias=True)\n",
       "        (v_proj): Linear(in_features=4, out_features=4, bias=True)\n",
       "        (c_proj): Linear(in_features=4, out_features=4, bias=True)\n",
       "      )\n",
       "      (ada_ln2): AdaLN(\n",
       "        (norm): LayerNorm((4,), eps=1e-05, elementwise_affine=False)\n",
       "        (action_mlp): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=4, out_features=8, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Linear(in_features=4, out_features=16, bias=True)\n",
       "        (gelu): GELU(approximate='tanh')\n",
       "        (c_proj): Linear(in_features=16, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_norm): AdaLN(\n",
       "    (norm): LayerNorm((4,), eps=1e-05, elementwise_affine=False)\n",
       "    (action_mlp): Sequential(\n",
       "      (0): SiLU()\n",
       "      (1): Linear(in_features=4, out_features=8, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor = ACPredictor(ACPredictorConfig(n_layer=1, heads=n_heads, embed_dim=n_embd, action_embed_dim=n_embd))\n",
    "predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e2841b-32fa-4d41-b7c2-115723f24903",
   "metadata": {},
   "source": [
    "### Forward Pass: Action Predictor\n",
    "The Action predictor tries to guess Target encoding given Context + Action.  The context is the ouput of the Student and the action is from our tokenized data.  The goal here is tha the output attempts to match the teacher. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd8dd03a-f9ef-45e9-910b-42ee77ab28cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1045,  802])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ccf6c1ff-330c-4f46-abca-35571ef07f91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.3139, -0.4016,  0.3994,  1.3179],\n",
       "         [ 1.5855, -0.7184, -0.8062, -0.0598],\n",
       "         [-0.3741,  1.6877, -0.8541, -0.4585],\n",
       "         ...,\n",
       "         [ 0.4311,  1.2780, -1.1620, -0.5462],\n",
       "         [-1.5899, -0.0922,  0.7758,  0.9081],\n",
       "         [ 1.3576, -0.6968, -1.1525,  0.4929]],\n",
       "\n",
       "        [[-1.3131, -0.4038,  0.4005,  1.3167],\n",
       "         [ 1.5844, -0.7207, -0.8036, -0.0603],\n",
       "         [-0.3742,  1.6872, -0.8518, -0.4595],\n",
       "         ...,\n",
       "         [ 0.4309,  1.2770, -1.1594, -0.5471],\n",
       "         [-1.5892, -0.0939,  0.7765,  0.9069],\n",
       "         [ 1.3567, -0.6989, -1.1497,  0.4921]]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_latents = predictor(context_latents, action_id)\n",
    "predicted_latents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb98c36-d1ad-42d4-8e61-2465bdedbd4d",
   "metadata": {},
   "source": [
    "# Loss Calculation\n",
    "\n",
    "We'll use L1 Loss (Mean Absolute Error) which measures the average absolute difference between predicted and actual values, making it robust to outliers.  We'll call this latent loss since we'll  use an L1 loss function within the latent space of a model to ensure the compressed representation (latent) accurately reflects the original data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09d24c1a-523a-4bae-b8ff-630df3b563f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0196, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = F.l1_loss(predicted_latents, target_latents)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3633ff-5398-4928-a40e-8133d2eaaad1",
   "metadata": {},
   "source": [
    "# Back Propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2abb5a2e-5d83-41b6-82a5-25122a59e4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c951691-68de-4246-9d0b-8a9f359f67ae",
   "metadata": {},
   "source": [
    "## Gradient Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "017d9b0a-fac7-4d10-a247-fece2a39732d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.5633)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.utils.clip_grad_norm_(predictor.parameters(), 1.0)\n",
    "torch.nn.utils.clip_grad_norm_(student.parameters(), 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f7402f-7255-4b0a-9010-5a67015c6647",
   "metadata": {},
   "source": [
    "## Update the Teacher\n",
    "\n",
    "Recall that our teacher was created from a copy of our student. This means the weights are initiated randomly. We also didn't include any backprop for our teacher as since our loss is calculated using the action predictor and the student model.  To update the teacher we'll use a mechanism called  **Exponential Moving Average (EMA)**\n",
    "\n",
    "In simple terms, this mechanism forces the Teacher to evolve very slowly, becoming a stable, wiser version of the Student.\n",
    "\n",
    "Here is the breakdown of the mechanics. Mathematically, it calculates:\n",
    "\n",
    "$Teacher_{new} = (m * Teacher_{old})+((1−m)*Student_{current})$\n",
    "\n",
    "With a typical momentum ($m$) of 0.996, the code tells the Teacher: *\"Keep 99.6% of your current weights, and mix in just 0.4% of what the Student learned in this last step.\"*\n",
    "\n",
    "The reason we do this is Stability. The Student network is updating rapidly and noisily using Gradient Descent ($Param←Param−LearningRate×Gradient$). If we simply copied the Student to the Teacher (`Teacher = Student`), the target we are trying to predict would move erratically every single step, making it impossible to learn (like trying to hit a bullseye that is vibrating wildly).\n",
    "\n",
    "By updating the Teacher this way, it effectively represents the **average** of the Student's weights over the last several thousand steps. This smooths out the noise and provides a steady, high-quality target for the Student to predict, preventing \"Model Collapse\" (where the model cheats by outputting zeros for everything).\n",
    "\n",
    "TO:DO  we could also replace the teacher with a pretrained model and use this as an update mechanism. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99ec34e1-1dea-4401-b158-2297bb877664",
   "metadata": {},
   "outputs": [],
   "source": [
    "m=0.996\n",
    "for param_s, param_t in zip(student.parameters(), teacher.parameters()):\n",
    "    param_t.data.mul_(m).add_((1 - m) * param_s.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaa6bf5-b32b-4e69-8161-d6537780580b",
   "metadata": {},
   "source": [
    "## Inference\n",
    "Now we've done a walkthrough of the forward pass of the model let's see how we can run an inference test since this is an encoder model. We'll reload our tokenized data and perturbation map so that we can convert our pertubation to text and sample from our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "402bffb4-12fa-4eb9-b884-316ba713fcb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('/Users/djemec/data/jepa/perturbation_map.json'),\n",
       " PosixPath('/Users/djemec/data/jepa/tokenized'))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_path, shard_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7be12085-8ccf-4153-916e-c87bd94fa53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load Map (ID -> Name)\n",
    "with open(metadata_path, \"r\") as f:\n",
    "    pert_map = json.load(f)\n",
    "# Invert map to: ID -> Name\n",
    "id_to_name = {v: k for k, v in pert_map.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f38e39-eafa-48f1-ac37-3f869edba02f",
   "metadata": {},
   "source": [
    "**Sample Data**\n",
    "We'll build a modification of our data loader which pulls a random file and a random perturbed cell example from it.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ca31291-7c99-49d2-9c16-5f0e20ae7311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_test_pair(shard_dir):\n",
    "    '''Grab a single real pair from a random shard'''\n",
    "    files = sorted(shard_dir.glob('*.npz'))\n",
    "    file_path = files[np.random.randint(len(files))]\n",
    "    \n",
    "    with np.load(file_path) as data:\n",
    "        idx = np.random.randint(data['action_ids'].shape[0])\n",
    "        \n",
    "        # Extract raw uint32\n",
    "        control_raw = data['control'][idx]\n",
    "        case_raw = data['case'][idx]\n",
    "        act_id = data['action_ids'][idx]\n",
    "        \n",
    "    # Dequantize\n",
    "    x_control = torch.tensor(control_raw.astype(np.float32)).unsqueeze(0).to(DEVICE) # [1, 1024]\n",
    "    x_case = torch.tensor(case_raw.astype(np.float32)).unsqueeze(0).to(DEVICE)\n",
    "    action_id = torch.tensor([act_id], dtype=torch.long).to(DEVICE)\n",
    "    \n",
    "    return x_control, x_case, action_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf31802f-d245-4f02-aea5-3a4b425279a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MTBP'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_control, x_case, action_id = get_random_test_pair(shard_dir)\n",
    "pert_name = id_to_name[action_id.item()]\n",
    "pert_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061ea2f8-ae43-4fda-9bf5-31431a3a7213",
   "metadata": {},
   "source": [
    "**Get Baseline**\n",
    "\n",
    "To start, we'll take our `teacher` model to create our baseline expected embedding for both our control and case examples.  These create our \"expected\" values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cd6633d4-3702-4cb1-82fc-797d3e252634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Get Baselines (Teacher View)\n",
    "# We need the Teacher to tell us where the \"Control\" and \"Real Treated\" \n",
    "# sit in the abstract latent space.\n",
    "with torch.no_grad():\n",
    "    z_control = teacher(x_control)       # Where the cell started\n",
    "    z_case = teacher(x_case)     # Where the cell actually went"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b4af47-2a7d-4c35-ab06-4be7e2dda23a",
   "metadata": {},
   "source": [
    "**Get Predicted**\n",
    "\n",
    "Now we need to run our predicted model. This requires first running our control (baseline cell) through our `student`, then use the output with our `predictor` along with the perturbation to see how good our predictor is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4634864b-7b17-4f43-a377-76c28687d8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Run The Physics Engine (Predictor)\n",
    "# Student encodes context -> Predictor adds Action -> Output\n",
    "with torch.no_grad():\n",
    "    z_context = student(x_control)\n",
    "    z_predicted = predictor(z_context, action_id) # Where the model thinks it went"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4baaf37-194a-458e-ad46-496f8486e4d3",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "Now we'll calculate 3 metrics:\n",
    "1. **Baseline Drift** - First we have to understand how much our teacher expects the cell state to change.  If the teacher is not expecting a change then we'd want to know that and the value would be `0`.  \n",
    "2. **Prediction Error** - Second we'll now check to see how far our predictor is from our expected case based on the teacher.  This is the value we'd like to minimize as best as possible. \n",
    "3. **Simulation Magnitude** - Third we'll see how much the predictor is different from the teacher based control. This value should ideally be as close to the drift value as possible since the predictor's output should share the latent space with the teacher\n",
    "\n",
    "With these values, we can understand a few things: If the error is larger than the drift, we can infer that the model is still worse than just predicting no change. Also, by comparing drift and simulated magnitude move we can understand how close directionally our predictor is to the teacher's impact. If that is close we can see that we're improving our model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "edfd4835-38f1-4b8c-9ea3-3187f6e6d55f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03951822593808174"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Metric 1: Baseline Drift (How much did the drug actually change the cell?)\n",
    "# If this is 0, the drug did nothing, so prediction is trivial.\n",
    "drift = F.l1_loss(z_control, z_case).item()\n",
    "drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e6166ec3-f9cb-4250-8b75-c1f6efd0efcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0133721828460693"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Metric 2: Prediction Error (How close is our guess to the real result?)\n",
    "error = F.l1_loss(z_predicted, z_case).item()\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5740f681-70a6-4b11-8d2a-0b3657f850b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0050748586654663"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Metric 3: Simulation Magnitude (How much did our model decide to move the cell?)\n",
    "sim_move = F.l1_loss(z_control,z_predicted).item()\n",
    "sim_move"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2a562f-050c-4d00-b379-ceb320f6d863",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "As you can see, untrained we have quite a lot of error.  Also, we're juggling 3 models and our predictor relies on a chain of two models meaning that overall we'll need a lot of training to start learning the cell physics. \n",
    "\n",
    "Also, looking at how this works we can see that we have a limitation on our training data currently as we're collapsing full networks down to a single number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09582e62-2447-4a0c-a94a-efd2951ab803",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
