{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cedd2e3-bcee-4a58-abcf-62858c07bd0f",
   "metadata": {},
   "source": [
    "# Bio-JEPA AC Explainer (NEEDS UPDATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ba0ded7-81b3-4afd-819b-299f9db18255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import copy\n",
    "import math\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cbf54b7-a62d-4cf0-9e81-ef07aec56ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25c49115-b74e-4283-ac4e-d98767492a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cpu\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    device = 'cpu'\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(1337)\n",
    "        device = 'cuda'\n",
    "    print(f'using {device}')\n",
    "    return device\n",
    "\n",
    "DEVICE = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54ca0e60-1213-4640-a688-bb7377085320",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('/Users/djemec/data/jepa')\n",
    "tok_dir = data_dir / 'tokenized'\n",
    "mask_path = data_dir / 'binary_pathway_mask.npy'\n",
    "metadata_path = data_dir / 'perturbation_map.json'\n",
    "checkpoint_dir = data_dir / 'checkpoint'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8e09de-cb6f-4922-a39d-32c9ef5e9383",
   "metadata": {},
   "source": [
    "## Text Prep/Tokenization\n",
    "\n",
    "We previously loaded and saved our data into a tokenized files.  For a full explainer, look at the data_prep_explainer.ipynb notebook.   In this step we'll pull the data from the files to use for training.  First we'll start by getting all the files in the tokenized directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "543054b1-a33c-4ccf-a20a-f1c5d783aad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 29 shards.\n"
     ]
    }
   ],
   "source": [
    "split = 'train'\n",
    "data_root = tok_dir / f'{split}'\n",
    "shards = list(data_root.glob('*.npz'))\n",
    "print(f'Found {len(shards)} shards.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba37f0b-d520-485e-9495-077e03653d4e",
   "metadata": {},
   "source": [
    "Now that we have the files, we need just a single file. Because of how we tokanized our files, we do not have to load them in any particular order so we'll select a random file from the list.  In training, we'd want to remove this from the file list to ensure we go through the full epoch before restarting.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efe3192c-11ee-47b7-8631-fee7be5d32c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, PosixPath('/Users/djemec/data/jepa/tokenized/train/shard_0005.npz'))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_file_idx = np.random.randint(len(shards))\n",
    "file_path = shards[random_file_idx]\n",
    "random_file_idx, file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952899be-68ad-4e37-8a9b-5b662d1d4482",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631425fd-ae6d-4b97-82cd-f307002812a2",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1043fd-c1c0-4e2f-a591-ac23bec2630f",
   "metadata": {},
   "source": [
    "To start, we need to get enough data to run the forward and backward passes.  Since our total dataset is likely too big to be held in memory all at once in real practice, we will read just enough file information into memory so that we can run the passes, leaving memory and compute to be used on the passes instead of static data holding. \n",
    "\n",
    "Recall that we saved the data in the following structure:\n",
    "\n",
    "```\n",
    "{\n",
    "    'control':[array of gene expression information],\n",
    "    'control_total':[array of total expression counts],\n",
    "    'case':[array of gene expression information],\n",
    "    'case_total':[array of total expressions],\n",
    "    'action_ids':[array of ids of the perturbation]\n",
    "}\n",
    "```\n",
    "\n",
    "Because of this, taking the same index out of each array gives us the control, case and perturbation informaiton. For training we'll do more than 1 cell at a time.  We'll use a variable `BATCH_SIZE` to control how much data to train on at a time.  In this case we'll start with 2. During training we'll want a increase our batch size.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "687c858b-51ee-4b9d-8b11-4aeaf3389cda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2816, 4658]),\n",
       " tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.4874]]),\n",
       " tensor([9.8303, 9.6755]),\n",
       " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([8.9352, 8.2506]),\n",
       " tensor([1901,   55]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 2\n",
    "with np.load(file_path) as data:\n",
    "    n_rows = data['action_ids'].shape[0]\n",
    "    row_idx = np.random.randint(n_rows, size=BATCH_SIZE)\n",
    "    \n",
    "    control_x = torch.from_numpy(data['control'][row_idx].astype(np.float32)).to(DEVICE)\n",
    "    control_tot = torch.from_numpy(data['control_total'][row_idx].astype(np.float32)).to(DEVICE)\n",
    "    case_x = torch.from_numpy(data['case'][row_idx].astype(np.float32)).to(DEVICE)\n",
    "    case_tot = torch.from_numpy(data['case_total'][row_idx].astype(np.float32)).to(DEVICE)\n",
    "    action_ids = torch.from_numpy(data['action_ids'][row_idx].astype(np.int64)).to(DEVICE)\n",
    "\n",
    "\n",
    "row_idx, control_x, control_tot, case_x, case_tot, action_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43148f73-be59-4bc8-9b53-3e9eca075646",
   "metadata": {},
   "source": [
    "## Forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3945b1b1-92c4-4be9-9554-2749e0f39367",
   "metadata": {},
   "source": [
    "![network_impact_math](../resources/biojepa_v1.png)\n",
    "\n",
    "Our current architecture has 3 models:  \n",
    "\n",
    "1. **The Student** - starts from the control cell and learns how to create a latent embedding of the cell state. \n",
    "2. **The Predictor** - learns how to adjust the student's embedded cell state based on different perturbations to hallucinate the future diseased state in the latent space shared with the teacher. \n",
    "3. **The Teacher** - starts from the actual perturbed cell and creates a latent space embedding for the pertured state to grade the predictors output against.\n",
    "\n",
    "\n",
    "We'll start by building the Pre-Norm Transformer Encoder block that are the basis for both the Teacher and Student models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739cbb8b-5abb-4de4-9753-785e6fabc05f",
   "metadata": {},
   "source": [
    "### Teacher/Student Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993c4bd8-bfac-46ac-ab69-a2da56dccf6f",
   "metadata": {},
   "source": [
    "**Multi-Headed Attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41791397-532a-4572-bfb2-f1a08bfbc62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioMultiHeadAttention(nn.Module):\n",
    "    # mirrors nn.MultiheadAttention(dim, heads, batch_first=True) \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        assert config.embed_dim % config.heads == 0\n",
    "        \n",
    "        self.head_dim = config.embed_dim // config.heads\n",
    "        self.heads = config.heads\n",
    "        \n",
    "        # Projections\n",
    "        self.q_proj = nn.Linear(config.embed_dim, config.embed_dim)\n",
    "        self.k_proj = nn.Linear(config.embed_dim, config.embed_dim)\n",
    "        self.v_proj = nn.Linear(config.embed_dim, config.embed_dim)\n",
    "        \n",
    "        self.c_proj = nn.Linear(config.embed_dim, config.embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # Batch, Seq, Embed Dim\n",
    "        \n",
    "        # 1. Project\n",
    "        q = self.q_proj(x).view(B, T, self.heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(x).view(B, T, self.heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(x).view(B, T, self.heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # 2. Standard Scaled Dot Product Attention (Permutation Invariant)\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=False)\n",
    "        \n",
    "        # 5. Reassemble\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.c_proj(y)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e754b8e-8536-44da-84cd-05f617f34af0",
   "metadata": {},
   "source": [
    "**MLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aeb98e8a-868c-4f8f-991c-84cdc4e887d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.c_fc = nn.Linear(config.embed_dim, int(config.mlp_ratio * config.embed_dim))\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "        self.c_proj = nn.Linear(int(config.mlp_ratio * config.embed_dim), config.embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0306eb8-042e-4dda-bc35-99a73d1e1ffb",
   "metadata": {},
   "source": [
    "**Hidden Transformer Block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90042ed3-5020-49be-8914-568d25a605c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CellStateBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.ln_1 = nn.LayerNorm(config.embed_dim)\n",
    "        self.attn = BioMultiHeadAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.embed_dim)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. Attention \n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "\n",
    "        # 2. MLP\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f6508f-895c-45c9-ace7-04f3fbda30c5",
   "metadata": {},
   "source": [
    "**Cell State Encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2151f336-a980-4466-a2b4-2b342f701e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CellStateEncoderConfig:\n",
    "    num_genes: int = 4096\n",
    "    num_pathways: int = 1024 \n",
    "    n_layer: int = 24 \n",
    "    heads: int = 12\n",
    "    embed_dim: int = 768\n",
    "    mlp_ratio: float = 4.0 # Changed to float for precision\n",
    "    mask_matrix: np.ndarray = None \n",
    "\n",
    "class CellStateEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Learnable Network, initialized based on known pathwasy 1 == connection\n",
    "        # wrapping as a \"parameter\" allows it to be learned\n",
    "        assert config.mask_matrix is not None, 'Must provide binary_pathway_mask!'\n",
    "        init_weights = torch.tensor(config.mask_matrix).float().T \n",
    "        self.pathway_weights = nn.Parameter(init_weights)\n",
    "        \n",
    "        # Learnable Gene Embeddings [num_genes, Dim]\n",
    "        self.gene_embeddings = nn.Parameter(torch.randn(config.num_genes, config.embed_dim) * 0.02)\n",
    "        \n",
    "        # Context Injector\n",
    "        self.total_count_proj = nn.Linear(1, config.embed_dim)\n",
    "\n",
    "        # Transfomer\n",
    "        self.blocks = nn.ModuleList([CellStateBlock(config) for _ in range(config.n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(config.embed_dim)\n",
    "\n",
    "        # Initiation \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None: \n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        \n",
    "    def forward(self, x_genes, x_total_ct):\n",
    "        # 1. Project Genes\n",
    "        x_genes = x_genes.unsqueeze(-1) \n",
    "        gene_repr = x_genes * self.gene_embeddings.unsqueeze(0)\n",
    "\n",
    "        # 2. Gene Embeddings @ pathway weights\n",
    "        x_pathway = self.pathway_weights @ gene_repr\n",
    "\n",
    "        # 3. Context Injection\n",
    "        x_total_ct = x_total_ct.unsqueeze(-1)\n",
    "        x_total_ct = self.total_count_proj(x_total_ct)\n",
    "        x_total_ct = x_total_ct.unsqueeze(1)\n",
    "        x = x_pathway + x_total_ct\n",
    "\n",
    "        # 4. Set Transformer\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # 5. Layer Norm\n",
    "        x = self.ln_f(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02752d4-46e3-4558-8468-d2e7843ba3ff",
   "metadata": {},
   "source": [
    "### Create The Student And Teacher Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "caa8f876-43cf-496f-a487-54da0f38f915",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_genes=4096\n",
    "num_pathways=1024\n",
    "n_layer=1\n",
    "heads=4\n",
    "embed_dim= 8 \n",
    "mlp_ratio= 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887d2031-c77d-449f-8500-ec042c0d6d95",
   "metadata": {},
   "source": [
    "**Binary Mask**\n",
    "Our student/teacher encoder models take in known gene network encodings to initiate the network embedding.  During tokenization we saved a one-hot encoded network.  We'll load this in as part of our initiation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5abafc22-6954-400d-8880-2ef9ff34dcd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask Loaded: 4096 Genes -> 1024 Pathways\n"
     ]
    }
   ],
   "source": [
    "mask_matrix = np.load(mask_path)\n",
    "N_GENES, N_PATHWAYS = mask_matrix.shape\n",
    "print(f'Mask Loaded: {N_GENES} Genes -> {N_PATHWAYS} Pathways')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f444253-5f8e-43fe-a679-3815f96bf8a4",
   "metadata": {},
   "source": [
    "**Student Model** Now we'll create our hyperparameters and initiate our student model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f56c881d-e7be-428e-a09f-4da8f3b4135b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CellStateEncoder(\n",
       "  (total_count_proj): Linear(in_features=1, out_features=8, bias=True)\n",
       "  (blocks): ModuleList(\n",
       "    (0): CellStateBlock(\n",
       "      (ln_1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): BioMultiHeadAttention(\n",
       "        (q_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "        (k_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "        (v_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "        (c_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Linear(in_features=8, out_features=32, bias=True)\n",
       "        (gelu): GELU(approximate='tanh')\n",
       "        (c_proj): Linear(in_features=32, out_features=8, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_conf = CellStateEncoderConfig(\n",
    "    num_genes=num_genes,\n",
    "    num_pathways=num_pathways,\n",
    "    n_layer=n_layer,\n",
    "    heads=heads,\n",
    "    embed_dim=embed_dim,\n",
    "    mlp_ratio=mlp_ratio,\n",
    "    mask_matrix=mask_matrix\n",
    ")\n",
    "student = CellStateEncoder(enc_conf) \n",
    "student"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09134ca7-76b4-47fd-a0b6-e9500da152f5",
   "metadata": {},
   "source": [
    "**Teacher** Now let's make a copy for the teacher. Since the teacher does not require updates, we can update the model to not require gradient updates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a257b3da-5334-46d2-9715-b3ec001efc56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CellStateEncoder(\n",
       "  (total_count_proj): Linear(in_features=1, out_features=8, bias=True)\n",
       "  (blocks): ModuleList(\n",
       "    (0): CellStateBlock(\n",
       "      (ln_1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): BioMultiHeadAttention(\n",
       "        (q_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "        (k_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "        (v_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "        (c_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Linear(in_features=8, out_features=32, bias=True)\n",
       "        (gelu): GELU(approximate='tanh')\n",
       "        (c_proj): Linear(in_features=32, out_features=8, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher = copy.deepcopy(student)\n",
    "teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3b34811d-ae3f-4d9a-97f9-b3d559a13191",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in teacher.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c22025e-cf3b-447d-a219-7562d1d5b71f",
   "metadata": {},
   "source": [
    "### Forward Pass: Teacher\n",
    "Teacher encodes the Target (Case). We'll make sure to run it without gradient accumulation. You may wonder at this point how the teacher is updated. The teacher's update is actually based on a downweighted version of the student's gradietns.  \n",
    "\n",
    "This teacher creates an embedding of the case cell for which the student + predictor try to match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c02b1af1-fbbb-4533-af8d-8792d97347d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    target_latents = teacher(case_x, case_tot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1e472d-8fab-4cc5-b9c1-960e233b2615",
   "metadata": {},
   "source": [
    "### Forward Pass: Student\n",
    "Student encodes the Context (Control), With the student we need to accumulate gradients since we need to be able to train the model. \n",
    "> TO DO: Add masking here if you want extra difficulty, but the task Control->Treated is enough for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "143c7081-8409-4d8f-af6f-c22b05f04454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3264,  0.8426, -0.6142,  ..., -0.7733,  1.5373, -1.7331],\n",
       "         [-0.4907,  0.5296, -0.3648,  ..., -0.8971,  1.5316, -1.6865],\n",
       "         [-0.0982, -0.3098, -1.2579,  ..., -0.0050,  1.7568, -1.2275],\n",
       "         ...,\n",
       "         [-1.0808, -0.4855,  0.0171,  ...,  0.3515,  0.9369, -0.3291],\n",
       "         [-0.6165,  0.7703, -0.0679,  ..., -0.3916,  1.8222, -1.8078],\n",
       "         [-0.6518, -0.0421, -0.7640,  ..., -0.4835,  1.5784, -1.4388]],\n",
       "\n",
       "        [[-0.2929,  0.9335, -0.3322,  ..., -0.6493,  1.7300, -1.8462],\n",
       "         [-0.3199,  0.5634, -0.3114,  ..., -0.8531,  1.7809, -1.7569],\n",
       "         [-0.1883, -0.3070, -1.4775,  ..., -0.1232,  1.9010, -0.8828],\n",
       "         ...,\n",
       "         [-0.8064,  0.1704,  0.8994,  ..., -0.6685,  0.8774, -0.9867],\n",
       "         [-0.3169,  0.3114,  0.0949,  ..., -0.8969,  1.7992, -1.7457],\n",
       "         [-0.3654, -0.2884, -0.8097,  ..., -0.6777,  1.8492, -1.2775]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_latents = student(control_x, control_tot)\n",
    "context_latents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f70a5f8-0f52-402e-8bd9-eeac1074f912",
   "metadata": {},
   "source": [
    "## Predictor Setup\n",
    "We need to create a predictor that tries to guess Target (from the teacher) given Context + Action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bb2ed4-6561-4b8e-9cb5-4bf857ddad91",
   "metadata": {},
   "source": [
    "**Adaptive Layer Normalization AdaLN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "77dc0a71-29c4-423d-ba89-9595cdac0bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaLN(nn.Module):\n",
    "    '''\n",
    "    Adaptive Layer Norm for conditioning the predictor on action embeddings.\n",
    "    The action vector regresses the Scale (gamma) and Shift (beta) of the normalization.\n",
    "    '''\n",
    "    def __init__(self, embed_dim, action_embed_dim):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(embed_dim, elementwise_affine=False)\n",
    "        self.action_mlp = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(action_embed_dim, 2 * embed_dim)\n",
    "        )\n",
    "        # Initialize to identity (gamma=0, beta=0 originally, effectively gamma=1 after logic)\n",
    "        # Zero-init the last layer so the action starts as a \"no-op\" (identity)\n",
    "        nn.init.zeros_(self.action_mlp[1].weight)\n",
    "        nn.init.zeros_(self.action_mlp[1].bias)\n",
    "\n",
    "    def forward(self, x, action_emb):\n",
    "        # x: [Batch, Seq, Dim]\n",
    "        # action_emb: [Batch, action_embed_dim]\n",
    "        \n",
    "        # Project action to style: [B, 2*D] -> [B, 1, 2*D]\n",
    "        style = self.action_mlp(action_emb).unsqueeze(1) \n",
    "        gamma, beta = style.chunk(2, dim=-1)\n",
    "        \n",
    "        # Apply affine transformation based on action\n",
    "        return self.norm(x) * (1 + gamma) + beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93608ce7-5ba4-488e-849e-05c298bfc50b",
   "metadata": {},
   "source": [
    "**Predictor Block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "009a430a-202c-4eff-974e-d5dfc9beca30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictorBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # 1. Conditioning (AdaLN) replaces standard LayerNorm\n",
    "        self.ada_ln1 = AdaLN(config.embed_dim, config.action_embed_dim)\n",
    "        \n",
    "        # 2. Attention (Using the shared BioMultiHeadAttention)\n",
    "        self.attn = BioMultiHeadAttention(config)\n",
    "        \n",
    "        # 3. Conditioning (AdaLN) for the MLP block\n",
    "        self.ada_ln2 = AdaLN(config.embed_dim, config.action_embed_dim)\n",
    "        \n",
    "        # 4. MLP (Using the shared MLP)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x, action_emb):\n",
    "        # 1. AdaLN -> Attention  -> Residual\n",
    "        x_norm = self.ada_ln1(x, action_emb)\n",
    "        x = x + self.attn(x_norm)\n",
    "        \n",
    "        # 2. AdaLN -> MLP -> Residual\n",
    "        x_norm = self.ada_ln2(x, action_emb)\n",
    "        x = x + self.mlp(x_norm)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27332d23-4dd4-4c4a-b8a2-24438e1bca6f",
   "metadata": {},
   "source": [
    "**Main Predictor Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3d4e7df9-10e8-4e35-bc69-623f5b4dae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ACPredictorConfig:\n",
    "    num_pathways: int = 1024\n",
    "    n_layer: int = 6 \n",
    "    heads: int = 4\n",
    "    embed_dim: int = 384\n",
    "    action_embed_dim: int = 256 \n",
    "    mlp_ratio: float = 4.0\n",
    "    max_perturb: int = 2058 ## eventually try to get to a 2**N power\n",
    "\n",
    "class ACPredictor(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Action Embedding (Discrete ID -> Vector)\n",
    "        self.action_embed = nn.Embedding(config.max_perturb, config.action_embed_dim)\n",
    "        \n",
    "        # Learnable Queries (\"Mask Tokens\") for the future state\n",
    "        # One query vector per pathway position\n",
    "        self.mask_queries = nn.Parameter(torch.randn(1, config.num_pathways, config.embed_dim) * 0.02)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            PredictorBlock(config) for _ in range(config.n_layer)\n",
    "        ])\n",
    "        \n",
    "        self.final_norm = AdaLN(config.embed_dim, config.action_embed_dim)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, context_latents, action_ids):\n",
    "        \"\"\"\n",
    "        context_latents: [Batch, N, Dim] (From Student Encoder)\n",
    "        action_ids: [Batch] (Ints)\n",
    "        \"\"\"\n",
    "        B, N, D = context_latents.shape\n",
    "        \n",
    "        # 1. Embed Action\n",
    "        action_emb = self.action_embed(action_ids) # [B, action_embed_dim]\n",
    "        \n",
    "        # 2. Construct Input: [Context, Mask_Queries]\n",
    "        # We concatenate the learned queries to the context. \n",
    "        # The predictor will attend to the context to update the queries.\n",
    "        queries = self.mask_queries.repeat(B, 1, 1) # [B, N, D]\n",
    "        sequence = torch.cat([context_latents, queries], dim=1) # [B, 2N, D]     \n",
    "        \n",
    "        # 3. Pass through AdaLN Blocks\n",
    "        for block in self.blocks:\n",
    "            sequence = block(sequence, action_emb)\n",
    "            \n",
    "        sequence = self.final_norm(sequence, action_emb)\n",
    "        \n",
    "        # 4. Return only the predicted part (The Queries corresponding to N..2N)\n",
    "        predictions = sequence[:, N:, :] \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a087de0-e0a3-49bf-b09e-b917d492eb5c",
   "metadata": {},
   "source": [
    "### Create the Predictor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b86f09d6-9258-46eb-af90-b21277f8b8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_conf = ACPredictorConfig(\n",
    "    num_pathways=num_pathways,\n",
    "    n_layer=n_layer,\n",
    "    heads=heads,\n",
    "    embed_dim=embed_dim,\n",
    "    action_embed_dim=embed_dim,\n",
    "    mlp_ratio=mlp_ratio\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b7f5531a-f3c3-4472-b638-5a3a17d18322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ACPredictor(\n",
       "  (action_embed): Embedding(2058, 8)\n",
       "  (blocks): ModuleList(\n",
       "    (0): PredictorBlock(\n",
       "      (ada_ln1): AdaLN(\n",
       "        (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=False)\n",
       "        (action_mlp): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=8, out_features=16, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (attn): BioMultiHeadAttention(\n",
       "        (q_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "        (k_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "        (v_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "        (c_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "      )\n",
       "      (ada_ln2): AdaLN(\n",
       "        (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=False)\n",
       "        (action_mlp): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=8, out_features=16, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Linear(in_features=8, out_features=32, bias=True)\n",
       "        (gelu): GELU(approximate='tanh')\n",
       "        (c_proj): Linear(in_features=32, out_features=8, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_norm): AdaLN(\n",
       "    (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=False)\n",
       "    (action_mlp): Sequential(\n",
       "      (0): SiLU()\n",
       "      (1): Linear(in_features=8, out_features=16, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor = ACPredictor(pred_conf)\n",
    "predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e2841b-32fa-4d41-b7c2-115723f24903",
   "metadata": {},
   "source": [
    "### Forward Pass: Action Predictor\n",
    "The Action predictor tries to guess Target encoding given Context + Action.  The context is the ouput of the Student and the action is from our tokenized data.  The goal here is tha the output attempts to match the teacher. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dd8dd03a-f9ef-45e9-910b-42ee77ab28cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1901,   55])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ccf6c1ff-330c-4f46-abca-35571ef07f91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0078,  1.6829, -1.1367,  ..., -1.0413,  0.1867,  1.3981],\n",
       "         [ 0.1166,  1.6274, -1.1412,  ..., -1.1031,  0.1151,  1.4478],\n",
       "         [ 0.2033,  1.6339, -1.0711,  ..., -1.2691,  0.2119,  1.3527],\n",
       "         ...,\n",
       "         [ 0.1031,  1.3992, -1.1596,  ..., -1.2680,  0.4441,  1.4944],\n",
       "         [ 0.2364,  1.3741, -1.1901,  ..., -1.3242,  0.2741,  1.5036],\n",
       "         [ 0.1904,  1.5286, -1.2348,  ..., -1.1161,  0.2004,  1.4497]],\n",
       "\n",
       "        [[-0.0266,  1.6780, -1.1313,  ..., -1.0113,  0.1162,  1.4369],\n",
       "         [ 0.0838,  1.6175, -1.1394,  ..., -1.0765,  0.0527,  1.4874],\n",
       "         [ 0.1740,  1.6284, -1.0700,  ..., -1.2384,  0.1352,  1.4009],\n",
       "         ...,\n",
       "         [ 0.0648,  1.3932, -1.1603,  ..., -1.2432,  0.3783,  1.5407],\n",
       "         [ 0.1993,  1.3728, -1.1908,  ..., -1.2878,  0.1942,  1.5496],\n",
       "         [ 0.1578,  1.5301, -1.2293,  ..., -1.0842,  0.1256,  1.4852]]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_latents = predictor(context_latents, action_ids)\n",
    "predicted_latents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb98c36-d1ad-42d4-8e61-2465bdedbd4d",
   "metadata": {},
   "source": [
    "# Loss Calculation\n",
    "\n",
    "We'll use L1 Loss (Mean Absolute Error) which measures the average absolute difference between predicted and actual values, making it robust to outliers.  We'll call this latent loss since we'll  use an L1 loss function within the latent space of a model to ensure the compressed representation (latent) accurately reflects the original data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "09d24c1a-523a-4bae-b8ff-630df3b563f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1376, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = F.l1_loss(predicted_latents, target_latents)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3633ff-5398-4928-a40e-8133d2eaaad1",
   "metadata": {},
   "source": [
    "# Back Propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2abb5a2e-5d83-41b6-82a5-25122a59e4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c951691-68de-4246-9d0b-8a9f359f67ae",
   "metadata": {},
   "source": [
    "## Gradient Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "017d9b0a-fac7-4d10-a247-fece2a39732d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4932)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.utils.clip_grad_norm_(predictor.parameters(), 1.0)\n",
    "torch.nn.utils.clip_grad_norm_(student.parameters(), 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f7402f-7255-4b0a-9010-5a67015c6647",
   "metadata": {},
   "source": [
    "## Update the Teacher\n",
    "\n",
    "Recall that our teacher was created from a copy of our student. This means the weights are initiated randomly. We also didn't include any backprop for our teacher as since our loss is calculated using the action predictor and the student model.  To update the teacher we'll use a mechanism called  **Exponential Moving Average (EMA)**\n",
    "\n",
    "In simple terms, this mechanism forces the Teacher to evolve very slowly, becoming a stable, wiser version of the Student.\n",
    "\n",
    "Here is the breakdown of the mechanics. Mathematically, it calculates:\n",
    "\n",
    "$Teacher_{new} = (m * Teacher_{old})+((1−m)*Student_{current})$\n",
    "\n",
    "With a typical momentum ($m$) of 0.996, the code tells the Teacher: *\"Keep 99.6% of your current weights, and mix in just 0.4% of what the Student learned in this last step.\"*\n",
    "\n",
    "The reason we do this is Stability. The Student network is updating rapidly and noisily using Gradient Descent ($Param←Param−LearningRate×Gradient$). If we simply copied the Student to the Teacher (`Teacher = Student`), the target we are trying to predict would move erratically every single step, making it impossible to learn (like trying to hit a bullseye that is vibrating wildly).\n",
    "\n",
    "By updating the Teacher this way, it effectively represents the **average** of the Student's weights over the last several thousand steps. This smooths out the noise and provides a steady, high-quality target for the Student to predict, preventing \"Model Collapse\" (where the model cheats by outputting zeros for everything).\n",
    "\n",
    "TO:DO  we could also replace the teacher with a pretrained model and use this as an update mechanism. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "99ec34e1-1dea-4401-b158-2297bb877664",
   "metadata": {},
   "outputs": [],
   "source": [
    "m=0.996\n",
    "for param_s, param_t in zip(student.parameters(), teacher.parameters()):\n",
    "    param_t.data.mul_(m).add_((1 - m) * param_s.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaa6bf5-b32b-4e69-8161-d6537780580b",
   "metadata": {},
   "source": [
    "## Inference\n",
    "Now we've done a walkthrough of the forward pass of the model let's see how we can run an inference test since this is an encoder model. We'll reload our tokenized data and perturbation map so that we can convert our pertubation to text and sample from our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "402bffb4-12fa-4eb9-b884-316ba713fcb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('/Users/djemec/data/jepa/gene_names.json'),\n",
       " PosixPath('/Users/djemec/data/jepa/perturbation_map.json'),\n",
       " PosixPath('/Users/djemec/data/jepa/tokenized/val'))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dir = tok_dir / 'val'\n",
    "gene_names_path = data_dir / 'gene_names.json'\n",
    "gene_names_path, metadata_path, val_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7be12085-8ccf-4153-916e-c87bd94fa53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Perturbation Map (ID -> Name)\n",
    "with open(metadata_path, \"r\") as f:\n",
    "    pert_map = json.load(f)\n",
    "id_to_pert = {v: k for k, v in pert_map.items()}\n",
    "\n",
    "# Load Gene Names\n",
    "with open(data_dir / 'gene_names.json', \"r\") as f:\n",
    "    gene_names = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f38e39-eafa-48f1-ac37-3f869edba02f",
   "metadata": {},
   "source": [
    "**Sample Data**\n",
    "We'll build a modification of our data loader which pulls a random file and a random perturbed cell example from it.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5ca31291-7c99-49d2-9c16-5f0e20ae7311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_test_pair(shard_dir):\n",
    "    '''Grab a single real pair from a random shard with Context'''\n",
    "    files = sorted(shard_dir.glob('*.npz'))\n",
    "        \n",
    "    file_path = files[np.random.randint(len(files))]\n",
    "    \n",
    "    with np.load(file_path) as data:\n",
    "        idx = np.random.randint(data['action_ids'].shape[0])\n",
    "        \n",
    "        # Extract Items (Now 5 items instead of 3)\n",
    "        c_raw = data['control'][idx]         # [2000]\n",
    "        ct_raw = data['control_total'][idx]  # Scalar\n",
    "        case_raw = data['case'][idx]         # [2000]\n",
    "        caset_raw = data['case_total'][idx]  # Scalar\n",
    "        act_id = data['action_ids'][idx]     # Scalar\n",
    "        \n",
    "    # Convert to Tensor & Add Batch Dim [1, ...]\n",
    "    xc = torch.tensor(c_raw).float().unsqueeze(0).to(DEVICE)\n",
    "    xct = torch.tensor(ct_raw).float().unsqueeze(0).to(DEVICE)\n",
    "    xt = torch.tensor(case_raw).float().unsqueeze(0).to(DEVICE)\n",
    "    xtt = torch.tensor(caset_raw).float().unsqueeze(0).to(DEVICE)\n",
    "    aid = torch.tensor([act_id]).long().to(DEVICE)\n",
    "    \n",
    "    return xc, xct, xt, xtt, aid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bf31802f-d245-4f02-aea5-3a4b425279a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ACD'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control_x, control_tot, case_x, case_tot, action_id = get_random_test_pair(val_dir)\n",
    "pert_name = id_to_pert[action_id.item()]\n",
    "pert_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "44ff25b1-633c-4be5-8564-04da57b10069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4096])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061ea2f8-ae43-4fda-9bf5-31431a3a7213",
   "metadata": {},
   "source": [
    "**Get Baseline**\n",
    "\n",
    "To start, we'll take our `teacher` model to create our baseline expected embedding for both our control and case examples.  These create our \"expected\" values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cd6633d4-3702-4cb1-82fc-797d3e252634",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    z_control = teacher(control_x, control_tot)       # Where the cell started\n",
    "    z_case = teacher(case_x, case_tot)     # Where the cell actually went"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b4af47-2a7d-4c35-ab06-4be7e2dda23a",
   "metadata": {},
   "source": [
    "**Get Predicted**\n",
    "\n",
    "Now we need to run our predicted model. This requires first running our control (baseline cell) through our `student`, then use the output with our `predictor` along with the perturbation to see how good our predictor is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4634864b-7b17-4f43-a377-76c28687d8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Run The Physics Engine (Predictor)\n",
    "# Student encodes context -> Predictor adds Action -> Output\n",
    "with torch.no_grad():\n",
    "    z_context = student(control_x, control_tot)\n",
    "    z_predicted = predictor(z_context, action_id) # Where the model thinks it went"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4baaf37-194a-458e-ad46-496f8486e4d3",
   "metadata": {},
   "source": [
    "### Metrics \n",
    "Now that we have our predicted latents and expected latents we can evaluate our prediction.  What we'll do is evaluate the change in the real control space, use the geneXnetwork matrix, and derive the most perturbed genes based on the action. \n",
    "\n",
    "*Note that currently our model is untrained so this will be BAD*\n",
    "\n",
    "\n",
    "First we'll start by calculating how much we saw the latent space change based on the perturbation. This means subtracting the predictor output from the teacher's control space. We use the teacher as the predictor output and teacher outputs should live in the same latent space so this delta should signal how much of a change the action caused.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bb10ea9c-0cc2-48a5-bb63-c033042c5551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.5257e-01,  8.6591e-01, -8.3257e-01,  ..., -7.6716e-02,\n",
       "         -1.4727e+00,  3.1437e+00],\n",
       "        [ 3.0687e-01,  1.3582e+00, -8.7432e-01,  ...,  1.2150e-01,\n",
       "         -1.6244e+00,  3.0174e+00],\n",
       "        [ 4.4986e-01,  2.1124e+00,  9.8932e-04,  ..., -8.9967e-01,\n",
       "         -1.9176e+00,  2.4622e+00],\n",
       "        ...,\n",
       "        [ 1.4239e+00,  1.9808e+00, -1.7845e+00,  ..., -1.0847e+00,\n",
       "         -1.1714e+00,  1.8653e+00],\n",
       "        [ 7.5283e-01,  7.1341e-01, -1.0429e+00,  ..., -6.0725e-01,\n",
       "         -1.6332e+00,  3.2135e+00],\n",
       "        [ 5.4587e-01,  1.6708e+00, -7.4546e-01,  ..., -5.5125e-02,\n",
       "         -1.6411e+00,  2.8595e+00]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate Latent Movement\n",
    "delta_latent = (z_predicted - z_control).squeeze(0) # [1024, 384]\n",
    "delta_latent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c83453a-62d8-4e2c-8dee-aa1fdb880351",
   "metadata": {},
   "source": [
    "**Pathway Energies**\n",
    "\n",
    "Next we want to score every gene based on how much of a shift the parent pathway had.  Our current latent space is the `[pathway,embedding]` dimension. we'll normalize our embeddings down to a single value per pathway, then use the learned weights of the `[gene,pathway]` to project the pathway impacts back out to the gene. This results in `Gene_Score = Sum(Mask_gp * Magnitude(Pathway_p))`  With this, we can determine things like if a gene is in 5 pathways that all moved violently, that gene is highly implicated. \n",
    "\n",
    "*Note that this is a very rudamentary way of validating.  full validation would require a trained head*\n",
    "\n",
    "First we'll normalize our latents down to a single value per network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9f062121-cb80-454a-9afb-58233994a630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.9492, 4.1390, 4.2119,  ..., 4.1820, 4.0623, 4.2419])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pathway_magnitudes = delta_latent.norm(dim=1) \n",
    "pathway_magnitudes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8991d7-870c-4981-ac64-db4a8103812f",
   "metadata": {},
   "source": [
    "**Network X Gene**\n",
    "\n",
    "Now we'll extract our pathway weights that are the learned gene to network mapping. We take the absolute value since a strong negative weight implies just as much biological \"impact\" for our testing as a positive value does.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e7398342-0fa3-4b02-8588-c4854d5042fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Access the Learnable Weights\n",
    "weights = student.pathway_weights.detach()\n",
    "\n",
    "# 2. Transpose back to [Genes, Pathways] for the projection\n",
    "routing_matrix = weights.T.abs() # Shape: [2000, 1024]\n",
    "routing_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6eff3fa-dd94-40de-b786-f57d99c0f1e5",
   "metadata": {},
   "source": [
    "**Matrix-vector product**\n",
    "Now we're ready to performs the matrix-vector product to expand our network impacts back to the genes in them. This will output a single vector for each gene based on the different networks it's been tied to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8dc79feb-aaf2-4cd1-96a9-72c60a62cadc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0.0000,   7.0570, 126.2627,  ...,   0.0000,   0.0000,   0.0000])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gene_impact_scores = torch.mv(routing_matrix, pathway_magnitudes)\n",
    "gene_impact_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230a14b5-1ce6-4252-a1f9-7b1f8bdd7f43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f5c6651-b3fd-4854-b1b4-ee81035262bf",
   "metadata": {},
   "source": [
    "### Validate top K genes impacted\n",
    "Now that we have our gene values, we need a way to compare this to our teacher.  In this case we'll take top K values and convert the index to a gene to signfiy it's been impacted. We'll then do the same on the teacher space and see if we got an overlap. \n",
    "\n",
    "Let's start by extracting the top gene impact scores and indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4cca1fee-0d6b-4532-a655-6c3ae4d702c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 100\n",
    "pred_values, pred_indices = torch.topk(gene_impact_scores, top_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd9ff63-6e89-40d2-9058-b53d40541feb",
   "metadata": {},
   "source": [
    "Now we'll convert those indeces into gene names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "54ce974e-615d-4246-8f37-4913048fa812",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Predicted\n",
      "CASP3      (Score: 1732.7908)\n",
      "VEGFA      (Score: 1525.5117)\n",
      "AURKB      (Score: 1229.8809)\n",
      "SLC7A1     (Score: 1091.5343)\n",
      "MYC        (Score: 1081.3484)\n",
      "ICAM1      (Score: 969.7882)\n",
      "SOD1       (Score: 918.0603)\n",
      "LDHA       (Score: 914.0148)\n",
      "NR3C1      (Score: 906.3495)\n",
      "CYCS       (Score: 887.7327)\n",
      "PRDX2      (Score: 885.9540)\n",
      "JUN        (Score: 880.2817)\n",
      "MCL1       (Score: 874.4275)\n",
      "CDT1       (Score: 858.1977)\n",
      "BCL2L1     (Score: 840.5076)\n",
      "NFKB1      (Score: 834.9631)\n",
      "NFE2L2     (Score: 817.7412)\n",
      "STAT3      (Score: 815.0062)\n",
      "PCNA       (Score: 799.4529)\n",
      "CASP8      (Score: 792.2278)\n",
      "NFKBIA     (Score: 763.6768)\n",
      "MAPK1      (Score: 737.3122)\n",
      "DDIT3      (Score: 728.9926)\n",
      "POLD4      (Score: 722.5864)\n",
      "DECR1      (Score: 715.1577)\n",
      "TNFRSF10B  (Score: 684.7576)\n",
      "EGR1       (Score: 674.3992)\n",
      "PMAIP1     (Score: 665.2002)\n",
      "HSPA1A     (Score: 664.4025)\n",
      "GADD45A    (Score: 662.5065)\n",
      "TAOK1      (Score: 662.2225)\n",
      "SAT1       (Score: 662.1367)\n",
      "MTR        (Score: 652.7802)\n",
      "GDF15      (Score: 624.2972)\n",
      "HSPA1B     (Score: 610.2079)\n",
      "DDIT4      (Score: 593.6105)\n",
      "CCNE2      (Score: 585.8037)\n",
      "HIF1A      (Score: 559.8604)\n",
      "TXNIP      (Score: 557.3527)\n",
      "LDLR       (Score: 554.9310)\n",
      "PPP1R15A   (Score: 554.6664)\n",
      "CBL        (Score: 554.0525)\n",
      "HSPA4      (Score: 541.6157)\n",
      "CD55       (Score: 538.5455)\n",
      "ROCK1      (Score: 536.5982)\n",
      "MAT2A      (Score: 532.4763)\n",
      "MDM2       (Score: 526.7744)\n",
      "TSC1       (Score: 526.0095)\n",
      "SLC7A11    (Score: 524.8771)\n",
      "HSPA5      (Score: 521.5704)\n",
      "KLF6       (Score: 509.8475)\n",
      "SGK1       (Score: 504.5777)\n",
      "TRIB3      (Score: 503.8374)\n",
      "SOD2       (Score: 494.8708)\n",
      "CCNA2      (Score: 488.0919)\n",
      "SCD        (Score: 487.1635)\n",
      "BHLHE40    (Score: 486.7702)\n",
      "PARP1      (Score: 485.7479)\n",
      "CDK1       (Score: 479.3636)\n",
      "FYN        (Score: 474.2670)\n",
      "VIM        (Score: 473.0419)\n",
      "SLC30A1    (Score: 472.3309)\n",
      "CCNB1      (Score: 471.1629)\n",
      "DNAJB1     (Score: 464.2626)\n",
      "KIT        (Score: 462.8766)\n",
      "SIRT1      (Score: 461.5601)\n",
      "CLK1       (Score: 458.8911)\n",
      "PTEN       (Score: 457.2065)\n",
      "INSIG1     (Score: 453.9344)\n",
      "SETD2      (Score: 452.2812)\n",
      "CEBPB      (Score: 450.2979)\n",
      "TNFSF10    (Score: 448.6425)\n",
      "SRSF7      (Score: 443.9391)\n",
      "BTG1       (Score: 443.7393)\n",
      "SLK        (Score: 441.2114)\n",
      "DHFR       (Score: 440.8657)\n",
      "H2AFX      (Score: 435.9324)\n",
      "HIST1H3H   (Score: 434.5414)\n",
      "DNAJB9     (Score: 433.3154)\n",
      "TXN        (Score: 433.0363)\n",
      "AREG       (Score: 429.1799)\n",
      "FOXO3      (Score: 428.0576)\n",
      "CDKN1B     (Score: 424.3093)\n",
      "MT2A       (Score: 418.2747)\n",
      "MT1X       (Score: 411.7454)\n",
      "IER3       (Score: 411.3596)\n",
      "ATM        (Score: 410.5863)\n",
      "CLK4       (Score: 408.1790)\n",
      "BRCA1      (Score: 406.1480)\n",
      "HMGCS1     (Score: 406.1208)\n",
      "HIST1H4C   (Score: 405.7154)\n",
      "HMGCR      (Score: 403.9939)\n",
      "N4BP2L2    (Score: 402.0149)\n",
      "IGF1R      (Score: 401.9859)\n",
      "BRD2       (Score: 401.8592)\n",
      "HSPA8      (Score: 401.3629)\n",
      "WDTC1      (Score: 399.2657)\n",
      "CAT        (Score: 396.8935)\n",
      "XBP1       (Score: 395.9756)\n",
      "CFLAR      (Score: 395.7438)\n"
     ]
    }
   ],
   "source": [
    "print('Model Predicted')\n",
    "for val, idx in zip(pred_values, pred_indices):\n",
    "    g_name = gene_names[idx.item()]\n",
    "    print(f'{g_name:<10} (Score: {val.item():.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dee9ce7-5a1f-43ab-8b3b-e4c9455c379c",
   "metadata": {},
   "source": [
    "**Ground Truth Comparison**\n",
    "Now we need to compare this list to our ground truth.  For the ground truth we can just compare our raw input variables `case_x, control_x` and pick the top K genes. \n",
    "\n",
    "Let's start by calculating the difference in expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ee9a96af-df7c-4d19-8634-67e9dce15dd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.5572, 0.0000,  ..., 0.5572, 0.0000, 0.0000])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_gene_delta = (case_x - control_x).abs().squeeze(0) \n",
    "real_gene_delta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc85899f-3701-4c75-bcbc-bf502b0ed271",
   "metadata": {},
   "source": [
    "Now we'll again extract the top K genes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "08558c59-c0ca-4f44-9a9b-9fbd15d78cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_values, real_indices = torch.topk(real_gene_delta, top_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3099a7d-b911-4ddf-aaca-d8839ae7301f",
   "metadata": {},
   "source": [
    "**Comparison of predicted and real**\n",
    "And now we'll print out the genes and, if they match, add a checkmark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f3616f14-8f46-4c59-9450-d2173546ca37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "❌ PFDN2      (Delta: 2.1350)\n",
      "❌ CMTM6      (Delta: 2.1350)\n",
      "❌ HBG1       (Delta: 2.0921)\n",
      "❌ CKB        (Delta: 2.0921)\n",
      "❌ KIF2C      (Delta: 2.0427)\n",
      "❌ UBC        (Delta: 2.0188)\n",
      "❌ HLTF       (Delta: 1.9583)\n",
      "❌ DNAJA1     (Delta: 1.9583)\n",
      "❌ ARF1       (Delta: 1.9410)\n",
      "❌ KDM5B      (Delta: 1.9410)\n",
      "❌ RAP1B      (Delta: 1.8593)\n",
      "❌ BTG2       (Delta: 1.8278)\n",
      "❌ ARL6IP1    (Delta: 1.8278)\n",
      "❌ TPP2       (Delta: 1.8038)\n",
      "❌ AC079466.1 (Delta: 1.8038)\n",
      "❌ SPDL1      (Delta: 1.8038)\n",
      "❌ CBLL1      (Delta: 1.8038)\n",
      "❌ NRDC       (Delta: 1.8038)\n",
      "❌ POP7       (Delta: 1.8038)\n",
      "❌ HDLBP      (Delta: 1.8038)\n",
      "❌ ACTG1      (Delta: 1.7579)\n",
      "❌ HDGF       (Delta: 1.7001)\n",
      "❌ ETF1       (Delta: 1.7001)\n",
      "❌ TRIB2      (Delta: 1.7001)\n",
      "❌ PTPN1      (Delta: 1.7001)\n",
      "❌ SDF4       (Delta: 1.7001)\n",
      "❌ PSAP       (Delta: 1.7001)\n",
      "❌ NCL        (Delta: 1.6382)\n",
      "❌ NDUFB6     (Delta: 1.6210)\n",
      "❌ ECI1       (Delta: 1.6210)\n",
      "❌ LMO2       (Delta: 1.6210)\n",
      "❌ NDUFC1     (Delta: 1.6210)\n",
      "❌ LRPPRC     (Delta: 1.6210)\n",
      "❌ RHOBTB3    (Delta: 1.6210)\n",
      "❌ PRKDC      (Delta: 1.6210)\n",
      "❌ KPNA2      (Delta: 1.5971)\n",
      "❌ CALM2      (Delta: 1.5971)\n",
      "❌ PPIB       (Delta: 1.5536)\n",
      "❌ IGFBP2     (Delta: 1.5536)\n",
      "❌ ASH1L      (Delta: 1.5536)\n",
      "❌ SLC20A1    (Delta: 1.5536)\n",
      "❌ MIDN       (Delta: 1.5536)\n",
      "❌ HIC2       (Delta: 1.5536)\n",
      "❌ DDX39A     (Delta: 1.5536)\n",
      "❌ EID1       (Delta: 1.5536)\n",
      "❌ BUD31      (Delta: 1.5536)\n",
      "❌ ZNF326     (Delta: 1.5536)\n",
      "❌ PDCD6IP    (Delta: 1.5536)\n",
      "❌ MRPL40     (Delta: 1.4777)\n",
      "❌ HBZ        (Delta: 1.4159)\n",
      "❌ DANCR      (Delta: 1.4027)\n",
      "❌ RBM28      (Delta: 1.4011)\n",
      "✅ LDHA       (Delta: 1.4011)\n",
      "❌ TBX1       (Delta: 1.3971)\n",
      "❌ SLC39A8    (Delta: 1.3971)\n",
      "❌ KTN1       (Delta: 1.3971)\n",
      "❌ RSRC2      (Delta: 1.3971)\n",
      "❌ GAL        (Delta: 1.3971)\n",
      "❌ PHGDH      (Delta: 1.3971)\n",
      "❌ DNAJC2     (Delta: 1.3971)\n",
      "❌ SNHG21     (Delta: 1.3971)\n",
      "❌ FAF2       (Delta: 1.3971)\n",
      "❌ IQGAP1     (Delta: 1.3971)\n",
      "❌ COMT       (Delta: 1.3971)\n",
      "❌ MS4A3      (Delta: 1.3971)\n",
      "❌ FXYD5      (Delta: 1.3971)\n",
      "❌ HELZ       (Delta: 1.3971)\n",
      "❌ ASAH1      (Delta: 1.3971)\n",
      "❌ MDK        (Delta: 1.3971)\n",
      "❌ DNAJC25    (Delta: 1.3971)\n",
      "❌ GUK1       (Delta: 1.3971)\n",
      "❌ NEAT1      (Delta: 1.3971)\n",
      "❌ NABP1      (Delta: 1.3971)\n",
      "❌ CTNNAL1    (Delta: 1.3971)\n",
      "❌ TNRC6A     (Delta: 1.3971)\n",
      "❌ DIP2B      (Delta: 1.3971)\n",
      "❌ CLSPN      (Delta: 1.3971)\n",
      "❌ RESF1      (Delta: 1.3971)\n",
      "❌ CD320      (Delta: 1.3971)\n",
      "✅ CCNA2      (Delta: 1.3971)\n",
      "❌ SLC2A3     (Delta: 1.3971)\n",
      "❌ RAB5B      (Delta: 1.3971)\n",
      "❌ HHEX       (Delta: 1.3820)\n",
      "❌ ANKRD10    (Delta: 1.3820)\n",
      "❌ FSCN1      (Delta: 1.3820)\n",
      "❌ WIPI2      (Delta: 1.3820)\n",
      "❌ ANLN       (Delta: 1.3820)\n",
      "❌ LIMD2      (Delta: 1.3820)\n",
      "❌ B3GNT2     (Delta: 1.3820)\n",
      "❌ RHOC       (Delta: 1.3820)\n",
      "❌ SFSWAP     (Delta: 1.3820)\n",
      "❌ TMSB10     (Delta: 1.3820)\n",
      "❌ PUM1       (Delta: 1.3820)\n",
      "❌ PPP1R18    (Delta: 1.3820)\n",
      "❌ ABCE1      (Delta: 1.3820)\n",
      "❌ CLIP1      (Delta: 1.3820)\n",
      "❌ TGFB1      (Delta: 1.3820)\n",
      "❌ FAM20B     (Delta: 1.3820)\n",
      "❌ INO80D     (Delta: 1.3820)\n",
      "❌ ISOC2      (Delta: 1.3820)\n"
     ]
    }
   ],
   "source": [
    "print(f'Ground Truth')\n",
    "for val, idx in zip(real_values, real_indices):\n",
    "    g_name = gene_names[idx.item()]\n",
    "    \n",
    "    # Check if our model also listed this gene in its top 10\n",
    "    match_icon = '✅' if idx in pred_indices else '❌'\n",
    "\n",
    "    print(f'{match_icon} {g_name:<10} (Delta: {val.item():.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5149dd2-9faf-46b1-a940-982c2837288d",
   "metadata": {},
   "source": [
    "**Precition Calculation**\n",
    "Now let's calcualte the precision which is a simple correct / top K  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "31277253-fc8d-4dbf-9ca5-419a4c193843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision @ 100 | 0.0200\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Precision@10 Metric ---\n",
    "# How many of the Top 10 real genes were in the Top 10 predicted genes?\n",
    "# (Set intersection)\n",
    "set_pred = set(pred_indices.tolist())\n",
    "set_real = set(real_indices.tolist())\n",
    "overlap = len(set_pred.intersection(set_real))\n",
    "\n",
    "print(f'Precision @ {top_k} | {overlap/top_k:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2a562f-050c-4d00-b379-ceb320f6d863",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "As you can see, untrained we have quite a lot of error.  Also, we're juggling 3 models and our predictor relies on a chain of two models meaning that overall we'll need a lot of training to start learning the cell physics. \n",
    "\n",
    "Also, looking at how this works we can see that we have a limitation on our training data currently as we're learning genes and networks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb4fe30-c097-496f-80be-2ed1b6610796",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
