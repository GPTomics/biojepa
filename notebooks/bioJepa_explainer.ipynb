{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cedd2e3-bcee-4a58-abcf-62858c07bd0f",
   "metadata": {},
   "source": [
    "# Bio-JEPA AC Explainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ba0ded7-81b3-4afd-819b-299f9db18255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import copy\n",
    "import math\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cbf54b7-a62d-4cf0-9e81-ef07aec56ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54ca0e60-1213-4640-a688-bb7377085320",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('/Users/djemec/data/jepa')\n",
    "shard_dir = data_dir / 'tokenized'\n",
    "metadata_path = data_dir / 'perturbation_map.json'\n",
    "checkpoint_dir = data_dir / 'checkpoint'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8e09de-cb6f-4922-a39d-32c9ef5e9383",
   "metadata": {},
   "source": [
    "## Text Prep/Tokenization\n",
    "\n",
    "We previously loaded and saved our data into a tokenized files.  For a full explainer, look at the data_prep_explainer.ipynb notebook.   In this step we'll pull the data from the files to use for training.  First we'll start by getting all the files in the tokenized directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "543054b1-a33c-4ccf-a20a-f1c5d783aad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30 shards.\n"
     ]
    }
   ],
   "source": [
    "data_files = sorted(shard_dir.glob('*.npz'))\n",
    "print(f'Found {len(data_files)} shards.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba37f0b-d520-485e-9495-077e03653d4e",
   "metadata": {},
   "source": [
    "Now that we have the files, we need just a single file. Because of how we tokanized our files, we do not have to load them in any particular order so we'll select a random file from the list.  In training, we'd want to remove this from the file list to ensure we go through the full epoch before restarting.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efe3192c-11ee-47b7-8631-fee7be5d32c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, PosixPath('/Users/djemec/data/jepa/tokenized/shard_0019.npz'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_file_idx = np.random.randint(len(data_files))\n",
    "file_path = data_files[random_file_idx]\n",
    "random_file_idx, file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952899be-68ad-4e37-8a9b-5b662d1d4482",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631425fd-ae6d-4b97-82cd-f307002812a2",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1043fd-c1c0-4e2f-a591-ac23bec2630f",
   "metadata": {},
   "source": [
    "\n",
    "To start, we need to get enough data to run the forward and backward passes.  Since our total dataset is likely too big to be held in memory all at once in real practice, we will read just enough file information into memory so that we can run the passes, leaving memory and compute to be used on the passes instead of static data holding. \n",
    "\n",
    "Recall that we saved the data in the following structure:\n",
    "\n",
    "```\n",
    "{\n",
    "    'control':[array of network expression information],\n",
    "    'case':[array of network expression information],\n",
    "    'action_ids':[array of ids of the perturbation]\n",
    "}\n",
    "```\n",
    "\n",
    "Because of this, taking the same index out of each array gives us our action_id, pertrubed data, and control data. For training we'll do more than 1 cell at a time.  We'll use a variable `BATCH_SIZE` to control how much data to train on at a time.  In this case we'll start with 2. During training we'll want a increase our batch size.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "687c858b-51ee-4b9d-8b11-4aeaf3389cda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([6448, 9579]),\n",
       " tensor([1499,  777]),\n",
       " tensor([[0.6825, 0.5222, 0.5322,  ..., 0.3849, 0.2634, 0.5255],\n",
       "         [0.7225, 0.5116, 0.6232,  ..., 0.4091, 0.3447, 0.5609]]),\n",
       " tensor([[0.7002, 0.6049, 0.5186,  ..., 0.3651, 0.1818, 0.5686],\n",
       "         [0.7559, 0.6287, 0.6083,  ..., 0.4122, 0.3186, 0.5887]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 2\n",
    "with np.load(file_path) as data:\n",
    "    # Keys: 'control', 'treated', 'action_ids'\n",
    "    n_rows = data['action_ids'].shape[0]\n",
    "    row_idx = np.random.randint(n_rows, size=BATCH_SIZE)\n",
    "    \n",
    "    control_raw = data['control'][row_idx]\n",
    "    case_raw = data['case'][row_idx]\n",
    "    act_id = data['action_ids'][row_idx]\n",
    "\n",
    "x_control = torch.tensor(control_raw)\n",
    "x_case = torch.tensor(case_raw)\n",
    "action_id = torch.tensor(act_id, dtype=torch.long)\n",
    "\n",
    "row_idx, action_id, x_control, x_case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43148f73-be59-4bc8-9b53-3e9eca075646",
   "metadata": {},
   "source": [
    "## Forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739cbb8b-5abb-4de4-9753-785e6fabc05f",
   "metadata": {},
   "source": [
    "**RoPE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ae7b27e-170e-4c31-89da-701fecff22ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_seq_len=2048):\n",
    "        super().__init__()\n",
    "        # Ensure dim is the head dimension, not the full embedding dimension\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        t = torch.arange(max_seq_len).type_as(inv_freq)\n",
    "        freqs = torch.outer(t, inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.register_buffer('emb', emb)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [Batch, Seq, Dim]\n",
    "        # We only care about the sequence length here\n",
    "        n = x.shape[1]\n",
    "        # Returns [Seq, Head_Dim]\n",
    "        return self.emb[:n, :].cos(), self.emb[:n, :].sin()\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin):\n",
    "    # q, k: [Batch, Heads, Seq, Head_Dim]\n",
    "    # cos, sin: [Seq, Head_Dim] -> reshape to [1, 1, Seq, Head_Dim]\n",
    "    \n",
    "    cos = cos.unsqueeze(0).unsqueeze(0)\n",
    "    sin = sin.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # Standard RoPE rotation logic\n",
    "    # split last dim into half\n",
    "    q_d = q.shape[-1] // 2\n",
    "    k_d = k.shape[-1] // 2\n",
    "    \n",
    "    q1, q2 = q[..., :q_d], q[..., q_d:]\n",
    "    k1, k2 = k[..., :k_d], k[..., k_d:]\n",
    "    \n",
    "    q_rotated = torch.cat((-q2, q1), dim=-1)\n",
    "    k_rotated = torch.cat((-k2, k1), dim=-1)\n",
    "    \n",
    "    q_out = (q * cos) + (q_rotated * sin)\n",
    "    k_out = (k * cos) + (k_rotated * sin)\n",
    "    \n",
    "    return q_out, k_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993c4bd8-bfac-46ac-ab69-a2da56dccf6f",
   "metadata": {},
   "source": [
    "**Multi-Headed Attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41791397-532a-4572-bfb2-f1a08bfbc62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioMultiHeadAttention(nn.Module):\n",
    "    # mirrors nn.MultiheadAttention(dim, heads, batch_first=True) \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.embed_dim % config.heads == 0\n",
    "        \n",
    "        self.head_dim = config.embed_dim // config.heads\n",
    "        self.heads = config.heads\n",
    "        self.embed_dim = config.embed_dim\n",
    "        \n",
    "        # Projections\n",
    "        self.q_proj = nn.Linear(config.embed_dim, config.embed_dim)\n",
    "        self.k_proj = nn.Linear(config.embed_dim, config.embed_dim)\n",
    "        self.v_proj = nn.Linear(config.embed_dim, config.embed_dim)\n",
    "        \n",
    "        self.c_proj = nn.Linear(config.embed_dim, config.embed_dim)\n",
    "\n",
    "    def forward(self, x, cos, sin):\n",
    "        B, T, C = x.size() # Batch, Seq, Embed Dim\n",
    "        \n",
    "        # 1. Project\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "\n",
    "        # 2. Reshape for multi-head attention\n",
    "        # (B, T, nh, hs) -> (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, T, self.heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # 3. Apply RoPE to Q and K (Rotary is applied per head)\n",
    "        q, k = apply_rotary_pos_emb(q, k, cos, sin)\n",
    "\n",
    "        # 4. Attention\n",
    "        # is_causal=False because this is a bidirectional encoder\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=False)\n",
    "        \n",
    "        # 5. Reassemble\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.c_proj(y)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e754b8e-8536-44da-84cd-05f617f34af0",
   "metadata": {},
   "source": [
    "**MLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aeb98e8a-868c-4f8f-991c-84cdc4e887d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.embed_dim, int(config.mlp_ratio * config.embed_dim))\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "        self.c_proj = nn.Linear(int(config.mlp_ratio * config.embed_dim), config.embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0306eb8-042e-4dda-bc35-99a73d1e1ffb",
   "metadata": {},
   "source": [
    "**Hidden Transformer Block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90042ed3-5020-49be-8914-568d25a605c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PathwayBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.embed_dim)\n",
    "        self.attn = BioMultiHeadAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.embed_dim)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x, cos, sin):\n",
    "        # 1. Attention with RoPE\n",
    "        x_norm = self.ln_1(x)\n",
    "        # Pass cos/sin into attn to be applied to Q/K\n",
    "        attn_out = self.attn(x_norm, cos, sin)\n",
    "        x = x + attn_out\n",
    "\n",
    "        # 2. MLP\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f6508f-895c-45c9-ace7-04f3fbda30c5",
   "metadata": {},
   "source": [
    "**Main Pathway Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2151f336-a980-4466-a2b4-2b342f701e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PathwayEncoderConfig:\n",
    "    num_pathways: int = 1024 \n",
    "    n_layer: int = 24 \n",
    "    heads: int = 12\n",
    "    embed_dim: int = 768\n",
    "    mlp_ratio: float = 4.0 # Changed to float for precision\n",
    "\n",
    "class PathwayEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            # input projects to embedding\n",
    "            input_proj = nn.Linear(1, config.embed_dim),\n",
    "            \n",
    "            # RoPE needs the HEAD dimension, not the full embedding dimension\n",
    "            rope = RotaryEmbedding(config.embed_dim // config.heads),\n",
    "            \n",
    "            # transformer block\n",
    "            blocks = nn.ModuleList([PathwayBlock(config) for _ in range(config.n_layer)]),\n",
    "            \n",
    "            # final layer norm\n",
    "            ln_f = nn.LayerNorm(config.embed_dim) \n",
    "        ))\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        std = 0.02\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [Batch, Num_Pathways]\n",
    "        x = x.unsqueeze(-1) # [B, N, 1]\n",
    "        x = self.transformer.input_proj(x) # [B, N, Dim]\n",
    "        \n",
    "        # Generate RoPE cache\n",
    "        # cos, sin are [Seq, Head_Dim]\n",
    "        cos, sin = self.transformer.rope(x)\n",
    "        cos, sin = cos.to(x.device), sin.to(x.device)\n",
    "        \n",
    "        for block in self.transformer.blocks:\n",
    "            x = block(x, cos, sin)\n",
    "            \n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02752d4-46e3-4558-8468-d2e7843ba3ff",
   "metadata": {},
   "source": [
    "### Create The Student And Teacher Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "caa8f876-43cf-496f-a487-54da0f38f915",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 4\n",
    "n_heads = 2\n",
    "n_pathways = 1024\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f56c881d-e7be-428e-a09f-4da8f3b4135b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PathwayEncoder(\n",
       "  (transformer): ModuleDict(\n",
       "    (input_proj): Linear(in_features=1, out_features=4, bias=True)\n",
       "    (rope): RotaryEmbedding()\n",
       "    (blocks): ModuleList(\n",
       "      (0): PathwayBlock(\n",
       "        (ln_1): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): BioMultiHeadAttention(\n",
       "          (q_proj): Linear(in_features=4, out_features=4, bias=True)\n",
       "          (k_proj): Linear(in_features=4, out_features=4, bias=True)\n",
       "          (v_proj): Linear(in_features=4, out_features=4, bias=True)\n",
       "          (c_proj): Linear(in_features=4, out_features=4, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=4, out_features=16, bias=True)\n",
       "          (gelu): GELU(approximate='tanh')\n",
       "          (c_proj): Linear(in_features=16, out_features=4, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student = PathwayEncoder(PathwayEncoderConfig(n_layer= 1, heads=n_heads, embed_dim= n_embd))\n",
    "student"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09134ca7-76b4-47fd-a0b6-e9500da152f5",
   "metadata": {},
   "source": [
    "**Teacher** Now let's make a copy for the teacher. Since the teacher does not require updates, we can update the model to not require gradient updates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a257b3da-5334-46d2-9715-b3ec001efc56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PathwayEncoder(\n",
       "  (transformer): ModuleDict(\n",
       "    (input_proj): Linear(in_features=1, out_features=4, bias=True)\n",
       "    (rope): RotaryEmbedding()\n",
       "    (blocks): ModuleList(\n",
       "      (0): PathwayBlock(\n",
       "        (ln_1): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): BioMultiHeadAttention(\n",
       "          (q_proj): Linear(in_features=4, out_features=4, bias=True)\n",
       "          (k_proj): Linear(in_features=4, out_features=4, bias=True)\n",
       "          (v_proj): Linear(in_features=4, out_features=4, bias=True)\n",
       "          (c_proj): Linear(in_features=4, out_features=4, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=4, out_features=16, bias=True)\n",
       "          (gelu): GELU(approximate='tanh')\n",
       "          (c_proj): Linear(in_features=16, out_features=4, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher = copy.deepcopy(student)\n",
    "teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b34811d-ae3f-4d9a-97f9-b3d559a13191",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in teacher.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c22025e-cf3b-447d-a219-7562d1d5b71f",
   "metadata": {},
   "source": [
    "### Forward Pass: Teacher\n",
    "Teacher encodes the Target (Case). We'll make sure to run it without gradient accumulation.  This teacher creates an embedding of the case cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c02b1af1-fbbb-4533-af8d-8792d97347d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.7197,  0.8458, -1.2067, -0.3588],\n",
       "         [ 0.6795,  0.8456, -1.1249, -0.4002],\n",
       "         [ 0.6304,  0.8405, -1.0339, -0.4369],\n",
       "         ...,\n",
       "         [ 0.4987,  0.8106, -0.8202, -0.4891],\n",
       "         [ 0.2261,  0.7062, -0.4481, -0.4842],\n",
       "         [ 0.6606,  0.8442, -1.0889, -0.4159]],\n",
       "\n",
       "        [[ 0.7269,  0.8607, -1.2420, -0.3456],\n",
       "         [ 0.6780,  0.8645, -1.1409, -0.4016],\n",
       "         [ 0.6682,  0.8644, -1.1218, -0.4108],\n",
       "         ...,\n",
       "         [ 0.5312,  0.8461, -0.8862, -0.4910],\n",
       "         [ 0.4273,  0.8181, -0.7313, -0.5140],\n",
       "         [ 0.6581,  0.8641, -1.1027, -0.4195]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    target_latents = teacher(x_case)\n",
    "target_latents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1e472d-8fab-4cc5-b9c1-960e233b2615",
   "metadata": {},
   "source": [
    "### Forward Pass: Student\n",
    "Student encodes the Context (Control), With the student we need to accumulate gradients since we need to be able to train the model. \n",
    "> TO DO: Add masking here if you want extra difficulty, but the task Control->Treated is enough for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "143c7081-8409-4d8f-af6f-c22b05f04454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.7095,  0.8514, -1.1911, -0.3698],\n",
       "         [ 0.6285,  0.8470, -1.0360, -0.4395],\n",
       "         [ 0.6349,  0.8478, -1.0474, -0.4353],\n",
       "         ...,\n",
       "         [ 0.5150,  0.8236, -0.8499, -0.4888],\n",
       "         [ 0.3613,  0.7735, -0.6292, -0.5055],\n",
       "         [ 0.6306,  0.8473, -1.0398, -0.4381]],\n",
       "\n",
       "        [[ 0.7263,  0.8471, -1.2229, -0.3504],\n",
       "         [ 0.6243,  0.8420, -1.0250, -0.4412],\n",
       "         [ 0.6869,  0.8480, -1.1414, -0.3935],\n",
       "         ...,\n",
       "         [ 0.5421,  0.8252, -0.8886, -0.4787],\n",
       "         [ 0.4739,  0.8062, -0.7850, -0.4951],\n",
       "         [ 0.6548,  0.8458, -1.0801, -0.4205]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_latents = student(x_control)\n",
    "context_latents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f70a5f8-0f52-402e-8bd9-eeac1074f912",
   "metadata": {},
   "source": [
    "## Predictor Setup\n",
    "We need to create a predictor that tries to guess Target (from the teacher) given Context + Action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bb2ed4-6561-4b8e-9cb5-4bf857ddad91",
   "metadata": {},
   "source": [
    "**Adaptive Layer Normalization AdaLN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77dc0a71-29c4-423d-ba89-9595cdac0bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaLN(nn.Module):\n",
    "    '''\n",
    "    Adaptive Layer Norm for conditioning the predictor on action embeddings.\n",
    "    The action vector regresses the Scale (gamma) and Shift (beta) of the normalization.\n",
    "    '''\n",
    "    def __init__(self, embed_dim, action_embed_dim):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(embed_dim, elementwise_affine=False)\n",
    "        self.action_mlp = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(action_embed_dim, 2 * embed_dim)\n",
    "        )\n",
    "        # Initialize to identity (gamma=0, beta=0 originally, effectively gamma=1 after logic)\n",
    "        # Zero-init the last layer so the action starts as a \"no-op\" (identity)\n",
    "        nn.init.zeros_(self.action_mlp[1].weight)\n",
    "        nn.init.zeros_(self.action_mlp[1].bias)\n",
    "\n",
    "    def forward(self, x, action_emb):\n",
    "        # x: [Batch, Seq, Dim]\n",
    "        # action_emb: [Batch, action_embed_dim]\n",
    "        \n",
    "        # Project action to style: [B, 2*D] -> [B, 1, 2*D]\n",
    "        style = self.action_mlp(action_emb).unsqueeze(1) \n",
    "        gamma, beta = style.chunk(2, dim=-1)\n",
    "        \n",
    "        # Apply affine transformation based on action\n",
    "        return self.norm(x) * (1 + gamma) + beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93608ce7-5ba4-488e-849e-05c298bfc50b",
   "metadata": {},
   "source": [
    "**Predictor Block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "009a430a-202c-4eff-974e-d5dfc9beca30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictorBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # 1. Conditioning (AdaLN) replaces standard LayerNorm\n",
    "        self.ada_ln1 = AdaLN(config.embed_dim, config.action_embed_dim)\n",
    "        \n",
    "        # 2. Attention (Using the shared BioMultiHeadAttention)\n",
    "        self.attn = BioMultiHeadAttention(config)\n",
    "        \n",
    "        # 3. Conditioning (AdaLN) for the MLP block\n",
    "        self.ada_ln2 = AdaLN(config.embed_dim, config.action_embed_dim)\n",
    "        \n",
    "        # 4. MLP (Using the shared MLP)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x, action_emb, cos, sin):\n",
    "        # 1. AdaLN -> Attention (with internal RoPE) -> Residual\n",
    "        x_norm = self.ada_ln1(x, action_emb)\n",
    "        \n",
    "        # Note: BioMultiHeadAttention handles q/k/v projection and apply_rotary_pos_emb internally\n",
    "        attn_out = self.attn(x_norm, cos, sin)\n",
    "        x = x + attn_out\n",
    "        \n",
    "        # 2. AdaLN -> MLP -> Residual\n",
    "        x_norm = self.ada_ln2(x, action_emb)\n",
    "        x = x + self.mlp(x_norm)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27332d23-4dd4-4c4a-b8a2-24438e1bca6f",
   "metadata": {},
   "source": [
    "**Main Predictor Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d4e7df9-10e8-4e35-bc69-623f5b4dae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ACPredictorConfig:\n",
    "    num_pathways: int = 1024\n",
    "    n_layer: int = 6 \n",
    "    heads: int = 4\n",
    "    embed_dim: int = 384\n",
    "    action_embed_dim: int=256 \n",
    "    mlp_ratio: float = 4.0\n",
    "    max_perturb: int= 2058 ## eventually try to get to a 2**N power\n",
    "\n",
    "class ACPredictor(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Action Embedding (Discrete ID -> Vector)\n",
    "        self.action_embed = nn.Embedding(config.max_perturb, config.action_embed_dim)\n",
    "        \n",
    "        # Learnable Queries (\"Mask Tokens\") for the future state\n",
    "        # One query vector per pathway position\n",
    "        self.mask_queries = nn.Parameter(torch.randn(1, config.num_pathways, config.embed_dim) * 0.02)\n",
    "        \n",
    "        # RoPE: initialized with HEAD dimension (dim // heads)\n",
    "        head_dim = config.embed_dim // config.heads\n",
    "        self.rope = RotaryEmbedding(head_dim)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            PredictorBlock(config) for _ in range(config.n_layer)\n",
    "        ])\n",
    "        \n",
    "        self.final_norm = AdaLN(config.embed_dim, config.action_embed_dim)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        std = 0.02\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "\n",
    "    def forward(self, context_latents, action_ids):\n",
    "        \"\"\"\n",
    "        context_latents: [Batch, N, Dim] (From Student Encoder)\n",
    "        action_ids: [Batch] (Ints)\n",
    "        \"\"\"\n",
    "        B, N, D = context_latents.shape\n",
    "        \n",
    "        # 1. Embed Action\n",
    "        action_emb = self.action_embed(action_ids) # [B, action_embed_dim]\n",
    "        \n",
    "        # 2. Construct Input: [Context, Mask_Queries]\n",
    "        # We concatenate the learned queries to the context. \n",
    "        # The predictor will attend to the context to update the queries.\n",
    "        queries = self.mask_queries.repeat(B, 1, 1) # [B, N, D]\n",
    "        sequence = torch.cat([context_latents, queries], dim=1) # [B, 2N, D]\n",
    "        \n",
    "        # 3. Generate RoPE for the full sequence (2N)\n",
    "        # cos, sin are [2N, Head_Dim]\n",
    "        cos, sin = self.rope(sequence)\n",
    "        cos, sin = cos.to(sequence.device), sin.to(sequence.device)\n",
    "        \n",
    "        # 4. Pass through AdaLN Blocks\n",
    "        for block in self.blocks:\n",
    "            sequence = block(sequence, action_emb, cos, sin)\n",
    "            \n",
    "        sequence = self.final_norm(sequence, action_emb)\n",
    "        \n",
    "        # 5. Return only the predicted part (The Queries corresponding to N..2N)\n",
    "        predictions = sequence[:, N:, :] \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a087de0-e0a3-49bf-b09e-b917d492eb5c",
   "metadata": {},
   "source": [
    "### Create the Predictor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7f5531a-f3c3-4472-b638-5a3a17d18322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ACPredictor(\n",
       "  (action_embed): Embedding(2058, 4)\n",
       "  (rope): RotaryEmbedding()\n",
       "  (blocks): ModuleList(\n",
       "    (0): PredictorBlock(\n",
       "      (ada_ln1): AdaLN(\n",
       "        (norm): LayerNorm((4,), eps=1e-05, elementwise_affine=False)\n",
       "        (action_mlp): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=4, out_features=8, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (attn): BioMultiHeadAttention(\n",
       "        (q_proj): Linear(in_features=4, out_features=4, bias=True)\n",
       "        (k_proj): Linear(in_features=4, out_features=4, bias=True)\n",
       "        (v_proj): Linear(in_features=4, out_features=4, bias=True)\n",
       "        (c_proj): Linear(in_features=4, out_features=4, bias=True)\n",
       "      )\n",
       "      (ada_ln2): AdaLN(\n",
       "        (norm): LayerNorm((4,), eps=1e-05, elementwise_affine=False)\n",
       "        (action_mlp): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=4, out_features=8, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Linear(in_features=4, out_features=16, bias=True)\n",
       "        (gelu): GELU(approximate='tanh')\n",
       "        (c_proj): Linear(in_features=16, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_norm): AdaLN(\n",
       "    (norm): LayerNorm((4,), eps=1e-05, elementwise_affine=False)\n",
       "    (action_mlp): Sequential(\n",
       "      (0): SiLU()\n",
       "      (1): Linear(in_features=4, out_features=8, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor = ACPredictor(ACPredictorConfig(n_layer=1, heads=n_heads, embed_dim=n_embd, action_embed_dim=n_embd))\n",
    "predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e2841b-32fa-4d41-b7c2-115723f24903",
   "metadata": {},
   "source": [
    "### Forward Pass: Action Predictor\n",
    "The Action predictor tries to guess Target encoding given Context + Action.  The context is the ouput of the Student and the action is from our tokenized data.  The goal here is tha the output attempts to match the teacher. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd8dd03a-f9ef-45e9-910b-42ee77ab28cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1499,  777])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ccf6c1ff-330c-4f46-abca-35571ef07f91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.3121, -0.4020,  0.3994,  1.3180],\n",
       "         [ 1.5843, -0.7185, -0.8059, -0.0594],\n",
       "         [-0.3733,  1.6866, -0.8537, -0.4579],\n",
       "         ...,\n",
       "         [ 0.4309,  1.2771, -1.1615, -0.5455],\n",
       "         [-1.5878, -0.0927,  0.7758,  0.9083],\n",
       "         [ 1.3567, -0.6971, -1.1521,  0.4932]],\n",
       "\n",
       "        [[-1.3147, -0.4032,  0.4006,  1.3160],\n",
       "         [ 1.5850, -0.7198, -0.8036, -0.0621],\n",
       "         [-0.3746,  1.6880, -0.8511, -0.4605],\n",
       "         ...,\n",
       "         [ 0.4306,  1.2780, -1.1584, -0.5485],\n",
       "         [-1.5906, -0.0935,  0.7768,  0.9061],\n",
       "         [ 1.3571, -0.6986, -1.1494,  0.4908]]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_latents = predictor(context_latents, action_id)\n",
    "predicted_latents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb98c36-d1ad-42d4-8e61-2465bdedbd4d",
   "metadata": {},
   "source": [
    "# Loss Calculation\n",
    "\n",
    "We'll use L1 Loss (Mean Absolute Error) which measures the average absolute difference between predicted and actual values, making it robust to outliers.  We'll call this latent loss since we'll  use an L1 loss function within the latent space of a model to ensure the compressed representation (latent) accurately reflects the original data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09d24c1a-523a-4bae-b8ff-630df3b563f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0230, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = F.l1_loss(predicted_latents, target_latents)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3633ff-5398-4928-a40e-8133d2eaaad1",
   "metadata": {},
   "source": [
    "# Back Propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2abb5a2e-5d83-41b6-82a5-25122a59e4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c951691-68de-4246-9d0b-8a9f359f67ae",
   "metadata": {},
   "source": [
    "## Gradient Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "017d9b0a-fac7-4d10-a247-fece2a39732d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.8341)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.utils.clip_grad_norm_(predictor.parameters(), 1.0)\n",
    "torch.nn.utils.clip_grad_norm_(student.parameters(), 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f7402f-7255-4b0a-9010-5a67015c6647",
   "metadata": {},
   "source": [
    "## Update the Teacher\n",
    "\n",
    "Recall that our teacher was created from a copy of our student. This means the weights are initiated randomly. We also didn't include any backprop for our teacher as since our loss is calculated using the action predictor and the student model.  To update the teacher we'll use a mechanism called  **Exponential Moving Average (EMA)**\n",
    "\n",
    "In simple terms, this mechanism forces the Teacher to evolve very slowly, becoming a stable, wiser version of the Student.\n",
    "\n",
    "Here is the breakdown of the mechanics. Mathematically, it calculates:\n",
    "\n",
    "$Teacher_{new} = (m * Teacher_{old})+((1−m)*Student_{current})$\n",
    "\n",
    "With a typical momentum ($m$) of 0.996, the code tells the Teacher: *\"Keep 99.6% of your current weights, and mix in just 0.4% of what the Student learned in this last step.\"*\n",
    "\n",
    "The reason we do this is Stability. The Student network is updating rapidly and noisily using Gradient Descent ($Param←Param−LearningRate×Gradient$). If we simply copied the Student to the Teacher (`Teacher = Student`), the target we are trying to predict would move erratically every single step, making it impossible to learn (like trying to hit a bullseye that is vibrating wildly).\n",
    "\n",
    "By updating the Teacher this way, it effectively represents the **average** of the Student's weights over the last several thousand steps. This smooths out the noise and provides a steady, high-quality target for the Student to predict, preventing \"Model Collapse\" (where the model cheats by outputting zeros for everything).\n",
    "\n",
    "TO:DO  we could also replace the teacher with a pretrained model and use this as an update mechanism. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "99ec34e1-1dea-4401-b158-2297bb877664",
   "metadata": {},
   "outputs": [],
   "source": [
    "m=0.996\n",
    "for param_s, param_t in zip(student.parameters(), teacher.parameters()):\n",
    "    param_t.data.mul_(m).add_((1 - m) * param_s.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20069d51-461a-406c-8861-1b4ab73d9830",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Now we've done a walkthrough of the forward pass of the model.  With this we can see a lot of room for improvement but a decent starting point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be12085-8ccf-4153-916e-c87bd94fa53b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
