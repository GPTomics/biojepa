{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71522d3b-8805-48e6-bb36-1c9a7bd15482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import pearsonr\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "\n",
    "import biojepa_ac_model as model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe98e32-54b3-4da6-adf9-8b3e76e3bc0b",
   "metadata": {},
   "source": [
    "## BioJEPA Model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bae5c1c-d7c4-4216-ab99-42a26b7aee75",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f67335c-63ea-4f80-9c10-96b4d3e09c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cpu\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    device = 'cpu'\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(1337)\n",
    "        device = 'cuda'\n",
    "    # elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    #     device = 'mps'\n",
    "    print(f'using {device}')\n",
    "    return device\n",
    "\n",
    "DEVICE = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5ec64e1-0d7a-4dee-b478-b99f901ce9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n_embd = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "025d6281-f460-46eb-89d9-842d1ca6fa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('/Users/djemec/data/jepa')\n",
    "eval_dir = data_dir / 'tokenized' / 'eval'\n",
    "\n",
    "mask_path = eval_dir / 'binary_pathway_mask.npy'\n",
    "metadata_path = eval_dir / 'perturbation_map.json'\n",
    "checkpoint_dir = data_dir / 'checkpoint'\n",
    "gene_names_path = eval_dir / 'gene_names.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9671701-5cc4-4016-a354-b71238c98947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Pathway Mask...\n",
      "Mask Loaded: 4096 Genes -> 1024 Pathways\n",
      "Loaded 2058 perturbations\n",
      "Loaded 4096 genes\n"
     ]
    }
   ],
   "source": [
    "print('Loading Pathway Mask...')\n",
    "binary_mask = np.load(mask_path)\n",
    "n_genes, n_pathways = binary_mask.shape\n",
    "print(f'Mask Loaded: {n_genes} Genes -> {n_pathways} Pathways')\n",
    "\n",
    "with open(metadata_path, 'r') as f:\n",
    "    pert_map = json.load(f)\n",
    "id_to_pert = {v: k for k, v in pert_map.items()}\n",
    "print(f'Loaded {len(id_to_pert.keys())} perturbations')\n",
    "\n",
    "with open(data_dir / 'gene_names.json', \"r\") as f:\n",
    "    gene_names = json.load(f)\n",
    "print(f'Loaded {len(gene_names)} genes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fe46b1b-aff7-4355-909f-9f71761f11c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = model.BioJepaConfig(\n",
    "    mask_matrix=binary_mask, \n",
    "    num_genes=n_genes,\n",
    "    num_pathways=n_pathways,\n",
    "    embed_dim=n_embd,\n",
    "    heads=1\n",
    ")\n",
    "model = model.BioJepa(config).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a609e628-943e-41c2-a4d7-a71b9075a640",
   "metadata": {},
   "source": [
    "**Load Checkpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4aac628e-6ab7-4768-955a-3f99e79f58ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = checkpoint_dir / 'bio_jepa_ckpt_4epoch_final.pt'\n",
    "checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "\n",
    "keys = model.load_state_dict(checkpoint['model'])\n",
    "keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8287a64f-9d0d-4fbe-8c9b-a03c463a1487",
   "metadata": {},
   "source": [
    "**Freeze Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63812672-368f-483a-be2a-8b65cc849a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c98b4a-5df8-4894-9c56-ae0dbcbc3acd",
   "metadata": {},
   "source": [
    "## Build Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3dec1f3-d9cc-4849-aaa8-6a7d60199432",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BenchmarkDecoderConfig:\n",
    "    embed_dim: int = 384\n",
    "    num_pathways: int = 1024\n",
    "    num_genes: int = 4096\n",
    "    \n",
    "class BenchmarkDecoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # Step 1: Collapse the embedding dimension (384 -> 1)\n",
    "        # This asks: \"How active is this pathway overall?\"\n",
    "        self.pool = nn.Linear(config.embed_dim, 1) \n",
    "        \n",
    "        # Step 2: Decode Pathway Activity -> Gene Expression\n",
    "        # This learns the specific contribution of each pathway to each gene\n",
    "        self.decode = nn.Linear(config.num_pathways, config.num_genes) \n",
    "        \n",
    "    def forward(self, latents):\n",
    "        # latents: [Batch, 1024, 384]\n",
    "        \n",
    "        # 1. Calculate Pathway Scores\n",
    "        # [B, 1024, 384] -> [B, 1024, 1] -> [B, 1024]\n",
    "        scores = self.pool(latents).squeeze(-1)\n",
    "        \n",
    "        # 2. Project to Genes\n",
    "        # [B, 1024] @ [1024, 2000] -> [B, 2000]\n",
    "        gene_preds = self.decode(scores)\n",
    "        \n",
    "        return gene_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c316fe59-e4aa-4d12-9db4-2e38781f53bc",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bfad6fe-82ef-4019-b0ae-0338089f11c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_shard(filename):\n",
    "    print(f'loading {filename}') # Optional: reduce noise\n",
    "    with np.load(filename) as data:\n",
    "        # Load all arrays into memory\n",
    "        # We convert to correct types immediately to save hassle later\n",
    "        control_x = data['control'].astype(np.float32)\n",
    "        control_tot = data['control_total'].astype(np.float32)\n",
    "        case_x = data['case'].astype(np.float32)\n",
    "        case_tot = data['case_total'].astype(np.float32)\n",
    "        action_ids = data['action_ids'].astype(np.int64)\n",
    "        \n",
    "    return control_x, control_tot, case_x, case_tot, action_ids\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, batch, split, device, tok_dir):\n",
    "        self.batch = batch\n",
    "        self.split = split\n",
    "        self.device = device\n",
    "        \n",
    "        # 1. Find Shards\n",
    "        data_root = tok_dir / f'{split}'\n",
    "        shards = list(data_root.glob('*.npz'))\n",
    "\n",
    "        self.total_files = len(shards)\n",
    "        self.shards = sorted(shards)\n",
    "\n",
    "        assert len(shards) > 0, f'no shards found for split {split}'\n",
    "        print(f'found {len(shards)} shards for split {split}')\n",
    "        \n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # Create a randomized queue of shards\n",
    "        self.remaining_shards = list(self.shards)\n",
    "        random.shuffle(self.remaining_shards)\n",
    "        \n",
    "        self.current_shard_idx = -1\n",
    "        self.load_next_shard()\n",
    "\n",
    "    def load_next_shard(self):\n",
    "        self.current_shard_idx += 1\n",
    "        \n",
    "        # If we ran out of shards, reset (Epoch done)\n",
    "        if self.current_shard_idx >= len(self.remaining_shards):\n",
    "            self.reset() # This resets shard_idx to -1 and reshuffles\n",
    "            return \n",
    "\n",
    "        # Load the file\n",
    "        filename = self.remaining_shards[self.current_shard_idx]\n",
    "        self.data_tuple = load_shard(filename)\n",
    "        \n",
    "        # Shuffle the items INSIDE the shard\n",
    "        # This is critical so we don't just memorize the sorted order of the shard\n",
    "        n_samples = len(self.data_tuple[0])\n",
    "        self.perm = np.random.permutation(n_samples)\n",
    "        self.current_position = 0\n",
    "        self.total_samples_in_shard = n_samples\n",
    "\n",
    "    def next_batch(self):\n",
    "        batch = self.batch\n",
    "        \n",
    "        # Check if we have enough data left in current shard\n",
    "        if self.current_position + batch > self.total_samples_in_shard:\n",
    "            self.load_next_shard()\n",
    "            # Recursively call to get batch from the new shard\n",
    "            return self.next_batch()\n",
    "            \n",
    "        # Get indices for this batch\n",
    "        indices = self.perm[self.current_position : self.current_position + batch]\n",
    "        self.current_position += batch\n",
    "        \n",
    "        # Slice data using the shuffled indices\n",
    "        # data_tuple structure: (xc, xct, xt, xtt, aid)\n",
    "        batch_cont_x  = torch.from_numpy(self.data_tuple[0][indices]).to(self.device)\n",
    "        batch_cont_tot = torch.from_numpy(self.data_tuple[1][indices]).to(self.device)\n",
    "        batch_case_x  = torch.from_numpy(self.data_tuple[2][indices]).to(self.device)\n",
    "        batch_case_t = torch.from_numpy(self.data_tuple[3][indices]).to(self.device)\n",
    "        batch_aid = torch.from_numpy(self.data_tuple[4][indices]).to(self.device)\n",
    "        \n",
    "        return batch_cont_x, batch_cont_tot, batch_case_x, batch_case_t, batch_aid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7f7811-5727-45e3-8194-505072bb16eb",
   "metadata": {},
   "source": [
    "**Data Loader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e1d5000-f278-4f9b-9227-9554f31e9a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 11 shards for split train\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/train/shard_train_0000.npz\n",
      "found 2 shards for split val\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/val/shard_val_0001.npz\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoaderLite(batch=batch_size, split='train', device=DEVICE, tok_dir=eval_dir)\n",
    "val_loader = DataLoaderLite(batch=batch_size, split='val', device=DEVICE, tok_dir=eval_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9025bf-05d7-4ff8-ae9a-c1b87471c3e5",
   "metadata": {},
   "source": [
    "## Training Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f484f1f2-2933-413d-80c0-f0c970656453",
   "metadata": {},
   "source": [
    "### Training Config/Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad700ee3-e710-43de-be89-4003c922a41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_decoder = 1e-2\n",
    "epochs = 2\n",
    "tok_file_chunk_size = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0494e7de-e96d-4989-b47b-9229c050a263",
   "metadata": {},
   "source": [
    "**Initialize Decoder** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74224c15-ff49-41d9-9552-b8ec9ed83dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BenchmarkDecoderConfig(\n",
    "    embed_dim= n_embd,\n",
    "    num_pathways= n_pathways,\n",
    "    num_genes= n_genes\n",
    ")\n",
    "\n",
    "decoder = BenchmarkDecoder(config).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2da30fa-9c44-40be-921c-5184c764f0e5",
   "metadata": {},
   "source": [
    "**Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b197e689-8b98-4cf9-8382-6116bf2ce440",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(decoder.parameters(), lr=lr_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958f11cf-8c9f-49b9-b526-74be69d2126f",
   "metadata": {},
   "source": [
    "**Training Lenght**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "009526cc-7703-497d-87cb-4a50f9c1c1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_total_examples = 101682\n",
    "val_total_examples = 11044\n",
    "test_total_examples = 38829"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "766a254e-2fd7-4274-91c4-7ceab21d8d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3177, 31770)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch = train_total_examples // batch_size\n",
    "max_steps = epochs * steps_per_epoch\n",
    "steps_per_epoch, max_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cf4803-b42e-4d1b-89db-c9b43d6e3558",
   "metadata": {},
   "source": [
    "**Scheduler**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c683627d-3da6-45e9-9827-06d8586a1f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=lr_decoder, total_steps=max_steps, pct_start=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15c5c7c-b741-486e-9ce2-99c2e253b05d",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82cb876c-e45d-43f7-94aa-2fa7f44b15c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossi = []\n",
    "step = 0\n",
    "total_epoch_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69468a01-365b-42b9-b041-5d6969e5b4ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.8973\n",
      "Step 0 | Loss: 0.91202 | LR: 1.60e-04\n",
      "Step 25 | Loss: 0.86784 | LR: 1.63e-04\n",
      "Step 50 | Loss: 0.89279 | LR: 1.70e-04\n",
      "Step 75 | Loss: 0.87640 | LR: 1.82e-04\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/val/shard_val_0000.npz\n",
      "test loss: 0.8991\n",
      "Step 100 | Loss: 0.92670 | LR: 1.98e-04\n",
      "Step 125 | Loss: 0.88284 | LR: 2.19e-04\n",
      "Step 150 | Loss: 0.89942 | LR: 2.45e-04\n",
      "Step 175 | Loss: 0.89937 | LR: 2.75e-04\n",
      "test loss: 0.9045\n",
      "Step 200 | Loss: 0.91058 | LR: 3.10e-04\n",
      "Step 225 | Loss: 0.91074 | LR: 3.49e-04\n",
      "Step 250 | Loss: 0.86770 | LR: 3.92e-04\n",
      "Step 275 | Loss: 0.92867 | LR: 4.39e-04\n",
      "test loss: 0.9067\n",
      "Step 300 | Loss: 0.92152 | LR: 4.91e-04\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/train/shard_train_0004.npz\n",
      "Step 325 | Loss: 0.89258 | LR: 5.46e-04\n",
      "Step 350 | Loss: 0.90006 | LR: 6.05e-04\n",
      "Step 375 | Loss: 0.93600 | LR: 6.67e-04\n",
      "test loss: 0.9016\n",
      "Step 400 | Loss: 0.90695 | LR: 7.33e-04\n",
      "Step 425 | Loss: 0.89810 | LR: 8.03e-04\n",
      "Step 450 | Loss: 0.90309 | LR: 8.75e-04\n",
      "Step 475 | Loss: 0.85644 | LR: 9.51e-04\n",
      "test loss: 0.9019\n",
      "Step 500 | Loss: 0.90745 | LR: 1.03e-03\n",
      "Step 525 | Loss: 0.88466 | LR: 1.11e-03\n",
      "Step 550 | Loss: 0.88117 | LR: 1.19e-03\n",
      "Step 575 | Loss: 0.91758 | LR: 1.28e-03\n",
      "test loss: 0.9029\n",
      "Step 600 | Loss: 0.87577 | LR: 1.37e-03\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/train/shard_train_0001.npz\n",
      "Step 625 | Loss: 0.86359 | LR: 1.45e-03\n",
      "Step 650 | Loss: 0.94857 | LR: 1.54e-03\n",
      "Step 675 | Loss: 0.90972 | LR: 1.64e-03\n",
      "test loss: 0.8958\n",
      "Step 700 | Loss: 0.92893 | LR: 1.73e-03\n",
      "Step 725 | Loss: 0.89447 | LR: 1.82e-03\n",
      "Step 750 | Loss: 0.88429 | LR: 1.92e-03\n",
      "Step 775 | Loss: 0.90446 | LR: 2.01e-03\n",
      "test loss: 0.8929\n",
      "Step 800 | Loss: 0.87027 | LR: 2.11e-03\n",
      "Step 825 | Loss: 0.89327 | LR: 2.20e-03\n",
      "Step 850 | Loss: 0.90531 | LR: 2.30e-03\n",
      "Step 875 | Loss: 0.86409 | LR: 2.39e-03\n",
      "test loss: 0.8887\n",
      "Step 900 | Loss: 0.89473 | LR: 2.48e-03\n",
      "Step 925 | Loss: 0.89368 | LR: 2.58e-03\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/train/shard_train_0010.npz\n",
      "Step 950 | Loss: 0.95177 | LR: 2.67e-03\n",
      "Step 975 | Loss: 0.94162 | LR: 2.76e-03\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/train/shard_train_0003.npz\n",
      "test loss: 0.8909\n",
      "Step 1000 | Loss: 0.91051 | LR: 2.85e-03\n",
      "Step 1025 | Loss: 0.91611 | LR: 2.93e-03\n",
      "Step 1050 | Loss: 0.89346 | LR: 3.02e-03\n",
      "Step 1075 | Loss: 0.91659 | LR: 3.10e-03\n",
      "test loss: 0.8890\n",
      "Step 1100 | Loss: 0.90230 | LR: 3.18e-03\n",
      "Step 1125 | Loss: 0.89004 | LR: 3.25e-03\n",
      "Step 1150 | Loss: 0.90744 | LR: 3.33e-03\n",
      "Step 1175 | Loss: 0.92451 | LR: 3.40e-03\n",
      "test loss: 0.8862\n",
      "Step 1200 | Loss: 0.90225 | LR: 3.47e-03\n",
      "Step 1225 | Loss: 0.91795 | LR: 3.53e-03\n",
      "Step 1250 | Loss: 0.88395 | LR: 3.59e-03\n",
      "Step 1275 | Loss: 0.87985 | LR: 3.65e-03\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/val/shard_val_0000.npz\n",
      "test loss: 0.8790\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/train/shard_train_0007.npz\n",
      "Step 1300 | Loss: 0.88216 | LR: 3.70e-03\n",
      "Step 1325 | Loss: 0.87980 | LR: 3.75e-03\n",
      "Step 1350 | Loss: 0.93730 | LR: 3.79e-03\n",
      "Step 1375 | Loss: 0.87812 | LR: 3.83e-03\n",
      "test loss: 0.8755\n",
      "Step 1400 | Loss: 0.88697 | LR: 3.87e-03\n",
      "Step 1425 | Loss: 0.88852 | LR: 3.90e-03\n",
      "Step 1450 | Loss: 0.89659 | LR: 3.93e-03\n",
      "Step 1475 | Loss: 0.88941 | LR: 3.95e-03\n",
      "test loss: 0.8770\n",
      "Step 1500 | Loss: 0.89399 | LR: 3.97e-03\n",
      "Step 1525 | Loss: 0.85482 | LR: 3.99e-03\n",
      "Step 1550 | Loss: 0.84578 | LR: 3.99e-03\n",
      "Step 1575 | Loss: 0.84757 | LR: 4.00e-03\n",
      "test loss: 0.8664\n",
      "Step 1600 | Loss: 0.89054 | LR: 4.00e-03\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/train/shard_train_0006.npz\n",
      "Step 1625 | Loss: 0.86059 | LR: 4.00e-03\n",
      "Step 1650 | Loss: 0.86741 | LR: 4.00e-03\n",
      "Step 1675 | Loss: 0.86907 | LR: 4.00e-03\n",
      "test loss: 0.8707\n",
      "Step 1700 | Loss: 0.92210 | LR: 4.00e-03\n",
      "Step 1725 | Loss: 0.88109 | LR: 4.00e-03\n",
      "Step 1750 | Loss: 0.85490 | LR: 4.00e-03\n",
      "Step 1775 | Loss: 0.88962 | LR: 4.00e-03\n",
      "test loss: 0.8630\n",
      "Step 1800 | Loss: 0.88814 | LR: 4.00e-03\n",
      "Step 1825 | Loss: 0.83307 | LR: 4.00e-03\n",
      "Step 1850 | Loss: 0.87259 | LR: 4.00e-03\n",
      "Step 1875 | Loss: 0.82316 | LR: 4.00e-03\n",
      "test loss: 0.8661\n",
      "Step 1900 | Loss: 0.87911 | LR: 4.00e-03\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/train/shard_train_0002.npz\n",
      "Step 1925 | Loss: 0.87936 | LR: 4.00e-03\n",
      "Step 1950 | Loss: 0.84759 | LR: 4.00e-03\n",
      "Step 1975 | Loss: 0.86668 | LR: 4.00e-03\n",
      "test loss: 0.8612\n",
      "Step 2000 | Loss: 0.88983 | LR: 4.00e-03\n",
      "Step 2025 | Loss: 0.86032 | LR: 4.00e-03\n",
      "Step 2050 | Loss: 0.88611 | LR: 4.00e-03\n",
      "Step 2075 | Loss: 0.87009 | LR: 4.00e-03\n",
      "test loss: 0.8639\n",
      "Step 2100 | Loss: 0.89335 | LR: 4.00e-03\n",
      "Step 2125 | Loss: 0.84077 | LR: 4.00e-03\n",
      "Step 2150 | Loss: 0.83567 | LR: 4.00e-03\n",
      "Step 2175 | Loss: 0.86577 | LR: 4.00e-03\n",
      "test loss: 0.8538\n",
      "Step 2200 | Loss: 0.86210 | LR: 4.00e-03\n",
      "Step 2225 | Loss: 0.88325 | LR: 4.00e-03\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/train/shard_train_0005.npz\n",
      "Step 2250 | Loss: 0.86481 | LR: 4.00e-03\n",
      "Step 2275 | Loss: 0.84975 | LR: 3.99e-03\n",
      "test loss: 0.8587\n",
      "Step 2300 | Loss: 0.83549 | LR: 3.99e-03\n",
      "Step 2325 | Loss: 0.87543 | LR: 3.99e-03\n",
      "Step 2350 | Loss: 0.82992 | LR: 3.99e-03\n",
      "Step 2375 | Loss: 0.86164 | LR: 3.99e-03\n",
      "test loss: 0.8549\n",
      "Step 2400 | Loss: 0.81588 | LR: 3.99e-03\n",
      "Step 2425 | Loss: 0.85793 | LR: 3.99e-03\n",
      "Step 2450 | Loss: 0.82866 | LR: 3.99e-03\n",
      "Step 2475 | Loss: 0.84049 | LR: 3.99e-03\n",
      "test loss: 0.8523\n",
      "Step 2500 | Loss: 0.86432 | LR: 3.99e-03\n",
      "Step 2525 | Loss: 0.81551 | LR: 3.99e-03\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/train/shard_train_0008.npz\n",
      "Step 2550 | Loss: 0.85983 | LR: 3.99e-03\n",
      "Step 2575 | Loss: 0.86983 | LR: 3.99e-03\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/val/shard_val_0001.npz\n",
      "test loss: 0.8433\n",
      "Step 2600 | Loss: 0.87863 | LR: 3.99e-03\n",
      "Step 2625 | Loss: 0.86858 | LR: 3.99e-03\n",
      "Step 2650 | Loss: 0.81201 | LR: 3.99e-03\n",
      "Step 2675 | Loss: 0.82870 | LR: 3.99e-03\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/val/shard_val_0001.npz\n",
      "test loss: 0.8363\n",
      "Step 2700 | Loss: 0.82969 | LR: 3.99e-03\n",
      "Step 2725 | Loss: 0.87680 | LR: 3.99e-03\n",
      "Step 2750 | Loss: 0.83810 | LR: 3.99e-03\n",
      "Step 2775 | Loss: 0.85415 | LR: 3.98e-03\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/val/shard_val_0000.npz\n",
      "test loss: 0.8425\n",
      "Step 2800 | Loss: 0.79670 | LR: 3.98e-03\n",
      "Step 2825 | Loss: 0.82361 | LR: 3.98e-03\n",
      "Step 2850 | Loss: 0.84055 | LR: 3.98e-03\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/train/shard_train_0009.npz\n",
      "Step 2875 | Loss: 0.82720 | LR: 3.98e-03\n",
      "test loss: 0.8391\n",
      "Step 2900 | Loss: 0.82951 | LR: 3.98e-03\n",
      "Step 2925 | Loss: 0.82830 | LR: 3.98e-03\n",
      "Step 2950 | Loss: 0.88686 | LR: 3.98e-03\n",
      "Step 2975 | Loss: 0.81779 | LR: 3.98e-03\n",
      "test loss: 0.8477\n",
      "Step 3000 | Loss: 0.88541 | LR: 3.98e-03\n",
      "Step 3025 | Loss: 0.85580 | LR: 3.98e-03\n",
      "Step 3050 | Loss: 0.83600 | LR: 3.98e-03\n",
      "Step 3075 | Loss: 0.84965 | LR: 3.98e-03\n",
      "test loss: 0.8432\n",
      "Step 3100 | Loss: 0.83694 | LR: 3.98e-03\n",
      "Step 3125 | Loss: 0.84491 | LR: 3.97e-03\n",
      "Step 3150 | Loss: 0.86961 | LR: 3.97e-03\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/train/shard_train_0004.npz\n",
      "Step 3175 | Loss: 0.82959 | LR: 3.97e-03\n",
      "=== Step 3177 Done. Avg Loss: 0.87587 ===\n",
      "test loss: 0.8334\n",
      "Step 3200 | Loss: 0.79866 | LR: 3.97e-03\n",
      "Step 3225 | Loss: 0.88170 | LR: 3.97e-03\n",
      "Step 3250 | Loss: 0.85840 | LR: 3.97e-03\n",
      "Step 3275 | Loss: 0.83549 | LR: 3.97e-03\n",
      "test loss: 0.8256\n",
      "Step 3300 | Loss: 0.82139 | LR: 3.97e-03\n",
      "Step 3325 | Loss: 0.84700 | LR: 3.97e-03\n",
      "Step 3350 | Loss: 0.83516 | LR: 3.97e-03\n",
      "Step 3375 | Loss: 0.87265 | LR: 3.97e-03\n",
      "test loss: 0.8398\n",
      "Step 3400 | Loss: 0.82324 | LR: 3.96e-03\n",
      "Step 3425 | Loss: 0.84211 | LR: 3.96e-03\n",
      "Step 3450 | Loss: 0.83392 | LR: 3.96e-03\n",
      "Step 3475 | Loss: 0.79231 | LR: 3.96e-03\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/train/shard_train_0007.npz\n",
      "test loss: 0.8340\n",
      "Step 3500 | Loss: 0.84534 | LR: 3.96e-03\n",
      "Step 3525 | Loss: 0.86753 | LR: 3.96e-03\n",
      "Step 3550 | Loss: 0.87392 | LR: 3.96e-03\n",
      "Step 3575 | Loss: 0.86334 | LR: 3.96e-03\n",
      "test loss: 0.8380\n",
      "Step 3600 | Loss: 0.82416 | LR: 3.96e-03\n",
      "Step 3625 | Loss: 0.85108 | LR: 3.96e-03\n",
      "Step 3650 | Loss: 0.87657 | LR: 3.95e-03\n",
      "Step 3675 | Loss: 0.84555 | LR: 3.95e-03\n",
      "test loss: 0.8429\n",
      "Step 3700 | Loss: 0.85497 | LR: 3.95e-03\n",
      "Step 3725 | Loss: 0.84212 | LR: 3.95e-03\n",
      "Step 3750 | Loss: 0.82800 | LR: 3.95e-03\n",
      "Step 3775 | Loss: 0.84133 | LR: 3.95e-03\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/train/shard_train_0008.npz\n",
      "test loss: 0.8357\n",
      "Step 3800 | Loss: 0.85640 | LR: 3.95e-03\n",
      "Step 3825 | Loss: 0.82868 | LR: 3.95e-03\n",
      "Step 3850 | Loss: 0.81655 | LR: 3.94e-03\n",
      "Step 3875 | Loss: 0.84733 | LR: 3.94e-03\n",
      "test loss: 0.8295\n",
      "Step 3900 | Loss: 0.82732 | LR: 3.94e-03\n",
      "Step 3925 | Loss: 0.81431 | LR: 3.94e-03\n",
      "Step 3950 | Loss: 0.81142 | LR: 3.94e-03\n",
      "Step 3975 | Loss: 0.84620 | LR: 3.94e-03\n",
      "test loss: 0.8317\n",
      "Step 4000 | Loss: 0.80973 | LR: 3.94e-03\n",
      "Step 4025 | Loss: 0.82079 | LR: 3.94e-03\n",
      "Step 4050 | Loss: 0.83449 | LR: 3.93e-03\n",
      "Step 4075 | Loss: 0.84660 | LR: 3.93e-03\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     16\u001b[39m     z_context = model.student(cont_x, cont_tot)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     z_pred = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mact_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# run new decoder\u001b[39;00m\n\u001b[32m     20\u001b[39m pred_delta = decoder(z_pred) - decoder(z_context)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/general/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/general/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/biojepa/notebooks/biojepa_ac_model.py:257\u001b[39m, in \u001b[36mACPredictor.forward\u001b[39m\u001b[34m(self, context_latents, action_ids)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;66;03m# 3. Pass through AdaLN Blocks\u001b[39;00m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks:\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m     sequence = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_emb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    259\u001b[39m sequence = \u001b[38;5;28mself\u001b[39m.final_norm(sequence, action_emb)\n\u001b[32m    261\u001b[39m \u001b[38;5;66;03m# 4. Return only the predicted part (The Queries corresponding to N..2N)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/general/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/general/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/biojepa/notebooks/biojepa_ac_model.py:197\u001b[39m, in \u001b[36mPredictorBlock.forward\u001b[39m\u001b[34m(self, x, action_emb)\u001b[39m\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, action_emb):\n\u001b[32m    195\u001b[39m     \u001b[38;5;66;03m# 1. AdaLN -> Attention  -> Residual\u001b[39;00m\n\u001b[32m    196\u001b[39m     x_norm = \u001b[38;5;28mself\u001b[39m.ada_ln1(x, action_emb)\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m     x = x + \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_norm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    199\u001b[39m     \u001b[38;5;66;03m# 2. AdaLN -> MLP -> Residual\u001b[39;00m\n\u001b[32m    200\u001b[39m     x_norm = \u001b[38;5;28mself\u001b[39m.ada_ln2(x, action_emb)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/general/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/general/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/biojepa/notebooks/biojepa_ac_model.py:44\u001b[39m, in \u001b[36mBioMultiHeadAttention.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     41\u001b[39m v = \u001b[38;5;28mself\u001b[39m.v_proj(x).view(B, T, \u001b[38;5;28mself\u001b[39m.heads, \u001b[38;5;28mself\u001b[39m.head_dim).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# 2. Standard Scaled Dot Product Attention (Permutation Invariant)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m y = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# 5. Reassemble\u001b[39;00m\n\u001b[32m     47\u001b[39m y = y.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m).contiguous().view(B, T, C)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for step in range(max_steps):\n",
    "\n",
    "    last_step = (step == max_steps - 1)\n",
    "\n",
    "    # once in a while evaluate our validation set loss\n",
    "    if step % 100 == 0 or last_step:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss_accum = 0.0\n",
    "            val_loss_steps = 25\n",
    "            for i in range(val_loss_steps):\n",
    "                cont_x, cont_tot, case_x, case_tot, act_id = val_loader.next_batch()\n",
    "\n",
    "                # run BioJEPA\n",
    "                with torch.no_grad():\n",
    "                    z_context = model.student(cont_x, cont_tot)\n",
    "                    z_pred = model.predictor(z_context, act_id)\n",
    "\n",
    "                # run new decoder\n",
    "                pred_delta = decoder(z_pred) - decoder(z_context)\n",
    "                real_delta = case_x - cont_x\n",
    "\n",
    "                loss = F.mse_loss(pred_delta, real_delta)\n",
    "                loss = loss / val_loss_steps\n",
    "                val_loss_accum += loss.detach()\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "        print(f'test loss: {val_loss_accum.item():.4f}')\n",
    "\n",
    "\n",
    "    # periodically save checkpoint\n",
    "    if step > 0 and (step % 1000 == 0 or step % steps_per_epoch ==0) and not last_step:\n",
    "        # Save Checkpoint\n",
    "        torch.save({\n",
    "            'model': decoder.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'step': step\n",
    "        }, checkpoint_dir / f'biojepa_decoder_ckpt_{step}.pt')\n",
    "\n",
    "    # actual training\n",
    "    decoder.train\n",
    "    cont_x, cont_tot, case_x, case_tot, act_id = train_loader.next_batch()\n",
    "\n",
    "    # run frozen BioJEPA\n",
    "    with torch.no_grad():\n",
    "        z_context = model.student(cont_x, cont_tot)\n",
    "        z_pred = model.predictor(z_context, act_id)\n",
    "\n",
    "    # run decoder\n",
    "    pred_delta = decoder(z_pred) - decoder(z_context)\n",
    "    real_delta = case_x - cont_x\n",
    "    real_delta = case_x - cont_x\n",
    "\n",
    "    # loss\n",
    "    pred_delta = decoder(z_pred) - decoder(z_context)\n",
    "    real_delta = case_x - cont_x\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss = F.mse_loss(pred_delta, real_delta)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    # loss caching\n",
    "    lossi.append(loss.item())\n",
    "    total_epoch_loss += loss.item()\n",
    "\n",
    "    if step % 25 == 0:\n",
    "        print(f\"Step {step} | Loss: {loss.item():.5f} | LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "    \n",
    "    \n",
    "    if step > 0 and step % steps_per_epoch == 0:   \n",
    "        avg_loss = total_epoch_loss / steps_per_epoch\n",
    "        print(f\"=== Step {step} Done. Avg Loss: {avg_loss:.5f} ===\")\n",
    "        total_epoch_loss = 0\n",
    "    \n",
    "    if last_step:\n",
    "        # Save Checkpoint\n",
    "        torch.save({\n",
    "            'model': decoder.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'step': step\n",
    "        }, checkpoint_dir / f'biojepa_decoder_ckpt_{step}_final.pt')\n",
    "\n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c924050-b2d8-4973-a3c1-f9dd0a3d1f03",
   "metadata": {},
   "source": [
    "**Training Loss Plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b11fde37-0cac-429d-be0c-351390bac22d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lossi)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8aaa78-e607-4bc4-8cde-49a529984c1b",
   "metadata": {},
   "source": [
    "## Trained Decoder Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "94fd5355-31c8-47fc-a2fd-9b1fa9f91576",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.eval()\n",
    "correlations = []\n",
    "mses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "417a9438-c9f1-4cb0-8da2-e9fe7ec831a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_steps_per_epoch = val_total_examples // batch_size\n",
    "test_steps_per_epoch = test_total_examples // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "27c89be7-508c-4e06-9dfc-9f4809807077",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  89%|        | 308/345 [02:33<00:18,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading /Users/djemec/data/jepa/tokenized/eval/val/shard_val_0001.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  99%| | 340/345 [02:49<00:02,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading /Users/djemec/data/jepa/tokenized/eval/val/shard_val_0000.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|| 345/345 [02:52<00:00,  2.00it/s]\n"
     ]
    }
   ],
   "source": [
    "for step in tqdm(range(val_steps_per_epoch), desc=\"Evaluating\"):\n",
    "    \n",
    "    # Custom Loader Call\n",
    "    cont_x, cont_tot, case_x, case_tot, act_id = val_loader.next_batch()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        z_context = model.student(cont_x, cont_tot)\n",
    "        z_pred = model.predictor(z_context, act_id)\n",
    "        \n",
    "        pred_delta = decoder(z_pred) - decoder(z_context)\n",
    "        real_delta = case_x - cont_x\n",
    "        \n",
    "    # Per-Sample Metrics\n",
    "    for i in range(len(pred_delta)):\n",
    "        p = pred_delta[i].cpu().numpy()\n",
    "        t = real_delta[i].cpu().numpy()\n",
    "        \n",
    "        # Metric: Pearson Correlation on Top 20 DEGs\n",
    "        top_20_idx = np.argsort(np.abs(t))[-20:]\n",
    "        \n",
    "        if np.std(p[top_20_idx]) > 1e-9 and np.std(t[top_20_idx]) > 1e-9:\n",
    "            corr, _ = pearsonr(p[top_20_idx], t[top_20_idx])\n",
    "            correlations.append(corr)\n",
    "            \n",
    "        mses.append(np.mean((p - t)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "750afdb9-ba92-4a7a-874d-0cc221525de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global MSE: 0.8301\n",
      "Top-20 Pearson R: 0.5648\n"
     ]
    }
   ],
   "source": [
    "mean_mse = np.mean(mses)\n",
    "mean_corr = np.mean(correlations)\n",
    "print(f'Global MSE: {mean_mse:.4f}')\n",
    "print(f'Top-20 Pearson R: {mean_corr:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "44680d47-980a-4b0e-b910-471b65efd220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " FUNCTIONAL (Better than random)\n"
     ]
    }
   ],
   "source": [
    "if mean_corr > 0.75:\n",
    "    print(' SOTA COMPETITIVE (Matches GEARS)')\n",
    "elif mean_corr > 0.40:\n",
    "    print(' FUNCTIONAL (Better than random)')\n",
    "else:\n",
    "    print(' NEEDS IMPROVEMENT')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6b2741-9067-4cee-b37f-2ee471d3c7e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
