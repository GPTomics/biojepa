{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71522d3b-8805-48e6-bb36-1c9a7bd15482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import pearsonr\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "\n",
    "import biojepa_ac_model as model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe98e32-54b3-4da6-adf9-8b3e76e3bc0b",
   "metadata": {},
   "source": [
    "## BioJEPA Model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bae5c1c-d7c4-4216-ab99-42a26b7aee75",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f67335c-63ea-4f80-9c10-96b4d3e09c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cpu\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    device = 'cpu'\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(1337)\n",
    "        device = 'cuda'\n",
    "    # elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    #     device = 'mps'\n",
    "    print(f'using {device}')\n",
    "    return device\n",
    "\n",
    "DEVICE = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5ec64e1-0d7a-4dee-b478-b99f901ce9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n_embd = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "025d6281-f460-46eb-89d9-842d1ca6fa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('/Users/djemec/data/jepa')\n",
    "eval_dir = data_dir / 'tokenized' / 'eval'\n",
    "\n",
    "mask_path = eval_dir / 'binary_pathway_mask.npy'\n",
    "metadata_path = eval_dir / 'perturbation_map.json'\n",
    "checkpoint_dir = data_dir / 'checkpoint'\n",
    "gene_names_path = eval_dir / 'gene_names.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9671701-5cc4-4016-a354-b71238c98947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Pathway Mask...\n",
      "Mask Loaded: 4096 Genes -> 1024 Pathways\n",
      "Loaded 2058 perturbations\n",
      "Loaded 4096 genes\n"
     ]
    }
   ],
   "source": [
    "print('Loading Pathway Mask...')\n",
    "binary_mask = np.load(mask_path)\n",
    "n_genes, n_pathways = binary_mask.shape\n",
    "print(f'Mask Loaded: {n_genes} Genes -> {n_pathways} Pathways')\n",
    "\n",
    "with open(metadata_path, 'r') as f:\n",
    "    pert_map = json.load(f)\n",
    "id_to_pert = {v: k for k, v in pert_map.items()}\n",
    "print(f'Loaded {len(id_to_pert.keys())} perturbations')\n",
    "\n",
    "with open(data_dir / 'gene_names.json', \"r\") as f:\n",
    "    gene_names = json.load(f)\n",
    "print(f'Loaded {len(gene_names)} genes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fe46b1b-aff7-4355-909f-9f71761f11c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = model.BioJepaConfig(\n",
    "    mask_matrix=binary_mask, \n",
    "    num_genes=n_genes,\n",
    "    num_pathways=n_pathways,\n",
    "    embed_dim=n_embd,\n",
    "    heads=1\n",
    ")\n",
    "model = model.BioJepa(config).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a609e628-943e-41c2-a4d7-a71b9075a640",
   "metadata": {},
   "source": [
    "**Load Checkpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4aac628e-6ab7-4768-955a-3f99e79f58ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = checkpoint_dir / 'bio_jepa_ckpt_4epoch_final.pt'\n",
    "checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "\n",
    "keys = model.load_state_dict(checkpoint['model'])\n",
    "keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8287a64f-9d0d-4fbe-8c9b-a03c463a1487",
   "metadata": {},
   "source": [
    "**Freeze Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63812672-368f-483a-be2a-8b65cc849a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c98b4a-5df8-4894-9c56-ae0dbcbc3acd",
   "metadata": {},
   "source": [
    "## Build Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3dec1f3-d9cc-4849-aaa8-6a7d60199432",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BenchmarkDecoderConfig:\n",
    "    embed_dim: int = 384\n",
    "    num_pathways: int = 1024\n",
    "    num_genes: int = 4096\n",
    "    \n",
    "class BenchmarkDecoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # Step 1: Collapse the embedding dimension (384 -> 1)\n",
    "        # This asks: \"How active is this pathway overall?\"\n",
    "        self.pool = nn.Linear(config.embed_dim, 1) \n",
    "        \n",
    "        # Step 2: Decode Pathway Activity -> Gene Expression\n",
    "        # This learns the specific contribution of each pathway to each gene\n",
    "        self.decode = nn.Linear(config.num_pathways, config.num_genes) \n",
    "        \n",
    "    def forward(self, latents):\n",
    "        # latents: [Batch, 1024, 384]\n",
    "        \n",
    "        # 1. Calculate Pathway Scores\n",
    "        # [B, 1024, 384] -> [B, 1024, 1] -> [B, 1024]\n",
    "        scores = self.pool(latents).squeeze(-1)\n",
    "        \n",
    "        # 2. Project to Genes\n",
    "        # [B, 1024] @ [1024, 2000] -> [B, 2000]\n",
    "        gene_preds = self.decode(scores)\n",
    "        \n",
    "        return gene_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c316fe59-e4aa-4d12-9db4-2e38781f53bc",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bfad6fe-82ef-4019-b0ae-0338089f11c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_shard(filename):\n",
    "    print(f'loading {filename}') # Optional: reduce noise\n",
    "    with np.load(filename) as data:\n",
    "        # Load all arrays into memory\n",
    "        # We convert to correct types immediately to save hassle later\n",
    "        control_x = data['control'].astype(np.float32)\n",
    "        control_tot = data['control_total'].astype(np.float32)\n",
    "        case_x = data['case'].astype(np.float32)\n",
    "        case_tot = data['case_total'].astype(np.float32)\n",
    "        action_ids = data['action_ids'].astype(np.int64)\n",
    "        \n",
    "    return control_x, control_tot, case_x, case_tot, action_ids\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, batch, split, device, tok_dir):\n",
    "        self.batch = batch\n",
    "        self.split = split\n",
    "        self.device = device\n",
    "        \n",
    "        # 1. Find Shards\n",
    "        data_root = tok_dir / f'{split}'\n",
    "        shards = list(data_root.glob('*.npz'))\n",
    "\n",
    "        self.total_files = len(shards)\n",
    "        self.shards = sorted(shards)\n",
    "\n",
    "        assert len(shards) > 0, f'no shards found for split {split}'\n",
    "        print(f'found {len(shards)} shards for split {split}')\n",
    "        \n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # Create a randomized queue of shards\n",
    "        self.remaining_shards = list(self.shards)\n",
    "        random.shuffle(self.remaining_shards)\n",
    "        \n",
    "        self.current_shard_idx = -1\n",
    "        self.load_next_shard()\n",
    "\n",
    "    def load_next_shard(self):\n",
    "        self.current_shard_idx += 1\n",
    "        \n",
    "        # If we ran out of shards, reset (Epoch done)\n",
    "        if self.current_shard_idx >= len(self.remaining_shards):\n",
    "            self.reset() # This resets shard_idx to -1 and reshuffles\n",
    "            return \n",
    "\n",
    "        # Load the file\n",
    "        filename = self.remaining_shards[self.current_shard_idx]\n",
    "        self.data_tuple = load_shard(filename)\n",
    "        \n",
    "        # Shuffle the items INSIDE the shard\n",
    "        # This is critical so we don't just memorize the sorted order of the shard\n",
    "        n_samples = len(self.data_tuple[0])\n",
    "        self.perm = np.random.permutation(n_samples)\n",
    "        self.current_position = 0\n",
    "        self.total_samples_in_shard = n_samples\n",
    "\n",
    "    def next_batch(self):\n",
    "        batch = self.batch\n",
    "        \n",
    "        # Check if we have enough data left in current shard\n",
    "        if self.current_position + batch > self.total_samples_in_shard:\n",
    "            self.load_next_shard()\n",
    "            # Recursively call to get batch from the new shard\n",
    "            return self.next_batch()\n",
    "            \n",
    "        # Get indices for this batch\n",
    "        indices = self.perm[self.current_position : self.current_position + batch]\n",
    "        self.current_position += batch\n",
    "        \n",
    "        # Slice data using the shuffled indices\n",
    "        # data_tuple structure: (xc, xct, xt, xtt, aid)\n",
    "        batch_cont_x  = torch.from_numpy(self.data_tuple[0][indices]).to(self.device)\n",
    "        batch_cont_tot = torch.from_numpy(self.data_tuple[1][indices]).to(self.device)\n",
    "        batch_case_x  = torch.from_numpy(self.data_tuple[2][indices]).to(self.device)\n",
    "        batch_case_t = torch.from_numpy(self.data_tuple[3][indices]).to(self.device)\n",
    "        batch_aid = torch.from_numpy(self.data_tuple[4][indices]).to(self.device)\n",
    "        \n",
    "        return batch_cont_x, batch_cont_tot, batch_case_x, batch_case_t, batch_aid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7f7811-5727-45e3-8194-505072bb16eb",
   "metadata": {},
   "source": [
    "**Data Loader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e1d5000-f278-4f9b-9227-9554f31e9a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 11 shards for split train\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/train/shard_train_0000.npz\n",
      "found 2 shards for split val\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/val/shard_val_0001.npz\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoaderLite(batch=batch_size, split='train', device=DEVICE, tok_dir=eval_dir)\n",
    "val_loader = DataLoaderLite(batch=batch_size, split='val', device=DEVICE, tok_dir=eval_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9025bf-05d7-4ff8-ae9a-c1b87471c3e5",
   "metadata": {},
   "source": [
    "## Training Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f484f1f2-2933-413d-80c0-f0c970656453",
   "metadata": {},
   "source": [
    "### Training Config/Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad700ee3-e710-43de-be89-4003c922a41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_decoder = 4e-3\n",
    "epochs = 10\n",
    "tok_file_chunk_size = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0494e7de-e96d-4989-b47b-9229c050a263",
   "metadata": {},
   "source": [
    "**Initialize Decoder** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74224c15-ff49-41d9-9552-b8ec9ed83dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BenchmarkDecoderConfig(\n",
    "    embed_dim= n_embd,\n",
    "    num_pathways= n_pathways,\n",
    "    num_genes= n_genes\n",
    ")\n",
    "\n",
    "decoder = BenchmarkDecoder(config).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2da30fa-9c44-40be-921c-5184c764f0e5",
   "metadata": {},
   "source": [
    "**Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b197e689-8b98-4cf9-8382-6116bf2ce440",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(decoder.parameters(), lr=lr_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958f11cf-8c9f-49b9-b526-74be69d2126f",
   "metadata": {},
   "source": [
    "**Training Lenght**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "009526cc-7703-497d-87cb-4a50f9c1c1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_total_examples = 101682\n",
    "val_total_examples = 11044\n",
    "test_total_examples = 38829"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "766a254e-2fd7-4274-91c4-7ceab21d8d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3177, 31770)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch = train_total_examples // batch_size\n",
    "max_steps = epochs * steps_per_epoch\n",
    "steps_per_epoch, max_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cf4803-b42e-4d1b-89db-c9b43d6e3558",
   "metadata": {},
   "source": [
    "**Scheduler**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c683627d-3da6-45e9-9827-06d8586a1f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=lr_decoder, total_steps=max_steps, pct_start=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15c5c7c-b741-486e-9ce2-99c2e253b05d",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82cb876c-e45d-43f7-94aa-2fa7f44b15c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossi = []\n",
    "step = 0\n",
    "total_epoch_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69468a01-365b-42b9-b041-5d6969e5b4ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.8973\n",
      "Step 0 | Loss: 0.91202 | LR: 1.60e-04\n",
      "Step 25 | Loss: 0.86784 | LR: 1.63e-04\n",
      "Step 50 | Loss: 0.89279 | LR: 1.70e-04\n",
      "Step 75 | Loss: 0.87640 | LR: 1.82e-04\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/val/shard_val_0000.npz\n",
      "test loss: 0.8991\n",
      "Step 100 | Loss: 0.92670 | LR: 1.98e-04\n",
      "Step 125 | Loss: 0.88284 | LR: 2.19e-04\n",
      "Step 150 | Loss: 0.89942 | LR: 2.45e-04\n"
     ]
    }
   ],
   "source": [
    "for step in range(max_steps):\n",
    "\n",
    "    last_step = (step == max_steps - 1)\n",
    "\n",
    "    # once in a while evaluate our validation set loss\n",
    "    if step % 100 == 0 or last_step:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss_accum = 0.0\n",
    "            val_loss_steps = 25\n",
    "            for i in range(val_loss_steps):\n",
    "                cont_x, cont_tot, case_x, case_tot, act_id = val_loader.next_batch()\n",
    "\n",
    "                # run BioJEPA\n",
    "                with torch.no_grad():\n",
    "                    z_context = model.student(cont_x, cont_tot)\n",
    "                    z_pred = model.predictor(z_context, act_id)\n",
    "\n",
    "                # run new decoder\n",
    "                pred_delta = decoder(z_pred) - decoder(z_context)\n",
    "                real_delta = case_x - cont_x\n",
    "\n",
    "                loss = F.mse_loss(pred_delta, real_delta)\n",
    "                loss = loss / val_loss_steps\n",
    "                val_loss_accum += loss.detach()\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "        print(f'test loss: {val_loss_accum.item():.4f}')\n",
    "\n",
    "\n",
    "    # periodically save checkpoint\n",
    "    if step > 0 and (step % 1000 == 0 or step % steps_per_epoch ==0) and not last_step:\n",
    "        # Save Checkpoint\n",
    "        torch.save({\n",
    "            'model': decoder.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'step': step\n",
    "        }, checkpoint_dir / f'biojepa_decoder_ckpt_{step}.pt')\n",
    "\n",
    "    # actual training\n",
    "    decoder.train\n",
    "    cont_x, cont_tot, case_x, case_tot, act_id = train_loader.next_batch()\n",
    "\n",
    "    # run frozen BioJEPA\n",
    "    with torch.no_grad():\n",
    "        z_context = model.student(cont_x, cont_tot)\n",
    "        z_pred = model.predictor(z_context, act_id)\n",
    "\n",
    "    # run decoder\n",
    "    pred_delta = decoder(z_pred) - decoder(z_context)\n",
    "    real_delta = case_x - cont_x\n",
    "    real_delta = case_x - cont_x\n",
    "\n",
    "    # loss\n",
    "    pred_delta = decoder(z_pred) - decoder(z_context)\n",
    "    real_delta = case_x - cont_x\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss = F.mse_loss(pred_delta, real_delta)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    # loss caching\n",
    "    lossi.append(loss.item())\n",
    "    total_epoch_loss += loss.item()\n",
    "\n",
    "    if step % 25 == 0:\n",
    "        print(f\"Step {step} | Loss: {loss.item():.5f} | LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "    \n",
    "    \n",
    "    if step > 0 and step % steps_per_epoch == 0:   \n",
    "        avg_loss = total_epoch_loss / steps_per_epoch\n",
    "        print(f\"=== Step {step} Done. Avg Loss: {avg_loss:.5f} ===\")\n",
    "        total_epoch_loss = 0\n",
    "    \n",
    "    if last_step:\n",
    "        # Save Checkpoint\n",
    "        torch.save({\n",
    "            'model': decoder.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'step': step\n",
    "        }, checkpoint_dir / f'biojepa_decoder_ckpt_{step}_final.pt')\n",
    "\n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c924050-b2d8-4973-a3c1-f9dd0a3d1f03",
   "metadata": {},
   "source": [
    "**Training Loss Plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11fde37-0cac-429d-be0c-351390bac22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lossi)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8aaa78-e607-4bc4-8cde-49a529984c1b",
   "metadata": {},
   "source": [
    "## Trained Decoder Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fd5355-31c8-47fc-a2fd-9b1fa9f91576",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.eval()\n",
    "correlations = []\n",
    "mses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417a9438-c9f1-4cb0-8da2-e9fe7ec831a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_steps_per_epoch = val_total_examples // batch_size\n",
    "test_steps_per_epoch = test_total_examples // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c89be7-508c-4e06-9dfc-9f4809807077",
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in tqdm(range(val_steps_per_epoch), desc=\"Evaluating\"):\n",
    "    \n",
    "    # Custom Loader Call\n",
    "    cont_x, cont_tot, case_x, case_tot, act_id = val_loader.next_batch()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        z_context = model.student(cont_x, cont_tot)\n",
    "        z_pred = model.predictor(z_context, act_id)\n",
    "        \n",
    "        pred_delta = decoder(z_pred) - decoder(z_context)\n",
    "        real_delta = case_x - cont_x\n",
    "        \n",
    "    # Per-Sample Metrics\n",
    "    for i in range(len(pred_delta)):\n",
    "        p = pred_delta[i].cpu().numpy()\n",
    "        t = real_delta[i].cpu().numpy()\n",
    "        \n",
    "        # Metric: Pearson Correlation on Top 20 DEGs\n",
    "        top_20_idx = np.argsort(np.abs(t))[-20:]\n",
    "        \n",
    "        if np.std(p[top_20_idx]) > 1e-9 and np.std(t[top_20_idx]) > 1e-9:\n",
    "            corr, _ = pearsonr(p[top_20_idx], t[top_20_idx])\n",
    "            correlations.append(corr)\n",
    "            \n",
    "        mses.append(np.mean((p - t)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750afdb9-ba92-4a7a-874d-0cc221525de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_mse = np.mean(mses)\n",
    "mean_corr = np.mean(correlations)\n",
    "print(f'Global MSE: {mean_mse:.4f}')\n",
    "print(f'Top-20 Pearson R: {mean_corr:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44680d47-980a-4b0e-b910-471b65efd220",
   "metadata": {},
   "outputs": [],
   "source": [
    "if mean_corr > 0.75:\n",
    "    print('üåü SOTA COMPETITIVE (Matches GEARS)')\n",
    "elif mean_corr > 0.40:\n",
    "    print('‚úÖ FUNCTIONAL (Better than random)')\n",
    "else:\n",
    "    print('‚ùå NEEDS IMPROVEMENT')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6b2741-9067-4cee-b37f-2ee471d3c7e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
