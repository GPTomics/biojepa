{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71522d3b-8805-48e6-bb36-1c9a7bd15482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import pearsonr\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "\n",
    "import biojepa_ac_model as model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe98e32-54b3-4da6-adf9-8b3e76e3bc0b",
   "metadata": {},
   "source": [
    "## BioJEPA Model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bae5c1c-d7c4-4216-ab99-42a26b7aee75",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f67335c-63ea-4f80-9c10-96b4d3e09c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cpu\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    device = 'cpu'\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(1337)\n",
    "        device = 'cuda'\n",
    "    # elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    #     device = 'mps'\n",
    "    print(f'using {device}')\n",
    "    return device\n",
    "\n",
    "DEVICE = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5ec64e1-0d7a-4dee-b478-b99f901ce9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n_embd = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "025d6281-f460-46eb-89d9-842d1ca6fa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('/Users/djemec/data/jepa')\n",
    "eval_dir = data_dir / 'tokenized' / 'eval'\n",
    "\n",
    "mask_path = eval_dir / 'binary_pathway_mask.npy'\n",
    "metadata_path = eval_dir / 'perturbation_map.json'\n",
    "checkpoint_dir = data_dir / 'checkpoint'\n",
    "gene_names_path = eval_dir / 'gene_names.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9671701-5cc4-4016-a354-b71238c98947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Pathway Mask...\n",
      "Mask Loaded: 4096 Genes -> 1024 Pathways\n",
      "Loaded 2058 perturbations\n",
      "Loaded 4096 genes\n"
     ]
    }
   ],
   "source": [
    "print('Loading Pathway Mask...')\n",
    "binary_mask = np.load(mask_path)\n",
    "n_genes, n_pathways = binary_mask.shape\n",
    "print(f'Mask Loaded: {n_genes} Genes -> {n_pathways} Pathways')\n",
    "\n",
    "with open(metadata_path, 'r') as f:\n",
    "    pert_map = json.load(f)\n",
    "id_to_pert = {v: k for k, v in pert_map.items()}\n",
    "print(f'Loaded {len(id_to_pert.keys())} perturbations')\n",
    "\n",
    "with open(data_dir / 'gene_names.json', \"r\") as f:\n",
    "    gene_names = json.load(f)\n",
    "print(f'Loaded {len(gene_names)} genes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fe46b1b-aff7-4355-909f-9f71761f11c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = model.BioJepaConfig(\n",
    "    mask_matrix=binary_mask, \n",
    "    num_genes=n_genes,\n",
    "    num_pathways=n_pathways,\n",
    "    embed_dim=n_embd,\n",
    "    heads=1\n",
    ")\n",
    "model = model.BioJepa(config).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a609e628-943e-41c2-a4d7-a71b9075a640",
   "metadata": {},
   "source": [
    "**Load Checkpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4aac628e-6ab7-4768-955a-3f99e79f58ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = checkpoint_dir / 'bio_jepa_ckpt_4epoch_final.pt'\n",
    "checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "\n",
    "keys = model.load_state_dict(checkpoint['model'])\n",
    "keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8287a64f-9d0d-4fbe-8c9b-a03c463a1487",
   "metadata": {},
   "source": [
    "**Freeze Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63812672-368f-483a-be2a-8b65cc849a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c98b4a-5df8-4894-9c56-ae0dbcbc3acd",
   "metadata": {},
   "source": [
    "## Build Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3dec1f3-d9cc-4849-aaa8-6a7d60199432",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BenchmarkDecoderConfig:\n",
    "    embed_dim: int = 384\n",
    "    num_pathways: int = 1024\n",
    "    num_genes: int = 4096\n",
    "    \n",
    "class BenchmarkDecoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # Step 1: Collapse the embedding dimension (384 -> 1)\n",
    "        # This asks: \"How active is this pathway overall?\"\n",
    "        self.pool = nn.Linear(config.embed_dim, 1) \n",
    "        \n",
    "        # Step 2: Decode Pathway Activity -> Gene Expression\n",
    "        # This learns the specific contribution of each pathway to each gene\n",
    "        self.decode = nn.Linear(config.num_pathways, config.num_genes) \n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None: \n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        \n",
    "    def forward(self, latents):\n",
    "        # latents: [Batch, 1024, 384]\n",
    "        \n",
    "        # 1. Calculate Pathway Scores\n",
    "        # [B, 1024, 384] -> [B, 1024, 1] -> [B, 1024]\n",
    "        scores = self.pool(latents).squeeze(-1)\n",
    "        \n",
    "        # 2. Project to Genes\n",
    "        # [B, 1024] @ [1024, 2000] -> [B, 2000]\n",
    "        gene_preds = self.decode(scores)\n",
    "        \n",
    "        return gene_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c316fe59-e4aa-4d12-9db4-2e38781f53bc",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bfad6fe-82ef-4019-b0ae-0338089f11c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_shard(filename):\n",
    "    print(f'loading {filename}') # Optional: reduce noise\n",
    "    with np.load(filename) as data:\n",
    "        # Load all arrays into memory\n",
    "        # We convert to correct types immediately to save hassle later\n",
    "        control_x = data['control'].astype(np.float32)\n",
    "        control_tot = data['control_total'].astype(np.float32)\n",
    "        case_x = data['case'].astype(np.float32)\n",
    "        case_tot = data['case_total'].astype(np.float32)\n",
    "        action_ids = data['action_ids'].astype(np.int64)\n",
    "        \n",
    "    return control_x, control_tot, case_x, case_tot, action_ids\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, batch, split, device, tok_dir):\n",
    "        self.batch = batch\n",
    "        self.split = split\n",
    "        self.device = device\n",
    "        \n",
    "        # 1. Find Shards\n",
    "        data_root = tok_dir / f'{split}'\n",
    "        shards = list(data_root.glob('*.npz'))\n",
    "\n",
    "        self.total_files = len(shards)\n",
    "        self.shards = sorted(shards)\n",
    "\n",
    "        assert len(shards) > 0, f'no shards found for split {split}'\n",
    "        print(f'found {len(shards)} shards for split {split}')\n",
    "        \n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # Create a randomized queue of shards\n",
    "        self.remaining_shards = list(self.shards)\n",
    "        random.shuffle(self.remaining_shards)\n",
    "        \n",
    "        self.current_shard_idx = -1\n",
    "        self.load_next_shard()\n",
    "\n",
    "    def load_next_shard(self):\n",
    "        self.current_shard_idx += 1\n",
    "        \n",
    "        # If we ran out of shards, reset (Epoch done)\n",
    "        if self.current_shard_idx >= len(self.remaining_shards):\n",
    "            self.reset() # This resets shard_idx to -1 and reshuffles\n",
    "            return \n",
    "\n",
    "        # Load the file\n",
    "        filename = self.remaining_shards[self.current_shard_idx]\n",
    "        self.data_tuple = load_shard(filename)\n",
    "        \n",
    "        # Shuffle the items INSIDE the shard\n",
    "        # This is critical so we don't just memorize the sorted order of the shard\n",
    "        n_samples = len(self.data_tuple[0])\n",
    "        self.perm = np.random.permutation(n_samples)\n",
    "        self.current_position = 0\n",
    "        self.total_samples_in_shard = n_samples\n",
    "\n",
    "    def next_batch(self):\n",
    "        batch = self.batch\n",
    "        \n",
    "        # Check if we have enough data left in current shard\n",
    "        if self.current_position + batch > self.total_samples_in_shard:\n",
    "            self.load_next_shard()\n",
    "            # Recursively call to get batch from the new shard\n",
    "            return self.next_batch()\n",
    "            \n",
    "        # Get indices for this batch\n",
    "        indices = self.perm[self.current_position : self.current_position + batch]\n",
    "        self.current_position += batch\n",
    "        \n",
    "        # Slice data using the shuffled indices\n",
    "        # data_tuple structure: (xc, xct, xt, xtt, aid)\n",
    "        batch_cont_x  = torch.from_numpy(self.data_tuple[0][indices]).to(self.device)\n",
    "        batch_cont_tot = torch.from_numpy(self.data_tuple[1][indices]).to(self.device)\n",
    "        batch_case_x  = torch.from_numpy(self.data_tuple[2][indices]).to(self.device)\n",
    "        batch_case_t = torch.from_numpy(self.data_tuple[3][indices]).to(self.device)\n",
    "        batch_aid = torch.from_numpy(self.data_tuple[4][indices]).to(self.device)\n",
    "        \n",
    "        return batch_cont_x, batch_cont_tot, batch_case_x, batch_case_t, batch_aid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7f7811-5727-45e3-8194-505072bb16eb",
   "metadata": {},
   "source": [
    "**Data Loader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e1d5000-f278-4f9b-9227-9554f31e9a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 11 shards for split train\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/train/shard_train_0000.npz\n",
      "found 2 shards for split val\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/val/shard_val_0001.npz\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoaderLite(batch=batch_size, split='train', device=DEVICE, tok_dir=eval_dir)\n",
    "val_loader = DataLoaderLite(batch=batch_size, split='val', device=DEVICE, tok_dir=eval_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9025bf-05d7-4ff8-ae9a-c1b87471c3e5",
   "metadata": {},
   "source": [
    "## Training Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f484f1f2-2933-413d-80c0-f0c970656453",
   "metadata": {},
   "source": [
    "### Training Config/Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad700ee3-e710-43de-be89-4003c922a41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_decoder = 1e-2\n",
    "epochs = 2\n",
    "tok_file_chunk_size = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0494e7de-e96d-4989-b47b-9229c050a263",
   "metadata": {},
   "source": [
    "**Initialize Decoder** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74224c15-ff49-41d9-9552-b8ec9ed83dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BenchmarkDecoderConfig(\n",
    "    embed_dim= n_embd,\n",
    "    num_pathways= n_pathways,\n",
    "    num_genes= n_genes\n",
    ")\n",
    "\n",
    "decoder = BenchmarkDecoder(config).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2da30fa-9c44-40be-921c-5184c764f0e5",
   "metadata": {},
   "source": [
    "**Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b197e689-8b98-4cf9-8382-6116bf2ce440",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(decoder.parameters(), lr=lr_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958f11cf-8c9f-49b9-b526-74be69d2126f",
   "metadata": {},
   "source": [
    "**Training Lenght**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "009526cc-7703-497d-87cb-4a50f9c1c1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_total_examples = 101682\n",
    "val_total_examples = 11044\n",
    "test_total_examples = 38829"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "766a254e-2fd7-4274-91c4-7ceab21d8d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3177, 6354)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch = train_total_examples // batch_size\n",
    "max_steps = epochs * steps_per_epoch\n",
    "steps_per_epoch, max_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cf4803-b42e-4d1b-89db-c9b43d6e3558",
   "metadata": {},
   "source": [
    "**Scheduler**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c683627d-3da6-45e9-9827-06d8586a1f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=lr_decoder, total_steps=max_steps, pct_start=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15c5c7c-b741-486e-9ce2-99c2e253b05d",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82cb876c-e45d-43f7-94aa-2fa7f44b15c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossi = []\n",
    "step = 0\n",
    "total_epoch_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69468a01-365b-42b9-b041-5d6969e5b4ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.8969\n",
      "Step 0 | Loss: 0.90620 | LR: 4.00e-04\n",
      "Step 25 | Loss: 0.88506 | LR: 5.59e-04\n",
      "Step 50 | Loss: 0.89774 | LR: 1.00e-03\n",
      "Step 75 | Loss: 0.94056 | LR: 1.70e-03\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/val/shard_val_0000.npz\n",
      "test loss: 0.9042\n",
      "Step 100 | Loss: 0.90679 | LR: 2.61e-03\n",
      "Step 125 | Loss: 0.92025 | LR: 3.69e-03\n",
      "Step 150 | Loss: 0.95356 | LR: 4.85e-03\n",
      "Step 175 | Loss: 0.91392 | LR: 6.04e-03\n",
      "test loss: 0.9020\n",
      "Step 200 | Loss: 0.88712 | LR: 7.17e-03\n",
      "Step 225 | Loss: 0.91492 | LR: 8.18e-03\n",
      "Step 250 | Loss: 0.89420 | LR: 9.02e-03\n",
      "Step 275 | Loss: 0.89694 | LR: 9.61e-03\n",
      "test loss: 0.9036\n",
      "Step 300 | Loss: 0.88010 | LR: 9.94e-03\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/train/shard_train_0004.npz\n",
      "Step 325 | Loss: 0.91948 | LR: 1.00e-02\n",
      "Step 350 | Loss: 0.91322 | LR: 1.00e-02\n",
      "Step 375 | Loss: 0.89820 | LR: 1.00e-02\n",
      "test loss: 0.8964\n",
      "Step 400 | Loss: 0.89644 | LR: 1.00e-02\n",
      "Step 425 | Loss: 0.90822 | LR: 9.99e-03\n",
      "Step 450 | Loss: 0.92842 | LR: 9.99e-03\n",
      "Step 475 | Loss: 0.89664 | LR: 9.98e-03\n",
      "test loss: 0.8966\n",
      "Step 500 | Loss: 0.88845 | LR: 9.98e-03\n",
      "Step 525 | Loss: 0.89775 | LR: 9.97e-03\n",
      "Step 550 | Loss: 0.89088 | LR: 9.96e-03\n",
      "Step 575 | Loss: 0.91092 | LR: 9.95e-03\n",
      "test loss: 0.8941\n",
      "Step 600 | Loss: 0.92314 | LR: 9.95e-03\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/train/shard_train_0001.npz\n",
      "Step 625 | Loss: 0.86776 | LR: 9.94e-03\n",
      "Step 650 | Loss: 0.86545 | LR: 9.92e-03\n",
      "Step 675 | Loss: 0.88868 | LR: 9.91e-03\n",
      "test loss: 0.8897\n",
      "Step 700 | Loss: 0.91370 | LR: 9.90e-03\n",
      "Step 725 | Loss: 0.87698 | LR: 9.89e-03\n",
      "Step 750 | Loss: 0.83917 | LR: 9.87e-03\n",
      "Step 775 | Loss: 0.89109 | LR: 9.86e-03\n",
      "test loss: 0.8921\n",
      "Step 800 | Loss: 0.88876 | LR: 9.84e-03\n",
      "Step 825 | Loss: 0.88949 | LR: 9.83e-03\n",
      "Step 850 | Loss: 0.87872 | LR: 9.81e-03\n",
      "Step 875 | Loss: 0.88474 | LR: 9.79e-03\n",
      "test loss: 0.8735\n",
      "Step 900 | Loss: 0.89011 | LR: 9.77e-03\n",
      "Step 925 | Loss: 0.86393 | LR: 9.75e-03\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/train/shard_train_0010.npz\n",
      "Step 950 | Loss: 0.87828 | LR: 9.73e-03\n",
      "Step 975 | Loss: 0.87619 | LR: 9.71e-03\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/train/shard_train_0003.npz\n",
      "test loss: 0.8756\n",
      "Step 1000 | Loss: 0.88677 | LR: 9.69e-03\n",
      "Step 1025 | Loss: 0.87572 | LR: 9.66e-03\n",
      "Step 1050 | Loss: 0.85193 | LR: 9.64e-03\n",
      "Step 1075 | Loss: 0.90764 | LR: 9.61e-03\n",
      "test loss: 0.8763\n",
      "Step 1100 | Loss: 0.87987 | LR: 9.59e-03\n",
      "Step 1125 | Loss: 0.86532 | LR: 9.56e-03\n",
      "Step 1150 | Loss: 0.86748 | LR: 9.54e-03\n",
      "Step 1175 | Loss: 0.88474 | LR: 9.51e-03\n",
      "test loss: 0.8684\n",
      "Step 1200 | Loss: 0.83729 | LR: 9.48e-03\n",
      "Step 1225 | Loss: 0.85964 | LR: 9.45e-03\n",
      "Step 1250 | Loss: 0.88186 | LR: 9.42e-03\n",
      "Step 1275 | Loss: 0.84945 | LR: 9.39e-03\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/val/shard_val_0000.npz\n",
      "test loss: 0.8676\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/train/shard_train_0007.npz\n",
      "Step 1300 | Loss: 0.86958 | LR: 9.36e-03\n",
      "Step 1325 | Loss: 0.86844 | LR: 9.33e-03\n",
      "Step 1350 | Loss: 0.86828 | LR: 9.29e-03\n",
      "Step 1375 | Loss: 0.86938 | LR: 9.26e-03\n",
      "test loss: 0.8567\n",
      "Step 1400 | Loss: 0.83254 | LR: 9.22e-03\n",
      "Step 1425 | Loss: 0.87592 | LR: 9.19e-03\n",
      "Step 1450 | Loss: 0.85898 | LR: 9.15e-03\n",
      "Step 1475 | Loss: 0.86942 | LR: 9.12e-03\n",
      "test loss: 0.8579\n",
      "Step 1500 | Loss: 0.87304 | LR: 9.08e-03\n",
      "Step 1525 | Loss: 0.87014 | LR: 9.04e-03\n",
      "Step 1550 | Loss: 0.84581 | LR: 9.00e-03\n",
      "Step 1575 | Loss: 0.84498 | LR: 8.96e-03\n",
      "test loss: 0.8555\n",
      "Step 1600 | Loss: 0.82952 | LR: 8.92e-03\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/train/shard_train_0006.npz\n",
      "Step 1625 | Loss: 0.85530 | LR: 8.88e-03\n",
      "Step 1650 | Loss: 0.83761 | LR: 8.84e-03\n",
      "Step 1675 | Loss: 0.86721 | LR: 8.80e-03\n",
      "test loss: 0.8511\n",
      "Step 1700 | Loss: 0.85228 | LR: 8.76e-03\n",
      "Step 1725 | Loss: 0.91461 | LR: 8.71e-03\n",
      "Step 1750 | Loss: 0.86034 | LR: 8.67e-03\n",
      "Step 1775 | Loss: 0.83972 | LR: 8.63e-03\n",
      "test loss: 0.8479\n",
      "Step 1800 | Loss: 0.82054 | LR: 8.58e-03\n",
      "Step 1825 | Loss: 0.86534 | LR: 8.54e-03\n",
      "Step 1850 | Loss: 0.82543 | LR: 8.49e-03\n",
      "Step 1875 | Loss: 0.86027 | LR: 8.44e-03\n",
      "test loss: 0.8413\n",
      "Step 1900 | Loss: 0.88124 | LR: 8.39e-03\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/train/shard_train_0002.npz\n",
      "Step 1925 | Loss: 0.83355 | LR: 8.35e-03\n",
      "Step 1950 | Loss: 0.86632 | LR: 8.30e-03\n",
      "Step 1975 | Loss: 0.87386 | LR: 8.25e-03\n",
      "test loss: 0.8470\n",
      "Step 2000 | Loss: 0.85161 | LR: 8.20e-03\n",
      "Step 2025 | Loss: 0.81435 | LR: 8.15e-03\n",
      "Step 2050 | Loss: 0.82187 | LR: 8.10e-03\n",
      "Step 2075 | Loss: 0.79298 | LR: 8.05e-03\n",
      "test loss: 0.8403\n",
      "Step 2100 | Loss: 0.82620 | LR: 7.99e-03\n",
      "Step 2125 | Loss: 0.83238 | LR: 7.94e-03\n",
      "Step 2150 | Loss: 0.81400 | LR: 7.89e-03\n",
      "Step 2175 | Loss: 0.86457 | LR: 7.84e-03\n",
      "test loss: 0.8440\n",
      "Step 2200 | Loss: 0.84877 | LR: 7.78e-03\n",
      "Step 2225 | Loss: 0.81118 | LR: 7.73e-03\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/train/shard_train_0005.npz\n",
      "Step 2250 | Loss: 0.79614 | LR: 7.67e-03\n",
      "Step 2275 | Loss: 0.85289 | LR: 7.62e-03\n",
      "test loss: 0.8362\n",
      "Step 2300 | Loss: 0.85275 | LR: 7.56e-03\n",
      "Step 2325 | Loss: 0.83351 | LR: 7.51e-03\n",
      "Step 2350 | Loss: 0.81757 | LR: 7.45e-03\n",
      "Step 2375 | Loss: 0.85293 | LR: 7.39e-03\n",
      "test loss: 0.8316\n",
      "Step 2400 | Loss: 0.83612 | LR: 7.34e-03\n",
      "Step 2425 | Loss: 0.80553 | LR: 7.28e-03\n",
      "Step 2450 | Loss: 0.82386 | LR: 7.22e-03\n",
      "Step 2475 | Loss: 0.79631 | LR: 7.16e-03\n",
      "test loss: 0.8318\n",
      "Step 2500 | Loss: 0.81942 | LR: 7.10e-03\n",
      "Step 2525 | Loss: 0.83462 | LR: 7.04e-03\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/train/shard_train_0008.npz\n",
      "Step 2550 | Loss: 0.84826 | LR: 6.98e-03\n",
      "Step 2575 | Loss: 0.81708 | LR: 6.92e-03\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/val/shard_val_0001.npz\n",
      "test loss: 0.8262\n",
      "Step 2600 | Loss: 0.82532 | LR: 6.86e-03\n",
      "Step 2625 | Loss: 0.81411 | LR: 6.80e-03\n",
      "Step 2650 | Loss: 0.82894 | LR: 6.74e-03\n",
      "Step 2675 | Loss: 0.81363 | LR: 6.68e-03\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/val/shard_val_0001.npz\n",
      "test loss: 0.8206\n",
      "Step 2700 | Loss: 0.88387 | LR: 6.62e-03\n",
      "Step 2725 | Loss: 0.81181 | LR: 6.56e-03\n",
      "Step 2750 | Loss: 0.83726 | LR: 6.50e-03\n",
      "Step 2775 | Loss: 0.87152 | LR: 6.43e-03\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/val/shard_val_0000.npz\n",
      "test loss: 0.8231\n",
      "Step 2800 | Loss: 0.80406 | LR: 6.37e-03\n",
      "Step 2825 | Loss: 0.80007 | LR: 6.31e-03\n",
      "Step 2850 | Loss: 0.81968 | LR: 6.25e-03\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/train/shard_train_0009.npz\n",
      "Step 2875 | Loss: 0.85252 | LR: 6.18e-03\n",
      "test loss: 0.8242\n",
      "Step 2900 | Loss: 0.85166 | LR: 6.12e-03\n",
      "Step 2925 | Loss: 0.85568 | LR: 6.06e-03\n",
      "Step 2950 | Loss: 0.83409 | LR: 5.99e-03\n",
      "Step 2975 | Loss: 0.80275 | LR: 5.93e-03\n",
      "test loss: 0.8195\n",
      "Step 3000 | Loss: 0.85828 | LR: 5.86e-03\n",
      "Step 3025 | Loss: 0.83555 | LR: 5.80e-03\n",
      "Step 3050 | Loss: 0.79976 | LR: 5.74e-03\n",
      "Step 3075 | Loss: 0.83996 | LR: 5.67e-03\n",
      "test loss: 0.8268\n",
      "Step 3100 | Loss: 0.83739 | LR: 5.61e-03\n",
      "Step 3125 | Loss: 0.84157 | LR: 5.54e-03\n",
      "Step 3150 | Loss: 0.83170 | LR: 5.48e-03\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/train/shard_train_0004.npz\n",
      "Step 3175 | Loss: 0.84382 | LR: 5.41e-03\n",
      "=== Step 3177 Done. Avg Loss: 0.86254 ===\n",
      "test loss: 0.8171\n",
      "Step 3200 | Loss: 0.81210 | LR: 5.35e-03\n",
      "Step 3225 | Loss: 0.78881 | LR: 5.28e-03\n",
      "Step 3250 | Loss: 0.83103 | LR: 5.22e-03\n",
      "Step 3275 | Loss: 0.87121 | LR: 5.15e-03\n",
      "test loss: 0.8147\n",
      "Step 3300 | Loss: 0.78994 | LR: 5.09e-03\n",
      "Step 3325 | Loss: 0.81512 | LR: 5.02e-03\n",
      "Step 3350 | Loss: 0.80603 | LR: 4.96e-03\n",
      "Step 3375 | Loss: 0.81385 | LR: 4.89e-03\n",
      "test loss: 0.8211\n",
      "Step 3400 | Loss: 0.80674 | LR: 4.83e-03\n",
      "Step 3425 | Loss: 0.84325 | LR: 4.76e-03\n",
      "Step 3450 | Loss: 0.80915 | LR: 4.70e-03\n",
      "Step 3475 | Loss: 0.81577 | LR: 4.63e-03\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/train/shard_train_0007.npz\n",
      "test loss: 0.8164\n",
      "Step 3500 | Loss: 0.80130 | LR: 4.57e-03\n",
      "Step 3525 | Loss: 0.80713 | LR: 4.50e-03\n",
      "Step 3550 | Loss: 0.82148 | LR: 4.44e-03\n",
      "Step 3575 | Loss: 0.82405 | LR: 4.37e-03\n",
      "test loss: 0.8175\n",
      "Step 3600 | Loss: 0.80449 | LR: 4.31e-03\n",
      "Step 3625 | Loss: 0.81659 | LR: 4.25e-03\n",
      "Step 3650 | Loss: 0.83539 | LR: 4.18e-03\n",
      "Step 3675 | Loss: 0.82502 | LR: 4.12e-03\n",
      "test loss: 0.8144\n",
      "Step 3700 | Loss: 0.83578 | LR: 4.05e-03\n",
      "Step 3725 | Loss: 0.79842 | LR: 3.99e-03\n",
      "Step 3750 | Loss: 0.81744 | LR: 3.93e-03\n",
      "Step 3775 | Loss: 0.83021 | LR: 3.86e-03\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/train/shard_train_0008.npz\n",
      "test loss: 0.8216\n",
      "Step 3800 | Loss: 0.77247 | LR: 3.80e-03\n",
      "Step 3825 | Loss: 0.81338 | LR: 3.74e-03\n",
      "Step 3850 | Loss: 0.79635 | LR: 3.67e-03\n",
      "Step 3875 | Loss: 0.79227 | LR: 3.61e-03\n",
      "test loss: 0.8130\n",
      "Step 3900 | Loss: 0.80645 | LR: 3.55e-03\n",
      "Step 3925 | Loss: 0.81234 | LR: 3.49e-03\n",
      "Step 3950 | Loss: 0.82667 | LR: 3.42e-03\n",
      "Step 3975 | Loss: 0.83637 | LR: 3.36e-03\n",
      "test loss: 0.8077\n",
      "Step 4000 | Loss: 0.81431 | LR: 3.30e-03\n",
      "Step 4025 | Loss: 0.81075 | LR: 3.24e-03\n",
      "Step 4050 | Loss: 0.80762 | LR: 3.18e-03\n",
      "Step 4075 | Loss: 0.77564 | LR: 3.12e-03\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/val/shard_val_0000.npz\n",
      "test loss: 0.8140\n",
      "Step 4100 | Loss: 0.77402 | LR: 3.06e-03\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/train/shard_train_0000.npz\n",
      "Step 4125 | Loss: 0.80431 | LR: 3.00e-03\n",
      "Step 4150 | Loss: 0.82486 | LR: 2.94e-03\n",
      "Step 4175 | Loss: 0.77803 | LR: 2.88e-03\n",
      "test loss: 0.8081\n",
      "Step 4200 | Loss: 0.78097 | LR: 2.82e-03\n",
      "Step 4225 | Loss: 0.80935 | LR: 2.76e-03\n",
      "Step 4250 | Loss: 0.84138 | LR: 2.71e-03\n",
      "Step 4275 | Loss: 0.80957 | LR: 2.65e-03\n",
      "test loss: 0.8048\n",
      "Step 4300 | Loss: 0.79957 | LR: 2.59e-03\n",
      "Step 4325 | Loss: 0.81743 | LR: 2.53e-03\n",
      "Step 4350 | Loss: 0.79327 | LR: 2.48e-03\n",
      "Step 4375 | Loss: 0.80652 | LR: 2.42e-03\n",
      "test loss: 0.8132\n",
      "Step 4400 | Loss: 0.80766 | LR: 2.37e-03\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/train/shard_train_0002.npz\n",
      "Step 4425 | Loss: 0.80192 | LR: 2.31e-03\n",
      "Step 4450 | Loss: 0.81935 | LR: 2.26e-03\n",
      "Step 4475 | Loss: 0.76666 | LR: 2.20e-03\n",
      "test loss: 0.8096\n",
      "Step 4500 | Loss: 0.79573 | LR: 2.15e-03\n",
      "Step 4525 | Loss: 0.77711 | LR: 2.10e-03\n",
      "Step 4550 | Loss: 0.81377 | LR: 2.04e-03\n",
      "Step 4575 | Loss: 0.80429 | LR: 1.99e-03\n",
      "test loss: 0.8062\n",
      "Step 4600 | Loss: 0.81869 | LR: 1.94e-03\n",
      "Step 4625 | Loss: 0.78360 | LR: 1.89e-03\n",
      "Step 4650 | Loss: 0.80488 | LR: 1.84e-03\n",
      "Step 4675 | Loss: 0.78398 | LR: 1.79e-03\n",
      "test loss: 0.7989\n",
      "Step 4700 | Loss: 0.79668 | LR: 1.74e-03\n",
      "Step 4725 | Loss: 0.81014 | LR: 1.69e-03\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/train/shard_train_0009.npz\n",
      "Step 4750 | Loss: 0.79480 | LR: 1.64e-03\n",
      "Step 4775 | Loss: 0.78922 | LR: 1.59e-03\n",
      "test loss: 0.7987\n",
      "Step 4800 | Loss: 0.81433 | LR: 1.54e-03\n",
      "Step 4825 | Loss: 0.79008 | LR: 1.50e-03\n",
      "Step 4850 | Loss: 0.77764 | LR: 1.45e-03\n",
      "Step 4875 | Loss: 0.78285 | LR: 1.41e-03\n",
      "test loss: 0.8018\n",
      "Step 4900 | Loss: 0.81318 | LR: 1.36e-03\n",
      "Step 4925 | Loss: 0.80303 | LR: 1.32e-03\n",
      "Step 4950 | Loss: 0.76486 | LR: 1.27e-03\n",
      "Step 4975 | Loss: 0.77087 | LR: 1.23e-03\n",
      "test loss: 0.7989\n",
      "Step 5000 | Loss: 0.84017 | LR: 1.19e-03\n",
      "Step 5025 | Loss: 0.81401 | LR: 1.15e-03\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/train/shard_train_0003.npz\n",
      "Step 5050 | Loss: 0.84719 | LR: 1.10e-03\n",
      "Step 5075 | Loss: 0.77094 | LR: 1.06e-03\n",
      "test loss: 0.8086\n",
      "Step 5100 | Loss: 0.80162 | LR: 1.02e-03\n",
      "Step 5125 | Loss: 0.77414 | LR: 9.85e-04\n",
      "Step 5150 | Loss: 0.78786 | LR: 9.47e-04\n",
      "Step 5175 | Loss: 0.78519 | LR: 9.09e-04\n",
      "test loss: 0.8036\n",
      "Step 5200 | Loss: 0.76716 | LR: 8.72e-04\n",
      "Step 5225 | Loss: 0.83016 | LR: 8.36e-04\n",
      "Step 5250 | Loss: 0.80006 | LR: 8.00e-04\n",
      "Step 5275 | Loss: 0.82132 | LR: 7.65e-04\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/val/shard_val_0001.npz\n",
      "test loss: 0.7916\n",
      "Step 5300 | Loss: 0.85494 | LR: 7.31e-04\n",
      "Step 5325 | Loss: 0.77111 | LR: 6.97e-04\n",
      "Step 5350 | Loss: 0.81536 | LR: 6.65e-04\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/train/shard_train_0005.npz\n",
      "Step 5375 | Loss: 0.80143 | LR: 6.33e-04\n",
      "test loss: 0.7935\n",
      "Step 5400 | Loss: 0.79586 | LR: 6.01e-04\n",
      "Step 5425 | Loss: 0.77820 | LR: 5.71e-04\n",
      "Step 5450 | Loss: 0.81136 | LR: 5.41e-04\n",
      "Step 5475 | Loss: 0.82777 | LR: 5.12e-04\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/val/shard_val_0001.npz\n",
      "test loss: 0.7959\n",
      "Step 5500 | Loss: 0.79395 | LR: 4.84e-04\n",
      "Step 5525 | Loss: 0.78260 | LR: 4.56e-04\n",
      "Step 5550 | Loss: 0.78384 | LR: 4.29e-04\n",
      "Step 5575 | Loss: 0.76310 | LR: 4.03e-04\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/val/shard_val_0000.npz\n",
      "test loss: 0.7928\n",
      "Step 5600 | Loss: 0.80862 | LR: 3.78e-04\n",
      "Step 5625 | Loss: 0.74592 | LR: 3.54e-04\n",
      "Step 5650 | Loss: 0.79256 | LR: 3.30e-04\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/train/shard_train_0001.npz\n",
      "Step 5675 | Loss: 0.81595 | LR: 3.07e-04\n",
      "test loss: 0.8064\n",
      "Step 5700 | Loss: 0.77036 | LR: 2.85e-04\n",
      "Step 5725 | Loss: 0.82094 | LR: 2.64e-04\n",
      "Step 5750 | Loss: 0.82375 | LR: 2.43e-04\n",
      "Step 5775 | Loss: 0.80113 | LR: 2.24e-04\n",
      "test loss: 0.8007\n",
      "Step 5800 | Loss: 0.81262 | LR: 2.05e-04\n",
      "Step 5825 | Loss: 0.78339 | LR: 1.87e-04\n",
      "Step 5850 | Loss: 0.78210 | LR: 1.70e-04\n",
      "Step 5875 | Loss: 0.78633 | LR: 1.53e-04\n",
      "test loss: 0.8031\n",
      "Step 5900 | Loss: 0.79082 | LR: 1.38e-04\n",
      "Step 5925 | Loss: 0.81584 | LR: 1.23e-04\n",
      "Step 5950 | Loss: 0.79710 | LR: 1.09e-04\n",
      "Step 5975 | Loss: 0.77778 | LR: 9.60e-05\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/train/shard_train_0010.npz\n",
      "test loss: 0.7968\n",
      "Step 6000 | Loss: 0.81672 | LR: 8.37e-05\n",
      "Step 6025 | Loss: 0.79671 | LR: 7.23e-05\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/train/shard_train_0006.npz\n",
      "Step 6050 | Loss: 0.79038 | LR: 6.17e-05\n",
      "Step 6075 | Loss: 0.79274 | LR: 5.19e-05\n",
      "test loss: 0.8048\n",
      "Step 6100 | Loss: 0.79985 | LR: 4.30e-05\n",
      "Step 6125 | Loss: 0.81194 | LR: 3.49e-05\n",
      "Step 6150 | Loss: 0.79899 | LR: 2.76e-05\n",
      "Step 6175 | Loss: 0.81039 | LR: 2.12e-05\n",
      "test loss: 0.7992\n",
      "Step 6200 | Loss: 0.77306 | LR: 1.57e-05\n",
      "Step 6225 | Loss: 0.78832 | LR: 1.10e-05\n",
      "Step 6250 | Loss: 0.78925 | LR: 7.08e-06\n",
      "Step 6275 | Loss: 0.80033 | LR: 4.05e-06\n",
      "test loss: 0.7986\n",
      "Step 6300 | Loss: 0.76316 | LR: 1.87e-06\n",
      "Step 6325 | Loss: 0.79315 | LR: 5.34e-07\n",
      "loading /Users/djemec/data/jepa/tokenized/eval/train/shard_train_0009.npz\n",
      "Step 6350 | Loss: 0.77101 | LR: 4.27e-08\n",
      "test loss: 0.7956\n"
     ]
    }
   ],
   "source": [
    "for step in range(max_steps):\n",
    "\n",
    "    last_step = (step == max_steps - 1)\n",
    "\n",
    "    # once in a while evaluate our validation set loss\n",
    "    if step % 100 == 0 or last_step:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss_accum = 0.0\n",
    "            val_loss_steps = 25\n",
    "            for i in range(val_loss_steps):\n",
    "                cont_x, cont_tot, case_x, case_tot, act_id = val_loader.next_batch()\n",
    "\n",
    "                # run BioJEPA\n",
    "                with torch.no_grad():\n",
    "                    z_context = model.student(cont_x, cont_tot)\n",
    "                    z_pred = model.predictor(z_context, act_id)\n",
    "\n",
    "                # run new decoder\n",
    "                pred_delta = decoder(z_pred) - decoder(z_context)\n",
    "                real_delta = case_x - cont_x\n",
    "\n",
    "                loss = F.mse_loss(pred_delta, real_delta)\n",
    "                loss = loss / val_loss_steps\n",
    "                val_loss_accum += loss.detach()\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "        print(f'test loss: {val_loss_accum.item():.4f}')\n",
    "\n",
    "\n",
    "    # periodically save checkpoint\n",
    "    if step > 0 and (step % 1000 == 0 or step % steps_per_epoch ==0) and not last_step:\n",
    "        # Save Checkpoint\n",
    "        torch.save({\n",
    "            'model': decoder.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'step': step\n",
    "        }, checkpoint_dir / f'biojepa_decoder_ckpt_{step}.pt')\n",
    "\n",
    "    # actual training\n",
    "    decoder.train\n",
    "    cont_x, cont_tot, case_x, case_tot, act_id = train_loader.next_batch()\n",
    "\n",
    "    # run frozen BioJEPA\n",
    "    with torch.no_grad():\n",
    "        z_context = model.student(cont_x, cont_tot)\n",
    "        z_pred = model.predictor(z_context, act_id)\n",
    "\n",
    "    # run decoder\n",
    "    pred_delta = decoder(z_pred) - decoder(z_context)\n",
    "    real_delta = case_x - cont_x\n",
    "    real_delta = case_x - cont_x\n",
    "\n",
    "    # loss\n",
    "    pred_delta = decoder(z_pred) - decoder(z_context)\n",
    "    real_delta = case_x - cont_x\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss = F.mse_loss(pred_delta, real_delta)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    # loss caching\n",
    "    lossi.append(loss.item())\n",
    "    total_epoch_loss += loss.item()\n",
    "\n",
    "    if step % 25 == 0:\n",
    "        print(f\"Step {step} | Loss: {loss.item():.5f} | LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "    \n",
    "    \n",
    "    if step > 0 and step % steps_per_epoch == 0:   \n",
    "        avg_loss = total_epoch_loss / steps_per_epoch\n",
    "        print(f\"=== Step {step} Done. Avg Loss: {avg_loss:.5f} ===\")\n",
    "        total_epoch_loss = 0\n",
    "    \n",
    "    if last_step:\n",
    "        # Save Checkpoint\n",
    "        torch.save({\n",
    "            'model': decoder.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'step': step\n",
    "        }, checkpoint_dir / f'biojepa_decoder_ckpt_{step}_final.pt')\n",
    "\n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c924050-b2d8-4973-a3c1-f9dd0a3d1f03",
   "metadata": {},
   "source": [
    "**Training Loss Plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b11fde37-0cac-429d-be0c-351390bac22d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkYAAAGdCAYAAAD3zLwdAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjYsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvq6yFwwAAAAlwSFlzAAAPYQAAD2EBqD+naQAAUEFJREFUeJzt3Qd4FOXWwPGTRgm9ht6R3ntHAQHLRewd4SoqWFG86lWw69VP9HotqFfFDjaQq0gHkRp6VXrvvQVI2+95J5uwm2yZ2Z3dmd39/54nbJKdnZkMye7Z8573vHEOh8MhAAAAkHirTwAAAMAuCIwAAACcCIwAAACcCIwAAACcCIwAAACcCIwAAACcCIwAAACcCIwAAACcEnM/gT7Z2dmyb98+KVGihMTFxVl9OgAAQAfVz/r06dNSpUoViY/3nhciMDJIBUXVq1e3+jQAAEAAdu/eLdWqVfN6P4GRQSpTlHthS5YsafXpAAAAHU6dOqUlNnJfx70hMDIod/hMBUUERgAARBZ/ZTAUXwMAADgRGAEAADgRGAEAADgRGAEAADgRGAEAADgRGAEAADgRGAEAADgRGAEAADgRGAEAADgRGAEAADgRGAEAADgRGAEAADgRGEUoh8Mhny/cIat2n7D6VAAAiBqJVp8AAvPr2v0yevJ67fMdr11p9ekAABAVyBhFqE0Hz1h9CgAARB0Coyjz86q9Mmb6Rm2oDQAAGMNQWoSK8/L9h8ev0m67X1JB2tYqG9ZzAgAg0pExilLHzqZbfQoAAEQcAiMAAAAnAqMoRYURAADGERgBAAA4ERhFqDhv1dcAACBgBEYAAABOBEYAAABOBEYAAABOBEYRKs5ri8ccNL4GAMA4AqMY9ca0v+TJH9ewdAgAAC4IjCLUwq1Hgnr8e3O2yvilu2XrYXssRjtp5V75eslOq08DABDjWCstQi3ZfsyU/aRnWp8xys52yCMTctZ4690oRVJKFrH6lAAAMYqMESznGpqduZBp4ZkAAGIdgVGMc9hg8RDqnAAAdkFgFLUiM9igoTcAwEoERrCVyAznAADRgsAIAADAicDIpr5ZsktSTZp5FkkYSgMAWInp+ja0eNtReXriWu3zHa9dKdGO4TMAgF2QMbKhnUfPhu1YTAgDAOAiAiMAAAAnAqMoFUmZoEg6VwBAdCMwQtBOnsuQc+lZpuwrLo7yawCAdSi+jjC/bzosB0+e97tduOILtYRHi+enS6HEeNn0Uv+I7b4NAIBCYGRDcT4mrQ/6NDWkw1Mrdh3XFnVtW6usru03Hjit3aZnZgd2QAAAbITACHkuZGbJte8v1D5f+9zlUqJIktWnBABAWFFjhDwXXLI+p8+Hb5V7iq8BAHZBYBSlgo01zIhVDp06Lx/P2yYn0tJN2BsAAKHHUFoMcnhJ0cTp2MaIOz5JlY0HT8uCrUdk3OD2Qe8PAIBQIzCKUhv2nZJLUopLvYolQrL/8xlZ8vD4lT7rkFRQpMzdeFj3flftPi7VyhSVpASSmQCA8OPVJ0q9O2eL9B4zz5TMj6e+RV3/NUemrT8oPyzfY+q+H52wWp76KWedOAAAwo3AyEYOnDwvPd+YI5/M3x62Y7rGTXqbKz4zaZ0cOXMhJOegmB1sAQCgF0NpNvJ/0zfKjqNpBb5/+PQF+XzhDrmpXfWAgo5Amj36SjSlbj9qfIcAAEQAAiMb8dQkUWVmhn+zQlK3H5OJK/ca3qeRgTSrFuMIVedrNYyY7RBJiGeZEQCAPgRGNpGZlS2TV+8r8P23ZmzSgiJl74lzhvebU2N0MTD4eslOKVescEDnqDpix0dQkDH0y+Wyds9JmfN4TylaKMHq0wEARAACI5v4cvFOj983a3FWZevhM/LPiesCeuxH87bKu7O3yA/3d5ZIMWPDwbz15fo1rWT16QAAIgCBkU2s3XsyJPt1HaQ6esZ3o0XXWqT8NUavTPlLux31c2CBlS90vgYA2AWz0qLA/M1HpMcbc2TR1qOGgw7X+h5fi9cCABALyBjZRRBZk9s/WaLd3vLxYg+71b9jPduGIrsT+oQRKSkAgD5kjGxu3mb9XaPNtGLXca3YGgCAWEJgZHNH/NQFBZvheeKHNR67Yz8yYZU8+/M62XggZ1kPI/sEACBSERjFuL8OnJbpztlb+X29ZJf0fXueti5aKHkKzNQ0ewAAwo3AKMrpye5MX3/Q57ZpLi0DtK5IYajRHvRZaugPAgBAPgRGNmHn0alQLETrtn8P3zt1LiOkxwQAwBMCoyina6aZkbDMzhEcAABBIjCKcmYke6yIhfQM1525kMnMOQCAqQiMopyesCG3saNDT3AVZ4/gTa0b13T0NLnxw0Uh2T8AIDYRGMHvUJrb/TYJMn5xLri7bOdxq08FABBFCIyivMB58qp9kpaeKe/P3SLbDp/xul1WtkN2HDnr8b7352w1fFxD0+1tEmwBAMCSIDbx69r9Idnv0xPXypZDZ+TTBdu9brPp4Gmp+/QUr/ePW7jDcKH2Q+NXSjD8rdtmJJYKR3sBAEB0IGNkExlZoUubLN/le7hp3d5Tph9TZaD0MjQrLgDUGAEA9CIwgmHpmdmhD3bI8gAALEBgBMPZl+NpNF8EAEQnAiMYkp7lP1tkFENdAAC7IDCKAVaPSoWi+JlgCgAQCsxKgyFxJgcuh06fl0OnLgR8HAAAzERgFAMys80f/vJWlH3iXLpULFHEb5CkeitNWXtAHv9+dVjODQAAPQiMYoCZ0/FPn8/0et8V7/yh9Uya8Wh3n/tYtPWofPD7Vpm36bBp5wUAgBkIjGDINi/dsRUVFCkqE+StxmjBliNy23+XSDhRjgQA0IvACCHhrcZo4dYjQRVsL952VOZsPCTJSfzqAgDMx6sLTBfKTtY3f7RYuy2dnBSyYwAAYhfT9WE6X6uB+FsDTe92J2gyCQAIAQIjhBxT7wEAkYLACCEvMKL4GQAQKQiMENahNAAA7Izia5ju3TlbpESRxKCWBwnFMiIAAPhDxggh4doIMpAYJy09SwZ9mqp1yA4W66oBAPQiMIJth9Z+33RYPluwQyLBc5PXy7uzN1t9GgCAIDGUhrDIznZIfLzeyfoXnbkQfMYo1FTH73ELcwK4By6rb/XpAACCQMYIYfHDij0BPc6MUqNQ1yudz8gK7QEAAGFDYISw+G3tfsuOTY0RAEAvAiMAAAAnAiOEF/PwAQA2RmAEW8u0QbdIaogAIHYQGMHWPpq3zdLjf79stzR8dqpMWLrL4/3pmdmSZYPgDQBgDgIjhFUgA2nzNh2Wp35a67XZ45/7T8ldn6XKur0nxWwjf1ij3f7jx7UFskjqo81LM2TAewtMPy4AwBr0MUJYxAVRW3Tnp6nabblihTzef9OHi+TU+UxZtPWobHypv4Sa6q3U8vnpthnqAwCYh4wRwmL2X4fkQmZwtTp7jqd5/L4KipQLmdkSDkt3HNMCokCCov0nz8krU/6U3cc8/ywAAGsRGCFsWr0wI6hJaYHmZhwBP9J8d3++TKubuuXjxVafCgDAAwIjhI1aGDYYP6/aJ5Fu/b5T2u2e4+esPhUAgAcERog5p89nBDeUZZ8EFADAZARGiDmtX5wh3V6fIzuPnrX6VAAANkNghLCKM2VZ2OBkZOWkfJZsP6b1IQIAIBeBEcLq80U7xC4mrtgrlzzzm/xn1marTwUAYBMERgirY2fTw37Mw6cvePz+om1Htds3Z2ySk2kZQbcNAABEPgIjRL3n/7fB7za7dBZjz9l4SJ79eb0JZwUAsCMCI8BAr6OvFu0M+bkAAKxDYASIyB2f5Cw7AgCIbQRGiConz2XIqfMZPtc58/Y4PYLp3A0AsD8WkUVUafnCdHE4RDa/7Hkx2SznVH0AADwhY4SoooIi5cDJ827fn7/5iHZ7LiO4ZUkAANEtpgOjgQMHSpkyZeT666+3+lRgsuzcCMnp9k+WyLbDZ+SBb1aE9LiOfMcFAESWmA6MHn74Yfniiy+sPg2EwJgZmwp8b/uRs7Js5/Eg9+y/yIjgCAAiV0wHRj179pQSJUpYfRoIgZ9X7SvwvXDEK2npWdLjjbky8vvVoT8YAMD6wOj06dPyyCOPSM2aNaVo0aLSuXNnWbp0qaknNW/ePLn66qulSpUqEhcXJ5MmTfK43XvvvSe1atWSIkWKSIcOHSQ1lSnX8M6MuMjfrLQpa/drzSK/X75Hdh292DQyLT3T52w5AECEBkZ33323zJgxQ7788ktZu3atXH755dK7d2/Zu3evx+0XLFggGRkFXxA2bNggBw8e9PiYs2fPSosWLbTAx5sJEybIiBEjZPTo0bJixQpt+759+8qhQ4fytmnZsqU0bdq0wMe+fQWzCYh+4Rjicj1C9zfmyJeLdmjHbTxqmjR/bnrIjw8ACGNgdO7cOfnxxx/l9ddfl+7du0u9evXkueee024/+OCDAttnZ2fL8OHD5dZbb5WsrIuzgTZu3CiXXXaZfP755x6P079/f3nppZe04mhvxowZI/fcc48MHjxYGjduLGPHjpXk5GT59NNP87ZZtWqVrFu3rsCHykQh9jgsOMhbMzdLNiVHABCdgVFmZqYW4KihK1dqSG3+/PkFdx4fL1OmTJGVK1fKnXfeqQVKW7du1YKia665Rp544omATjo9PV2WL1+uZapcj6W+XrRokYSCyl6pAKxdu3Yh2T9CLxw1Rs/8vC70BwEA2CMwUoXKnTp1khdffFEbjlJB0ldffaUFI/v37/f4GJWdmT17thY4qcyRCopUAOMpw6TXkSNHtGOnpKS4fV99feDAAd37Uedxww03aMFbtWrVfAZVKvOlhv/MrqdCOAUfGfmbk5aemV2gbUAkN8tWdVHr9520+jQAwL6dr1Vt0ZAhQ6Rq1aqSkJAgrVu3lltuuUXL4HhTo0YN7XE9evSQOnXqyCeffKIVVVtt5syZVp8CwsiKWfQn0iK74Pqy/5srR86kyzd3d5DO9cpbfToAYL/i67p168rvv/8uZ86ckd27d2szwVRxtQp4vFFF1kOHDtVmmqWlpcmjjz4a1EmXL19eC8ryF2+rrytVqhTUvoFYcvp8hrw5faP8deCUx/tVUKRM3+B5ogQARJuA+xgVK1ZMKleuLMePH5dp06bJgAEDvA579erVSxo1aiQ//fSTzJo1S5tR9vjjjwd80oUKFZI2bdpo+8ql6pfU12qoD/DEX8LovHO5EL0Lypp1XCu99ttf8p/ZW6Tf239YfSoAEJlDaSoIUtOPGzRoIFu2bJGRI0dKw4YNtdlh+algRc0wUz2PVDCUmJioFTCr6f6q1kgNx3nKHqlslNp3ru3bt2szzMqWLasNyylqqv6gQYOkbdu20r59e3n77be1af6ezgPwtExIfuNTd0mlUkXkvq9WyL3d68hTVzQqsE0gI8Br9pyQUFC1PwdPnZfLGrrX2hmxdi/1QwAQVGB08uRJeeqpp2TPnj1aoHLdddfJyy+/LElJSQW2VTPFXnnlFenWrZuW5cmleg6p+p4KFSp4PMayZcvk0ksvzftaBUGKCoTGjRunfX7TTTfJ4cOHZdSoUVrBtepZNHXq1AIF2YBeZ9Oz5IX/bdA+/3DeNo+BUSAGvr9QQuHKd3Jmgk5/tLtckhJYB3dWLwGAIAOjG2+8UfvQq0+fPh6/36pVK59LdehpxvfAAw9oH0A4goCfV+2V/SfPm3U62u+4GZMQth0+G3BgBAAIMjAConUoTQUYvrZ4ePwqU89HnY4ZkzNza6NCiYVxAcSKmF5EFnD144o9ETm09MiEVbLl0OmAHuvQWRq+9fDZgPaP2KLWB/x84Y6wBOtAqBAYAQEECuYcy7Nz6Vlyw9iF8u7szbr39fnCnV7vW77zuDw7aV1Qs+3mbzkScPCF2NFrzFwZPXm9/HuW/t9dwG4IjAAX4cwY7TqWJhv2ndKmzKsO07m+X75blu44Lv83fVOBx5xIS5d7v1wm09br7/B+3QcL5cvFO+W13/4M6nwXbTsW1OMR/TKycv6AFm87avWpAAGjxghwcS6MQwA3f7RIDp66oH2usjmvXttM+/xChvuyIq7enL5Jpq0/qH0YtfVQweGwSBw6BIBQImOEmKGnePr0+cy8zxuPmhrS88kNipQN+z13ns7v8OmLjzEqdccxbYmP5TvJ/CC0CLgRyQiMAC/S0rMi5pXEtTZKzSD7btlubZguv21HzsqtHy8J6lgAEM0IjACbcZ3C//G8bbL18BlDj1f1R0/8sEaueMfzMh8XMr0P1flz9sLFjBoARCMCI8DGXp7yp/R683f5YtEOeXvmJpnqpeg6Ti5GU+s9ZIrMSFSNnbtVmoyeJr+s2af/QQAQYSi+BmzgrwO+p8KP+nm9rqG0k2kZkpWtP9oxMoC398Q57Xbk92vkquZVDDwSACIHgRFgA8EMb+XaefSs9Hhjrq5t1fBc3QrFvd5/4OR5+fiPbUGfE2ITtdeIZAylAVFi8ir9Q1y5i+V6c+9Xy+WT+dslHFSW667PUmXy6n2mBIfzNh025byAYPx14JR0fGWWTFi6y+pTgUEERkCUMPNd+urdJyRc3pq5SeZuPCwPfbsy6H2pjNmdn6bSkgCWU0POB06dl3/8uNbqU4FBBEZAjCySa1YQZfayKcfT0sVsq3ef1LXdur0nZfNBljoxHY2MJCMr+OFxWIMaI8Bm4lzn64f4tUj1PIpVagjvqv/M1z7f8dqVVp8OAJsgYxTBapZLtvoUYCOOIAMiVXCtWgIcOn1eYsHBGPk5ARhDxiiCDepUS174xXcRLWKHkezPH5uPyGPfrXab2j/o01TZePC0Vu8TToHlxwAgNMgYATYTaKBgdFTsxxV7ZPOhi121VVCkrPJTeH0+I1vemrFJ7MzoaGQsDykCcEdgBEQB9bpupPg6WP+etVnS0jMtranyhTgHQKAIjCJYCF5PYAM7jp4N6HHvz90q4eSvF1IkIZAyF5cTkYzACLCJ3HqfLxbtjIgg2XXdtt3H0rQmjQu3HtE9I2zcgu1y5MwFw9fo1Sl/ypy/DvncjjcNAAJFYATYxLuzt0RsxuORCau0ou1bP16ic/uV8tz/NsiQcUsN10V9OG+bDDb4uFA5kZYuo35eJ2v2hK8hJoDQIjACbGLcwsCX4Ji32dplMNRUfyPmOGe+rdmjrxFjrn3OhWzNFmhc+dzk9VqG72/vLjD5jABYhcAoQtzcrrrhxzRIKRGSc0FoBJP02X0sNAFDOHga9Tp5LkNGfLdK/rA44PNn08GLs/oARAcCowjmr4zi26Edw3QmiEV6ynimrjugNY10nQ6//chZvzt6Y9pf8tOKvXLHJ6k+hwzPXMiUsxeCnx3HdH1zcTkRyQiMolDxwonyy4NdpWyxQn63/WlY57CcE/w7kZYh/565WSKRt2Ln+75aLm/P3Czzt1wsyu771jy/+9t73H8GLD0zW5qOniZNRk9za1QZThR5wxuCw8hFYBSFvru3kzStWkrXtq1rlAn5+cDYSvPRwrUe6PDpi7PP0g0urqnqeDzVFh07m+6WObLDoqFknoDIR2AEwFRzN+ZMpZ/mMp3fcCdql8/HLdwhQ79cVuD7vvZpNJHjL5xRw3UTV+7Rap+8af7cdBn65XKDRwZgNwRGUa5iicJWnwJigGuQctdnOVPpn3dpABkX5Ipo6/ae8r1BiBM1T/60Vh6dsFrudQZonpzLyJIZGw6G9kQAhByBUQTTs5TCj/dTQwTrmVWL47obM8t78o+A/bZ2v3yzZFfe1/9bvU+7XbztWED7V8unLNtxTLItqoVC+FF/FrkIjCJEscKJAT2uetlk088FkTesZWdGsknewgqHl3u+Td0lT/ywOq84W9UA6alHuv/rFfL0xLWy08/yLHpf/FTjy+vHLpIvFxvvah6JvP1/xBLKzSIXgVEEaF+7rDx4WT2rTwMRKHdYy+rgJhQLxerx1E9r5btle/KGuB74dqU2k+3P/X6G5lxmCpph1e6cztjfL99tyv4AhA6BUYTMMiud7H/qPWAVPXGP6l80ZvpG895t5zumryn7p87nBDi/rtmv3X62YLuuDIdaDy7T4Cw6AJEtsPEZ2AJj2LCSkSyQ2rLf2/PkQmaQQYaXiGnfifNy00dzfB7f1Ypd+tY2+2DuVimTnOT2vdW7T0iL6qWd+zX2R8jwCmB/ZIwgS57uJfUrFrf6NBDlgg6K8nENSsb+vlVOn3evHVq87ajXx245dEZ3wJJ/ptmA91gXDf5RZxW5CIxsLhyzylJKFpGJw7vIqKsay5SHuoX8eIgOqsGi6j6thpoy8gU9d3yyxO1rtZ0RRl9SPG1/80eLDe4FZiEzhkjGUJrNJcTHhW0ZkSFda8tJk4pNERven7tFvl+2R/adPO/2/T82X1wCRHns+9WmH9t1JC9/Bqjgtv7/jtbtPSlFCyVI3QrRlT09n5ElFzKypVS+IUEAnpExgpt4fiNgwLxNh2Wvh+U6QvUCv9lLALTneFpQs+COnk2Xq/4zX3q9+XuB+4JtTmmmZyetkxvGLjRUEN7+5ZnS4oXpvOkJI9UHa9NB38E67IuXwQgWiqfrEkWSZGj3OvL3rrWlaFJCCI4A+OdpzbFur8+R39Yd8LJ9cMfbfcw9sHLbt4+BvWwdBzZz/TTVB2npjuNui/L6c8pZe7Vmr76CczPE+lCa6oOFyMVQWoRrkFJCNh48beo+n76ikXbr2vkX8CScr3+uC9GawbVY+5c1OZ2tjVq/z3c/pI/mbZVP5ru3BjCDr9YEAIJDxiiCTBzWWcbe3sbte0O61grZ8S5vkhKyfSM65J8JFuhyGcHyl5WZ7FzSw9V1HyzM+/yrxRffBOw6mm9YLojc7CtT/pKDpy4Y/lk2HzztM/iJ9YyMnZmZIYQ1CIxszvUpuVWNMtKvaaWwHfvlgc3CdixEJn9Fz3p4Gh7T89riuo1DRy3UaWeTR38GvDdfrKQyTH3emicjfzC/YB2hN/wbhtEiHYERfM5UA6ygp3batfZHTyB1LiNL17GP5ytS9lRjpBo/mhEUevLOrM3a7U8r9oZk/witKWs918EhchAY2VCvhhX1bUjra0Qpo6MR4W6m96+pf0nvMQVnsOUufXL9Bwtlzl+HdP9c2dkOuZCZE7jp+UkYrAFCh8DIhtSssFw8ASIW6Qp0XIfSHP7fMwSa4TFaY/TI+JWybOdxGTxO/wK+qpt28+emy9kLmbr+6O1ex2Lvs4td41N3ycfztll9GrbHWEkMUms/qeGCq5pXtvpUgLAtN3Lrx+7duENl9Z6Thh+zdm/OY1RAFSrBFJGrQEzNwKufUlwKJ9LGI1I9+dNa7faK5pWlaumiVp+ObZExsjl/T2XVyiQb3ufkB7rKE/0aBFRcrdoDKKythlA1xtObDfEVDNk5oaIyV6pZZTDC/eN9tWSX1gDz7s+XhfnICAUtMwmvCIxsrlb5Yj6Dps51y8kzVzaSL//eXvcyItXLJsuwnvWkVFHjSwRMfrCLvHNLK5lwbyfDjwX0NMabs7FgbY4nquFjpNmw/5RWm3TD2EUSScYt2O5xqRe7m7vxkHR7fbYs8bGgMJAfgZFNrRrVR1v13l/wopY+uLtbHelWv4Lc2qGGXN44RS5Jcc/mfH9fJ234zAwqjf63FlWkbLFCYraBraqavk9EnlW7Twad8bH7vITcoTNPdBVf2zgjZid3fbZUdh87Jzd/bP2CwjTljBwERjak/nxKJxfSVr034pWBzeSjO9sWWCeqXa2y8syVjQM6lx/v7yzhMubGFvLXi/3CdjzAjuxWWK06jm89fNayn0HtK9j9WX1JX/3tT2n5/PQCa/rF8u+VnVF8DZ/a1CwTtmOpgK4I67PFPDOSPT+a2QMojNkn/YcK7EXu0/nbpV7F4tL9kgp+t1VLAq3efUIb/jPDqfMZkhQfL0ULJRh6Mb/pw8XaLMXv7u1keHFgu/jw95yZYO/N2SqvXkvjXLsjMAJgu2VGgn1zq17QTRMlb7RTtx+Vd2Zv0T7f8dqVfrd/emLODCa91vkYHjyXnqW1I1BxzfZX/R8717Gz6ZK645j2+dGz6VK+eGFD5wQEgqE0G9L7nsjIm6dgntsrlODJCOHz6YLtYW/YaCehqjHac+KcmGHrYc/9oNSsNW9U00slmIA3MnNFiEQERvDrrRtbWn0KgHV4RXYz/Gv/a4H9deC02+LAZgS6oQyVT6ZlyBX//kPG/r41hEdBpCAwikC3daghNcomyzUtmcWF6LTvxHmxi9TtOUM54ZI/qzJl7X7ZsO9U0EFCMA0eXR1PS9e13di5+oOMg6fOy/IQNrf056M/tmq1VK/99leIj2RdJtRutddZ2Q5tKRw7osbIhvz9qqjGjKoo0UghYjBPibE8rAFr7Dpm3ewdq7n+vS3feUyGOTM0rnVBgbzIhfvveN/J87rPt8Mrs7TbicM6S6saZcKeuLuQ4d4s9NDp81qrFLp8h0ZmVrb0GvO7lCiSKP97oKvtiurJGEUou/0iATDmk/nb5dCp8z4LxdWQVKSbvv6ADNG5bpyv7Fy4wrodR85K+5dnSZ8x88J0xNiz4+hZ2Xk0TdbtPSV2TBoRGCEkKfgpD3ULybkA0eLFXzZojQfVArJ26Vc0aeVeuZAZ3HIl+Q39crkcOn3B6/0/r9prqzeAMzYc1JW1VENBaomU16caGX7jDW0kIDCKEcEE5YGk4BtXKRnEEYHYsM1P40Rvb0pCMSx23QcL5ZEJq2TMjE0+az88DYt56uq8zDnN3p+Hx68Sq+m5mvM2HZa9LjP7Fm09KjP/PCjvG6ilspINEzO2RWBkQ5HynqKHjiZxAPRTCRLXwMNbABSKQtrcDMn09Qflb+95n3qvsj+1nvzVbf0x1W8ovx1HI79ObOfRs/LED6vl84U75M5PU6XLa7Pz7kvPCiSzRniS//fXjq93BEY2FCl/Ov+6rrl2m1KSPkeAWS8Yrn///5y4Lu/z9ftOhuU54tS5DK32w5+bPvK//tgva/b5vP+MzVd5H/Rpqny3bI+Mnrze6lOJWg6xHwIjm1DrnEWaSqWKyMaX+smiJ3tpSwzUKV/M6lMCIl56pvsMqVxXvuM9i2MmM8t6Hvhmpdf7MrKypenoaWJnkZr1Uv2Yvlu22+/MMKvE2TFN5ILAyCZu7VAjpPsP1e+hms4aHx8nXwxpLy9e0zRERwFig94eQaEVnletf8/cLNFCDbmpobaFW49YfSpal3HVj+mJH9Z4XURW9cVq+OxUy/4PHHZME7mgj1GMCKr4WueD29YqozWerFOBzBEQiD82639hPXkuQ+u1Y3S2ldVUUfYHc7fKrL8ORfSLqWth/EPjV2ltF1SBdijbCKh1BJtVK+V3KNSfl37dIJnZDnlr5iZ5uHd9E88yOhAYwdTs0dzHe9o+TQpEundnb5aHvl0pVzSrJO/e0lrL2vqjXlT1COTv18hjrh+7yPgBRGTquv1yR6daYjW1IO66fSflv/O35X3vwMng1qHT07C35//N1W4XP9VLK2MI7nhimcmr97llr3I+t9eLBkNpMJV6gg5F75F+TSqZvk8gUm06mLOQ65S1B+T75bvlRFq6vDdni9t08kDZ6yXqomd/LlgArYaE/rfad4G32dTSITeMXSQLthw1Z3/7TknrF2fIF4t26G6OGKxA46LsbIfW7ypQi7cd1QJ6O7Ro8IXACH7ZIYP92nWRV5wOhCMomfPXYRn5wxp5Y9pGuTHAbIzbOdg1MvLginf+kAe/Xan1FDKL/5/fYeo6dP/4cY0cT8uQUR4CP7s9X9/9xTJp9/JMLcAJxKaDkdHJncAIEaF4YUZ9AU9Ur6P5ztok14zR3I2HtOEno06d0z+FfuD7C+TrJTslVPSGGxsPFGwv8KXODEx+4V6HzuhjAx0Gc+i8rhlZ2bJi13G3WWuqT5X6/mxnXdhnC7ZLNOPVJkY0CaITdbBvIOtXLK4VDP60wnvrf1e9GlY0XJgJwH3I467P9K1Plt+5DP2NC1fuOqF99LXhULenoTc91AKyoeIpqDG73ifY5+tnJq6TCct2y12da8lzf2uizbjr8cZc7XncroXwZiNjFCMaVS4p397TUSuONirYvwFVKPj6dc11r5/27q2t5eu7O7h9z+y6pQcurWfq/gAzBFT47OGlMJJft9SMtVBmoTxxfaH/+A/j2ZBghtLW7ztl7Fhx7gHwyTT3WWjLdh73uw9fvx8TnP2Pxi3MybhNXXdAu918KKeuzWx2/F0lYxRDOtUtZ9mxExPida+fVrRQgnSpVz6k51OldNGQ7h8IhOqyHMhQTCjWTjNCDbOYYc/xc/Ll4pyg6LYONXU/zorFZl0dOKUvy2TGaboGcXWenqLd/vJgV2latVTe4sRmcuj8XjQhY2RDhROj479lYKuq2u2wnp6zM3d0NPDEZ9pZAdHlwKkLcj4j22368++bwjsU/eSPa03Zz1mLlggxErDk1tlYTc1my/WVM5j0xczhL0eUR0bR8QocJR7tfYlc17qatKxeWuykceXA6pPG3NhCVo3q4zVTRSNIIHiqsaArNWtoyLhlYT2H381qbOgSoLj2uvGXpVKF5uF6oX9vztaQHWfcgu1yXmeN19GzgU+bd3X9Bwtll8GlT/YcT9Nmpvn7P4rUN7gERjaiOpC+eWMLy9PC+VUoUVj+eOJSWflsH0OPUz9H6eRC3u/XvZ/ImkIMWOnIGTssKxIYIz2JsrIvvijP2XhY9pnQw8lqz/1vg7wzK/TLdLgGNKomaeQPqw09/q8Dp+XmjxbLL2v2Bz8sZ8PsE4ERdKleNlnKFPMe5OhVsojxsjYVEwUSLH4+pL3cFuI16IBItPuYPRdHzchy7Yhs7Ly3hKg4ONyWbD9mapful37ZIEt3uO9T9U1ydfRsujZF/0JmwWyVr8Bl2vqcwuxoQ2AEy+gNduIDTBf1uKSCvDyQxpBAfrkrr6teNaop5DOTzKkRMlv+P/1ur88JuNBbTTtXs93SM61bVV4Po8NTvqhu6P+dv13r1O2rQF8Flde+v1Ae/Galof17yxipa6xqoMz8WcKJWWkIq0D+TPIHRj0bVJABLavIoxNWB5yZYmgOsUy9XqnFXN+ds0VSdxzTPuz4XOHpdVUtKfFk/4aSUtLYemGqF09uA8v7e9YVuzIzlNh2xFgWbbpJCw3f/cUybUHdplVLys3tasjtBiba2AEZI9he/iBmZN8GMrBVNa/bq8Bp/NBOoT8xIIKpxVznbgzdavCh8tu6AzLo01SfAYUaQvImdbt5y4dYKZjeSUY4AgjVVFCkrNt7Sp6ZtC6v15KnM7a61YQnBEawPaPZnXGD27v1TJo0vEuBbUoEUOsERAs7vhh5XnHe8307/MyiUvUy3thtcoud3TB2oSnBs6faJTsjMIJl9D4/5b4z6lC7rDbFv0FKCUPHUe0P2tUq4/a9/k0rG9oHgPDzVaKS/75IrWcJZVAbbFZp6Y7jkmpiMXikIDCC7cU7/7bHD+0oMx/toXXRDkbvRimSkLtTD/rZcN0nwEyREEOE6hRVg8a+b82Tk+cytGBKrY123kYZDX//N2/N2CSPf7/a45tM14Vfje4XFzGegPByGOsAfiEzW1uANjcFHmgW3PWdU6miSX77SU2N0mmoQCT5de1+Q32NBn+WKq1qlJHWNdwzxPltPHhaWjw/XdrULCPLdawtZhY9wYlDRxZHfeTPnD87KWfxVztzSGQgYwTb+vWhrtoKz+/c3MrU/aoZLUAsi4QXqDV7TmiFu3qHlFRPHdXoccyMTbqPEc6gKBAqA/Tzqr3yuXNBV191O2qNOdu2IojTFywu3HJEax2Rlm7N0jC5yBghvAxkfOpVLCHP/a2J1/tfuqapzydOX528faE2E9FOrWBvd1sPnzW0vet6cdHi8rfmybYjxq6DN5NX6e8qHir+nlpv/e8S7bZkkSR5op91b2DJGCG8XN4dBBt/qN4Y17bOWagWQHSJMzgs5fqGxq5vboyel6+gyOjsuid+XCPhMPybFUHvY/dxa5d3ITBCREv0UUQd7n1WLmWs4RyAwIf7fN0fKYXGHhe/dTgkO9thzsK8YQ4QL2Rmya8G10+zIwIjhFWKa/Bgwtu6EX0aSLUyReWJfg1k3OB2MnFYZ92Pnf1YD/nfA119bnNJSnHd+1v0VC/d2wLw7eN52yTa5A/Y7vpsaYFtVu85KeOX7vbYxNL4ASUsvlmyS3q8MUd2eekvtff4OW15EjUTMBJQY4SwGnt7Gxk9eZ08eFl9OWrCKuCVShWR+f+4LKDH1qngP+hR7exf+GVDQPsHELjNfhaFjea+RWNmbPS7jZqxa+asvmA8PTFnrb1RP6/3eP/A9xdKJCEwQljVq1hcvr67Y970WpU0svr5Lf85uH5u11oFINZFb1gkckTHm8aP5tmvgD4jwAV+7YahNFhGNVl8cUBTsZMHL6vn9nWo46Iel1QI8RGA6GT1G6pgqGzXt6m7gtpHNM7CswsCI8Q81+BHTRMNp1evbRbW4wHRou1LM7wGSqqexY5yM9AzNhyUp37KGX4yw7ep9mjsGBcXHQEugREsZYehqk51y/m8/+WBoctqJQW5vAkQqzKyvL+iLtp2VOwoNwjYeOC01acCH3hWhqXs8G4hf2dtR75eIfUrGlu01hV9lgDkUkt21H7qV3nTQHfuSBIX7v4AIUJghNjg4++1XHHfnbCDcWmDiqatlA3A3tnnSHkzCN8IjBAbgngyql+xuOEn5af6N5Sv/t5BrmpeOfADA0AESd1xzJT9WB3jEhjBUnZ78+Qpg1OmWCH5/r5OPh83eXjXAlmorvXLG27bDwCxxGG7VwECI8QKA/GJpz5G/pb7aFatVGDnZb/nBCAi8RYEZiEwQmxwBFc8GEzmp2rpol7vU9moXHd0rBnwMYBYR3YWZiEwAvKldYNJ7TatWlK77dngYuPGBU9eJr+P7OlxcVo1XX/Nc5fL2uculwd7uTeXNMvjl18Skv0CdkJYZI1nJ60zZT+ZNuqazZIgiA1x4elM/fPwrnI+I0uKFXb/06pZrpjEa+9oCwZduU0lSxRJknkjL5Xub8wRM1XxkbECgGB8uXhnUI/Pdojc+WmqzNt0WOyCjBHgQ42yyYbiKrXMSf6gyNDxyuUcL7/LGvqe9g8AkajjK7NsFRQpBEaAz+JreyToiyYlBPWODIh2P63ca/UpIABnLmSK3RAYISYkJQQX4JgSHwW5jyZVSlpxWACIKQRGiIk2sC8MaKrNDnvxmqYR25n2g9vaBPS4amWoMQIAvQiMEBPqViiuzQ6zckp8MJkbNVNO1R/99862cnvHGl63a1m9dMHj2mQ4EAAiAYERkC9bpAqoQ7E4YrbOVFTrGgWDm1y9G6fIfT3qer2/lofibeIiANCPwAjI55KU4lofouvbVMv7XqmiOVPqg5Gpswr6nVtaBXwMskMAEBz6GAEuVFyhgotxg9u7fb9ooQRtGOvuL5YFvG+9tUt2rXECgHCw+v0dGSNAZ1DSoFKJoPY9ok9OB+oyycFnn0Klfe2yVp8CAFiKwAgIkwcvqyczR3SXUVc3Dmo/ifHxht5pGXnz1buR70aSfRqnGNgbAEQeAiNYKpZGjdQQXb2KJZxLgwSuUqkiEip3da4dsn0DQCQgMALCrFyxwkHvw9OCtGYolOj7KYHSbgDRjuJrIMy61Csn9/esKw291Cx5qnPSW5AdqoAJAGIFGSPAJaDoUq98WIbU/tGvoQxoWdX0fQ/qXMvD8aJntggAhBoZI0BElj3TWw6cOi8NKwW2HpkdLP1nb6lQIvhhOl/MaHQJAHZGxggQkdLJhWwTFKnlPwLhKyiqX7F4EGcEALGDwAiwmVA0eIyl2X8AEAwCIyDqgyXzhr+oMQIQ7QiMYCmWvwgPh0kXmsAIQLQjMAJsxmHjfVJ8DSDUzmdkiZViOjAaOHCglClTRq6//nqrTwUwxLJEm4e4qFBCTD+NADDZ2QsERpZ5+OGH5YsvvrD6NICQDHuFYvhraLc6bl9/PqS9lCteSKx0fZtqlh4fQHSJ6cCoZ8+eUqJEcCumI3aEq75GT1hk+FRMirVaVC/t9nWPSyqYs2MACLJliWWBUVZWljz77LNSu3ZtKVq0qNStW1defPFFU9/lzps3T66++mqpUqWK1iV40qRJHrd77733pFatWlKkSBHp0KGDpKammnYOQLTIH0Q91Kt+QPu5tUMN7bZEEfe+sFZXHVHAD0SXOIufVQx3vv7Xv/4lH3zwgXz++efSpEkTWbZsmQwePFhKlSolDz30UIHtFyxYIO3bt5ekpCS372/YsEHKlSsnKSkpBR5z9uxZadGihQwZMkSuvfZaj+cxYcIEGTFihIwdO1YLit5++23p27evbNy4USpWrKht07JlS8nMzCzw2OnTp2tBFxCp76D0xgKq/qdR5ZJu24/oc4kkxMXJWzM36T7+q9c2k5vbVfd4cOISAGZK3X5MIiowWrhwoQwYMECuvPJK7WuVsfn22289Zmuys7Nl+PDhUr9+fRk/frwkJCRo31fBy2WXXaYFNk888USBx/Xv31/78GXMmDFyzz33aEGZogKkX3/9VT799FN58sknte+tWrVKzKKyU+pDZcxg33qaaGDWJVGL1E4a3kWKJCUUuM7tapXRvZ+SRRJlQMuc7K2Z5/vbw91k+c7j8sykdYHtAEBUSs/KjqyhtM6dO8usWbNk06acd5urV6+W+fPnewxk4uPjZcqUKbJy5Uq58847tUBp69atWlB0zTXXeAyK9EhPT5fly5dL79693Y6lvl60aJGEggrwVJZr6dKlIdk/7K9q6aLSvnZZiRQJ8XFaUKQUzzf81aluORnWs67ffUx7pLssf7aPJBe6+HiHSbVXKpN1e8eagT0YAOwSGKlszM033ywNGzbUhsdatWoljzzyiNx2220et1dDVrNnz9aCp1tvvVULilQAo4bjAnXkyBEtc5N/GE59feDAAd37Uedxww03aMFbtWrVQhZUITqojMmEoR3DcCTzs2hv39RSyyC9f1vrvJ/liX4N/T6uQaUSkpRvOr7dsnxqmFEN9QGAJUNp3333nXz99dfyzTffaDVGarhKBUYqABo0aJDHx9SoUUO+/PJL6dGjh9SpU0c++eQTr2n5cJo5c6bVp4AIY4ffW71c45d6FUvI1Ee6h+Q4drgit7SvIWv3npRvluyy+lQAxFrGaOTIkXlZo2bNmskdd9whjz76qLz66qteH3Pw4EEZOnSoNtMsLS1N2z4Y5cuX1+qV1H7zH6dSpUpB7Rvh1Tzf9G+I1ClfXFpWLy09G9hzKnz+fJG98kcAEObASAU2qp7HlQpSVP2Qt2GvXr16SaNGjeSnn37S6pPUjLLHH3884JMuVKiQtGnTRttXLnV89XWnTp0C3i/Cr3WNMlqTwLmP97T6VGwjPj5OJg7rLJ/d1S6o/YQquZV/JK162eTQHAgAImEoTWV9Xn75ZW14TA2lqcJqNUNMTa3PTwUrqii7Zs2aWjCUmJgojRs3lhkzZmi1RlWrVvWYPTpz5oxs2bIl7+vt27drQ3Zly5bVjquoGW1q6K5t27ZaOwA1XV9N88+dpYbIQZPA0AzZGS0FevoK/zVHSkrJwrLjaFre13d1rmXt9FpSVgCsDIz+85//aA0ehw0bJocOHdJqi+69914ZNWpUgW1VZumVV16Rbt26aVmeXKpHkarvqVDB8wui6o106aWX5n2tgiBFBULjxo3TPr/pppvk8OHD2nFVwbXqWTR16lSPfZEA+PbywKZyWwd9M8Q+vrOtPPvzOnmk9yUBr5V2beuqhh8DALYMjNQSGio7oz706NOnj8fvq9lsvpbq0DPz5YEHHtA+gFinZpyFS/2UEjJ+aHBD1q8MbBaVReAAYjAwAhB+vt4n3NK+ugzrWS+cpwMAUSumF5EFAqHWGmtSpaTYxavXNncrgK5Wpqh2269pJVutTeR6zbyVUPVuVFFKJ7svH+QPJUYAzETGCDBIrTWmPmo9+avY0c/Du2jF0L0b26ve7pcHu8r9X62QYoUTpXBiTkduT/VLWdkOqffP38J+fgCgEBgBUaZc8cLSv1llQ48JZhKc3seqmXZj72jjd5vEBH07rFE2WXYdS5MrDf6sAOALgRFgY61qlJaVu07Ize2dK9vbdFkOK0x5uJvsOHI2b4jOX4DWvlZZSd1h7ardAOyPGiPAxiYM7SSzH+shlzUM37DYY31ypuE/d3VjsbPihROladVS+ns+5dtszI0tQnJeACIbGSPAxgolxkudCsVDfhzXmOHBXvXllg41pHzxwhJV8iXZ2tQsY9WZALAxMkYACghnUFS1dM4sum71y4f0OA6XyOi1a5tJzXLFQno8AJGJjBEQgbThI5vUGQW7esn4oR3l+2W75a4utT3eP7hLLenZoKIcPXNBRny3OuDjuF4uNQQHAJ6QMQIC9ES/BtrtPd08v6CHktnF18EEN5VK5mR8AqV6MI24vIGULXZx2SBXHWqX1dbTu7Z1NXnzBuqCAIQWgREQoPt71JV5Iy+Vp69oJJHqprbVtWnvVzWvEvA+GlcpKa9e20w+H9JeQi0h3nsEVybZPbDq1bCiaX2h6lUMfZ0XAHsgMAKCGM6qUS5Z/6woG/rX9c3l95E9taaLwbilfQ0tqxNqfZtUkprlkuW61tUK3De0ex25tMHFc8j//2Ikx3ZNy4uBYovqpeXGtgWPByA6ERgBMS4UgZ3K7Mx6rIfpQ19FCyXI3Md7ypseptqXKJIknw1u7zUU8jT82CClhNfjhHvJFAD2QGAEwHQqjKhbobhULl3EskBOTxnWT8M6a0Nl+WVn6zsXHyN7ACIUgREQgW7vWFO7dR06shN7zJfzf05qCFENleWXnS+qcl2kN9fY29vIgicvM/UcAViPwAiIQP+8spF8NridvHdba4l+cSEPzsrlmxGX/7F9m6TkzULM1a9pJalcKrgZeV/f3SGoxwMwH4EREIHU6vSXNqgoyYVoRaaXSgLltgTwN8usZJGkAsN3w3rWk851y5l2PqWKuh8DgD0QGAEIWY7HyqLl/MXW6qvFT/WSP1/oJ0WS3IurXbe8s1NNGX5pXS/7NOfckhLiZJKH2iYA1iMwAmBr5Yp7bvzob4mRK5pV9rj2XP4ZZ/m9MKCplCteWAuiVo++XEJh88tXSO3yxSxrXv7QZfWsOTAQAQiMAISMGZ0A2hpc7HXKQ93k23s6eux15E0n5xCZa61RpVJFomK4a8MLfQt8T3UaN4Na3041OQXs3t3fCAoUAJjmtg415Oslu2TE5ZfkZWgCLUqesHS3jL66seE+S6WSk/ICHTc+nmhfGdhMmlQpKVcH0QE82IVtQyV/HZqZCwTHO5ucAmZTf65W9c4lMAJgmhcHNJUhXWtLnfI5K9e3ql5armxWWaqVNTZ7q0u98tqHmXyFICozpIqrw80m6wADtuOw8NgMpQEwTXx8nNbYMTfLo25VS4Gn+lu/npyq6bEzNSzli1qL7kETa4P+e2db0/YFmC1/L7FwIjACEBOeu7qJ7Ya+XPf25d999zRSa9E1qVLKtGP3bpxi+DHF862pd18P99l7dSvYO/hE5HBYmDIiMAIQ9a5oVknK5GviaLZ7utUOeh+5fZaU9c/3lVFXNZZBnWrKzBE9/D62TLK+QvH6fno4eaM6hLd2KYRf+WyfvFquCUM7aj2ePrwjOrJQ+Rt+IvzIGAFACAxsVVW7vbe7575ERvnqy9SvaWWZ/49L5YshrgvZGuy15PK1Wq5E1Ws9P6Cp34aUysInexkOvozI/5O7Bpod6pSTb+7pqOs8A3Fzu+oSTrMf6xnW48FeCIwARK0xN7aQdc/39bgeWiiG0qqVSfY8I87r/syj+jMFGphc1bxgzye7eOeWVvLqtc3Cekw1sxHWImMEACGgir/z18WE7lhiuWevahzQ49RL0OzHesgLA4KvwzLb1c0rG27ZEEnMbJ8QTQolWBeeEBgBgAlyX7qTEuKlTwCFzWZoWKlEwI+tU6G43NmplthNNAdFyq3twztMGCkSCYwAILK5voB/rHcqfL7RgmAHD1JKFpGZI7r7LMz2Nxz49661pXWN0lr7gH/f3FLMoJZpUU07VU8ruCvKQtC2w/8IAJigYgl7DInUqxhA1sjhfTju4fGrLm4WYN1H90sq5DXt/PXJXwPaR7RqXs28FgwwBxkjAAjC9/d1kk8GtZUqzsVrjcifvXnzhhba7dNXNJRoEuWjYUGJhWvz3q2tJZKQMQKAILSrVda0ffVqlCJ/vdhPiiQlSDTV6UTi0idqOHHFrhMhP46vFhB2UqxQgpxNzwrosYG2iLAKGSMAsOgF3tP+fAVFdQLsLO267tyIPg0K3J8Qb96Lc24909Utwrsgb6CNK7357t5OXu8b0SdnkWREJwIjAIiQQOuSlBIBrXH20jVN5cn+DWXeyEs99jpK1BkY5QY7Ncsle91m8gNd5Yl+DbRj5jKSsGpVo7Q82tt44DGyb8GAz26zotSyLp7c2qGG2MmDHtbkC2fW8fm/Wds2gsAIAHQK5LXhxrbVvN5XqVSRoNc4e3FAExl7u+8ajlJFk7R1zWp4CWjU4r96XN+6mrb8hwp+vKleNlmG9aynHTMQRRITpGcDzwGEL5c3qSRmL+Z7fRvP/3dxQXZiz+9vYc6u+dO4cskC3wvngF+gvztmITACgBAOpV3VvIpseqm/x/uaVi2lNVX87K52AZ/TbR1qasuRBKNDbX11UiqAUst/hPKFy8qOx7mtBTwFSa6STW4a2rFOOZk0vIvYhUPn96IVxdcAEGKFEuO1rMDk1fsK3BdsU8VgRjjmPt5TVu4+LgNaeM5kmCUyyosvrlHnS5uaZSQpIc70/6uWASxbo/YXijjS4YjtmXdkjAAgDMx8rTFrbbNa5YvJwFbVdA2lBfPiZuRnv6Ft4J2gK5U0PjTpql2tMh7ra1xVL2O8LUOoDOtpzuLI+VUsWbAnVzCxTZVS9rlmehAYAYBOdhlOuLd7XdtMxTebyqx5+pFcu3d/dEcbj4+d/XgPmfN4z4CP/f19naV0cninlvvrRG7FVP8iiea2i1C1bXZeqDg/AiMACAMzX8KsiIVCVfrz2eB2MnNED78/2+zHemqF39tfvUIua1jR4zbJhRKldvnAWhrAPVj7Rz/jTUbL+ehXZNX6gYEgMAIAneKiINtkt+t2aYOKbsGQt23LFCukFX6HM0NW2cMQUCz8PzocIvfnH6bTcdkLJ8abElgHk0UzA4ERAOgUCy+K3oQyHjG663AFR+1rl5XiQcxA+znEM838lYaRPQsMgREAhEGgC7B63pdENSuWyXjgUs9F10O61CqQ4dLLaKfyajoKu69tXdXQOn6BqBtgF3FfAavVWSAjCIwAALYJxvQcJi5ELRX8GTe4nQxoqb8ZY4kiSYYyXarWqp+PRpWXN04xNPOufPGCs8v8Wflsn6CyZNGAwAgAIkyUTUQrEDDUrVjMZ82Kai/QpErB7syh1lOrh4rTFThc2cz4LCy1Tl79FO/ZGnWZhnSt7f4Nk5XxUkAdFyOL5SoERgAQYawYSgtlMOa6+np8XJw2u2ztc5fL6tGX+1xTLVD/vrllge+1MNBgMZQL5OZfHmRQp5pu/++BZIHswMFQGgAA+oIqtcTIj/d3ll8e7CoJzopiNQylMijePHRZ/YDPxTX4eOP65vLhHW28LvDqSZKXBWa3vOx56Rcj8u+7ToWCGaReDStK6eQk6d1If70T9IvtgUQAMMIR+EMi5/2y+U3/9GS41FIbRlQsWUTa1yorqTuOGT4X16E7NXOrba2ypqTKEr0ETHm7EpEPbmste46fk5en/KnzfAtevP8OaitZ2Q7ZeSxNwiXO5TosebqXLNhyREZ8t1r34w1N17f4j4WMEQBEmMZVSmo1Nt4aHZrpzRtaSL2KxeXVa5uJ7cTZeIjQy377N6ss93SvE1zxeVyc3yDMbHEuP09KySIysFVo19ezEhkjAIgwarhJDTuFw3Vtqmkf0Vos7jc7YXX6wiYcjoLBWf2KxWXzoTN53yucZE6DR6uRMQKACKRemKJtnTSjVObCaDNElf0Kpd6NUjz2P9IrlP+j1csGvphrnIcT+78bWrh9/e4trSUaEBgBQDiE8R2zKs6NBaOuaqytwfXZXe38bvvLg920outPBrU1dhAvwacqGPdELXC74tk+0qZmwbolq+PYJ/s1MnV/LaqXllcGNnMb4vXG6p/dCAIjAIgyneqWk1hQoURh+fjOtnKpjkBQvWi/c0srqVnOnGUyvA1lqh5Lru0HjPbycYQ5nu5Qu6xULR14JkkvhtIAIApFSi+WK5zNBVtUK2X1qUSt6mWTJRpc06qqTH6gi1zbqqp8d6/xJUQi5W/CCIqvASDKVCldVNY8d7kUK2S/p3jVXPHh8avCdrxa5ZLlutbBFY9bycwRKBXEqAzRku3uLQ7KFS8sY25qGdB5OHTGRZEUPpExAoAQLmuQ6G8J9BApWSQpr1minQxoWVUaVw7Pch739agrc0deKg/2CqwZpB34CigC+d/9bHA7GT+048X964xY4rwUCZm5OLJdEBgBQAiGDQZ3qSXd6peXjnXKRe2Qg93XyUouFHhTSrPMebxn3ueeYgu1/EkoqN8/T9Txcn8njejf1PPitrXKm1Oz5crqWMt+eVYAiAKjr25i9SnYVsWShWXDfokJqrO2J89c2Uh2HUvzWwcWSJCgAjAzf/9ubldd2tcuK18v2VXgvq71ystL1zSVhpVKFLjv7Zta+pypluuq5pXllzX2+YUgMAKAMFDTt6esPSCxqGFl9xdN1UX7yR/Xyl0B9voxk1W9oO7u5r37dbDM/onqVSzu9Tqp79/e8eJCt/kLu/V4/frm2oSBYV+vEDsgMAIAnbz1rtHjzk41pWhSgnSs42NdriijprQv3nZUbm5Xw+37lUsVlc+HtBcr3di2muw8miYtq5cWO4uk/j+++KpFUsN7uTMp7YDACAB0en5AUzmettJr/Ya/VdNv7eAeIES7plVLaR929Pr17l2bg5lhp/oW6WkcqSfGqVK6iAlDaeZHU3ESOwiMAEAn1Qjvx/s7W30aCKM4PzPs1IeZCicmyLrn+0rT0dPyvqfqd/46cFquDbLtQKBFzfFxcZKUEFxoFElTDwiMAAAIA72JnOKF3V+aJw7rIjuOnvVY4OzxOAbOyddsyXu61ZZZfx6SG9tVl8KJ8VoBtt2HHs1AYAQAgM2o/leZ2Q7pXK+cFC2UII1M6P1kdITtn1c21j5yGemM/bCf3lEq0LqQmW3L7BKBEQAANrP8mT5y+Mx5qVdRX5ZIuaZlFZm0ap/W2NIqM0d0lxW7Tsj1fob9rm1dVb5N3S12RGAEAEBY6E/ZlEpO0j587i1fCuiNG1rI4C61vRa8h6NxYr2KJQwFc3ZEYAQAgIXNHs2aAq9mPrYwWAMUti7TDokYBEYAAIS4n9OBk+elgc7iaViLwAgAgAjs5+Q6lKaKme3MUSBl5H1YsVUNa2e+ERgBABDh1Jpl/qhFje0ipWThAt9b9kxvOXLmgtStUFysRGAEAEAELsmhOm7nen6A70VjXxnYTEonX9zeavd2r6stotuvSaW875UvXlj7sBqBEQAAVhcnB7h230/DOkuhhHipVibZ57ZFC9lrqK1ooQQZc2NLsSMCIwAAIlTrGmWsPoWoY68QEgCAIOV2ib6yuX1WbLersM3Wd0jEIGMEAIgqkx/oIqfOZUg5G9SroKAu9cqJnZExAgBEFdXo0KygyM7F10bEGVpa1nyuCaMvh3QQOyMwAgAAYRMfb+9ok8AIAADAicAIAIBYFUlV0WFCYAQAgBe3dagpxQsnyk1tq1t9KggTZqUBAOBFhRKFZdWoPpKYQB4hVhJT/E8DAOADQVFs4X8bAIAo563tQNgaPErkpIwIjAAAAJwIjAAAiFHh6ihUrlghiRQUXwMAEKPCNcB1eeNKMqhTTWkVAYveEhgBAICQd7t+fkBTiQQMpQEAEKPa1Spr9SnYDhkjAABizNJ/9paDp85Lo8olrT4V2yEwAgAgBhtXqg8UxFAaAACAE4ERAACAE4ERAABRrla5YlafQsSgxggAgCg1cVhn2XUsTVpUL231qUQMAiMAAKKUaqgYCU0V7YShNAAAACcCIwAAACcCIwAAACcCIwAAACcCIwAAACcCIwAAACcCIwAAACcCIwAAACcCIwAAACcCIwAAACcCIwAAACcCIwAAACcCIwAAAKfE3E+gj8Ph0G5PnTpl9akAAACdcl+3c1/HvSEwMuj06dPabfXq1a0+FQAAEMDreKlSpbzeH+fwFzrBTXZ2tuzbt09KlCghcXFxpkayKtjavXu3lCxZ0rT9RgOujXdcG++4Nt5xbbzj2kTvtVHhjgqKqlSpIvHx3iuJyBgZpC5mtWrVQrZ/9csWib9w4cC18Y5r4x3XxjuujXdcm+i8Nr4yRbkovgYAAHAiMAIAAHAiMLKJwoULy+jRo7VbuOPaeMe18Y5r4x3XxjuujXexcm0ovgYAAHAiYwQAAOBEYAQAAOBEYAQAAOBEYAQAAOBEYGQT7733ntSqVUuKFCkiHTp0kNTUVIkm8+bNk6uvvlrrOKo6hk+aNMntfjUHYNSoUVK5cmUpWrSo9O7dWzZv3uy2zbFjx+S2227TGouVLl1a/v73v8uZM2fctlmzZo1069ZNu46qQ+vrr78udvfqq69Ku3bttG7qFStWlGuuuUY2btzots358+dl+PDhUq5cOSlevLhcd911cvDgQbdtdu3aJVdeeaUkJydr+xk5cqRkZma6bTN37lxp3bq1NqukXr16Mm7cOLGzDz74QJo3b57XUK5Tp07y22+/Saxfl/xee+017e/qkUceyfteLF+b5557Trserh8NGzbMuz+Wr83evXvl9ttv13529VzbrFkzWbZsWd79jhh+Ls6jZqXBWuPHj3cUKlTI8emnnzrWr1/vuOeeexylS5d2HDx40BEtpkyZ4vjnP//p+Omnn9QsSMfEiRPd7n/ttdccpUqVckyaNMmxevVqx9/+9jdH7dq1HefOncvbpl+/fo4WLVo4Fi9e7Pjjjz8c9erVc9xyyy159588edKRkpLiuO222xzr1q1zfPvtt46iRYs6PvzwQ4ed9e3b1/HZZ59p57xq1SrHFVdc4ahRo4bjzJkzedvcd999jurVqztmzZrlWLZsmaNjx46Ozp07592fmZnpaNq0qaN3796OlStXate7fPnyjqeeeipvm23btjmSk5MdI0aMcGzYsMHxn//8x5GQkOCYOnWqw64mT57s+PXXXx2bNm1ybNy40fH00087kpKStGsVy9fFVWpqqqNWrVqO5s2bOx5++OG878fytRk9erSjSZMmjv379+d9HD582BHr1+bYsWOOmjVrOu666y7HkiVLtJ9h2rRpji1btuRt81oMPxfnIjCygfbt2zuGDx+e93VWVpajSpUqjldffdURjfIHRtnZ2Y5KlSo53njjjbzvnThxwlG4cGHtD0pRTzzqcUuXLs3b5rfffnPExcU59u7dq339/vvvO8qUKeO4cOFC3jb/+Mc/HA0aNHBEkkOHDmk/6++//553LVQw8P333+dt8+eff2rbLFq0SPtaPXHHx8c7Dhw4kLfNBx984ChZsmTe9XjiiSe0FwtXN910kxaYRRL1f/zf//6X6+JwOE6fPu2oX7++Y8aMGY4ePXrkBUaxfm1UYKReuD2J5Wujng+7du3q9X6ei3MwlGax9PR0Wb58uZaudF2PTX29aNEiiQXbt2+XAwcOuF0DtZ6NGlLMvQbqVqVs27Ztm7eN2l5dqyVLluRt0717dylUqFDeNn379tWGpY4fPy6R4uTJk9pt2bJltVv1+5GRkeF2fdSwQI0aNdyuj0qJp6SkuP3satHH9evX523juo/cbSLl9ywrK0vGjx8vZ8+e1YbUuC6iDQep4Z7858+1EW34Rw3d16lTRxv2UUNjsX5tJk+erD2H3nDDDdrwYKtWreTjjz/Ou5/n4hwERhY7cuSI9oTv+geoqK/VL2gsyP05fV0Ddav+kF0lJiZqwYPrNp724XoMu8vOztbqRLp06SJNmzbNO3f1BKOejHxdH38/u7dt1JP9uXPnxK7Wrl2r1YGoOo777rtPJk6cKI0bN47566KCxBUrVmg1avnF+rVRL+Sq3mfq1KlanZp6wVf1Lmpl9Vi+Ntu2bdOuR/369WXatGly//33y0MPPSSff/65dj/PxTkSnbcAbJIBWLduncyfP9/qU7GNBg0ayKpVq7RM2g8//CCDBg2S33//XWLZ7t275eGHH5YZM2Zoxa1w179//7zPVfG+CpRq1qwp3333nVZQHKvUGy+V6XnllVe0r1XGSD3fjB07Vvu7Qg4yRhYrX768JCQkFJgRob6uVKmSxILcn9PXNVC3hw4dcrtfzRBRsyNct/G0D9dj2NkDDzwgv/zyi8yZM0eqVauW93117mrI9cSJEz6vj7+f3ds2amaJnV8s1Lt7NeOnTZs2WnakRYsW8u9//zumr4saDlJ/D2pGlHq3rj5UsPjOO+9on6t357F6bTxR2aFLLrlEtmzZEtO/N2qmmcq2umrUqFHeMCPPxTkIjGzwpK+e8GfNmuUW1auvVR1FLKhdu7b2x+J6DVQ6Wo1X514DdaueyNQLQq7Zs2dr10q9G8zdRrUFUPUDudQ7apVxKFOmjNiVqkdXQZEaIlI/k7oertTvR1JSktv1UWP16snM9fqoISfXJyz1s6sn6dwnQrWN6z5yt4m03zP1f37hwoWYvi69evXSfi6VScv9UJkAVUuT+3msXhtP1FTyrVu3aoFBLP/eqCH6/K1ANm3apGXTlFh/Ls7jLMKGxdP1VdX/uHHjtIr/oUOHatP1XWdERDo1e0ZNe1Uf6tduzJgx2uc7d+7MmyKqfuaff/7ZsWbNGseAAQM8ThFt1aqVNs10/vz52mwc1ymiavaEmiJ6xx13aFNE1XVV02ntPkX0/vvv16bHzp071216cVpamtv0YjWFf/bs2dr04k6dOmkf+acXX3755dqUfzVluEKFCh6nF48cOVKbhfPee+/Zfnrxk08+qc3O2759u/Z7ob5Ws1+mT58e09fFE9dZabF+bR577DHt70n93ixYsECbdq+m26sZn7F8bVRrh8TERMfLL7/s2Lx5s+Prr7/Wfoavvvoqb5vXYvi5OBeBkU2oHhjqD1X1M1LT91V/iGgyZ84cLSDK/zFo0KC8aaLPPvus9sekgsRevXppfWtcHT16VPvjK168uDZtdvDgwVrA5Ur13VDTUdU+qlatqv2R252n66I+VG+jXOpJadiwYdoUWPUEM3DgQC14crVjxw5H//79tX4h6kVAvThkZGQU+H9o2bKl9ntWp04dt2PY0ZAhQ7S+K+p81QuT+r3IDYpi+broCYxi+dqoafOVK1fWzlk9D6ivXXv1xPK1+d///qcFfeo5smHDho6PPvrI7f7sGH4uzhWn/rmYPwIAAIhd1BgBAAA4ERgBAAA4ERgBAAA4ERgBAAA4ERgBAAA4ERgBAAA4ERgBAAA4ERgBAAA4ERgBAAA4ERgBAAA4ERgBAAA4ERgBAABIjv8HRYD6Ywu1gCYAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lossi)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8aaa78-e607-4bc4-8cde-49a529984c1b",
   "metadata": {},
   "source": [
    "## Trained Decoder Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "94fd5355-31c8-47fc-a2fd-9b1fa9f91576",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.eval()\n",
    "correlations = []\n",
    "mses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "417a9438-c9f1-4cb0-8da2-e9fe7ec831a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_steps_per_epoch = val_total_examples // batch_size\n",
    "test_steps_per_epoch = test_total_examples // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "27c89be7-508c-4e06-9dfc-9f4809807077",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  27%|                                                          | 94/345 [00:37<01:43,  2.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading /Users/djemec/data/jepa/tokenized/eval/val/shard_val_0000.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|| 345/345 [02:22<00:00,  2.42it/s]\n"
     ]
    }
   ],
   "source": [
    "for step in tqdm(range(val_steps_per_epoch), desc=\"Evaluating\"):\n",
    "    \n",
    "    # Custom Loader Call\n",
    "    cont_x, cont_tot, case_x, case_tot, act_id = val_loader.next_batch()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        z_context = model.student(cont_x, cont_tot)\n",
    "        z_pred = model.predictor(z_context, act_id)\n",
    "        \n",
    "        pred_delta = decoder(z_pred) - decoder(z_context)\n",
    "        real_delta = case_x - cont_x\n",
    "        \n",
    "    # Per-Sample Metrics\n",
    "    for i in range(len(pred_delta)):\n",
    "        p = pred_delta[i].cpu().numpy()\n",
    "        t = real_delta[i].cpu().numpy()\n",
    "        \n",
    "        # Metric: Pearson Correlation on Top 20 DEGs\n",
    "        top_20_idx = np.argsort(np.abs(t))[-20:]\n",
    "        \n",
    "        if np.std(p[top_20_idx]) > 1e-9 and np.std(t[top_20_idx]) > 1e-9:\n",
    "            corr, _ = pearsonr(p[top_20_idx], t[top_20_idx])\n",
    "            correlations.append(corr)\n",
    "            \n",
    "        mses.append(np.mean((p - t)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "750afdb9-ba92-4a7a-874d-0cc221525de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global MSE: 0.7983\n",
      "Top-20 Pearson R: 0.6156\n"
     ]
    }
   ],
   "source": [
    "mean_mse = np.mean(mses)\n",
    "mean_corr = np.mean(correlations)\n",
    "print(f'Global MSE: {mean_mse:.4f}')\n",
    "print(f'Top-20 Pearson R: {mean_corr:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "44680d47-980a-4b0e-b910-471b65efd220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " FUNCTIONAL (Better than random)\n"
     ]
    }
   ],
   "source": [
    "if mean_corr > 0.75:\n",
    "    print(' SOTA COMPETITIVE (Matches GEARS)')\n",
    "elif mean_corr > 0.40:\n",
    "    print(' FUNCTIONAL (Better than random)')\n",
    "else:\n",
    "    print(' NEEDS IMPROVEMENT')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6b2741-9067-4cee-b37f-2ee471d3c7e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
